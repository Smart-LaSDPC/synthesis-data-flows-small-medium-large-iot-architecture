@inproceedings{10.1145/3090354.3090357,
author = {Laassiri, Jalal},
title = {Data Security and risks for IoT in intercommunicating objects},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090357},
doi = {10.1145/3090354.3090357},
abstract = {Nowadays Internet of Things" (IoT) codes are passive entities that encode information, The goal of this work is to give explicit interconnections between IoT specifications and interpreting IoT codes and information's in order to exchange information to fulfill tasks for the users. Deploying existing data security solutions to the Internet of Things (IoT) is not straightforward because of device heterogeneity, highly dynamic and possibly unprotected environments, and large scale.It is well-known the existence of a closed relation between IoT systems over finite fields and Security and risks systems that allow to understand some properties of computational security codes and to construct them. An accurate look at the IoT structure of information using techniques of smart systems information as well a study of input-state-output representation smart control systems, For the Internet of Things security based Internet Language we intend to analyze both the theoretical security mechanism.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {3},
numpages = {4},
keywords = {IOT codes, Internet of Things, Internet programming, Network security, UML meta-models, XSS and OWASP, attack vector, risk},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3018896.3065844,
author = {Kendrick, Phillip and Hussain, Abir and Criado, Natalia and Randles, Martin},
title = {Multi-agent systems for scalable internet of things security},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3065844},
doi = {10.1145/3018896.3065844},
abstract = {Providing effective and scalable real-time security to Internet of Things devices can be a challenging task given the limited computational capacity of portable devices and a significant volume of network traffic. Multi-Agent Systems have proven to be a valuable tool in the areas of cyber security, distributed networks and legacy systems because of their scalable and flexible architecture. In this paper, we present a novel implementation of a Multi-Agent System for use within, or to support, Internet of Things networks through the distributed processing of security events to offload the computational cost of processing data from Internet of Things devices. In particular, domain experts can add new agents to existing systems which can automatically work with preexisting agents without manual reconfiguration. The scalability of this deployment model makes it suitable for a broad range of environments including dynamic and large-area networks.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {193},
numpages = {6},
keywords = {cyber security, internet of things, multi-agent system, scalable systems},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/2939672.2945395,
author = {Gupta, Ashish and Agarwal, Neera},
title = {Streaming Analytics},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945395},
doi = {10.1145/2939672.2945395},
abstract = {Recently we have seen emergence and huge adoption of social media, internet of things for home, industrial internet of things, mobile applications and online transactions. These systems generate streaming data at very large scale. Building technologies and distributed systems that can capture, process and analyze this streaming data in real time is very important for gaining real time insights. Real-time analysis of streaming data can be used for applications as diverse as fraud detection, in-session targeting and recommendations, control systems for transportation systems and smarter cities, earthquake prediction and control of autonomous vehicles. This tutorial will provide overview of streaming systems and hands on tutorial on building streaming analytics systems using open source technologies.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2123},
numpages = {1},
keywords = {IoT, analytics, distributed systems, machine learning, stream},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3338500.3360334,
author = {\'{A}lvarez, Flor and Almon, Lars and Hahn, Ann-Sophie and Hollick, Matthias},
title = {Toxic Friends in Your Network: Breaking the Bluetooth Mesh Friendship Concept},
year = {2019},
isbn = {9781450368322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338500.3360334},
doi = {10.1145/3338500.3360334},
abstract = {Bluetooth Low Energy is the dominant wireless technology empowering the Internet-of-Things. It has recently been amended with Bluetooth Mesh, which promises secure low energy multi-hop wireless connectivity. Even older Bluetooth devices can become part of the network with a software-only upgrade. Bluetooth Mesh claims to be suitable for building large-scale multi-hop sensor networks with thousands of devices and up to 127 hops. In particular, it introduces the friendship concept, allowing low power Internet-of-Things devices to save energy by going into sleep mode, while their associated friend node caches their packets.In this paper, we show that the security model underlying the friendship concept introduces a number of simplifying assumptions that can be harnessed against the Bluetooth Mesh network. We demonstrate three fundamental vulnerabilities in the security model that lead to denial-of-service and impersonation attacks. Furthermore, we experimentally prove that our denial-of-service attack significantly affects the battery life of power-constrained Internet-of-Things devices from a normal duration of two years to just few days.In addition, we introduce btlemesh, an open-source tool to analyze Bluetooth Mesh and perform the aforementioned security tests in practice. Finally, we discuss possible countermeasures to mitigate these vulnerabilities.},
booktitle = {Proceedings of the 5th ACM Workshop on Security Standardisation Research Workshop},
pages = {1–12},
numpages = {12},
keywords = {bluetooth mesh, denial-of-service, internet-of-things},
location = {London, United Kingdom},
series = {SSR'19}
}

@inproceedings{10.5555/3324320.3324376,
author = {Provoost, Michiel and Weyns, Danny},
title = {Demo: DingNet: A Simulator for Large-Scale&nbsp;IoT Systems with Mobile Devices},
year = {2019},
isbn = {9780994988638},
publisher = {Junction Publishing},
address = {USA},
abstract = {The Internet-of-Things (IoT) provides an enabling layer for smart cities. However, engineering such large-scale IoT deployments is challenging since these systems typically consist of complex distributed infrastructure of gateways and devices. To investigate and test IoT solutions before deployment in the field, engineers can make use of simulation. In our research, we study large-scale IoT systems for smart city applications that comprise of mobile devices that sense parameters of the environment and send the sensor data to a user application via stationary gateways. As existing IoT simulators are not sufficient to support our research, we developed a novel simulator of DingNet, an IoT system that is deployed in Leuven. In this paper, we give an overview of the simulator design and we illustrate how we used for the study of an IoT scenario with multiple mobile devices.},
booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
pages = {267–269},
numpages = {3},
location = {Beijing, China},
series = {EWSN '19}
}

@inproceedings{10.1145/3339252.3341482,
author = {Ankele, Ralph and Marksteiner, Stefan and Nahrgang, Kai and Vallant, Heribert},
title = {Requirements and Recommendations for IoT/IIoT Models to automate Security Assurance through Threat Modelling, Security Analysis and Penetration Testing},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3341482},
doi = {10.1145/3339252.3341482},
abstract = {The factories of the future require efficient interconnection of their physical machines into the cyber space to cope with the emerging need of an increased uptime of machines, higher performance rates, an improved level of productivity and a collective collaboration along the supply chain. With the rapid growth of the Internet of Things (IoT), and its application in industrial areas, the so called Industrial Internet of Things (IIoT)/Industry 4.0 emerged. However, further to the rapid growth of IoT/IIoT systems, cyber attacks are an emerging threat and simple manual security testing can often not cope with the scale of large IoT/IIoT networks.In this paper, we suggest to extract metadata from commonly used diagrams and models in a typical software development process, to automate the process of threat modelling, security analysis and penetration testing, without detailed prior security knowledge. In that context, we present requirements and recommendations for metadata in IoT/IIoT models that are needed as necessary input parameters of security assurance tools.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {102},
numpages = {8},
keywords = {Automation, IIoT, Industry 4.0, IoT, Penetration Testing, Recommendations, Requirements, Security Analysis, Threat Modelling},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3147234.3148105,
author = {Carvalho, Ot\'{a}vio and Roloff, Eduardo and Navaux, Philippe O.A.},
title = {A Distributed Stream Processing based Architecture for IoT Smart Grids Monitoring},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148105},
doi = {10.1145/3147234.3148105},
abstract = {Sensor networks have become ubiquitous -- being present from personal smartphones to smart cities deployments -- and are producing large volumes of data at increasing rates. Distributed event stream processing systems, in its turn, are a specific kind of systems that help us to parallelize event processing. Therefore, they provide us capabilities to produce quick insights and decisions, in near real-time, on top of multiple data streams. However, current systems for large scale processing do not focus on Internet of Things and Sensor Network workloads, which makes the performance decrease quickly as the workload size increases. In order to process large scale events with acceptable latency percentiles and high throughput, special systems are needed, such as distributed event stream processing systems. In this work, we propose an architecture for Internet of Things data workloads, in a combination of sensor networks data sources and distributed event stream processing systems, focused on smart grid data profiles. In our evaluations, the system was able to process up to 45K messages per second using 8 processing nodes, while providing stable latencies for micro-batches above 30 seconds.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {9–14},
numpages = {6},
keywords = {cloud computing, distributed processing, internet of things, sensor networks, smart grids, stream processing},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@inproceedings{10.1145/3299874.3319484,
author = {Kong, Yuyao and Yang, Jun},
title = {In-memory Processing based on Time-domain Circuit},
year = {2019},
isbn = {9781450362528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299874.3319484},
doi = {10.1145/3299874.3319484},
abstract = {Deep Neural Networks (DNN) have emerged as a dominant algorithm for machine learning (ML). High performance and extreme energy efficiency are critical for deployments of DNN, especially in mobile platforms such as autonomous vehicles, cameras, and other devices of internet of things. However, DNNs lead to massive data movement and memory accesses, which prevents it from being integrated into always-on Internet-of-Things (IoT) devices. Recently, computing in-memory (CIM) architectures embeds analog computation circuits in/near the memory arrays. It significantly reduces data movement energy. This paper summarizes the most recent novel methods on the CIM architectures based on the time-domain computation. Compared with voltage-domain and frequency-domain analog computing method, time-domain computation provides more flexibility, higher accuracy and greater scalability for larger neural networks. Thereafter, the first in-memory binary weight network (BWN) processor based on pulse-width modulation in which the feature is stored in memory is also presented. This work significantly reduces memory accesses (4x), and achieves state-of-the-art peak energy efficiency of 119.7TOPS/W.},
booktitle = {Proceedings of the 2019 on Great Lakes Symposium on VLSI},
pages = {435–438},
numpages = {4},
keywords = {bwn, computing in-memory, time-domain},
location = {Tysons Corner, VA, USA},
series = {GLSVLSI '19}
}

@article{10.1145/2850416,
author = {V\"{o}gler, Michael and Schleicher, Johannes M. and Inzinger, Christian and Dustdar, Schahram},
title = {A Scalable Framework for Provisioning Large-Scale IoT Deployments},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2850416},
doi = {10.1145/2850416},
abstract = {Internet of Things (IoT) devices are usually considered external application dependencies that only provide data or process and execute simple instructions. The recent emergence of IoT devices with embedded execution environments allows practitioners to deploy and execute custom application logic directly on the device. This approach fundamentally changes the overall process of designing, developing, deploying, and managing IoT systems. However, these devices exhibit significant differences in available execution environments, processing, and storage capabilities. To accommodate this diversity, a structured approach is needed to uniformly and transparently deploy application components onto a large number of heterogeneous devices. This is especially important in the context of large-scale IoT systems, such as in the smart city domain. In this article, we present LEONORE, an infrastructure toolset that provides elastic provisioning of application components on resource-constrained and heterogeneous edge devices in large-scale IoT deployments. LEONORE supports push-based as well as pull-based deployments. To improve scalability and reduce generated network traffic between cloud and edge infrastructure, we present a distributed provisioning approach that deploys LEONORE local nodes within the deployment infrastructure close to the actual edge devices. We show that our solution is able to elastically provision large numbers of devices using a testbed based on a real-world industry scenario.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {11},
numpages = {20},
keywords = {IoT, framework, gateway, large-scale, provisioning, resource-constrained}
}

@inproceedings{10.5555/3108009.3108047,
author = {Liu, Xiuming and Khankan, Abdul Rahman and Alsioufi, Mohamad and He, Zhitao and Ngai, Edith C.H.},
title = {Poster: Cloud-Based Data Fusion in GreenIoT for Smart Cities},
year = {2017},
isbn = {9780994988614},
publisher = {Junction Publishing},
address = {USA},
abstract = {Applications based on internet-of-things (IoT) for smart cities attract intensive attentions recently. One of the important elements in a IoT platform is processing the sensor data. The motivation of abstracting useful information from large scale and heterogeneous sensory data drive us to design and implement a cloud-based sensor data fusion system, which bridges the sensor networks and external applications. A system architecture is illustrated and tested with two use cases in this poster.},
booktitle = {Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks},
pages = {216–217},
numpages = {2},
location = {Uppsala, Sweden},
series = {EWSN ’17}
}

@inproceedings{10.1145/2939948.2949688,
author = {Forshaw, Matthew and Thomas, Nigel and McGough, A. Stephen},
title = {The case for energy-aware simulation and modelling of internet of things (IoT)},
year = {2016},
isbn = {9781450344197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939948.2949688},
doi = {10.1145/2939948.2949688},
abstract = {Existing approaches to energy management of large scale distributed systems are ill-equipped to handle the challenges introduced by the dynamic and self-adaptive nature of the Internet of Things. In this position paper we motivate the need for energy-aware modelling and simulation approaches for IoT infrastructures, to facilitate what-if analyses, and support design decisions and runtime optimisation. We identify open challenges and research opportunities in the energy-aware simulation and modelling of IoT.},
booktitle = {Proceedings of the 2nd International Workshop on Energy-Aware Simulation},
articleno = {5},
numpages = {4},
keywords = {energy, internet of things, modeling, simulation},
location = {Waterloo, Ontario, Canada},
series = {ENERGY-SIM '16}
}

@inproceedings{10.1145/2675743.2774215,
author = {Hasan, Souleiman and Curry, Edward},
title = {Tackling variety in event-based systems},
year = {2015},
isbn = {9781450332866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675743.2774215},
doi = {10.1145/2675743.2774215},
abstract = {Event-based systems follow an interaction model based on three decoupling dimensions: space, time, and synchronization. However, event producers and consumers are tightly coupled by event semantics: types, attributes, and values. That limits scalability in large-scale heterogeneous environments with significant variety such as the Internet of Things (IoT) due to difficulties in establishing semantic agreements at such scales. This paper studies this problem and investigates the suitability of different traditional and emerging approaches for tackling the issue.},
booktitle = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems},
pages = {256–265},
numpages = {10},
keywords = {coupling, event processing systems, internet of things, semantics, thingsonomy},
location = {Oslo, Norway},
series = {DEBS '15}
}

@inproceedings{10.1145/2968456.2973275,
author = {Dutt, Nikil and Kurdahi, Fadi J. and Ernst, Rolf and Herkersdorf, Andreas},
title = {Conquering MPSoC complexity with principles of a self-aware information processing factory},
year = {2016},
isbn = {9781450344838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968456.2973275},
doi = {10.1145/2968456.2973275},
abstract = {We introduce the idea of an Information Processing Factory as a step towards autonomous many-core platforms in Cyber-Physical Systems (CPS) and the Internet of Things (IoT). It represents a paradigm shift in platform design moving long term, with robust and independent platform operation in the focus of platform-centric design rather than existing semiconductor device or software technology, as mostly seen today. Such a refocus is not only needed to prepare for the time when technology scaling comes to an end, but is a necessity to manage complexity when networked IT systems not only by far outnumber humans but when their complexity by far exceeds that of human societies. While such applications complexity growth is visible to large parts of the society, and is publicly discussed, it is important to note that the complexity growth equally affects the platforms (computers, communication, storage, etc.) which enable such growing applications. This challenge is partly addressed by the new areas of Cyber-Physical Systems and the Internet-of-Things, but the changing challenges to the operation of complex microelectronic systems is mostly left to computer and software architects, and to semiconductor technology. This perspective paper motivates the necessity for a paradigm shift in IT system design and its application to autonomous systems as a main application.},
booktitle = {Proceedings of the Eleventh IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {37},
numpages = {4},
keywords = {cyber-physical systems, self-awareness, system-on-chip},
location = {Pittsburgh, Pennsylvania},
series = {CODES '16}
}

@inproceedings{10.1145/2808797.2809361,
author = {Barreto, Luciano and Celesti, Antonio and Villari, Massimo and Fazio, Maria and Puliafito, Antonio},
title = {An Authentication Model for IoT Clouds},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2809361},
doi = {10.1145/2808797.2809361},
abstract = {Nowadays, the combination between Cloud computing and Internet of Things (IoT) is pursuing new levels of efficiency in delivering services, representing a tempting business opportunity for IT operators of increasing their revenues. However, security is considered as one of the major factors that slows down the rapid and large scale adoption and deployment of both IoT and Cloud computing. In this paper, considering such an IoT Cloud scenario, we present an architectural model and several use cases that allow different types of users to access IoT devices.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1032–1035},
numpages = {4},
keywords = {Cloud computing, authentication, internet of things, security},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3361149.3361174,
author = {Dobaj, J\"{u}rgen and Schuss, Markus and Krisper, Michael and Boano, Carlo Alberto and Macher, Georg},
title = {Dependable mesh networking patterns},
year = {2019},
isbn = {9781450362061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361149.3361174},
doi = {10.1145/3361149.3361174},
abstract = {In our daily life, we are increasingly relying on connected systems ranging from smart health care devices to industrial and intelligent transportation systems, as well as smart homes and cities. The unavailability or malfunctioning of these systems could threaten human life, cause environmental damage, and significant financial loss. To prevent such large scale and mission-critical systems from malfunctioning, it is of utmost importance to establish and guaranty reliable connections to attain a dependable networked system. Generally, mesh networking technologies are used for building such systems since mesh networks provide the best performance characteristics regarding fault-tolerance, throughput, resource usage, and service level flexibility.In this paper, we summarize the major challenges in dependable network design, to subsequently present three patterns that approach redundancy on the hardware level, software-defined networking, and cross-cutting concerns like monitoring and service discovery within distributed networked systems. These three patterns should help designers and engineers in choosing the appropriate technologies for building dependable networked systems at all scales. Since dependable network engineering requires a holistic system-wide design and engineering approach, we also present a pattern map guiding to complementary and closely related patterns. System architects and system engineers responsible for building mixed-criticality systems, internet-of-things (IoT), and industrial Internet-of-Things (IIoT) systems are the target audience of the patterns presented in this paper.},
booktitle = {Proceedings of the 24th European Conference on Pattern Languages of Programs},
articleno = {25},
numpages = {14},
keywords = {IIoT, IaaS, IoT, SoA, cloud architecture, dependability, industry 4.0, infrastructure-as-a-service, logical mesh pattern, microservices, network architecture, network design, physical mesh pattern, service mesh pattern, service-oriented architecture},
location = {Irsee, Germany},
series = {EuroPLop '19}
}

@inproceedings{10.1145/3102304.3102348,
author = {Barker, Peter and Hammoudeh, Mohammad},
title = {A Survey on Low Power Network Protocols for the Internet of Things and Wireless Sensor Networks},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102348},
doi = {10.1145/3102304.3102348},
abstract = {Low power communication is becoming an increasingly critical factor in the design and implementation of large-scale Internet of Things (IoT) and Wireless Sensor Networks (WSN). Recently, new protocols have been introduced to help reduce such system's power can cost. This paper presents a survey of recent research on low power consumption networking for IoT and WSN systems, highlighting the move from battery life of hours or days to months and years Then the paper flags some Cyber Security vulnerabilities of specific IoT interest as well as identifying key areas for further work.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {44},
numpages = {8},
keywords = {BLE, Backfi, Battery Life, Bluetooth Low Energy, Internet of Things, IoT, NFC, Neul, SigFox LoRaWAN, Thread, Wi-Fi, Z-Wave, Zigbee},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3180445.3180455,
author = {Khan, Latifur},
title = {Big IoT Data Stream Analytics with Issues in Privacy and Security},
year = {2018},
isbn = {9781450356343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180445.3180455},
doi = {10.1145/3180445.3180455},
abstract = {Internet of Things (IoT) Devices are monitoring and controlling systems that interact with the physical world by collecting, processing and transmitting data using the internet. IoT devices include home automation systems, smart grid, transportation systems, medical devices, building controls, manufacturing and industrial control systems. With the increase in deployment of IoT devices, there will be a corresponding increase in the amount of data generated by these devices, therefore, resulting in the need of large scale data processing systems to process and extract information for efficient and impactful decision making that will improve quality of living.},
booktitle = {Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics},
pages = {22},
numpages = {1},
keywords = {IoT, data analytics, security and privacy, stream data},
location = {Tempe, AZ, USA},
series = {IWSPA '18}
}

@inproceedings{10.1145/3135932.3135939,
author = {Gascon-Samson, Julien and Rafiuzzaman, Mohammad and Pattabiraman, Karthik},
title = {SmartJS: dynamic and self-adaptable runtime middleware for next-generation IoT systems},
year = {2017},
isbn = {9781450355148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3135932.3135939},
doi = {10.1145/3135932.3135939},
abstract = {The Internet of Things (IoT) has gained wide popularity both in the academic and industrial contexts. However, IoT-based systems exhibit many important challenges across many dimensions. In this work, we propose SmartJS, a rich Javascript-based middleware platform and runtime environment that abstracts the complexity of the various IoT platforms by providing a high-level framework for IoT system developers. SmartJS abstracts large-scale distributed system considerations, such as scheduling, monitoring and self-adaptation, and proposes a rich inter-device Javascript-based code migration framework. Finally, it provides debugging and monitoring techniques to analyze performance and observe system-wide security properties.},
booktitle = {Proceedings Companion of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {51–52},
numpages = {2},
keywords = {Code Migration, Dependability, Internet of Things, IoT, Javascript, Scheduling, Security},
location = {Vancouver, BC, Canada},
series = {SPLASH Companion 2017}
}

@inproceedings{10.1145/2737095.2742927,
author = {Rajaraman, Vasanth and Misra, Prasant and Dhotrad, Kumaresh and Warrior, Jay},
title = {Enabling plug-n-play for the internet of things with self describing devices},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2742927},
doi = {10.1145/2737095.2742927},
abstract = {The primal problem with the Internet of Things is the lack of interoperability at various levels, and more predominately at the device level. While there exists multitude of platforms from multiple manufacturers, the existing ecosystem still remains highly closed. In this work, we propose SNaaS or Sensor/Network as a Service: a service layer that enables the creation of the plug-n-play infrastructure, across platforms from multiple vendors, necessary for interoperability and successful deployment of large-scale systems. We present the design and implementation of SNaaS, along with preliminary microbenchmarks.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {374–375},
numpages = {2},
keywords = {IEEE 1451, TEDS, internet of things, plug and play},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/3323503.3345027,
author = {Santana, Cleber and Andrade, Leandro and Mello, Brenno and Batista, Ernando and Sampaio, Jos\'{e} Vitor and Prazeres, C\'{a}ssio},
title = {A reliable architecture based on reactive microservices for IoT applications},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3345027},
doi = {10.1145/3323503.3345027},
abstract = {Microservices has recently been employed in Cloud Computing to support the construction of large-scale systems that are resilient, elastic and best suited to meet today's demands. In Internet of Things (IoT) a set of smart applications can be built in the most different scenarios and that can impact the daily routine of people's lives. The development of applications and services at IoT brings challenges such as deployment, elasticity and resilience. Microservices developed with the characteristics of resilience, elasticity and addressed to messages are considered reactive Microservices. Thus, this paper proposes an architectural model based on reactive Microservices to improve the reliability of IoT applications from the perspective of availability.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {15–19},
numpages = {5},
keywords = {IoT applications, availability, internet of things, reactive microservices, reliability},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/2988272.2988280,
author = {Sicari, Sabrina and Rizzardi, Alessandra and Miorandi, Daniele and Coen-Porisini, Alberto},
title = {Internet of Things: Security in the Keys},
year = {2016},
isbn = {9781450345040},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988272.2988280},
doi = {10.1145/2988272.2988280},
abstract = {Security threats may hinder the large scale adoption of the emerging Internet of Things (IoT) technologies. Besides efforts have already been made in the direction of data integrity preservation, confidentiality and privacy, several issues are still open. The existing solutions are mainly based on encryption techniques, but no attention is actually paid to key management. A clever key distribution system, along with a key replacement mechanism, are essentials for assuring a secure approach. In this paper, two popular key management systems, conceived for wireless sensor networks, are integrated in a real IoT middleware and compared in order to evaluate their performance in terms of overhead, delay and robustness towards malicious attacks.},
booktitle = {Proceedings of the 12th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {129–133},
numpages = {5},
keywords = {internet of things, key management, prototype, security},
location = {Malta, Malta},
series = {Q2SWinet '16}
}

@inproceedings{10.1145/3330431.3330446,
author = {Duisebekova, Kulyanda S. and Sarsenova, Zhibek N. and Pyagay, Viktor T. and Tuyakova, Zamira N. and Duzbayev, Nurzhan T. and Aitmagambetov, Altay Z. and Amanzholova, Saule T.},
title = {Environmental monitoring system for analysis of climatic and ecological changes using LoRa technology},
year = {2019},
isbn = {9781450372121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330431.3330446},
doi = {10.1145/3330431.3330446},
abstract = {In this article, the problem of monitoring of climatic and ecological condition of the region is considered. The problem of environmental pollution in large cities is very significant, and modern monitoring systems have a number of significant drawbacks: low speed of deployment, large size of stations, and high cost of maintenance. The authors propose a new approach to the construction of such systems using the technologies of the "Internet of things". This will make it possible to create easily scalable low-cost systems with high energy efficiency through the use of modern communication technologies.},
booktitle = {Proceedings of the 5th International Conference on Engineering and MIS},
articleno = {15},
numpages = {6},
keywords = {IoT, LoRa, LoraWAN, PWAN, ecology, environment},
location = {Astana, Kazakhstan},
series = {ICEMIS '19}
}

@inproceedings{10.1145/2959689.2960077,
author = {Meiklejohn, Chris},
title = {Lasp: A Model For Distributed, Convergent, Edge Computation},
year = {2016},
isbn = {9781450344647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959689.2960077},
doi = {10.1145/2959689.2960077},
abstract = {Consistency is hard and coordination is expensive. As we move into the world of connected 'Internet of Things' style applications, or large-scale mobile applications, devices have less power, periods of limited connectivity, and operate over unreliable asynchronous networks. This poses a problem with shared state: how do we handle concurrent operations over shared state, while clients are offline,and ensure that values converge to a desirable result without making the system unavailable? We look at a new programming model, called Lasp. This programming model combines distributed convergent data structures with a dataflow execution model designed for distribution over large-scale applications.},
booktitle = {Applicative 2016},
location = {New York, NY, USA},
series = {Applicative 2016}
}

@inproceedings{10.1145/2714576.2737091,
author = {Zhang, Zhi-Kai and Cho, Michael Cheng Yi and Shieh, Shiuhpyng},
title = {Emerging Security Threats and Countermeasures in IoT},
year = {2015},
isbn = {9781450332453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2714576.2737091},
doi = {10.1145/2714576.2737091},
abstract = {IoT (Internet of Things) diversifies the future Internet, and has drawn much attention. As more and more gadgets (i.e. Things) connected to the Internet, the huge amount of data exchanged has reached an unprecedented level. As sensitive and private information exchanged between things, privacy becomes a major concern. Among many important issues, scalability, transparency, and reliability are considered as new challenges that differentiate IoT from the conventional Internet. In this paper, we enumerate the IoT communication scenarios and investigate the threats to the large-scale, unreliable, pervasive computing environment. To cope with these new challenges, the conventional security architecture will be revisited. In particular, various authentication schemes will be evaluated to ensure the confidentiality and integrity of the exchanged data.},
booktitle = {Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security},
pages = {1–6},
numpages = {6},
keywords = {authentication, communication, iot, privacy, security},
location = {Singapore, Republic of Singapore},
series = {ASIA CCS '15}
}

@inproceedings{10.1145/3286978.3287023,
author = {Fan, Xinxin and Chai, Qi},
title = {Roll-DPoS: A Randomized Delegated Proof of Stake Scheme for Scalable Blockchain-Based Internet of Things Systems},
year = {2018},
isbn = {9781450360937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286978.3287023},
doi = {10.1145/3286978.3287023},
abstract = {Delegated Proof-of-Stake (DPoS) is an efficient, decentralized, and flexible consensus framework available in the blockchain industry. However, applying DPoS to the decentralized Internet of Things (IoT) applications is quite challenging due to the nature of IoT systems such as large-scale deployments and huge amount of data. To address the unique challenge for IoT based blockchain applications, we present Roll-DPoS, a randomized delegated proof of stake algorithm. Roll-DPoS inherits all the advantages of the original DPoS consensus framework and further enhances its capability in terms of decentralization as well as extensibility to complex blockchain architectures. A number of modern cryptographic techniques have been utilized to optimize the consensus process with respect to the computational and communication overhead.},
booktitle = {Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {482–484},
numpages = {3},
keywords = {Bilinear Pairing, Delegated Proof-of-Stake, Distributed Key Generation, Randomization, Threshold Signature},
location = {New York, NY, USA},
series = {MobiQuitous '18}
}

@inproceedings{10.1145/2957319.2957372,
author = {Casadei, Roberto and Viroli, Mirko},
title = {Towards Aggregate Programming in Scala},
year = {2016},
isbn = {9781450347754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957319.2957372},
doi = {10.1145/2957319.2957372},
abstract = {Recent works in the context of large-scale adaptive systems, such as those for the Internet of Things (IoT) scenario, promote aggregate programming [3], a development approach for distributed systems in which one programs the aggregate of computational devices instead of individual ones. This makes the resulting behaviour highly insensitive to network size, density, and topology, and as such, intrinsically robust to failures and changes to working conditions (e.g., location of computational load, communication technology, and computational infrastructure).In this paper we are concerned with how this approach can impact mainstream software development, and hence outline a Scala-based support of aggregate programming, leveraging Scala advanced type system, DSL support, and actors mechanisms.},
booktitle = {First Workshop on Programming Models and Languages for Distributed Computing},
articleno = {5},
numpages = {7},
keywords = {DSL, Scala, aggregate programming, complex adaptive systems, distributed platform},
location = {Rome, Italy},
series = {PMLDC '16}
}

@inproceedings{10.1145/2857705.2857729,
author = {Voas, Jeffrey},
title = {Decoding the Mystery of the Internet of Things},
year = {2016},
isbn = {9781450339353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857705.2857729},
doi = {10.1145/2857705.2857729},
abstract = {System primitives allow formalisms, reasoning, simulations, and reliability and security risk-tradeoffs to be formulated and argued. In this work, six core primitives belonging to most distributed systems are presented. These primitives apply well to systems with large amounts of data, scalability concerns, heterogeneity concerns, temporal concerns, and elements of unknown pedigree with possible nefarious intent. These primitives form the basic building blocks for a Network of 'Things' (NoT), including the Internet of Things (IoT). This keynote offers an underlying and foundational science to IoT. To my knowledge, the ideas and the manner in which the science underlying IoT is presented here is unique. Further, this talk reflects my personal viewpoints and not those of NIST.},
booktitle = {Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy},
pages = {1–2},
numpages = {2},
keywords = {reliability, security},
location = {New Orleans, Louisiana, USA},
series = {CODASPY '16}
}

@article{10.1145/2786984.2786991,
author = {Samuel, Arjmand and Mohamedally, Dean and Banerjee, Nilanjan and Brush, A. J. and Mahajan, Ratul},
title = {Lab of Things in Education},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-0529},
url = {https://doi.org/10.1145/2786984.2786991},
doi = {10.1145/2786984.2786991},
abstract = {Lately, much has been said about the Internet of Things and how it is going to change how we will live, work and play. Strategy Analytics forecasts that by 2020, every person on the planet will carry four connected devices [1]. The majority of this growth will be driven by the interconnection of devices, sensors, smart objects and the like, and is expected to usher in changes to almost all aspects of our lives. This change will require, on the one hand, large-scale design, development and deployment of cloud and network systems; and on the other hand, design of hardware sensors, actuators, software middleware, and network protocols.},
journal = {GetMobile: Mobile Comp. and Comm.},
month = {jun},
pages = {18–24},
numpages = {7}
}

@inproceedings{10.1145/3054977.3054988,
author = {Hall, Jared and Iqbal, Razib},
title = {CoMPES: A Command Messaging Service for IoT Policy Enforcement in a Heterogeneous Network},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3054988},
doi = {10.1145/3054977.3054988},
abstract = {In this paper, we present a scalable and adaptive model which efficiently and quickly enforces control schemas for the Internet of Things (IoT) via a novel command messaging service. This is achieved by utilizing the n-tier scalability of the cloud to generate vast networks of virtual machines. Within each of these virtual machines exists a cluster of "compute pages". Each cluster of compute pages contains an instance of our policy enforcement algorithm. This algorithm processes device-level telemetry into device-level policies. Our proposed model is built with a particular focus on the efficiency and scalability of the service. We demonstrate the performance of the service with a small-scale real world experiment and provide some analysis of the results.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {37–43},
numpages = {7},
keywords = {IoT control, IoT policy, cloud computing, heterogeneous networks, messaging service, policy enforcement},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.5555/3320516.3320605,
author = {Fujimoto, Richard and Barjis, Joseph and Blasch, Erik and Cai, Wentong and Jin, Dong and Lee, Seunghan and Son, Young-Jun},
title = {Dynamic data driven application systems: research challenges and opportunities},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Dynamic Data Driven Applications Systems (DDDAS) is a paradigm where data is dynamically integrated into an executing application, and in reverse, the application dynamically steers the measurement process in a feedback control loop. Since its inception in 2000, the DDDAS concept has been successfully applied to a host of application areas. New technologies are emerging such as big data, the Internet of Things, and cloud/edge computing. With these trends DDDAS is poised to have large-scale impacts in areas such as smart cities, manufacturing, health care, and security, to name a few. Each author describes their views concerning the important research challenges facing the DDDAS paradigm and opportunities for impact in the years ahead.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {664–678},
numpages = {15},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@article{10.14778/3137765.3137805,
author = {Ren, Xiangnan and Cur\'{e}, Olivier and Ke, Li and Lhez, Jeremy and Belabbess, Badre and Randriamalala, Tendry and Zheng, Yufan and Kepeklian, Gabriel},
title = {Strider: an adaptive, inference-enabled distributed RDF stream processing engine},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137805},
doi = {10.14778/3137765.3137805},
abstract = {Real-time processing of data streams emanating from sensors is becoming a common task in industrial scenarios. An increasing number of processing jobs executed over such platforms are requiring reasoning mechanisms. The key implementation goal is thus to efficiently handle massive incoming data streams and support reasoning, data analytic services. Moreover, in an on-going industrial project on anomaly detection in large potable water networks, we are facing the effect of dynamically changing data and work characteristics in stream processing. The Strider system addresses these research and implementation challenges by considering scalability, fault-tolerance, high throughput and acceptable latency properties. We will demonstrate the benefits of Strider on an Internet of Things-based real world and industrial setting.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1905–1908},
numpages = {4}
}

@inproceedings{10.5555/2872550.2872561,
author = {Frey, Michael and G\"{u}nes, Mesut},
title = {Follow the pheromone trail: on studying ant routing algorithms in simulation and wireless testbeds},
year = {2015},
isbn = {9781510801004},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The Internet of Things (IoT) proposes the interconnection of millions of physical and virtual objects enabling us to interact with our environment in smarter ways. While there are multiple intepretations of the IoT, typically all visions share one commonality - the communication is going to be mostly wireless. Providing adaptive and scalable wireless networks for the IoT is one of the most challenging tasks. Researchers propose the application of bio-inspired networking algorithms in order to tackle these issues. However, studying these algorithms for the IoT opposes its own set of problems and challenges. We present a methodology and a framework to study a particular class of bio-inspired algorithms. The framework enables researchers to study and compare them on the algorithmic level and an approach to run large simulation studies in a comprehensive way.},
booktitle = {Proceedings of the 18th Symposium on Communications &amp; Networking},
pages = {68–74},
numpages = {7},
keywords = {ant routing algorithms, self-organization, wireless communication},
location = {Alexandria, Virginia},
series = {CNS '15}
}

@inproceedings{10.1145/3009925.3009935,
author = {Bouget, Simon},
title = {Position paper: Toward an holistic approach of Systems of Systems},
year = {2016},
isbn = {9781450346658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009925.3009935},
doi = {10.1145/3009925.3009935},
abstract = {Large scale distributed systems have become ubiquitous, from on-line social networks to the Internet-of-things. To meet rising expectations (scalability, robustness, flexibility,...) these systems increasingly espouse complex distributed architectures, that are hard to design, deploy and maintain. To grasp this complexity, developers should be allowed to assemble large distributed systems from smaller parts using a seamless, high-level programming paradigm. We present such an assembly-based programming framework, enabling developers to easily define and realize complex distributed topologies as a construction of simpler blocks (e.g. rings, grids). It does so by harnessing the power of self-organizing overlays, that is made accessible to developers through a high-level Domain Specific Language and self-stabilizing runtime. Our evaluation further shows that our approach is generic, expressive, low-overhead and robust.},
booktitle = {Proceedings of the Doctoral Symposium of the 17th International Middleware Conference},
articleno = {10},
numpages = {4},
location = {Trento, Italy},
series = {Middleware Doctoral Symposium'16}
}

@inproceedings{10.5555/3199700.3199748,
author = {Fallahzadeh, Ramin and Alini, Parastoo and Ghasemzadeh, Hassan},
title = {Learn-on-the-go: autonomous cross-subject context learning for internet-of-things applications},
year = {2017},
publisher = {IEEE Press},
abstract = {Developing machine learning algorithms for applications of Internet-of-Things requires collecting a large amount of labeled training data, which is an expensive and labor-intensive process. Upon a minor change in the context, for example utilization by a new user, the model will need re-training to maintain the initial performance. To address this problem, we propose a graph model and an unsupervised label transfer algorithm (learn-on-the-go) which exploits the relations between source and target user data to develop a highly-accurate and scalable machine learning model. Our analysis on real-world data demonstrates 54% and 22% performance improvement against baseline and state-of-the-art solutions, respectively.},
booktitle = {Proceedings of the 36th International Conference on Computer-Aided Design},
pages = {360–367},
numpages = {8},
keywords = {activity recognition, autonomous transfer learning, cross-subject boosting},
location = {Irvine, California},
series = {ICCAD '17}
}

@inproceedings{10.1145/3287921.3287979,
author = {Tran, Huu Tam and Jahl, Alexander and Geihs, Kurt and Kuppili, Ramaprasad and Nguyen, Xuan Thang and Huynh, Thi Thanh Binh},
title = {DECOM: A framework to support evolution of IoT services},
year = {2018},
isbn = {9781450365390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287921.3287979},
doi = {10.1145/3287921.3287979},
abstract = {In the heterogeneous and dynamic Internet of Things (IoT), applications and services are frequently subject to change for various reasons such as maintaining their functionality, reliability, availability, and performance. Detecting and communicating these changes are still performed manually by responsible developers and administrators. Such a mechanism will not be adequate anymore in the future of large-scale IoT environments. Therefore, we present a comprehensive framework named DECOM for automatic detection and communication of service changes. Here, we assume that capabilities and interfaces of IoT devices are described and provided through REST services. To be able to detect syntactic as well as semantic changes, we transform an extended version of the interface description into a logic program and apply a sequence of analysis steps to detect changes. The feasibility and applicability of the framework are demonstrated in an IoT application scenario.},
booktitle = {Proceedings of the 9th International Symposium on Information and Communication Technology},
pages = {389–396},
numpages = {8},
keywords = {Answer Set Programming, Change Detection, Internet of Things, Service Co-evolution, Service Evolution},
location = {Danang City, Viet Nam},
series = {SoICT '18}
}

@inproceedings{10.1145/3357251.3357581,
author = {Kang, Runchang and Guo, Anhong and Laput, Gierad and Li, Yang and Chen, Xiang 'Anthony'},
title = {Minuet: Multimodal Interaction with an Internet of Things},
year = {2019},
isbn = {9781450369756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357251.3357581},
doi = {10.1145/3357251.3357581},
abstract = {A large number of Internet-of-Things (IoT) devices will soon populate our physical environments. Yet, IoT devices’ reliance on mobile applications and voice-only assistants as the primary interface limits their scalability and expressiveness. Building off of the classic ‘Put-That-There’ system, we contribute an exploration of the design space of voice + gesture interaction with spatially-distributed IoT devices. Our design space decomposes users’ IoT commands into two components—selection and interaction. We articulate how the permutations of voice and freehand gesture for these two components can complementarily afford interaction possibilities that go beyond current approaches. We instantiate this design space as a proof-of-concept sensing platform and demonstrate a series of novel IoT interaction scenarios, such as making ‘dumb’ objects smart, commanding robotic appliances, and resolving ambiguous pointing at cluttered devices.},
booktitle = {Symposium on Spatial User Interaction},
articleno = {2},
numpages = {10},
keywords = {Internet-of-Things, gesture, multimodal interaction, voice},
location = {New Orleans, LA, USA},
series = {SUI '19}
}

@inproceedings{10.1145/3091478.3162383,
author = {Banerjee, Agniva and Dalal, Raka and Mittal, Sudip and Joshi, Karuna Pande},
title = {Generating Digital Twin Models using Knowledge Graphs for Industrial Production Lines},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3162383},
doi = {10.1145/3091478.3162383},
abstract = {Digital Twin models are computerized clones of physical assets that can be used for in-depth analysis. Industrial production lines tend to have multiple sensors to generate near real-time status information for production. Industrial Internet of Things datasets are difficult to analyze and infer valuable insights such as points of failure, estimated overhead. etc. In this paper we introduce a simple way of formalizing knowledge as digital twin models coming from sensors in industrial production lines. We present a way on to extract and infer knowledge from large scale production line data, and enhance manufacturing process management with reasoning capabilities, by introducing a semantic query mechanism. Our system primarily utilizes a graph-based query language equivalent to conjunctive queries and has been enriched with inference rules.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {425–430},
numpages = {6},
keywords = {big data, digital twin, industrial internet of things, knowledge graph, semantic web},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@article{10.1145/2872332,
author = {Wu, Jianjia and Zhao, Wei},
title = {Design and Realization of WInternet: From Net of Things to Internet of Things},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/2872332},
doi = {10.1145/2872332},
abstract = {In recent years, Internet of Things (IoT) has attracted great attention from academia, industry, and government. IoT is considered to be a networking infrastructure that can connect enormous physical objects and has great potential to extend mankind's capabilities in monitoring, analyzing, and controlling the physical space using cyber technologies. Extensive studies on IoT have been carried out and many IoT prototype systems have been built. However, most of these systems are usually suitable for domain-specific applications and operate in a local region. They are really “Nets of Things (NoT)” as they miss mechanisms for large-scale interconnection. As such, how to build a globally interconnected IoT is still an open problem. In this article, by reviewing the development of the Internet, we derive a pathway that may lead to successful development and deployment of a global IoT. We propose and examine a novel IoT architecture, namely, WInternet, which aims at interconnecting small-scale domain-specific NoTs into a globally connected IoT.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {nov},
articleno = {2},
numpages = {12},
keywords = {Internet of Things, WInternet, netlet computing, pipe protocol}
}

@article{10.1145/3001934,
author = {Xue, Yuankun and Li, Ji and Nazarian, Shahin and Bogdan, Paul},
title = {Fundamental Challenges Toward Making the IoT a Reachable Reality: A Model-Centric Investigation},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/3001934},
doi = {10.1145/3001934},
abstract = {Constantly advancing integration capability is paving the way for the construction of the extremely large scale continuum of the Internet where entities or things from vastly varied domains are uniquely addressable and interacting seamlessly to form a giant networked system of systems known as the Internet-of-Things (IoT). In contrast to this visionary networked system paradigm, prior research efforts on the IoT are still very fragmented and confined to disjoint explorations of different applications, architecture, security, services, protocol, and economical domains, thus preventing design exploration and optimization from a unified and global perspective. In this context, this survey article first proposes a mathematical modeling framework that is rich in expressivity to capture IoT characteristics from a global perspective. It also sets forward a set of fundamental challenges in sensing, decentralized computation, robustness, energy efficiency, and hardware security based on the proposed modeling framework. Possible solutions are discussed to shed light on future development of the IoT system paradigm.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {apr},
articleno = {53},
numpages = {25},
keywords = {Internet of Things, challenges, mathematical modeling, optimization}
}

@inproceedings{10.1145/3131542.3131548,
author = {Nikolopoulos, Basil and Dimopoulos, Alexandros C. and Nikolaidou, Mara and Dimitrakopoulos, George and Anagnostopoulos, Dimosthenis},
title = {The role of autonomous aggregators in IoT multi-core systems},
year = {2017},
isbn = {9781450353182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131542.3131548},
doi = {10.1145/3131542.3131548},
abstract = {The Internet of Things constitutes a prominent field, integrating smart devices and people into complex systems that may vary in scale. To ensure the constant availability and performance of provided services, alternative distributed architectures should be explored, promoting system scalability. To this end, alternative architectures for the IoT are proposed. Commonly an intermediate layer consisting of aggregators, controlling sensors and actuators and providing a service interface to IoT applications, is incorporated in such architectures. To promote scalability of IoT systems, aggrerators should to operate as autonomous entities. For an aggregator to become autonomous, self-management policies should be enforced. In the paper, we discuss autonomous aggregator software, running on multi-core IoT systems to efficiently implement such policies. A demonstrator for smart buildings, developed as a proof of concept for the proposed concepts, is also presented.},
booktitle = {Proceedings of the Seventh International Conference on the Internet of Things},
articleno = {17},
numpages = {8},
keywords = {aggregators and sensors, autonomy, internet of things, mulit-core systems},
location = {Linz, Austria},
series = {IoT '17}
}

@inproceedings{10.1145/3079368.3079395,
author = {De Troyer, Christophe and Nicolay, Jens and De Meuter, Wolfgang and Scholliers, Christophe},
title = {Abstractions for Distributed Event-Driven Applications: Position Paper},
year = {2017},
isbn = {9781450348362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079368.3079395},
doi = {10.1145/3079368.3079395},
abstract = {The Internet of Things (IoT) requires us to rethink the way distributed event-driven applications are programmed. IoT applications differ from traditional distributed applications on a number of points. First, they are comprised of an order of magnitude more devices that operate within a dynamic network. Second, failure in large dynamic networks is no longer an exceptional state but a given and thus needs to be part of the core semantics when programming such networks. Third, the hardware in these networks is not homogeneous so that a common software stack is impossible. We believe that contemporary event-driven languages do not offer appropriate abstractions to write IoT applications. We propose a novel computational model for programming IoT applications by identifying four key abstractions for designating network nodes and handle failures that facilitate writing large-scale IoT applications.},
booktitle = {Companion Proceedings of the 1st International Conference on the Art, Science, and Engineering of Programming},
articleno = {18},
numpages = {2},
keywords = {Distributed Programming, Internet of Things, Runtimes},
location = {Brussels, Belgium},
series = {Programming '17}
}

@inproceedings{10.1145/3139937.3139949,
author = {DeMarinis, Nicholas and Fonseca, Rodrigo},
title = {Toward Usable Network Traffic Policies for IoT Devices in Consumer Networks},
year = {2017},
isbn = {9781450353960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139937.3139949},
doi = {10.1145/3139937.3139949},
abstract = {The Internet of Things (IoT) revolution has brought millions of small, low-cost, connected devices into our homes, cities, infrastructure, and more. However, these devices are often plagued by security vulnerabilities that pose threats to user privacy or can threaten the Internet architecture as a whole. Home networks can be particularly vulnerable to these threats as they typically have no network administrator and often contain unpatched or otherwise vulnerable devices.In this paper, we argue that the unique security challenges of home networks require a new network-layer architecture to both protect against external threats and mitigate attacks from compromised devices. We present initial findings based on traffic analysis from a small-scale IoT testbed toward identifying predictable patterns in IoT traffic that may allow construction of a policy-based framework to restrict malicious traffic. Based on our observations, we discuss key features for the design of this architecture to promote future developments in network-layer security in smart home networks.},
booktitle = {Proceedings of the 2017 Workshop on Internet of Things Security and Privacy},
pages = {43–48},
numpages = {6},
keywords = {home networks, internet of things (iot), intrusion detection, network security},
location = {Dallas, Texas, USA},
series = {IoTS&amp;P '17}
}

@inproceedings{10.5555/3283535.3283551,
author = {Mohan, Sibin and Asplund, Mikael and Bloom, Gedare and Sadeghi, Ahmad-Reza and Ibrahim, Ahmad and Salajageh, Negin and Griffioen, Paul and Sinopoli, Bruno},
title = {The future of IoT security: special session},
year = {2018},
isbn = {9781538655641},
publisher = {IEEE Press},
abstract = {The Internet-of-Things (IoT) is a large and complex domain. These systems are often constructed using a very diverse set of hardware, software and protocols. This, combined with the ever increasing number of IoT solutions/services that are rushed to market means that most such systems are rife with security holes. Recent incidents (e.g., the Mirai botnet) further highlight such security issues.With emerging technologies such as blockchain and software-defined networks (SDNs), new security solutions are possible in the IoT domain. In this paper we will explore future trends in IoT security: (a) the use of blockchains in IoT security, (b) data provenance for sensor information, (c) reliable and secure transport mechanisms using SDNs (d) scalable authentication and remote attestation mechanisms for IoT devices and (e) threat modeling and risk/maturity assessment frameworks for the domain.},
booktitle = {Proceedings of the International Conference on Embedded Software},
articleno = {16},
numpages = {7},
location = {Turin, Italy},
series = {EMSOFT '18}
}

@article{10.1145/3012000,
author = {Chang, Chii and Srirama, Satish Narayana and Buyya, Rajkumar},
title = {Mobile Cloud Business Process Management System for the Internet of Things: A Survey},
year = {2016},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3012000},
doi = {10.1145/3012000},
abstract = {The Internet of Things (IoT) represents a comprehensive environment that consists of a large number of smart devices interconnecting heterogeneous physical objects to the Internet. Many domains such as logistics, manufacturing, agriculture, urban computing, home automation, ambient assisted living, and various ubiquitous computing applications have utilized IoT technologies. Meanwhile, Business Process Management Systems (BPMSs) have become a successful and efficient solution for coordinated management and optimized utilization of resources/entities. However, past BPMSs have not considered many issues they will face in managing large-scale connected heterogeneous IoT entities. Without fully understanding the behavior, capability, and state of the IoT entities, the BPMS can fail to manage the IoT integrated information systems. In this article, we analyze existing BPMSs for IoT and identify the limitations and their drawbacks based on a Mobile Cloud Computing perspective. Later, we discuss a number of open challenges in BPMS for IoT.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {70},
numpages = {42},
keywords = {Internet of Things, business process management system, challenge, mobile cloud computing, review, service oriented, state of the art}
}

@inproceedings{10.1145/3285017.3285021,
author = {Grimaldi, Matteo and Pugliese, Federico and Tenace, Valerio and Calimera, Andrea},
title = {A compression-driven training framework for embedded deep neural networks},
year = {2018},
isbn = {9781450365987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285017.3285021},
doi = {10.1145/3285017.3285021},
abstract = {Deep neural networks (DNNs) are brain-inspired machine learning methods designed to recognize key patterns from raw data. State-of-the-art DNNs, even the simplest ones, require a huge amount of memory to store and retrieve data during computation. This prevents a practical mapping onto platforms with very limited resources, like those deployed in the end-nodes of the Internet-of-Things (IoT). The aim of this paper is to describe an efficient compression-driven training framework for embedded DNNs. The learning algorithm, which consists of a modified version of the Stochastic Gradient Descent, limits the original training space in a (-σ, 0, σ) ternarized subspace, with σ a hyperparameter learned layer-wise. Tested on medium- and large-scale DNNs, both Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), the obtained σ-ary DNNs enable an efficient use of sparse-matrix representations, hence high compression rate (up to 77\texttimes{} for CNN and 95\texttimes{} for RNNs), at the cost of a limited accuracy loss.},
booktitle = {Proceedings of the Workshop on INTelligent Embedded Systems Architectures and Applications},
pages = {45–50},
numpages = {6},
keywords = {compression, deep learning, learning algorithms},
location = {Turin, Italy},
series = {INTESA '18}
}

@inproceedings{10.1145/3361570.3361593,
author = {Chouaib, Boulkamh and Lakhdar, Derdouri and Lokmane, Zendaoui},
title = {Smart Home Energy Management System Architecture Using IoT},
year = {2019},
isbn = {9781450362924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361570.3361593},
doi = {10.1145/3361570.3361593},
abstract = {In recent years the rate of energy consumption has been increased considerably, therefore the adoption of an Energy Management System (EMS) is of paramount importance. The advent of the Internet of Things (IoT) has offered promising technologies for tackling the challenges that arise on large scale by creating a massive world-wide network of interconnected smart objects embedded with electronics, software, sensors, and network connectivity. This paper presents a proposition of a general microcontroller based Smart Home Energy Management System (SHEMS) architecture. The proposed model aim to minimize the power consumption in smart home, this can be achieved by controlling and monitoring electrical home appliances. Indeed, microcontroller manages home appliances to avoid the wastage of energy by dimming or turning off the appliances when not in use. In our work the proposed model has been used to design and implement an efficient lighting system to reducing power consumption in home.},
booktitle = {Proceedings of the 9th International Conference on Information Systems and Technologies},
articleno = {18},
numpages = {5},
keywords = {Energy, IoT, Smart home},
location = {Cairo, Egypt},
series = {ICIST '19}
}

@inproceedings{10.1145/3152141.3152391,
author = {Gascon-Samson, Julien and Rafiuzzaman, Mohammad and Pattabiraman, Karthik},
title = {ThingsJS: towards a flexible and self-adaptable middleware for dynamic and heterogeneous IoT environments},
year = {2017},
isbn = {9781450351706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152141.3152391},
doi = {10.1145/3152141.3152391},
abstract = {The Internet of Things (IoT) has gained wide popularity both in academic and industrial contexts. Nowadays, such systems exhibit many important challenges across many dimensions. In this work, we propose ThingsJS, a rich Javascript-based middleware platform and runtime environment that abstracts the inherent complexity of such systems by providing a high-level framework for IoT system developers, built over Javascript. ThingsJS abstracts several large-scale distributed systems considerations, such as scheduling, monitoring and self-adaptation, by means of a rich constraint model, a multi-dimensional resource prediction approach and a SMT-based scheduler to properly schedule and manage the execution of high-level, large-scale distributed applications on heterogeneous physical IoT devices. ThingsJS also provides a rich inter-device communication framework built on top of the widely-used publish/subscribe/MQTT paradigm. Finally, ThingsJS also proposes a rich inter-device Javascript-based code migration framework to support the transparent migration of live IoT components between heterogeneous devices.},
booktitle = {Proceedings of the 4th Workshop on Middleware and Applications for the Internet of Things},
pages = {11–16},
numpages = {6},
keywords = {IoT, Javascript, MQTT, code migration, dependability, internet of things, publish/subscribe, scheduling},
location = {Las Vegas, Nevada},
series = {M4IoT '17}
}

@article{10.1145/2812810,
author = {Makhoul, Abdallah and Guyeux, Christophe and Hakem, Mourad and Bahi, Jacques M.},
title = {Using an Epidemiological Approach to Maximize Data Survival in the Internet of Things},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/2812810},
doi = {10.1145/2812810},
abstract = {The Internet of Things (IoT) has gained worldwide attention in recent years. It transforms the everyday objects that surround us into proactive actors of the Internet, generating and consuming information. An important issue related to the appearance of such a large-scale self-coordinating IoT is the reliability and the collaboration between the objects in the presence of environmental hazards. High failure rates lead to significant loss of data. Therefore, data survivability is a main challenge of the IoT. In this article, we have developed a compartmental e-Epidemic SIR (Susceptible-Infectious-Recovered) model to save the data in the network and let it survive after attacks. Furthermore, our model takes into account the dynamic topology of the network where natural death (crashing nodes) and birth are defined and analyzed. Theoretical methods and simulations are employed to solve and simulate the system of equations developed and to analyze the model.},
journal = {ACM Trans. Internet Technol.},
month = {jan},
articleno = {5},
numpages = {15},
keywords = {Internet of things, data survivability and availability, epidemic models, security, wireless sensor networks}
}

@inproceedings{10.1145/3139937.3139954,
author = {Mergendahl, Samuel and Sisodia, Devkishen and Li, Jun and Cam, Hasan},
title = {Source-End DDoS Defense in IoT Environments},
year = {2017},
isbn = {9781450353960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139937.3139954},
doi = {10.1145/3139937.3139954},
abstract = {While the Internet of Things (IoT) becomes increasingly popular and pervasive in everyday objects, IoT devices often remain unprotected and can be exploited to launch large-scale distributed denial-of-service (DDoS) attacks. One could attempt to employ traditional DDoS defense solutions, but these solutions are hardly suitable in IoT environments since they seldom consider the resource constraints of IoT devices.This paper presents FR-WARD which defends against DDoS attacks launched from an IoT network. FR-WARD is an adaptation of the classic DDoS defense system D-WARD. While both solutions are situated near the attack sources and drop packets to throttle DDoS traffic, FR-WARD utilizes the fast retransmit mechanism in TCP congestion control to minimize resource penalties on benign IoT devices. Based on our analysis and simulation results, FR-WARD not only effectively throttles DDoS traffic but also minimizes retransmission overhead for benign IoT devices.},
booktitle = {Proceedings of the 2017 Workshop on Internet of Things Security and Privacy},
pages = {63–64},
numpages = {2},
keywords = {ddos, fast retransmit, iot, network security, source-end ddos defense, tcp},
location = {Dallas, Texas, USA},
series = {IoTS&amp;P '17}
}

@inproceedings{10.1145/3078468.3078496,
author = {Vernik, Gil and Factor, Michael and Kolodner, Elliot K. and Ofer, Effi and Michiardi, Pietro and Pace, Francesco},
title = {Stocator: a high performance object store connector for spark},
year = {2017},
isbn = {9781450350358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078468.3078496},
doi = {10.1145/3078468.3078496},
abstract = {Data is the natural resource of the 21st century. It is being produced at dizzying rates, e.g., for genomics by sequencers, for Media and Entertainment with very high resolution formats, and for Internet of Things (IoT) by multitudes of sensors. Object Stores such as AWS S3, Azure Blob storage, and IBM Cloud Object Storage, are highly scalable distributed storage systems that offer high capacity, cost effective storage for this data. But it is not enough just to store data; we also need to derive value from it. Apache Spark is the leading big data analytics processing engine. It runs up to one hundred times faster than Hadoop MapReduce and combines SQL, streaming and complex analytics. In this poster we present Stocator, a high performance storage connector, that enables Spark to work directly on data stored in object storage systems.},
booktitle = {Proceedings of the 10th ACM International Systems and Storage Conference},
articleno = {27},
numpages = {1},
location = {Haifa, Israel},
series = {SYSTOR '17}
}

@inproceedings{10.1145/3009912.3009919,
author = {Giang, Nam K. and Lea, Rodger and Blackstock, Michael and Leung, Victor C. M.},
title = {On Building Smart City IoT Applications: a Coordination-based Perspective},
year = {2016},
isbn = {9781450346672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009912.3009919},
doi = {10.1145/3009912.3009919},
abstract = {In the Internet of Things (IoT), Internet-connected things provide an influx of data and resources that offer unlimited possibility for applications and services. Smart City IoT systems refer to the things that are distributed over wide physical areas covering a whole city. While the new breed of data and resources looks promising, building applications in such large scale IoT systems is a difficult task due to the distributed and dynamic natures of entities involved, such as sensing, actuating devices, people and computing resources. In this paper, we explore the process of developing Smart City IoT applications from a coordination-based perspective. We show that a distributed coordination model that oversees such a large group of distributed components is necessary in building Smart City IoT applications. In particular, we propose Adaptive Distributed Dataflow, a novel Dataflow-based programming model that focuses on coordinating city-scale distributed systems that are highly heterogeneous and dynamic.},
booktitle = {Proceedings of the 2nd International Workshop on Smart},
articleno = {7},
numpages = {6},
keywords = {Coordination Models, Distributed Systems, Fog Computing, Internet of Things, Smart City},
location = {Trento, Italy},
series = {SmartCities '16}
}

@inproceedings{10.1145/3167132.3167215,
author = {Xia, Ye and Etchevers, Xavier and Letondeur, Lo\"{\i}c and Coupaye, Thierry and Desprez, Fr\'{e}d\'{e}ric},
title = {Combining hardware nodes and software components ordering-based heuristics for optimizing the placement of distributed IoT applications in the fog},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167215},
doi = {10.1145/3167132.3167215},
abstract = {As fog computing brings compute and storage resources to the edge of the network, there is an increasing need for automated placement (i.e., selection of hosting devices) to deploy distributed applications. Such a placement must conform to applications' resource requirements in a heterogeneous fog infrastructure. The placement decision-making is further complicated by Internet of Things (IoT) applications that are tied to geographical locations of physical objects/things. This paper presents a model, an objective function, and a mechanism to address the problem of placing distributed IoT applications in the fog. Based on a backtrack search algorithm and accompanied heuristics, the proposed mechanism is able to deal with large scale problems, and to efficiently make placement decisions that fit the objective---to lower placed applications' response time. The proposed approach is validated through comparative simulations of different combinations of the algorithms and heuristics on varying sizes of infrastructures and applications.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {751–760},
numpages = {10},
keywords = {IoT, fog computing, heuristics, placement},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.5555/3324320.3324357,
author = {Provoost, Michiel and Weyns, Danny},
title = {Poster: Towards Dependable IoT Systems Using Self-Adaptation},
year = {2019},
isbn = {9780994988638},
publisher = {Junction Publishing},
address = {USA},
abstract = {Realizing dependable Internet-of-Things (IoT) comes with numerous challenges. For large-scale IoT deployments, one particular challenge is handling uncertain operating conditions, such as interference in the wireless communication between devices and gateways or changing availability of services. Without proper mitigation of such uncertainties, dependability goals of IoT systems, such as reliability and energy efficiency, may be jeopardized. In our research, we study how self-adaptation can be applied to realize dependable IoT systems regardless of uncertainties. Self-adaptation equips an IoT system with a feedback loop that tracks the uncertainties and adapts the system as needed to maintain its quality goals. Our focus is on large-scale IoT systems with mobile devices. In this paper, we study a typical adaptation problem of IoT where mobile devices need to adapt their power settings while moving in an heterogeneous environment to ensure reliable and energy efficient communication. We evaluate the approach using a simulator of DingNet, an IoT system that is deployed in Leuven, Belgium.},
booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
pages = {228–229},
numpages = {2},
location = {Beijing, China},
series = {EWSN '19}
}

@inproceedings{10.1145/2851613.2851962,
author = {Cecchinel, Cyril and Mosser, S\'{e}bastien and Collet, Philippe},
title = {Towards a (de)composable workflow architecture to define data collection policies},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851962},
doi = {10.1145/2851613.2851962},
abstract = {Sensor networks are classically used in the Internet of Things to collect data, typically supporting Smart Cities or Smart Homes use cases. However, a deep knowledge of these networks is needed to properly develop applications over the deployed systems. This leads to a target mismatch: developers know how to exploit the collected data to develop large-scale "smart" systems, but do not have enough knowledge to technically enact and compose such behaviors on a given sensor network. In this paper, we envision a tooled approach that supports data collection policies management at a higher level of abstraction, fostering reuse. We discuss an architectural abstraction based on workflow concepts assisting developers in expressing data collection policies. The resulting architectures are then composable to be enacted on the same sensor network, but they can also be partially reused through a selection operator.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1344–1346},
numpages = {3},
keywords = {sensor network, software architecture, software reuse},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2818869.2818898,
author = {Tang, Bo and Chen, Zhen and Hefferman, Gerald and Wei, Tao and He, Haibo and Yang, Qing},
title = {A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818898},
doi = {10.1145/2818869.2818898},
abstract = {The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {28},
numpages = {6},
keywords = {Fog computing, big data analysis, distributed computing architecture, pipeline safety monitoring, smart city},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.1145/3199919.3199924,
author = {Ciortea, Andrei and Boissier, Olivier and Ricci, Alessandro},
title = {Beyond Physical Mashups: Autonomous Systems for the Web of Things},
year = {2017},
isbn = {9781450363440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199919.3199924},
doi = {10.1145/3199919.3199924},
abstract = {By abstracting devices to Web resources, the Web of Things (WoT) fosters innovation and rapid prototyping in the Internet of Things (IoT): it enables developers to use standard Web technologies for creating mashups of Web services that perceive and act on the physical world (a.k.a. physical mashups). In recent years, however, it has become apparent that current programming paradigms for Web development have important shortcomings when it comes to engineering IoT systems: static Web mashups cannot adapt to dynamic IoT environments, and manually mashing-up the IoT does not scale. To address these limitations, WoT researchers started to look for means to engineer WoT systems that are more autonomous in pursuit of their design objectives. The engineering of autonomous systems has already been explored to a large extent in the scientific literature on artificial intelligence. In this position paper, we distill that large body of research into a coherent set of abstractions for engineering autonomous WoT systems.},
booktitle = {Proceedings of the Eighth International Workshop on the Web of Things},
pages = {16–20},
numpages = {5},
keywords = {Autonomous agents, Web of Things, multi-agent systems},
location = {Linz, CA, USA},
series = {WoT 2017}
}

@inproceedings{10.1145/3230833.3232806,
author = {Michaels, Alan J.},
title = {Improved RNS-based PRNGs},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3232806},
doi = {10.1145/3230833.3232806},
abstract = {In developing pseudorandom number generation mechanisms for low-power systems like the Internet of Things (IoT), there exists a large tradeoff between computational complexity and the resulting security enabled by the generator. For most communications applications, the use of any PRNG stream must be performed in a synchronizable, and sometimes invertible, fashion. This paper focuses on improvements to prior residue number space (RNS)-based PRNGs, configured to support extremely low-power IoT applications via internal dynamics like switching between PRNG components, simpler permutation-based mappings as opposed to pre-defined polynomials, dynamic indexing processes for improved multiple access operation, and computationally efficient sequence combination techniques. While similarly scalable to applications of prior RNS-based PRNGs, simulation and hardware prototyping results on an MSP430 and Altera FPGAs (Cyclone V and Arria 10) equally validate the suitability of the proposed PRNG techniques for microprocessor-level low-power implementations like industrial IoT.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {20},
numpages = {5},
keywords = {Residue number system, pseudorandom number generation},
location = {Hamburg, Germany},
series = {ARES '18}
}

@inproceedings{10.1145/2820975.2820979,
author = {Shahriar, Md. Sumon and Rahman, M. Sabbir},
title = {Urban Sensing and Smart Home Energy Optimisations: A Machine Learning Approach},
year = {2015},
isbn = {9781450338387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820975.2820979},
doi = {10.1145/2820975.2820979},
abstract = {Energy efficiency for smart home applications is proposed using urban sensing data with machine learning techniques. We exploit Internet of Things (IoTs) enabled environmental and energy panel sensor data, smart home sensing data and opportunistic crowd-sourced data for energy efficient applications in a smart urban home. We present some applications where data from the IoT enabled sensors can be utilised using machine learning techniques. Prediction of small scale renewable energy using solar photovoltaic panels and environmental sensor data is used in energy management such as water heating system. Smart meter data and motion sensor data are used in household appliance monitoring applications with machine learning techniques towards energy savings. Further event detection from environmental and traffic sensor data is proposed in planning and optimising energy usage of smart electric vehicles for a smart urban home. Initial experimental results show the applicability of developing energy efficient applications using machine learning techniques with IoT enabled sensor data.},
booktitle = {Proceedings of the 2015 International Workshop on Internet of Things towards Applications},
pages = {19–22},
numpages = {4},
keywords = {energy efficiency, machine learning, urban sensing},
location = {Seoul, South Korea},
series = {IoT-App '15}
}

@inproceedings{10.1145/3318216.3363333,
author = {Wang, An and Zha, Zili and Guo, Yang and Chen, Songqing},
title = {A SDN-based network layer for edge computing: poster},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363333},
doi = {10.1145/3318216.3363333},
abstract = {Driven by big data analytical capabilities and ever-expanding Internet-of-Things (IoT) devices and applications, the new computing paradigm, Edge Computing significantly improves network performance by collecting and processing data locally or in a nearby edge data center. It offers a far less expensive route to scalability, allowing service providers to expand their computing capability as needed. Meanwhile, emerging network technologies, such as Software Defined Networking (SDN) and programmale data plane, can facilitate cost-efficient networking and direct network management. Such advancements stimulate recent efforts to adopt these technologies by Edge Computing to further improve efficiency and reduce latency. However, albeit a lot of efforts have been spent on the edge computing, little has been developed from the SDN perspective. In particular, a generic framework that integrates programmable network control is still missing. In this poster, we discuss the relevant challenges and propose an initial design of such a framework.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {334–336},
numpages = {3},
location = {Arlington, Virginia},
series = {SEC '19}
}

@inproceedings{10.1145/2991079.2991090,
author = {Agadakos, Ioannis and Hallgren, Per and Damopoulos, Dimitrios and Sabelfeld, Andrei and Portokalidis, Georgios},
title = {Location-enhanced authentication using the IoT: because you cannot be in two places at once},
year = {2016},
isbn = {9781450347716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991079.2991090},
doi = {10.1145/2991079.2991090},
abstract = {User location can act as an additional factor of authentication in scenarios where physical presence is required, such as when making in-person purchases or unlocking a vehicle. This paper proposes a novel approach for estimating user location and modeling user movement using the Internet of Things (IoT). Our goal is to utilize its scale and diversity to estimate location more robustly, than solutions based on smartphones alone, and stop adversaries from using compromised user credentials (e.g., stolen keys, passwords, etc.), when sufficient evidence physically locates them elsewhere. To locate users, we leverage the increasing number of IoT devices carried and used by them and the smart environments that observe these devices. We also exploit the ability of many IoT devices to "sense" the user. To demonstrate our approach, we build a system, called Icelus. Our experiments with it show that it exhibits a smaller false-rejection rate than smartphone-based location-based authentication (LBA) and it rejects attackers with few errors (i.e., false acceptances).},
booktitle = {Proceedings of the 32nd Annual Conference on Computer Security Applications},
pages = {251–264},
numpages = {14},
keywords = {authentication, internet of things, location-based services, trust},
location = {Los Angeles, California, USA},
series = {ACSAC '16}
}

@inproceedings{10.1145/3054977.3054997,
author = {Zeitz, Kimberly and Cantrell, Michael and Marchany, Randy and Tront, Joseph},
title = {Designing a Micro-Moving Target IPv6 Defense for the Internet of Things},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3054997},
doi = {10.1145/3054977.3054997},
abstract = {As the use of low-power and low-resource embedded devices continues to increase dramatically with the introduction of new Internet of Things (IoT) devices, security techniques are necessary which are compatible with these devices. This research advances the knowledge in the area of cyber security for the IoT through the exploration of a moving target defense to apply for limiting the time attackers may conduct reconnaissance on embedded systems while considering the challenges presented from IoT devices such as resource and performance constraints. We introduce the design and optimizations for a Micro-Moving Target IPv6 Defense including a description of the modes of operation, needed protocols, and use of lightweight hash algorithms. We also detail the testing and validation possibilities including a Cooja simulation configuration, and describe the direction to further enhance and validate the security technique through large scale simulations and hardware testing followed by providing information on other future considerations.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {179–184},
numpages = {6},
keywords = {Embedded Systems, IoT, Moving Target Defense},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00022,
author = {Koziolek, Heiko and Burger, Andreas and Platenius-Mohr, Marie and R\"{u}ckert, Julius and Stomberg, G\"{o}sta},
title = {OpenPnP: a plug-and-produce architecture for the industrial internet of things},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00022},
doi = {10.1109/ICSE-SEIP.2019.00022},
abstract = {Industrial control systems are complex, softwareintensive systems that manage mission-critical production processes. Commissioning such systems requires installing, configuring, and integrating thousands of sensors, actuators, and controllers and is still a largely manual and costly process. Therefore, practitioners and researchers have been working on "plug and produce" approaches that automate commissioning for more than 15 years, but have often focused on network discovery and proprietary technologies. We introduce the vendor-neutral OpenPnP reference architecture, which can largely automate the configuration and integration tasks for commissioning. Using an example implementation, we demonstrate that OpenPnP can reduce the configuration and integration effort up to 90 percent and scales up to tens of thousands of communicated signals per second for large Industrial Internet-of-Things (IIoT) systems. OpenPnP can serve as a template for practitioners implementing IIoT applications throughout the automation industry and streamline commissioning processes in many thousands of control system installations.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {131–140},
numpages = {10},
keywords = {client-server systems, control engineering, internet of things, real-time systems, software architecture},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1145/2821650.2821669,
author = {Bhattacharya, Arka and Ploennigs, Joern and Culler, David},
title = {Short Paper: Analyzing Metadata Schemas for Buildings: The Good, the Bad, and the Ugly},
year = {2015},
isbn = {9781450339810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2821650.2821669},
doi = {10.1145/2821650.2821669},
abstract = {Commercial buildings account for a large amount of delivered energy in the U.S., nearly 42% of which is consumed in buildings with digital control systems {EIA}. These buildings are a ripe venue to deploy novel applications because of (a) access to sensors and actuators that are used in their digital control systems, (b) deployed wireless sensor networks, and (c) the advent of smart "internet-of-things" sensors. However, these novel applications face a fundamental scalability challenge because the sensor metadata across buildings do not follow any common schema. In this paper, we quantify the shortcomings of three metadata schemas which have gained traction in modeling the contextual, spatial and functional relationships between sensors in the built environment: (1) Project Haystack, (2) Industry Foundation Classes (3) and Semantic Sensor Web against three commercial buildings and an extensive list of smart-building applications.},
booktitle = {Proceedings of the 2nd ACM International Conference on Embedded Systems for Energy-Efficient Built Environments},
pages = {33–34},
numpages = {2},
keywords = {building metadata schemas, portable applications},
location = {Seoul, South Korea},
series = {BuildSys '15}
}

@inproceedings{10.1145/2837060.2837108,
author = {Igorevich, Rustam Rakhimov and Min, Dugki},
title = {Smart Car Use Case: Dynamic Reconfigurable IoT Convergence with BigData},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837108},
doi = {10.1145/2837060.2837108},
abstract = {Big Data becomes un-detachable part of the highly scalable applications in Internet of Things. In this work we introduce use case of Dynamic Reconfigurable IoT convergence with Big Data. Dynamic Reconfigurable IoT framework was developed based on OSGi (Open Service Gateway initiative) to be cross-platform and flexible framework. Suggested Dynamic Reconfigurable IoT Framework has been applied to Smart Car with CAN, MOST Networks. Drowsiness detection module was suggested as an application module for Dynamic Reconfigurable IoT framework. Usage of Big Data techniques in storing and processing IoT generated data is necessary to provide additional services. Smart Car uses MQTT protocol to communicate with the IoT Server to deliver generated sensor and video data. When the sensor and video data delivered to IoT Server, it is saved in HDFS to perform further processing and final processed data stored in MongoDB. The owner of the Smart Car can get different services and advices from the IoT Server through the popular REST web interface.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {254–258},
numpages = {5},
keywords = {Big Data, CAN, Dynamic, JMeter, MOST, OBDII, OSGi, RaspberryPi, Reconfigurable},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.1145/2820783.2820843,
author = {Wang, Ran and Chow, Chi-Yin and Lyu, Yan and Lee, Victor C. S. and Kwong, Sam and Li, Yanhua and Zeng, Jia},
title = {TaxiRec: recommending road clusters to taxi drivers using ranking-based extreme learning machines},
year = {2015},
isbn = {9781450339674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820783.2820843},
doi = {10.1145/2820783.2820843},
abstract = {Utilizing large-scale GPS data to improve taxi services becomes a popular research problem in the areas of data mining, intelligent transportation, and the Internet of Things. In this paper, we utilize a large-scale GPS data set generated by over 7,000 taxis in a period of one month in Nanjing, China, and propose TaxiRec; a framework for discovering the passenger-finding potentials of road clusters, which is incorporated into a recommender system for taxi drivers to hunt passengers. In TaxiRec, we first construct the road network by defining the nodes and road segments. Then, the road network is divided into a number of road clusters through a clustering process on the mid points of the road segments. Afterwards, a set of features for each road cluster is extracted from real-life data sets, and a ranking-based extreme learning machine (ELM) model is proposed to evaluate the passenger-finding potential of each road cluster. Experimental results demonstrate the feasibility and effectiveness of the proposed framework.},
booktitle = {Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {53},
numpages = {4},
keywords = {extreme learning machine, passenger-finding potential, recommender system, taxi trajectory data},
location = {Seattle, Washington},
series = {SIGSPATIAL '15}
}

@inproceedings{10.1145/2801948.2801974,
author = {Amaxilatis, Dimitrios and Chatzigiannakis, Ioannis and Mylonas, Georgios and Pocero, Lidia and Zarras, Dimitrios and Koskeris, Andreas},
title = {Green mindset: using IoT to promote energy efficiency and sustainability in Greek public schools},
year = {2015},
isbn = {9781450335515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801948.2801974},
doi = {10.1145/2801948.2801974},
abstract = {The Internet of Things is shaping up to be the ideal vehicle for introducing pervasive computing in our everyday lives, especially in the form of smart home and building management systems. However, although such technologies are gradually becoming more mainstream, there is still a lot of ground to be covered with respect to public buildings and specifically ones in the educational sector. We discuss here "Green Mindset", an action focusing on energy efficiency and sustainability in Greek public schools. A large-scale sensor infrastructure has been deployed to 12 public school buildings across diverse settings. We report on the overall design and implementation of the system, as well as on some first results coming from the data produced. Our system provides a flexible and efficient basis for realizing a unified approach to monitoring energy consumption and environmental parameters, that can be used both for building administration and educational purposes.},
booktitle = {Proceedings of the 19th Panhellenic Conference on Informatics},
pages = {297–302},
numpages = {6},
keywords = {IoT, deployment, education, sensor network, sustainability},
location = {Athens, Greece},
series = {PCI '15}
}

@inproceedings{10.1145/3289100.3289105,
author = {Stevens, Mason and Horne, Matt and Yao, Yizhou and Wang, Jiaqi and Abdelgawad, Ahmed},
title = {Using Internet of Vehicle Technology for Decreased Medical Response Times},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289105},
doi = {10.1145/3289100.3289105},
abstract = {Internet of Things technology has great potential to be integrated into the automotive industry. Cheap electronics systems such as the Raspberry Pi can be used to communicate GPS information between emergency and pedestrian vehicles. This information can be used to instruct pedestrian vehicles to pull over preemptively, decreasing the response time of emergency vehicles. Further, emergency vehicles can take advantage of cloud computing to give hospitals vitals data from a patient, such as their heart rate, breathing rate, or temperature in order to achieve a quicker diagnosis. This paper displays a proof of concept for this idea and a small-scale test for accuracy and latency of this communication, as well as many of the challenges in designing a project of this nature. At the conclusion of this paper, the reader should have a basic understanding of the process of implementation and obstacles this technology faces in the future.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {27–32},
numpages = {6},
keywords = {Ambulance, Automation, Internet of Things, Internet of Vehicles, Real-Time Video Streaming, ThingSpeak, eHealth Sensor Kit},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@inproceedings{10.5555/3042094.3042329,
author = {Volovoi, Vitali},
title = {Simulation of maintenance processes in the big data era},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {Maintenance processes of repairable systems have been extensively studied in the past. The resulting simple solutions have proven to be remarkably effective. It requires complex and time-consuming simulations to improve on those simple solutions, and reliable input data is even harder to get. However, new technologies, epitomized by Big Data and the Internet of Things, change the data-availability part of the equation. As a result, there are new exciting possibilities for modeling more subtle effects, and developing processes for easily (and therefore frequently) updated inputs. Modeling decisions can be repeatedly tested on the data, and the models can be quickly adjusted to better reflect reality and even to compensate for missing pieces of the data. In this context, the transparency and simplicity of models becomes a larger virtue. Several examples of the insights based on real-world large-scale applications of predictive analytics using simulation are discussed.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {1872–1883},
numpages = {12},
location = {Arlington, Virginia},
series = {WSC '16}
}

@inproceedings{10.1145/3299902.3311062,
author = {Maasoumy, Mehdi},
title = {Enterprise-wide AI-enabled Digital Transformation},
year = {2019},
isbn = {9781450362535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299902.3311062},
doi = {10.1145/3299902.3311062},
abstract = {Having solved the data integration problem, we discuss how convergence of 4 technology vectors, namely Big Data, Artificial Intelligence, Cloud Computing, and Internet of Things (IoT) has, for the first time, enabled us to solve a class of problems previously deemed as unsolvable at massive scales. AI applications such as predictive maintenance, fraud detection, sensor network health, supply chain optimization, energy management, anti-money laundering, and customer engagement are among the set of problems that are now solvable at enterprise scale. This is possible, thanks to the platform that brings together all infrastructure, micro services, data sources and research and data scientist on the same platform, and in doing so improves the productivity of the development team by a factor of 10-100x. We will discuss the "Predictive Maintenance" problem applied to distribution networks, aircraft systems and oil and gas assets, "Inventory Optimization" problem solved in the manufacturing industry, and "Fraud Detection" in the electricity space as well as banking space.},
booktitle = {Proceedings of the 2019 International Symposium on Physical Design},
pages = {103},
numpages = {1},
keywords = {fraud detection, inventory optimization, predictive maintenance},
location = {San Francisco, CA, USA},
series = {ISPD '19}
}

@inproceedings{10.1145/3184558.3192307,
author = {Ahlers, Dirk and Wilde, Erik and Schifanella, Rossano and Alowibdi, Jalal S. and Zubair Shafiq, Muhammad},
title = {LocWeb2018 Chairs' Welcome &amp; Organization},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3192307},
doi = {10.1145/3184558.3192307},
abstract = {It is our great pleasure to welcome you to the 8th International Workshop on Location and the Web (LocWeb2018) at WWW 2018. LocWeb 2018 will continue a successful workshop series at the intersection of location-based services and Web architecture. It focuses on Web-scale services and systems facilitating location-aware information access as well as on Spatial Social Behavior Analytics on the Web as part of social computing. The location topic is seen as a cross-cutting issue equally concerning information access, semantics and standards, social analysis and mining, and Web-scale systems and services. The workshop is an integrated venue where location and spatio-social aspects can be discussed in depth with an interested community. New application areas for Web architecture, such as the Internet of Things (IoT) and the Web of Things (WoT), will lead to increasingly rich and large sets of applications for which location is highly relevant as the connection to the physical world. Location has high importance in Web-based designs, and it continues to provide challenging research questions.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1188–1189},
numpages = {2},
keywords = {locweb, welcome, workshop},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3279996.3280012,
author = {Lazreg, Amel ben and Arbia, Dhafer Ben and Arbia, Anis Ben and Youssef, Habib},
title = {A novel cloudlet-based communication framework for resource-constrained devices},
year = {2018},
isbn = {9781450365369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279996.3280012},
doi = {10.1145/3279996.3280012},
abstract = {In this paper, we present a new emerging technology for building mobility-enhanced and small-scaled mobile to cloud applications called Cloudlet. We address the reliability challenges in affording a powerful computing resources to the mobile applications. The proposed system connects to different end points (mobile, sensors, computer) in order to collect information and send it to the cloud server. In addition, the integrated system allows users to make autonomic applications and send intelligent commands to the cloud in both on-line and off-line mode. The system integrates heterogeneous wireless devices (embadded system, sensors, integrators) and various communicating technologies (WiFi IEEE802.11n, GSM/GPRS, 3G, etc) to enable connectivity which is monitored by a cloud Internet-of-Things platform. The performance of GPRS/GSM based framework is evaluated under realistic conditions to ensure data synchronization between Cloudlet and Cloud. The results showed that the proposed approach increases the downstream throughput from the Cloud to the Cloudlet.},
booktitle = {Proceedings of the First International Conference on Data Science, E-Learning and Information Systems},
articleno = {16},
numpages = {6},
keywords = {GPRS/GSM technology, cloud server, cloudlet, end-to-end communication, internet of things},
location = {Madrid, Spain},
series = {DATA '18}
}

@inproceedings{10.1145/3312614.3312649,
author = {He, Xinchi and Alqahtani, Sarra and Gamble, Rose and Papa, Mauricio},
title = {Securing Over-The-Air IoT Firmware Updates using Blockchain},
year = {2019},
isbn = {9781450366403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312614.3312649},
doi = {10.1145/3312614.3312649},
abstract = {Over-the-air (OTA) firmware updates are very common in IoT (Internet of Things) devices with wireless capabilities. Although they are convenient, they may also be open to attack since physical access may not be needed. In addition, most frameworks use a centralized architecture to update a potentially large number of devices increasing the threat landscape. An alternative solution, that relies on a blockchain framework with smart contracts, is proposed in this paper to protect the integrity of the firmware update process. The proposed system is suitable for use in smart cities or scenarios with a large number of devices and service providers where nodes are authenticated, communications protected, and update conditions specified and enforced through smart contracts. A proof--of--concept system was implemented and tested using an open--source blockchain framework and a WiFi--capabable ESP8266--based board. The system was evaluated for scalability and response to denial of service (DoS) and man--in--the--middle (MitM) attacks. Preliminary experimental results show that the approach is feasible and a viable substitute for a centralized solution.},
booktitle = {Proceedings of the International Conference on Omni-Layer Intelligent Systems},
pages = {164–171},
numpages = {8},
keywords = {Firmware update, Internet of Things, blockchain, network security},
location = {Crete, Greece},
series = {COINS '19}
}

@inproceedings{10.1145/3289100.3289103,
author = {Hossain, Shadeeb and Abdelgawad, Ahmed},
title = {Smart Refrigerator Based on Internet of Things (IoT): An Approach to Efficient Food Management},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289103},
doi = {10.1145/3289100.3289103},
abstract = {This paper focuses on the technology of the smart refrigerator connected through the Internet of Things (IoT) platform. It deals with the issue of food wastage and efficient food distribution through a network of connected refrigerators within a defined neighborhood. This paper defines the three-layer architecture: front-end layer, gateway layer and the back-end layer that can allow the implementation of the system. This low-cost network can be created through a microcontroller and wireless sensors at the front-end architecture. The process was discussed to determine: the stock item below a defined quantity or threshold limit, determining the product expiration date and eventually determining the purchase or exchange of food items across the network. It also includes the proof of concept technology by allowing connection of an ultrasonic sensor to an IoT platform that can allow the exchange of information on the positioning of the food items inside the refrigerator. This eventually can allow for a large-scale commercial implementation of the project to deal with the current issue of food distribution.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {15–18},
numpages = {4},
keywords = {Cloud architecture, Internet of Things, Raspberry Pi, Smart refrigerator},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@inproceedings{10.1145/3131542.3131564,
author = {Missier, Paolo and Bajoudah, Shaimaa and Capossele, Angelo and Gaglione, Andrea and Nati, Michele},
title = {Mind my value: a decentralized infrastructure for fair and trusted IoT data trading},
year = {2017},
isbn = {9781450353182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131542.3131564},
doi = {10.1145/3131542.3131564},
abstract = {Internet of Things (IoT) data are increasingly viewed as a new form of massively distributed and large scale digital assets, which are continuously generated by millions of connected devices. The real value of such assets can only be realized by allowing IoT data trading to occur on a marketplace that rewards every single producer and consumer, at a very granular level. Crucially, we believe that such a marketplace should not be owned by anybody, and should instead fairly and transparently self-enforce a well defined set of governance rules. In this paper we address some of the technical challenges involved in realizing such a marketplace. We leverage emerging blockchain technologies to build a decentralized, trusted, transparent and open architecture for IoT traffic metering and contract compliance, on top of the largely adopted IoT brokered data infrastructure. We discuss an Ethereum-based prototype implementation and experimentally evaluate the overhead cost associated with Smart Contract transactions, concluding that a viable business model can indeed be associated with our technical approach.},
booktitle = {Proceedings of the Seventh International Conference on the Internet of Things},
articleno = {15},
numpages = {8},
location = {Linz, Austria},
series = {IoT '17}
}

@inproceedings{10.1145/3365871.3365880,
author = {Toro-Betancur, Ver\'{o}nica and Zamora, Jos\'{e} Viquez and Antikainen, Markku and Di Francesco, Mario},
title = {A Scalable Software Update Service for IoT Devices in Urban Scenarios},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365880},
doi = {10.1145/3365871.3365880},
abstract = {Devices in the Internet of Things (IoT) are software-driven, thus, they need not be only programmed before deployment, but also continuously updated. IoT deployments in urban scenarios are particularly relevant as enablers of smart city applications. For such a context, this work addresses the reliability and security aspects of distributing software updates to a large number of IoT devices. Specifically, it presents a design and implementation of a software update framework for IoT devices in urban scenarios. The proposed approach leverages long-range wireless broadcast to update a large number of IoT devices at the same time, which scales up to the massive networks that are typical of densely-populated and built-up metropolitan areas. Experiments on a real testbed demonstrate that the proposed approach obtains a long range (up to 350 m) and a success rate higher than 99% with a single transmission, for IoT devices deployed both outdoors and indoors. In particular, broadcast updates are always more efficient than standard updates over the Internet through enterprise WiFi for typical urban IoT deployments.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {9},
numpages = {8},
keywords = {Long-range broadcast, Scalability, Security, Software updates},
location = {Bilbao, Spain},
series = {IoT '19}
}

@inproceedings{10.1145/3006299.3006326,
author = {Chen, Yi and Bordbar, Behzad},
title = {DRESS: a rule engine on spark for event stream processing},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006326},
doi = {10.1145/3006299.3006326},
abstract = {Rule-based systems process event streams and trigger actions according to pre-defined rule-sets. Over the last three decades, such systems have been widely used in businesses, governments and organisations. However, with today's need to process larger event streams such as events produced in Internet of Things (IoT), current rule-based systems face serious challenges in terms of speed, scalability and fault tolerance. Spark Streaming is emerging as a novel solution to address these challenges. This paper presents an approach for adapting rule-based systems to work with Spark Streaming. We focus on Rete algorithm which is behind many of the current rule engines. We present DRESS (Distributed Rule Engine on Spark Streaming), an infrastructure for executing Rete algorithm on Spark Streaming. In addition, we present an automated method of transforming rules written in Drools' style to be executed on DRESS. The performance of our system was evaluated with the help of a case study.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {46–51},
numpages = {6},
keywords = {apache spark, event stream processing, rule engine},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/3287921.3287935,
author = {Ly-Trong, Nhan and Dang-Le-Bao, Chuong and Huynh-Van, Dang and Le-Trung, Quan},
title = {UiTiOt v3: A Hybrid Testbed for Evaluation of Large-Scale IoT Networks},
year = {2018},
isbn = {9781450365390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287921.3287935},
doi = {10.1145/3287921.3287935},
abstract = {The 21st century is promising to be the era of technology standing out with the thrive of the Internet of Things (IoT). The rapid growth of the IoT leads to an increasingly urgent need of testbed systems. Taking a new step in our long-term research, in this paper, we introduce the third version of our testbed named UiTiOt v3. It is actually a hybrid system which puts virtual wireless nodes together with real physical devices to carry out a variety of IoT experiments. Utilizing the power of the emulation tool, namely QOMET along with the two-layer virtualization based on two state-of-the-art virtualization techniques (Container and OpenStack), our research is towards a large-scale testbed having great advantages in terms of reliability, availability as well as cost-and-effort-effectiveness. After giving details of the overall architectural design, we make a discussion on one typical experiment out of a vast number of test cases we have already done to demonstrate the feasibility and potential of our system.},
booktitle = {Proceedings of the 9th International Symposium on Information and Communication Technology},
pages = {155–162},
numpages = {8},
keywords = {Emulation, Hybrid testbed, IoT networks, Large-scale testbed, UiTiOt},
location = {Danang City, Viet Nam},
series = {SoICT '18}
}

@inproceedings{10.1145/3292006.3300024,
author = {Tambe, Amit and Aung, Yan Lin and Sridharan, Ragav and Ochoa, Mart\'{\i}n and Tippenhauer, Nils Ole and Shabtai, Asaf and Elovici, Yuval},
title = {Detection of Threats to IoT Devices using Scalable VPN-forwarded Honeypots},
year = {2019},
isbn = {9781450360999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292006.3300024},
doi = {10.1145/3292006.3300024},
abstract = {Attacks on Internet of Things (IoT) devices, exploiting inherent vulnerabilities, have intensified over the last few years. Recent large-scale attacks, such as Persirai, Hakai, etc. corroborate concerns about the security of IoT devices. In this work, we propose an approach that allows easy integration of commercial off-the-shelf IoT devices into a general honeypot architecture. Our approach projects a small number of heterogeneous IoT devices (that are physically at one location) as many (geographically distributed) devices on the Internet, using connections to commercial and private VPN services. The goal is for those devices to be discovered and exploited by attacks on the Internet, thereby revealing unknown vulnerabilities. For detection and examination of potentially malicious traffic, we devise two analysis strategies: (1) given an outbound connection from honeypot, backtrack into network traffic to detect the corresponding attack command that caused the malicious connection and use it to download malware, (2) perform live detection of unseen URLs from HTTP requests using adaptive clustering. We show that our implementation and analysis strategies are able to detect recent large-scale attacks targeting IoT devices (IoT Reaper, Hakai, etc.) with overall low cost and maintenance effort.},
booktitle = {Proceedings of the Ninth ACM Conference on Data and Application Security and Privacy},
pages = {85–96},
numpages = {12},
keywords = {adaptive clustering, attack attribution, high-interaction iot honeypot, intrusion detection, network traffic analysis},
location = {Richardson, Texas, USA},
series = {CODASPY '19}
}

@inproceedings{10.1145/3126908.3126933,
author = {Amaral, Marcelo and Polo, Jord\`{a} and Carrera, David and Seelam, Seetharami and Steinder, Malgorzata},
title = {Topology-aware GPU scheduling for learning workloads in cloud environments},
year = {2017},
isbn = {9781450351140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126908.3126933},
doi = {10.1145/3126908.3126933},
abstract = {Recent advances in hardware, such as systems with multiple GPUs and their availability in the cloud, are enabling deep learning in various domains including health care, autonomous vehicles, and Internet of Things. Multi-GPU systems exhibit complex connectivity among GPUs and between GPUs and CPUs. Workload schedulers must consider hardware topology and workload communication requirements in order to allocate CPU and GPU resources for optimal execution time and improved utilization in shared cloud environments.This paper presents a new topology-aware workload placement strategy to schedule deep learning jobs on multi-GPU systems. The placement strategy is evaluated with a prototype on a Power8 machine with Tesla P100 cards, showing speedups of up to ≈1.30x compared to state-of-the-art strategies; the proposed algorithm achieves this result by allocating GPUs that satisfy workload requirements while preventing interference. Additionally, a large-scale simulation shows that the proposed strategy provides higher resource utilization and performance in cloud systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {17},
numpages = {12},
keywords = {GPU, multi-GPU, performance analysis, placement, resource contention, scheduling, workload interference and deep learning},
location = {Denver, Colorado},
series = {SC '17}
}

@inproceedings{10.5555/3091125.3091415,
author = {Rocha, Vladimir and Brand\~{a}o, Anarosa Alves Franco},
title = {MATe: Multiagent Architecture for Taming e-Devices},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In recent years, an explosive growth has been observed in the use of wireless devices, mainly due to the decrease in cost, size, and energy consumption. Researches in the Internet of Things have focused on how to continuously monitor these devices in different scenarios, such as vehicle, weather, and biodiversity tracking, considering both scalability and efficiency while searching and updating their information. For this, current alternatives use a combination of a widely recognized method, called data aggregation, and a widely adopted distributed structure, called Distributed Hash Table, which minimize the number of transmissions and save energy. However, scalability is still a key challenge when the group comprises a large number of devices. In this paper, we propose a scalable architecture that distributes the data aggregation responsibility to the devices of group frontier, and creates agents to manage groups and the interaction among them. Experimental results showed the viability of adopting this architecture if compared to the most widely used approaches.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1716–1718},
numpages = {3},
keywords = {DHT, IoT, data aggregation, multiagent system},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/3022227.3022304,
author = {Murakami, Masaya and Leibnitz, Kenji and Kominami, Daichi and Shimokawa, Tetsuya and Murata, Masayuki},
title = {Constructing virtual IoT network topologies with a brain-inspired connectivity model},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022304},
doi = {10.1145/3022227.3022304},
abstract = {Wireless sensor networks will be one of the fundamental technologies for realizing the future Internet of Things (IoT) environment. In IoT, the number of connected devices is expected to increase drastically and there will be a wide variety of requirements for application services, which will lead to frequent modifications or construction/destruction of topologies. In such situations, it is essential to know how power-saving, low-latency, and highly efficient IoT network topologies can be constructed. In this paper, we take inspiration from the brain's network of interconnecting neurons is known for its efficient properties. We propose a virtual IoT network construction method based on the Exponential Distance Rule (EDR) model that describes the connection structure of the areas in the cerebral cortex. Since the original EDR model deals with large-scale networks with an enormous number of neurons and generates links between nodes considering physical distance constraints, the virtual IoT network constructed by the proposed method is able to achieve high scalability, low latency, and high communication efficiency at a relatively low cost.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {78},
numpages = {8},
keywords = {brain network, exponential distance rule (EDR), internet of things (IoT), virtual network, wireless sensor networks (WSN)},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@inproceedings{10.1145/3371425.3371447,
author = {Wang, Kai and Yan, Yingjian and Zhu, Chunsheng},
title = {Exploiting wavelet transform and support vector machine algorithm to perform side channel attacks on advanced encryption standard (AES)},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371447},
doi = {10.1145/3371425.3371447},
abstract = {As the scale of the Internet of Things continues to expand, cryptographic chips have become one of the key elements to protect sensitive data. Although the cryptographic algorithms used in the chips are mathematically safe, they can still be cracked through side channel attacks (SCA) based on the leaked power information. However, a successful attack is time consuming and large data is needed to be processed. In this paper, a novel SCA method based on wavelet transform and machine learning algorithms was proposed to improve the attack efficiency. In this method, the collected power information of the Advanced Encryption Standard (AES) algorithm was preprocessed by wavelet transform, data normalization and principal component analysis to extract the data feature and remove the unrelated data. Then, the preprocessed data were adopted to train the support vector machine, and the unknown key could be recovered by the trained model finally. Compared with the existing support vector machine (SVM) method, this proposed technique could improve the key recovery capability by 20%.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {69},
numpages = {7},
keywords = {advanced encryption standard (AES), side channel attack, support vector machine, wavelet transform},
location = {Sanya, China},
series = {AIIPCC '19}
}

@inproceedings{10.1145/2988287.2989163,
author = {Bor, Martin C. and Roedig, Utz and Voigt, Thiemo and Alonso, Juan M.},
title = {Do LoRa Low-Power Wide-Area Networks Scale?},
year = {2016},
isbn = {9781450345026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988287.2989163},
doi = {10.1145/2988287.2989163},
abstract = {New Internet of Things (IoT) technologies such as Long Range (LoRa) are emerging which enable power efficient wireless communication over very long distances. Devices typically communicate directly to a sink node which removes the need of constructing and maintaining a complex multi-hop network. Given the fact that a wide area is covered and that all devices communicate directly to a few sink nodes a large number of nodes have to share the communication medium. LoRa provides for this reason a range of communication options (centre frequency, spreading factor, bandwidth, coding rates) from which a transmitter can choose. Many combination settings are orthogonal and provide simultaneous collision free communications. Nevertheless, there is a limit regarding the number of transmitters a LoRa system can support. In this paper we investigate the capacity limits of LoRa networks. Using experiments we develop models describing LoRa communication behaviour. We use these models to parameterise a LoRa simulation to study scalability. Our experiments show that a typical smart city deployment can support 120 nodes per 3.8 ha, which is not sufficient for future IoT deployments. LoRa networks can scale quite well, however, if they use dynamic communication parameter selection and/or multiple sinks.},
booktitle = {Proceedings of the 19th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {59–67},
numpages = {9},
keywords = {lora, low-power wide-area network, scalability analysis},
location = {Malta, Malta},
series = {MSWiM '16}
}

@inproceedings{10.1145/3054977.3054984,
author = {de Leon Barido, D. Ponce and Suffian, S. and Rosa, J. and Brewer, E. and Kammen, D. M.},
title = {Enabling Micro-level Demand-Side Grid Flexiblity in Resource Constrained Environments},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3054984},
doi = {10.1145/3054977.3054984},
abstract = {The increased penetration of uncertain and variable renewable energy presents various resource and operational electric grid challenges1. Micro-level (household and small commercial) demand-side grid flexibility could be a cost-effective strategy to integrate high penetrations of wind and solar energy, but literature and field deployments exploring the necessary information and communication technologies (ICTs) are scant. This paper presents an exploratory framework for enabling information driven grid flexibility through the Internet of Things (IoT), and a proof-of-concept wireless sensor gateway (FlexBox) to collect the necessary parameters for adequately monitoring and actuating the micro-level demand-side. In the summer of 2015, thirty sensor gateways were deployed in the city of Managua (Nicaragua) to develop a baseline for a near future small-scale demand response pilot implementation. FlexBox field data has begun shedding light on relationships between ambient temperature and load energy consumption, load and building envelope energy efficiency challenges, latency communication network challenges, and opportunities to engage existing demand-side user behavioral patterns. Information driven grid flexibility strategies present great opportunity to develop new technologies, system architectures, and implementation approaches that can easily scale across regions, incomes, and levels of development.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {233–245},
numpages = {13},
keywords = {Internet of Things (IoT), electric grid flexibility, energy management, energy reporting, network architecture, wireless sensor networks},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.1145/3019612.3019689,
author = {Din, Sadia and Ahmad, Awais and Paul, Anand},
title = {Human enabled green IoT in 5G networks},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019689},
doi = {10.1145/3019612.3019689},
abstract = {Internet of Things (IoT) plays a major role in connecting the physical world with the cyber world through new services and seamless interconnection between heterogeneous devices. Such heterogeneous devices tend to generate a massive volume of Big Data. However, exploiting green schemes for IoT is still a challenge since IoT attains a large scale and becomes more multifaceted, the current trends of analyzing Big Data are not directly applicable to it. Similarly, achieving green IoT through the use of 5G also poses new challenges when it comes to transferring huge volume of data in an efficient way. To address the challenges above, this paper presents a scheme for human- enabled green IoT in 5G network. Green IoT is achieved by grouping mobile nodes in a cluster. Also, a mobility management model is designed that helps in triggering efficient handover and selecting optimal networks based on multi-criteria decision modeling. Afterward, we design a network architecture that integrates green IoT with 5G network. Moreover, the 5G network architecture is supported by proposed protocol stack, which maps Internet Protocol (IP), Medium Access Protocol (MAC), and Location identifiers (LOC). The proposed scheme is also implemented using C programming language to validate mobility model in 5G, regarding cost, energy, and Quality of Service.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {208–213},
numpages = {6},
keywords = {5G, big data, clustering mechanism, green internet of things, network architecture},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@article{10.1145/3186592,
author = {Mahmud, Redowan and Ramamohanarao, Kotagiri and Buyya, Rajkumar},
title = {Latency-Aware Application Module Management for Fog Computing Environments},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3186592},
doi = {10.1145/3186592},
abstract = {The fog computing paradigm has drawn significant research interest as it focuses on bringing cloud-based services closer to Internet of Things (IoT) users in an efficient and timely manner. Most of the physical devices in the fog computing environment, commonly named fog nodes, are geographically distributed, resource constrained, and heterogeneous. To fully leverage the capabilities of the fog nodes, large-scale applications that are decomposed into interdependent Application Modules can be deployed in an orderly way over the nodes based on their latency sensitivity. In this article, we propose a latency-aware Application Module management policy for the fog environment that meets the diverse service delivery latency and amount of data signals to be processed in per unit of time for different applications. The policy aims to ensure applications’ Quality of Service (QoS) in satisfying service delivery deadlines and to optimize resource usage in the fog environment. We model and evaluate our proposed policy in an iFogSim-simulated fog environment. Results of the simulation studies demonstrate significant improvement in performance over alternative latency-aware strategies.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {9},
numpages = {21},
keywords = {Internet of things, application QoS, application management, application placement, fog computing, latency awareness, resource optimization}
}

@inproceedings{10.1145/3018896.3025168,
author = {Cloete, A. H. and Booysen, M. J. and Sandell, R. C. and van der Merwe, A. B.},
title = {Smart electric water heaters: a system architecture proposal for scalable IoT},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3025168},
doi = {10.1145/3018896.3025168},
abstract = {Energy and water are scarce resources in South Africa, and the management thereof is receiving increased attention. One rare place where energy and water coincide is in water heaters, which consume over 30% of household energy in South Africa and collectively pass over 500 billion litres of water annually. These resource-hungry devices are not well managed and tend to be physically inaccessible and difficult to understand for users. The advent of wireless and cloud computing technology brought expectations of simple management of a multitude of devices and the so-called Internet of Things. A few solutions have been proposed for remote water heater management, but these are not scalable, and have not been benchmarked for scalability above tens of units. This paper proposes an end-to-end architecture to monitor and control EWHs based on MQTT, and specific emphasis is placed on the technologies suitable for large-scale cross-domain interoperability. The performance of the system with 10,000 connections and the results of a pilot deployment are presented.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {119},
numpages = {7},
keywords = {electric water heaters, internet of things, smart cities},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/2695664.2695999,
author = {Kambona, Kennedy and Boix, Elisa Gonzalez and De Meuter, Wolfgang},
title = {Serena: scalable middleware for real-time web applications},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695999},
doi = {10.1145/2695664.2695999},
abstract = {With the advent of the Internet of Things, an increasing number of devices are expected to contribute to larger software systems hosted on the web. The client devices send massive amounts of data to servers that need to send feedback to clients in a real-time fashion. The supporting technologies on these servers have little or no capacity to handle processing of continuous data with dynamic constraints whilst at the same time responding to the effects of the data in real time. In this paper we present Serena, a middleware for real-time web applications. Serena utilizes a scoped, rule-based approach to ease the dynamic definition of requirements or constraints and to support the scalable processing of real-time data, giving instantaneous feedback. Serena also abstracts the underlying infrastructure needed to support real-time communication. We evaluate our approach by implementing a non-trivial web-based application with dynamic constraints and real-time responses to clients, comparing it with a similar implementation using mainstream technology.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {802–805},
numpages = {4},
keywords = {middleware, reactive, real-time, rete, web applications},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3109761.3109783,
author = {Bacciu, Davide and Chessa, Stefano and Gallicchio, Claudio and Micheli, Alessio},
title = {On the need of machine learning as a service for the internet of things},
year = {2017},
isbn = {9781450352437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109761.3109783},
doi = {10.1145/3109761.3109783},
abstract = {In recent years we are witnessing a rapid increase in the diffusion of the Internet of Things (IoT) technology, with a large scale adoption of interconnected heterogeneous devices that are pervasively collecting information through the interaction with humans in their environment. The adoption of Machine Learning (ML) methodologies can play a fundamental role, allowing smarter IoT applications to continuously adapt to evolving environmental conditions and user's needs. In this context, the time is now ripe for a decisive step forward in the direction of a systematic integration of ML functionalities within the IoT platform.In this paper, we outline the principles that should guide the realization of a ML service for the IoT, proposing a conceptual architecture of such a learning service, integrated within the IoT reference model. Our proposal leverages on the experience of recent successful European initiatives that led to the realization of intelligent sensor networks built on the synergy between resource efficient ML models for temporal data processing and wireless sensor networks. The relevant impact of ML in applicative domains of interest for the IoT is also enucleated through a brief summary of recent results.},
booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
articleno = {22},
numpages = {8},
keywords = {adaptive IoT applications, distributed learning service, intelligent sensor networks, internet of things, machine learning service},
location = {Liverpool, United Kingdom},
series = {IML '17}
}

@inproceedings{10.1145/3132479.3132490,
author = {Wang, Jianyu and Pan, Jianli and Esposito, Flavio},
title = {Elastic urban video surveillance system using edge computing},
year = {2017},
isbn = {9781450355285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132479.3132490},
doi = {10.1145/3132479.3132490},
abstract = {During the past decade, the concepts and applications of Internet of Things (IoT) are pervasively propagated to the academia and industries. The widely distributed IoT devices contribute to building an effective smart urban surveillance system, which manages the regular operations and handles emergencies. The real time monitoring uploads massive amounts of data to the backbone network and requires prompt feedbacks. The recent rapid development of "Edge Computing" (also called "Fog Computing" or Mobile Edge Computing in different literature) aims at pushing the computation and storage resources from the remote data center to the edge of network for reducing the burden of backbone and the computing latency In this paper, we design a three-tier edge computing system architecture to elastically adjust computing capacity and dynamically route data to proper edge servers for the real-time surveillance applications. A system prototype integrating Network Functions Virtualization (NFV) and Software-Defined Networking (SDN) is implemented in an OpenStack based virtualization environment. Moreover, we introduce schemes of resource reallocation and workload balance in urgent situations. Experimental results of the prototype show the great potentials of using edge computing for future large-scale and distributed smart urban surveillance applications.},
booktitle = {Proceedings of the Workshop on Smart Internet of Things},
articleno = {7},
numpages = {6},
keywords = {edge computing, internet of things, real-time surveillance, smart city},
location = {San Jose, California},
series = {SmartIoT '17}
}

@inproceedings{10.1145/3047273.3047274,
author = {Shrivastava, Swapnil and Pal, Supriya N.},
title = {A Big Data Analytics Framework for Enterprise Service Ecosystems in an e-Governance Scenario},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047274},
doi = {10.1145/3047273.3047274},
abstract = {In the recent times we have been seeing a fundamental shift from Enterprise Applications towards large scale Enterprise Service Ecosystems. Enterprise Service Ecosystems are developed by modularizing and bundling of individual business rules and functions in the form of services. These services are loosely coupled, distributed and heterogeneous components which orchestrate amongst themselves in a seamless manner. Ecosystem components record the events that are related to the activities performed by them. These components could span across Data Centre, Cloud Infrastructure and Internet of Things. Aadhaar Authentication Ecosystem and e-Governance Service Exchange are examples of Enterprise Service Ecosystems which recently emerged in national e-Governance scenario. A Big Data Analytics Framework for comprehensive mining and analyzing event data of Enterprise Service Ecosystems is proposed in this paper. The offered framework facilitates interesting real time analytics (e.g. Process Conformance Checking, Bottleneck Detection) as well as performing offline analytics (e.g. Process Discovery). The application of the proposed framework for real time analytics is explained using Aadhaar (Unique Identity) Authentication Ecosystem case study.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {5–11},
numpages = {7},
keywords = {Aadhaar Authentication Ecosystem, Big Data Analytics, Complex Event Processing, Enterprise Service Ecosystem, Event Data, Graph Analytics, Process Mining, e-Governance},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3281022.3281025,
author = {Baldassarre, Maria Teresa and Caivano, Danilo and Serrano, Diego and Stroulia, Eleni},
title = {“Smart Traffic”: an IoT traffic monitoring system based on open source technologies on the cloud},
year = {2018},
isbn = {9781450360548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281022.3281025},
doi = {10.1145/3281022.3281025},
abstract = {The constantly increasing importance of cloud computing and Internet of Things (IoT) has led to solutions able to integrate heterogenous and diverse systems as well manage big data. This is especially true in Smart City environments with respect to traffic monitoring. Furthermore, cloud computing, and the various technologies around it are quickly becoming a must in the education domain. Unlike traditional education, it promotes the use of computing infrastructures anywhere and at any time, without restrictions. In this paper, we present our experience in using cloud computing technologies for a computing science course on Software Quality, with fourth-year undergraduate students at the University of Alberta, Canada. In particular, the paper illustrates how students have been actively involved in carrying out a real project and coordinated their project work among the class groups thanks to cloud technologies. Project work consisted in building a scalable system for an urban IoT environment of traffic monitoring and routing based on open source technologies and publicly available data from the city of Edmonton, Alberta in Canada.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering},
pages = {13–18},
numpages = {6},
keywords = {Traffic-based routing, cloud computing, internet of things, open data, open source, real time systems},
location = {Lake Buena Vista, FL, USA},
series = {EnSEmble 2018}
}

@inproceedings{10.1145/2899015.2899029,
author = {Lee, Robert P. and Markantonakis, Konstantinos and Akram, Raja Naeem},
title = {Binding Hardware and Software to Prevent Firmware Modification and Device Counterfeiting},
year = {2016},
isbn = {9781450342889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2899015.2899029},
doi = {10.1145/2899015.2899029},
abstract = {Embedded systems are small scale computing devices that are increasingly located in more of the items we use and own. The number of embedded systems in the world is increasing dramatically as the "internet of things" concept becomes more prevalent in the market. The value of the market for embedded systems is predicted to increase to being worth trillions of dollars by 2020. With great value in the embedded system market, there is a need for preventing unauthorised firmware tampering or product counterfeiting. Here is presented a technique for binding software to hardware instances that uses hardware intrinsic security properties of the devices being protected. The proposed technique provides assurance to manufacturers that only they can perform their hardware and software binding and create their products. Also presented is an FPGA implementation of the described scheme that binds the hardware and software together with only a 6.7% increase in execution time. Thus, making it difficult for an attacker to either counterfeit the device or extract the (software) Intellectual Property.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Cyber-Physical System Security},
pages = {70–81},
numpages = {12},
keywords = {binding, counterfeiting, firmware, hardware, intrinsic, modification, puf, security, software},
location = {Xi'an, China},
series = {CPSS '16}
}

@inproceedings{10.1145/3139937.3139938,
author = {Loi, Franco and Sivanathan, Arunan and Gharakheili, Hassan Habibi and Radford, Adam and Sivaraman, Vijay},
title = {Systematically Evaluating Security and Privacy for Consumer IoT Devices},
year = {2017},
isbn = {9781450353960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139937.3139938},
doi = {10.1145/3139937.3139938},
abstract = {Internet-of-Things (IoT) devices such as smart bulbs, cameras, and health monitors are being enthusiastically adopted by consumers, with numbers projected to rise to the billions. However, such devices are also easily attacked, or used for launching attacks, at large scale and at increasing frequency. This paper is an attempt at developing a systematic method to identify the security and privacy shortcomings of various IoT devices, with a view towards alerting consumers, manufacturers, and regulators to the associated risks. We categorize the threats along four dimensions: confidentiality of private data sent to/from the IoT device; integrity of data from the IoT device to internal/external entities; access control of the IoT device; and reflective attacks that can be launched from an IoT device. We develop scripts to automate the security testing along each of these dimensions, subject twenty market-ready consumer IoT devices to our test suite, and reveal findings that give a fairly comprehensive picture of the security/privacy posture of these devices. Our methodology can be used as a basis for a star-based security ratings system for IoT devices being brought to market.},
booktitle = {Proceedings of the 2017 Workshop on Internet of Things Security and Privacy},
pages = {1–6},
numpages = {6},
keywords = {iot, privacy, security},
location = {Dallas, Texas, USA},
series = {IoTS&amp;P '17}
}

@article{10.1145/3204412,
author = {Wang, Ping and Ma, Meng and Chu, Chao-Hsien},
title = {Long-Term Event Processing over Data Streams in Cyber-Physical Systems},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3204412},
doi = {10.1145/3204412},
abstract = {Event processing is a crucial cornerstone supporting the revolution of Internet of Things (IoT) and Cyber-Physical Systems (CPS) by integrating physical-layer networking and providing intelligent computation and real-time control abilities. In various IoT and CPS application scenarios, the event processing systems are required to detect complex event patterns using large time window, namely long-term events. The detection of long-term event usually leads to a large number of redundant runtime instances and calculations that significantly deteriorates the system efficiency. In this article, we propose an efficient long-term event processing model, named Long-Term Complex Event Processing (LTCEP). It leverages the semantic constraints calculus to split long-term event into sub-models. We establish a long-term query and intermediate result buffering mechanism to optimize the real-time response ability and throughput performance. Experimental results show that LTCEP can effectively reduce more than 50% redundant runtime states, which provides over 60% faster response performance and around 30% higher system throughput comparing to other selected benchmarks. The results also imply that LTCEP model has better stability and scalability in large-scale event processing applications.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {jun},
articleno = {14},
numpages = {23},
keywords = {Cyber-Physical Systems, data stream, event processing, internet of things, long-term}
}

@inproceedings{10.1145/3349341.3349486,
author = {Chen, Chunli and Liu, Huifang and Wang, Zhenhua},
title = {Analysis and Design of Urban Traffic Congestion in Urban Intelligent Transportation System Based on Big Data and Internet of Things},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349486},
doi = {10.1145/3349341.3349486},
abstract = {With the rapid development of big cities, the pressure of traffic congestion is increasing, and intelligent transportation is the fundamental way to solve traffic problems. Intelligent transportation system is a transportation management system that effectively integrates information technology, data communication technology, sensor technology, control technology and data processing technology. Large-scale traffic data management, integration and mining are key technologies. This paper briefly introduces the current situation of intelligent transportation in various countries, analyses the causes of urban traffic congestion in detail, and discusses the concept, composition and relationship between various subsystems of urban intelligent transportation system. Using the Internet of Things technology to collect traffic big data, based on the Big Data analysis technology to construct a traffic prediction model for traffic situation prediction, combined with real-time monitoring and analysis, the combination of remote traffic indicator control and optimization strategy control for traffic collaborative management. The causes of traffic congestion are analyzed in detail, specific solutions are proposed, and suggestions for transportation infrastructure planning are proposed.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {659–665},
numpages = {7},
keywords = {Big Data, Intelligent Transportation System, Internet of Things, Traffic Congestion},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3176258.3176337,
author = {Ammar, Mahmoud and Daniels, Wilfried and Crispo, Bruno and Hughes, Danny},
title = {SPEED: Secure Provable Erasure for Class-1 IoT Devices},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176337},
doi = {10.1145/3176258.3176337},
abstract = {The Internet of Things (IoT) consists of embedded devices that sense and manage our environment in a growing range of applications. Large-scale IoT systems such as smart cities require significant investment in both equipment and personnel. To maximize return on investment, IoT platforms should support multiple third-party applications and adaptation of infrastructure over time. Realizing the vision of shared IoT platforms demands strong security guarantees. That is particularly challenging considering the limited capability and resource constraints of many IoT devices.In this paper, we present SPEED, an approach to secure erasure with verifiability in IoT. Secure erasure is a fundamental property when it comes to share an IoT platform with other users which guarantees the cleanness of a device's memory at the beginning of the application deployment as well as at the time of releasing the underlying IoT device. SPEED relies on two security primitives: memory isolation and distance bounding protocol. We evaluate the performance of SPEED by implementing it on a simple bare-metal IoT device belongs to Class-1. Our evaluation results show a limited overhead in terms of memory footprint, time, and energy consumption.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {111–118},
numpages = {8},
keywords = {distance bounding, iot security, memory isolation, secure erasure},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1145/3313150.3313228,
author = {Tange, Koen and De Donno, Michele and Fafoutis, Xenofon and Dragoni, Nicola},
title = {Towards a systematic survey of industrial IoT security requirements: research method and quantitative analysis},
year = {2019},
isbn = {9781450366984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313150.3313228},
doi = {10.1145/3313150.3313228},
abstract = {Industry 4.0 and, in particular, Industrial Internet of Things (IIoT) represent two of the major automation and data exchange trends of the 21st century, driving a steady increase in the number of smart embedded devices used by industrial applications. However, IoT devices suffer from numerous security flaws, resulting in a number of large scale cyber-attacks. In this light, Fog computing, a relatively new paradigm born from the necessity of bridging the gap between Cloud computing and IoT, can be used as a security solution for the IIoT. To achieve this, the first step is to clearly identify the security requirements of the IIoT that can be subsequently used to design security solutions based on Fog computing. With this in mind, our paper represents a preliminary work towards a systematic literature review of IIoT security requirements. We focus on two key steps of the review: (1) the research method that will be used in the systematic work and (2) a quantitative analysis of the results produced by the study selection process. This lays the necessary foundations to enable the use of Fog computing as a security solution for the IIoT.},
booktitle = {Proceedings of the Workshop on Fog Computing and the IoT},
pages = {56–63},
numpages = {8},
keywords = {IIoT, fog computing, industrial internet of things, industry 4.0, security, systematic literature review},
location = {Montreal, Quebec, Canada},
series = {IoT-Fog '19}
}

@inproceedings{10.1145/2675743.2771880,
author = {Khare, Shweta and An, Kyoungho and Gokhale, Aniruddha and Tambe, Sumant and Meena, Ashish},
title = {Reactive stream processing for data-centric publish/subscribe},
year = {2015},
isbn = {9781450332866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675743.2771880},
doi = {10.1145/2675743.2771880},
abstract = {The Internet of Things (IoT) paradigm has given rise to a new class of applications wherein complex data analytics must be performed in real-time on large volumes of fast-moving and heterogeneous sensor-generated data. Such data streams are often unbounded and must be processed in a distributed and parallel manner to ensure timely processing and delivery to interested subscribers. Dataflow architectures based on event-based design have served well in such applications because events support asynchrony, loose coupling, and helps build resilient, responsive and scalable applications. However, a unified programming model for event processing and distribution that can naturally compose the processing stages in a dataflow while exploiting the inherent parallelism available in the environment and computation is still lacking. To that end, we investigate the benefits of blending Reactive Programming with data distribution frameworks for building distributed, reactive, and high-performance stream-processing applications. Specifically, we present insights from our study integrating and evaluating Microsoft .NET Reactive Extensions (Rx) with OMG Data Distribution Service (DDS), which is a standards-based publish/subscribe middleware suitable for demanding industrial IoT applications. Several key insights from both qualitative and quantitative evaluation of our approach are presented.},
booktitle = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems},
pages = {234–245},
numpages = {12},
keywords = {data distribution service (DDS), publish/subscribe, reactive extensions (Rx), reactive programming, stream processing},
location = {Oslo, Norway},
series = {DEBS '15}
}

@article{10.1145/3325097,
author = {Adhikari, Mainak and Amgoth, Tarachand and Srirama, Satish Narayana},
title = {A Survey on Scheduling Strategies for Workflows in Cloud Environment and Emerging Trends},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3325097},
doi = {10.1145/3325097},
abstract = {Workflow scheduling is one of the challenging issues in emerging trends of the distributed environment that focuses on satisfying various quality of service (QoS) constraints. The cloud receives the applications as a form of a workflow, consisting of a set of interdependent tasks, to solve the large-scale scientific or enterprise problems. Workflow scheduling in the cloud environment has been studied extensively over the years, and this article provides a comprehensive review of the approaches. This article analyses the characteristics of various workflow scheduling techniques and classifies them based on their objectives and execution model. In addition, the recent technological developments and paradigms such as serverless computing and Fog computing are creating new requirements/opportunities for workflow scheduling in a distributed environment. The serverless infrastructures are mainly designed for processing background tasks such as Internet-of-Things (IoT), web applications, or event-driven applications. To address the ever-increasing demands of resources and to overcome the drawbacks of the cloud-centric IoT, the Fog computing paradigm has been developed. This article also discusses workflow scheduling in the context of these emerging trends of cloud computing.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {68},
numpages = {36},
keywords = {Cloud computing, Fog computing, QoS constraint, scientific problems, serverless computing, workflow scheduling}
}

@inproceedings{10.1145/3035918.3058731,
author = {Mansour, Essam and Abdelaziz, Ibrahim and Ouzzani, Mourad and Aboulnaga, Ashraf and Kalnis, Panos},
title = {A Demonstration of Lusail: Querying Linked Data at Scale},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3058731},
doi = {10.1145/3035918.3058731},
abstract = {There has been a proliferation of datasets available as interlinked RDF data accessible through SPARQL endpoints. This has led to the emergence of various applications in life science, distributed social networks, and Internet of Things that need to integrate data from multiple endpoints.We will demonstrate Lusail; a system that supports the need of emerging applications to access tens to hundreds of geo-distributed datasets. Lusail is a geo-distributed graph engine for querying linked RDF data. Lusail delivers out- standing performance using (i) a novel locality-aware query decomposition technique that minimizes the intermediate data to be accessed by the subqueries, and (ii) selectivity-awareness and parallel query execution to reduce network latency and to increase parallelism. During the demo, the audience will be able to query actually deployed RDF end- points as well as large synthetic and real benchmarks that we have deployed in the public cloud. The demo will also show that Lusail outperforms state-of-the-art systems by orders of magnitude in terms of scalability and response time.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1603–1606},
numpages = {4},
keywords = {decentralized rdf graphs, federated sparql queries, linked data, parallel query processing, query processing, rdf data, sparql},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3054977.3054995,
author = {Garcia-Luna-Aceves, J. J.},
title = {ADN: An Information-Centric Networking Architecture for the Internet of Things},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3054995},
doi = {10.1145/3054977.3054995},
abstract = {Forwarding data by name has been assumed to be a necessary aspect of an information-centric redesign of the current Internet architecture that makes content access, dissemination, and storage more efficient. The Named Data Networking (NDN) and Content-Centric Networking (CCNx) architectures are the leading examples of such an approach. However, forwarding data by name incurs storage and communication complexities that are orders of magnitude larger than solutions based on forwarding data using addresses. Furthermore, the specific algorithms used in NDN and CCNx have been shown to have a number of limitations. The Addressable Data Networking (ADN) architecture is introduced as an alternative to NDN and CCNx. ADN is particularly attractive for large-scale deployments of the Internet of Things (IoT), because it requires far less storage and processing in relaying nodes than NDN. ADN allows things and data to be denoted by names, just like NDN and CCNx do. However, instead of replacing the waist of the Internet with named-data forwarding, ADN uses an address-based forwarding plane and introduces an information plane that seamlessly maps names to addresses without the involvement of end-user applications. Simulation results illustrate the order of magnitude savings in complexity that can be attained with ADN compared to NDN.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {27–36},
numpages = {10},
keywords = {Content-centric networking, IoT, forwarding},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.1145/3019612.3019700,
author = {Taherkordi, Amir and Eliassen, Frank and Horn, Geir},
title = {From IoT big data to IoT big services},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019700},
doi = {10.1145/3019612.3019700},
abstract = {The large-scale deployments of Internet of Things (IoT) systems have introduced several new challenges in terms of processing their data. The massive amount of IoT-generated data requires design solutions to speed up data processing, scale up with the data volume and improve data adaptability and extensibility. Beyond existing techniques for IoT data collection, filtering, and analytics, innovative service computing technologies are required for provisioning data-centric and scalable IoT services. This paper presents a service-oriented design model and framework for realizing scalable and efficient acquisition, processing and integration of data-centric IoT services. In this approach, data-centric IoT services are organized in a service integrating tree structure, adhering to the architecture of many large-scale IoT systems, including recent fog-based IoT computing models. A service node in the tree is called a Big Service and acts as an integrator, collecting data from lower level Big Services, processing them, and delivering the result to higher level IoT Big Services. The service tree thereby encapsulates required data processing functions in a hierarchical manner in order to achieve scalable and real-time data collection and processing. We have implemented the IoT Big Services framework leveraging a popular cloud-based service and data platform called Firebase, and evaluated its performance in terms of real-time requirements.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {485–491},
numpages = {7},
keywords = {big data, big services, internet of things},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3131885.3131918,
author = {Dautov, Rustem and Distefano, Salvatore and Merlino, Giovani and Bruneo, Dario and Longo, Francesco and Puliafito, Antonio},
title = {Towards a Global Intelligent Surveillance System},
year = {2017},
isbn = {9781450354875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131885.3131918},
doi = {10.1145/3131885.3131918},
abstract = {Recent technological advances have led to the rapid development of Intelligent Surveillance Systems (ISSs), ubiquitously present in modern urban spaces are constantly generating streams of raw data. As most of the actual Internet traffic is nowadays constituted by visual data streams, often originated by ISSs, it is important to properly manage these avalanches of data so as to support sustainability of this technological trend, which will very likely saturate the current network bandwidth in few years. This paper aims to combine existing technologies and paradigms from the Internet of Things, Cloud, Edge Computing and Big Data into a common framework to enable a shared approach for ISSs at a wide geographical scale, thus envisioning a Global ISS. The proposed solution is based on the idea of pushing data processing tasks as close to data sources as possible, thus increasing security and performance levels, usually critical to surveillance systems. To demonstrate the feasibility and the effectiveness of the proposed approach, the paper presents a case study based on a distributed ISS scenario in a crowded area, implemented on clustered edge devices able to offload tasks in a 'horizontal' manner.},
booktitle = {Proceedings of the 11th International Conference on Distributed Smart Cameras},
pages = {119–124},
numpages = {6},
keywords = {Cloud, Edge, Intelligent Surveillance Systems, IoT, Stream Processing},
location = {Stanford, CA, USA},
series = {ICDSC 2017}
}

@inproceedings{10.1145/3277593.3277600,
author = {Le-Tuan, Anh and Hayes, Conor and Wylot, Marcin and Le-Phuoc, Danh},
title = {RDF4Led: an RDF engine for lightweight edge devices},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277600},
doi = {10.1145/3277593.3277600},
abstract = {Semantic interoperability for the Internet of Things(IoT) is being enabled by standards and technologies from the Semantic Web. As recent research suggests a move towards decentralised IoT architectures, our focus is on how to enable scalable and robust RDF engines that can be embedded throughout the architecture, in particular at edge nodes. RDF processing at edge enables the creation of semantic integration gateways for locally connected low-level devices. We introduce a lightweight RDF engine, which comprises of RDF storage and SPARQL processor, for the lightweight edge devices, called RDF4Led. RDF4Led follows the RISCstyle (Reduce Instruction Set Computer) design philosophy. The design comprises a flash-aware storage structure, an indexing scheme and a low-memory-footprint join algorithm which improves scalability as well as robustness over competing solutions. With a significantly smaller memory footprint, we show that RDF4Led can handle 2 to 5 times more data than RDF engines such as Jena TDB and Virtuoso. On three types of ARM boards, RDF4Led requires 10--30% memory of its competitors to operate up to 30 million triples dataset; it can perform faster updates and can scale better than Jena TDB and Virtuoso. Furthermore, we demonstrate considerably faster query operations than Jena TDB.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {2},
numpages = {8},
keywords = {RDF engine, edge device},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@article{10.1145/3122785,
author = {Lu, Sixing and Lysecky, Roman},
title = {Time and Sequence Integrated Runtime Anomaly Detection for Embedded Systems},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3122785},
doi = {10.1145/3122785},
abstract = {Network-connected embedded systems grow on a large scale as a critical part of Internet of Things, and these systems are under the risk of increasing malware. Anomaly-based detection methods can detect malware in embedded systems effectively and provide the advantage of detecting zero-day exploits relative to signature-based detection methods, but existing approaches incur significant performance overheads and are susceptible to mimicry attacks. In this article, we present a formal runtime security model that defines the normal system behavior including execution sequence and execution timing. The anomaly detection method in this article utilizes on-chip hardware to non-intrusively monitor system execution through trace port of the processor and detect malicious activity at runtime. We further analyze the properties of the timing distribution for control flow events, and select subset of monitoring targets by three selection metrics to meet hardware constraint. The designed detection method is evaluated by a network-connected pacemaker benchmark prototyped in FPGA and simulated in SystemC, with several mimicry attacks implemented at different levels. The resulting detection rate and false positive rate considering constraints on the number of monitored events supported in the on-chip hardware demonstrate good performance of our approach.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {dec},
articleno = {38},
numpages = {27},
keywords = {Embedded system security, anomaly detection, medical device security, software security, timing based detection}
}

@inproceedings{10.1145/3055186.3055192,
author = {Guarnizo, Juan David and Tambe, Amit and Bhunia, Suman Sankar and Ochoa, Martin and Tippenhauer, Nils Ole and Shabtai, Asaf and Elovici, Yuval},
title = {SIPHON: Towards Scalable High-Interaction Physical Honeypots},
year = {2017},
isbn = {9781450349567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055186.3055192},
doi = {10.1145/3055186.3055192},
abstract = {In recent years, the emerging Internet-of-Things (IoT) has led to rising concerns about the security of networked embedded devices. In this work, we propose the SIPHON architecture---a Scalable high-Interaction Honeypot platform for IoT devices. Our architecture leverages IoT devices that are physically at one location and are connected to the Internet through so-called emph{wormholes} distributed around the world. The resulting architecture allows exposing few physical devices over a large number of geographically distributed IP addresses. We demonstrate the proposed architecture in a large scale experiment with 39 wormhole instances in 16 cities in 9 countries. Based on this setup, five physical IP cameras, one NVR and one IP printer are presented as 85 real IoT devices on the Internet, attracting a daily traffic of 700MB for a period of two months. A preliminary analysis of the collected traffic indicates that devices in some cities attracted significantly more traffic than others (ranging from 600 000 incoming TCP connections for the most popular destination to less than 50 000 for the least popular). We recorded over 400 brute-force login attempts to the web-interface of our devices using a total of 1826 distinct credentials, from which 11 attempts were successful. Moreover, we noted login attempts to Telnet and SSH ports some of which used credentials found in the recently disclosed Mirai malware.},
booktitle = {Proceedings of the 3rd ACM Workshop on Cyber-Physical System Security},
pages = {57–68},
numpages = {12},
keywords = {high-interaction honeypot, internet of things, low-interaction honeypot, scalability},
location = {Abu Dhabi, United Arab Emirates},
series = {CPSS '17}
}

@inproceedings{10.5555/3233397.3233491,
author = {Shang, Lei and Lin, Ching Yeh and Atif, Muhammad and Williams, Allan},
title = {Evaluation of high density GPUs as sustainable smart city infrastructure},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {Internet of things (IoT) is driving the big data revolution in smart cities. In order to make informed, accurate and real-time decisions, smart cities have to invest in powerful computing infrastructure with the minimal total cost of ownership. Smart city infrastructure will need to process data from various scientific and engineering domains like weather variability, traffic management, disease control etc in real-time while keeping the operational costs to minimum. In this paper we build a case for using General Purpose GPUs (GPGPU) as an alternate to the traditional CPU based computing. Utilising the GPUs in development of smart city infrastructure is an attractive alternate as it provides an efficient computing capacity when compared with traditional CPU only solutions. However, we find that naive deployment of applications on high-density GPUs results in lower scalability and performance. We show that designing a NUMA and GPU affinity aware parallel execution model can lead to substantial speed-ups. Our results show that smart cities can save over 45% in infrastructure power and over 90% in data centre space if high-density GPU solutions are used.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {482–487},
numpages = {6},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@article{10.1145/3314397,
author = {Fomichev, Mikhail and Maass, Max and Almon, Lars and Molina, Alejandro and Hollick, Matthias},
title = {Perils of Zero-Interaction Security in the Internet of Things},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3314397},
doi = {10.1145/3314397},
abstract = {The Internet of Things (IoT) demands authentication systems which can provide both security and usability. Recent research utilizes the rich sensing capabilities of smart devices to build security schemes operating without human interaction, such as zero-interaction pairing (ZIP) and zero-interaction authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and reported promising results. However, those schemes were often evaluated under conditions which do not reflect realistic IoT scenarios. In addition, drawing any comparison among the existing schemes is impossible due to the lack of a common public dataset and unavailability of scheme implementations.In this paper, we address these challenges by conducting the first large-scale comparative study of ZIP and ZIA schemes, carried out under realistic conditions. We collect and release the most comprehensive dataset in the domain to date, containing over 4250 hours of audio recordings and 1 billion sensor readings from three different scenarios, and evaluate five state-of-the-art schemes based on these data. Our study reveals that the effectiveness of the existing proposals is highly dependent on the scenario they are used in. In particular, we show that these schemes are subject to error rates between 0.6% and 52.8%.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {10},
numpages = {38},
keywords = {Authentication, Context-based Security, Internet-of-Things, Secure Device Pairing}
}

@article{10.1145/3287066,
author = {Sasatani, Takuya and Yang, Chouchang Jack and Chabalko, Matthew J. and Kawahara, Yoshihiro and Sample, Alanson P.},
title = {Room-Wide Wireless Charging and Load-Modulation Communication via Quasistatic Cavity Resonance},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287066},
doi = {10.1145/3287066},
abstract = {The rise of the Internet of Things (IoT) has led to a significant increase in the number of connected devices that stream data in our homes, offices and industrial spaces. However, as the number of these devices increases, the costs of actively maintaining and replacing batteries becomes prohibitive at scale. Recent work on Quasistatic Cavity Resonance (QSCR), offers the possibility of seamless wireless power transfer (WPT) to receivers placed anywhere inside large indoor spaces. This work aims to solve two unexplored and critical missing pieces needed to realize this vision of ubiquitous WPT. First, we demonstrate a full end-to-end QSCR-based WPT system that is capable of simultaneously charging multiple custom designed nodes nearly anywhere in the 4.9 m x 4.9 m x 2.3 m test room. Second, this work utilizes the WPT mechanism as a communication channel, where nodes communicate with a centralized reader and to each other via load modulation. Through analysis and experiments, the proposed system shows that 10 receiver nodes can be safely and efficiently wirelessly charged and the end node to end node communication rate can achieve from 1 kbps without occurring any errors, up to 5 kbps with 6% BER while the end node to central unit can achieve 10 kbps without occurring any errors.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {dec},
articleno = {188},
numpages = {23},
keywords = {Wireless sensor networks, load-modulation communication, quasistatic cavity resonance, wireless power transfer}
}

@inproceedings{10.1145/2957265.2970370,
author = {Iakovakis, Dimitrios and Hadjileontiadis, Leontios},
title = {Standing hypotension prediction based on smartwatch heart rate variability data: a novel approach},
year = {2016},
isbn = {9781450344135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957265.2970370},
doi = {10.1145/2957265.2970370},
abstract = {The number of wearable and smart devices which are connecting every day in the Internet of Things (IoT) is continuously growing. We have a great opportunity though to improve the quality of life (QoL) standards by adding medical value to these devices. Especially, by exploiting IoT technology, we have the potential to create useful tools which utilize the sensors to provide biometric data. This novel study aims to use a smartwatch, independent from other hardware, to predict the Blood Pressure (BP) drop caused by postural changes. In cases that the drop is due to orthostatic hypotension (OH) can cause dizziness or even faint factors, which increase the risk of fall in the elderly but, as well as, in younger groups of people. A mathematical prediction model is proposed here which can reduce the risk of fall due to OH by sensing heart rate variability (data and drops in systolic BP after standing in a healthy group of 10 subjects. The experimental results justify the efficiency of the model, as it can perform correct prediction in 86.7% of the cases, and are encouraging enough for extending the proposed approach to pathological cases, such as patients with Parkinson's disease, involving large scale experiments.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {1109–1112},
numpages = {4},
keywords = {blood pressure drop, heart rate variability, regression, smartwatch},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1145/3344341.3368820,
author = {Salama, Maria and Elkhatib, Yehia and Blair, Gordon},
title = {IoTNetSim: A Modelling and Simulation Platform for End-to-End IoT Services and Networking},
year = {2019},
isbn = {9781450368940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344341.3368820},
doi = {10.1145/3344341.3368820},
abstract = {Internet-of-Things (IoT) systems are becoming increasingly complex, heterogeneous and pervasive, integrating a variety of physical devices and virtual services that are spread across architecture layers (cloud, fog, edge) using different connection types. As such, research and design of such systems have proven to be challenging. Despite the influx in IoT research and the significant benefits of simulation-based approaches in supporting research, there is a general lack of appropriate modelling and simulation platforms to create a detailed representation of end-to-end IoT services, i.e. from the underlying IoT nodes to the application layer in the cloud along with the underlying networking infrastructure. To aid researchers and practitioners in overcoming these challenges, we propose IoTNetSim, a novel self-contained extendable platform for modelling and simulation of end-to-end IoT services. The platform supports modelling heterogeneous IoT nodes (sensors, actuators, gateways, etc.) with their fine-grained details (mobility, energy profile, etc.), as well as different models of application logic and network connectivity. The proposed work is distinct from the current literature, being an all-in-one tool for end-to-end IoT services with a multi-layered architecture that allows modelling IoT systems with different structures. We experimentally validate and evaluate our IoTNetSim implementation using two very large-scale real-world cases from the natural environment and disaster monitoring IoT domains.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
pages = {251–261},
numpages = {11},
keywords = {cloud computing, edge computing, fog computing, internet of things, simulation},
location = {Auckland, New Zealand},
series = {UCC'19}
}

@inproceedings{10.1145/3102304.3102324,
author = {Boulakbech, Marwa and Messai, Nizar and Sam, Yacine and devogele, Thomas and Hammoudeh, Mohammad},
title = {IoT Mashups: From IoT Big Data to IoT Big Service},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102324},
doi = {10.1145/3102304.3102324},
abstract = {Internet of Things (IoT) addresses the challenge to provide a transparent access to a huge number of IoT resources that can be either physical devices or just data resources. Moreover, because of the large number of resource-constrained devices and the dynamic nature of IoT environments, integrating the resulted data becomes a non trivial task. We believe that the use of mashups, a way to compose new services from existing ones, can be a solution to the above challenge if each resource exposes its functionalities as a Web service. In the IoT environment, this will constitute a Web of Things where mashups development will take advantage of the connected physical world. The huge amounts of IoT-generated data from physical devices and data sources, called IoT Big Data, requires new design solutions to speed up data processing, scale up with the data volume and improve data adaptability. Besides existing techniques for IoT data collection, filtering, and analytics, we present in this article a mashup oriented model, called IoT Big Services, for provisioning data-centric IoT services in the context of IOT mashups. These IoT services are organized in tree structure where each node, called an IoT Big Service, acts as an integrator that collects data from lower level, processes them and delivers the results to higher level in the architecture.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {20},
keywords = {Big Data, Big Services, Internet of Things, Mashup application},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3210284.3210293,
author = {Rivetti, Nicolo and Zacheilas, Nikos and Gal, Avigdor and Kalogeraki, Vana},
title = {Probabilistic Management of Late Arrival of Events},
year = {2018},
isbn = {9781450357821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210284.3210293},
doi = {10.1145/3210284.3210293},
abstract = {In a networked world, events are transmitted from multiple distributed sources into CEP systems, where events are related to one another along multiple dimensions, e.g., temporal and spatial, to create complex events. The big data era brought with it an increase in the scale and frequency of event reporting. Internet of Things adds another layer of complexity with multiple, continuously changing event sources, not all of which are perfectly reliable, often suffering from late arrivals. In this work we propose a probabilistic model to deal with the problem of reduced reliability of event arrival time. We use statistical theories to fit the distributions of inter-generation at the source and network delays per event type. Equipped with these distributions we propose a predictive method for determining whether an event belonging to a window has yet to arrive. Given some user-defined tolerance levels (on quality and timeliness), we propose an algorithm for dynamically determining the amount of time a complex event time-window should remain open. Using a thorough empirical analysis, we compare the proposed algorithm against state-of-the-art mechanisms for delayed arrival of events and show the superiority of our proposed method.},
booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
pages = {52–63},
numpages = {12},
keywords = {Complex Event Processing, Late arrivals, Probabilistic Prediction, Sliding Window},
location = {Hamilton, New Zealand},
series = {DEBS '18}
}

@inproceedings{10.1145/2837060.2837067,
author = {Rathore, M. Mazhar and Ahmad, Awais and Paul, Anand},
title = {Big Data and Internet of Things: An Asset for Urban Planning},
year = {2015},
isbn = {9781450338462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837060.2837067},
doi = {10.1145/2837060.2837067},
abstract = {The growing city population demands the delivery of services and infrastructure. The use of Internet of Things (IoT) devices, such as sensors, actuators, and smartphones, etc., and the smart system is the valuable source in order to meet increasing demands. However, thousands of interconnecting IoT devices effects in producing an enormous volume of data, termed as Big Data. To integrate IoT services and processing Big Data in an efficient way to achieve smooth urban planning is a challenging task. In this paper, we propose an IoT-based urban planning system using Big Data Analytics. The system consists of various types of IoT-based smart system including smart home, vehicular networking, weather and water system, smart parking, and surveillance objects, etc. A four-tier architecture is proposed that includes 1) Bottom Tier-1: responsible for IoT data generation, and collections 2) Intermediate Tier-1: responsible for all type of communication between sensors, relays, base stations, Internet, etc. 3) Intermediate Tier 2: it is responsible for data management and processing using Hadoop framework, and 4) Top Tier: is responsible for application and usage of the data analysis and results generated. The proposed system is implemented in Hadoop ecosystem environment with MapReduce programming. The system produced results with higher throughput and low processing time, which proves the scalability and efficiency of the system.},
booktitle = {Proceedings of the 2015 International Conference on Big Data Applications and Services},
pages = {58–65},
numpages = {8},
keywords = {Big Data, IoT, Smart City, Urban Planning},
location = {Jeju Island, Republic of Korea},
series = {BigDAS '15}
}

@inproceedings{10.5555/3172795.3172821,
author = {Jim\'{e}nez, Miguel and Villegas, Norha M. and Tamura, Gabriel and M\"{u}ller, Hausi A.},
title = {Deployment Specification challenges in the context of large scale systems},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Traditionally, the focus of software deployment has been mainly on the infrastructure to realise deployment and configuration (D&amp;C) of complex and distributed systems, with an increasing interest in deployment of internet of things and cyber-physical systems. Advances in job scheduling, storage orchestration, containerized applications, along with agile practices such as continuous integration and microservices architecture, have improved the state of the practice. However, little effort has been devoted to the need for D&amp;C specifications to support the various levels of detail and abstraction present in large-scale systems. The understanding of the software components hierarchy has shifted from the comprehension of design artefacts, usually specified with static diagrams, to the understanding of runtime concepts. The DevOps movement has dramatically influenced how and when deployment is realised, but little has been done from the software perspective in terms of documentation and linkage between design and runtime artefacts in the sense of software specification as such. This paper presents an overview of the state of the art of deployment requirements for large-scale, distributed and complex software and its automation and characterises a set of deployment specification challenges intended as starting points for advancing the field of software deployment.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {220–226},
numpages = {7},
keywords = {DevOps, continuous integration continuous configuration, continuous software deployment, deployment specification, models at runtime, runtime artefacts},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3266276.3266283,
author = {Gosain, Abhimanyu},
title = {Platforms for Advanced Wireless Research: Helping Define a New Edge Computing Paradigm},
year = {2018},
isbn = {9781450359313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266276.3266283},
doi = {10.1145/3266276.3266283},
abstract = {The Platforms for Advanced Wireless Research (PAWR) program [1] aims to enable experimental wireless communications research across devices, communication techniques, networks, systems, and services conceived by the US academic and industrial wireless research community and deployed in partnership with local communities. PAWR seeks to accelerate the wireless innovation ecosystem, thereby enhancing broadband connectivity; enabling the emerging Internet of Things (IoT), edge computing and heterogeneous wireless connectivity technologies. Each research platform conceived under the PAWR program will enable at-scale experimentation by supporting the geographic size, technical diversity, and user density representative of a small city/community. From chipmakers to networking companies to software companies to application developers to vertical technology providers and users, the industry is devoting significant efforts to "moving past science experiments" into developing use cases for edge computing technologies. This calls for fundamental rethinking of computing and networking architectures that can disrupt existing business models and reshape industry landscapes. This talk details the edge computing ecosystem developed by the first two platforms; COSMOS [2] and POWDER [3]. We present the system architecture and components from radio clients, transport X-Haul, near edge cloud, and core cloud to rapidly develop and test Use-cases such as IoT Security via Edge AI, Smart City and Machine Vision, AR/VR and Automotive Edge (safety, navigation, automation + infotainment) on PAWR Platforms.},
booktitle = {Proceedings of the 2018 on Technologies for the Wireless Edge Workshop},
pages = {33},
numpages = {1},
location = {New Delhi, India},
series = {WirelessEdge '18}
}

@inproceedings{10.1145/3229565.3229569,
author = {Alexopoulos, Nikolaos and Habib, Sheikh Mahbub and M\"{u}hlh\"{a}user, Max},
title = {Towards Secure Distributed Trust Management on a Global Scale: An analytical approach for applying Distributed Ledgers for authorization in the IoT},
year = {2018},
isbn = {9781450359054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229565.3229569},
doi = {10.1145/3229565.3229569},
abstract = {Authorization, and more generally Trust Management (TM), is an indispensable part of the correct operation of most IT systems. The advent of the Internet of Things (IoT), with its cyber-physical and distributed nature, creates new challenges, that existing TM systems cannot adequately address, such as for example the need for non-interactive exclusive access enforcement. In the meantime, a line of thought in the research community is that Distributed Ledgers (DLs), like the one implemented by the Ethereum blockchain, can provide strong security guarantees for distributed access control. However, this approach has not yet been examined in a scientific, systematic manner, and has many pitfalls, with arguably the most important one being scalability.In this paper, we critically explore the shortcomings of existing solutions for trust management in distributed networks, pinpoint which of these shortcomings can be addressed by utilizing DLs, and offer a conceptual design for a scalable, secure TM system. Our design approaches the problem in three layers, namely a global, an intermediate group or shard layer, and a local layer, corresponding to the set of embedded devices behind an internet access point. We view our design as a novel first step, helping the community to produce more secure and realistic authorization solutions for the IoT, in the near future.},
booktitle = {Proceedings of the 2018 Workshop on IoT Security and Privacy},
pages = {49–54},
numpages = {6},
location = {Budapest, Hungary},
series = {IoT S&amp;P '18}
}

@article{10.1145/3149647.3149655,
author = {Riegler, Michael and Shamma, David Ayman},
title = {An interview with David Ayman Shamma},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3149647.3149655},
doi = {10.1145/3149647.3149655},
abstract = {About David Ayman Shamma:I am a Principal Investigator and Senior Scientist at Centrum Wiskunde &amp; Informatica (CWI) where I lead a team looking at Social Computing, Internet of Things (IoT), and fashion. Formerly, I was Director of Research at Yahoo Labs where I ran the HCI Research Group and I was the scientific liaison to Flickr (where I co-founded the Data-science group there). Broadly speaking, I design and prototype systems for multimedia-mediated communication, as well as, develops targeted methods and metrics for understanding how people communicate online in small environments and at web scale. Additionally, I create media art installations that have been reviewed by The New York Times, International Herald Tribune, and Chicago Magazine and exhibited internationally, including Second City Chicago, the Berkeley Art Museum, SIGGRAPH ETECH, Chicago Improv Festival, and Wired NextFest/NextMusic.I have a Ph.D. in Computer Science from the Intelligent Information Laboratory at Northwestern University and a B.S./M.S. from the Institute for Human and Machine Cognition at The University of West Florida. Before Yahoo!, I was an instructor at the Medill School of Journalism; I have also taught courses in Computer Science and Studio Art departments. Prior to receiving my Ph.D., I was a visiting research scientist for the Center for Mars Exploration at NASA Ames Research Center.},
journal = {SIGMultimedia Rec.},
month = {oct},
articleno = {8}
}

@inproceedings{10.1145/2968456.2974043,
author = {M\"{o}stl, Mischa and Schlatow, Johannes and Ernst, Rolf and Hoffmann, Henry and Merchant, Arif and Shraer, Alexander},
title = {Self-aware systems for the internet-of-things},
year = {2016},
isbn = {9781450344838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968456.2974043},
doi = {10.1145/2968456.2974043},
abstract = {The IoT will host a large number of co-existing cyber-physical applications. Continuous change, application interference, environment dynamics and uncertainty lead to complex effects which must be controlled to give performance and application guarantees. Application and platform self-configuration and self-awareness are one paradigm to approach this challenge. They can leverage context knowledge to control platform and application functions and their interaction. They could play a dominant role in large scale cyber-physical systems and systems-of-systems, simply because no person can oversee the whole system functionality and dynamics.IoT adds a new dimension because Internet based services will increasingly be used in such system functions. Autonomous vehicles accessing cloud services for efficiency and comfort as well as to reach the required level of safety and security are an example. Such vehicle platforms will communicate with a service infrastructure that must be reliable and highly responsive. Automated continuous self-configuration of data storage might be a good basis for such services up to the point where the different self-x strategies might affect each other, in a positive or negative form.This paper contains three contributions from different domains representing the current status of self-aware systems as they will meet in the Internet-of-Things and closes with a short discussion of upcoming challenges.},
booktitle = {Proceedings of the Eleventh IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
articleno = {21},
numpages = {9},
location = {Pittsburgh, Pennsylvania},
series = {CODES '16}
}

@inproceedings{10.1145/3338499.3357355,
author = {Gardiner, Joseph and Craggs, Barnaby and Green, Benjamin and Rashid, Awais},
title = {Oops I Did it Again: Further Adventures in the Land of ICS Security Testbeds},
year = {2019},
isbn = {9781450368315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338499.3357355},
doi = {10.1145/3338499.3357355},
abstract = {Research efforts in the security of Industrial Control Systems (ICS) have dramatically increased over the past few years. However, there is a limiting factor when work cannot be evaluated on real-world systems due to safety and operational reasons. This has led to multiple deployments of ICS testbeds covering multiple sectors including water treatment, power distribution and transportation networks.Over the last five years, we have designed and constructed ICS testbeds to support cyber security research. Our prior work in building testbeds culminated in a set of design principles and lessons learnt, formulated to support other researchers in designing and building their own ICS testbeds. In the last two years we have taken these lessons and used them to guide our own greenfield large-scale, complex and process-diverse security testbed affording a rare opportunity to design and build from the ground up -- one in which we have been able to look back and validate those past lessons and principles.In this work we describe the process of building our new ICS and Industrial Internet of Things (IIoT) testbed, and give an overview of its architecture. We then reflect on our past lessons, and contribute five previously unrecognised additional lessons based on this experience.},
booktitle = {Proceedings of the ACM Workshop on Cyber-Physical Systems Security &amp; Privacy},
pages = {75–86},
numpages = {12},
keywords = {cps, cyber security, ics, iiot, industrial control systems, operational technology, ot, scada, testbeds},
location = {London, United Kingdom},
series = {CPS-SPC'19}
}

@article{10.1145/3325822,
author = {Wedaj, Samuel and Paul, Kolin and Ribeiro, Vinay J.},
title = {DADS: Decentralized Attestation for Device Swarms},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2471-2566},
url = {https://doi.org/10.1145/3325822},
doi = {10.1145/3325822},
abstract = {We present a novel scheme called Decentralized Attestation for Device Swarms (DADS), which is, to the best of our knowledge, the first to accomplish decentralized attestation in device swarms. Device swarms are smart, mobile, and interconnected devices that operate in large numbers and are likely to be part of emerging applications in Cyber-Physical Systems (CPS) and Industrial Internet of Things (IIoTs). Swarm devices process and exchange safety, privacy, and mission-critical information. Thus, it is important to have a good code verification technique that scales to device swarms and establishes trust among collaborating devices. DADS has several advantages over current state-of-the-art swarm attestation techniques: It is decentralized, has no single point of failure, and can handle changing topologies after nodes are compromised. DADS assures system resilience to node compromise/failure while guaranteeing only devices that execute genuine code remain part of the group. We conduct performance measurements of communication, computation, memory, and energy using the TrustLite embedded systems architecture in OMNeT++ simulation environment. We show that the proposed approach can significantly reduce communication cost and is very efficient in terms of computation, memory, and energy requirements. We also analyze security and show that DADS is very effective and robust against various attacks.},
journal = {ACM Trans. Priv. Secur.},
month = {jul},
articleno = {19},
numpages = {29},
keywords = {Remote attestation, decentralized attestation, dynamic networks, security, swarm attestation}
}

@inproceedings{10.1145/3326285.3329057,
author = {Zhang, Lu and Li, Chao and Wang, Pengyu and Liu, Yunxin and Hu, Yang and Chen, Quan and Guo, Minyi},
title = {Characterizing and orchestrating NFV-ready servers for efficient edge data processing},
year = {2019},
isbn = {9781450367783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326285.3329057},
doi = {10.1145/3326285.3329057},
abstract = {The fast-growing Internet of Things (IoT) and Artificial intelligence (AI) applications mandate high-performance edge data analytics. This requirement cannot be fully fulfilled by prior works that focus on either small architectures (e.g., accelerators) or large infrastructure (e.g., cloud data centers). Sitting in between the edge and cloud, there have been many server-level designs for augmenting edge data processing. However, they often require specialized hardware resources and lack scalability as well as agility.Other than reinventing the wheel, we explore tapping into underutilized network infrastructure in the incoming 5G era for augmenting edge data analytics. Specifically, we focus on efficiently deploying edge data processing applications on Network Function Virtualization (NFV) enabled commodity servers. In such a way, we can benefit from the service flexibility of NFV while greatly reducing the cost of many servers deployed in the edge network. We perform extensive experiments to investigate the characteristics of packet processing in a DPDK-based NFV platform and discover the resource under-utilization issue when using the DPDK polling-mode. Then, we propose a framework named EdgeMiner, which can harvest the potentially idle cycles of the cores for data processing purpose. Meanwhile, it can also guarantee the Quality of Service (QoS) of both the Virtualized Network Functions (VNFs) and Edge Data Processing (EDP) applications when they are co-running on the same server.},
booktitle = {Proceedings of the International Symposium on Quality of Service},
articleno = {22},
numpages = {10},
keywords = {QoS, edge data processing, network function virtualization, resource under-utilization},
location = {Phoenix, Arizona},
series = {IWQoS '19}
}

@inproceedings{10.1145/3306309.3306327,
author = {B\l{}aszczyszyn, Bart\l{}tomiej and M\"{u}hlethaler, Paul},
title = {Analyzing LoRa long-range, low-power, wide-area networks using stochastic geometry},
year = {2019},
isbn = {9781450365963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306309.3306327},
doi = {10.1145/3306309.3306327},
abstract = {In this paper we present a simple, stochastic-geometric model of a wireless access network exploiting the LoRA (Long Range) protocol, which is a non-expensive technology allowing for long-range, single-hop connectivity for the Internet of Things. We assume a space-time Poisson model of packets transmitted by LoRA nodes to a fixed base station. Following previous studies of the impact of interference [8, 10], we assume that a given packet is successfully received when no interfering packet arrives with similar power before the given packet payload phase. This is as a consequence of LoRa using different transmission rates for different link budgets (transmissions with smaller received powers use larger spreading factors) and LoRa intra-technology interference treatment. Using our model, we study the scaling of the packet reception probabilities per link budget as a function of the spatial density of nodes and their rate of transmissions. We consider both the parameter values recommended by the LoRa provider, as well as proposing LoRa tuning to improve the equality of performance for all link budgets. We also consider spatially non-homogeneous distributions of LoRa nodes. We show also how a fair comparison to non-slotted Aloha can be made within the same framework.},
booktitle = {Proceedings of the 12th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {119–126},
numpages = {8},
keywords = {Internet of Things, LoRa, Low-Power, Poisson process, Wide-Area Network, propagation process, reception probability, stochastic geometry},
location = {Palma, Spain},
series = {VALUETOOLS 2019}
}

@inproceedings{10.1145/3131473.3133333,
author = {Schulz, Matthias and Knapp, Fabian and Deligeorgopoulos, Efstathios and Wegemer, Daniel and Gringoli, Francesco and Hollick, Matthias},
title = {Demo: Nexmon in Action: Advanced Applications Powered by the Nexmon Firmware Patching Framework},
year = {2017},
isbn = {9781450351478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131473.3133333},
doi = {10.1145/3131473.3133333},
abstract = {Smartphones and Internet of Things (IoT) devices are widely available and offer interfaces for wireless communication. This makes them perfect candidates for large-scale wireless testbeds. To reduce energy consumption, those devices contain FullMAC Wi-Fi chips. They run proprietary firmwares that abstract from low-layer mechanisms on the data link (MAC) and physical layer (PHY). This hinders researchers to influence their operation and evaluate new communication schemes on off-the-shelf devices. Using our Nexmon firmware patching framework, we gain access to chip internals to extend their functionalities by writing patches comfortably in C. In this work, we use a Raspberry Pi 3 to offer workshop attendees a hands-on experience on how to get starting with Nexmon by extending the firmware of the Pi's Wi-Fi chip. Additionally, we use Android smartphones to present our wireless penetration testing app based on monitor mode and frame injection patches, as well as our reactive Wi-Fi jamming app based on patches to the Wi-Fi chip's real-time processor. The demonstrations show how easily Nexmon enables us to implement even complex applications in a Wi-Fi chip resulting in very low processing latencies and low energy consumption. As open-source projects, all our demos can be reproduced by fellow researchers in their own laboratories by using widely available off-the-shelf hardware.},
booktitle = {Proceedings of the 11th Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; CHaracterization},
pages = {101–102},
numpages = {2},
keywords = {broadcom, firmware reverse engineering, jamming, nexmon, raspberry pi, smartphones, wireless security},
location = {Snowbird, Utah, USA},
series = {WiNTECH '17}
}

@inproceedings{10.1145/2939672.2939777,
author = {Silva, Arlei and Dang, Xuan Hong and Basu, Prithwish and Singh, Ambuj and Swami, Ananthram},
title = {Graph Wavelets via Sparse Cuts},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939777},
doi = {10.1145/2939672.2939777},
abstract = {Modeling information that resides on vertices of large graphs is a key problem in several real-life applications, ranging from social networks to the Internet-of-things. Signal Processing on Graphs and, in particular, graph wavelets can exploit the intrinsic smoothness of these datasets in order to represent them in a compact and accurate manner. However, how to discover wavelet bases that capture the geometry of the data with respect to the signal as well as the graph structure remains an open problem. In this paper, we study the problem of computing graph wavelet bases via sparse cuts in order to produce low-dimensional encodings of data-driven bases. This problem is connected to known hard problems in graph theory (e.g. multiway cuts) and thus requires an efficient heuristic. We formulate the basis discovery task as a relaxation of a vector optimization problem, which leads to an elegant solution as a regularized eigenvalue computation. Moreover, we propose several strategies in order to scale our algorithm to large graphs. Experimental results show that the proposed algorithm can effectively encode both the graph structure and signal, producing compressed and accurate representations for vertex values in a wide range of datasets (e.g. sensor and gene networks) and significantly outperforming the best baseline.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1175–1184},
numpages = {10},
keywords = {graph mining, spectral theory, wavelets},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2909827.2930795,
author = {Dang, Hung and Chong, Yun Long and Brun, Francois and Chang, Ee-Chien},
title = {Practical and Scalable Sharing of Encrypted Data in Cloud Storage with Key Aggregation},
year = {2016},
isbn = {9781450342902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909827.2930795},
doi = {10.1145/2909827.2930795},
abstract = {We study a sensor network setting in which samples are encrypted individually using different keys and maintained on a cloud storage. For large systems, e.g. those that generate several millions of samples per day, fine-grained sharing of encrypted samples is challenging. Existing solutions, such as Attribute-Based Encryption (ABE) and Key Aggregation Cryptosystem (KAC), can be utilized to address the challenge, but only to a certain extent. They are often computationally expensive and thus unlikely to operate at scale. We propose an algorithmic enhancement and two heuristics to improve KAC's key reconstruction cost, while preserving its provable security. The improvement is particularly significant for range and down-sampling queries -- accelerating the reconstruction cost from quadratic to linear running time. Experimental study shows that for queries of size 32k samples, the proposed fast reconstruction techniques speed-up the original KAC by at least 90 times on range and down-sampling queries, and by eight times on general (arbitrary) queries. It also shows that at the expense of splitting the query into 16 sub-queries and correspondingly issuing that number of different aggregated keys, reconstruction time can be reduced by 19 times. As such, the proposed techniques make KAC more applicable in practical scenarios such as sensor networks or the Internet of Things.},
booktitle = {Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security},
pages = {69–80},
numpages = {12},
keywords = {cloud security, key aggregation cryptosystem, security and privacy, sensor network},
location = {Vigo, Galicia, Spain},
series = {IH&amp;MMSec '16}
}

@inproceedings{10.1145/3269206.3269223,
author = {Das, Ariyam and Gandhi, Sahil M. and Zaniolo, Carlo},
title = {ASTRO: A Datalog System for Advanced Stream Reasoning},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269223},
doi = {10.1145/3269206.3269223},
abstract = {The rise of the Internet of Things (IoT) and the recent focus on a gamut of 'Smart City' initiatives world-wide have pushed for new advances in data stream systems to (1) support complex analytics and evolving graph applications as continuous queries, and (2) deliver fast and scalable processing on large data streams. Unfortunately current continuous query languages (CQL) lack the features and constructs needed to support the more advanced applications. For example recursive queries are now part of SQL, Datalog, and other query languages, but they are not supported by most CQLs, a fact that caused a significant loss of expressive power, which is further aggravated by the limitation that only non-blocking queries can be supported. To overcome these limitations we have developed an a dvanced st ream r easo ning system ASTRO that builds on recent advances in supporting aggregates in recursive queries. In this demo, we will briefly elucidate the formal Streamlog semantics, which combined with the Pre-Mappability (PreM) concept, allows the declarative specification of many complex continuous queries, which are then efficiently executed in real-time by the portable ASTRO architecture. Using different case studies, we demonstrate (i) the ease-of-use, (ii) the expressive power and (iii) the robustness of our system, as compared to other state-of-the-art declarative CQL systems.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1863–1866},
numpages = {4},
keywords = {complex event processing (cep) system, continuous query languages (cql), data stream management system (dsms), datalog, evolving graphs, recursive queries, stream reasoning applications},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3286719.3286724,
author = {Thangarajan, Ashok Samraj and Yang, Fan and Joosen, Wouter and Hughes, Danny},
title = {Real-time Distributed In-Situ Benchmarking of Energy Harvesting IoT Devices},
year = {2018},
isbn = {9781450361187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286719.3286724},
doi = {10.1145/3286719.3286724},
abstract = {The deployment of Internet of Things (IoT) devices is accelerating across a wide range of applications. The majority of today's IoT devices are powered by batteries that can operate for a maximum of a few years, after which they need to be replaced. This introduces two problems. First, the effort that is required to manually replace batteries cannot economically scale to support the next billion IoT devices. Secondly, treating billions of toxic batteries as disposable is not environmentally friendly. Together, these problems form a critical road-block in deploying IoT solutions. The biggest problem facing the designers of IoT applications is ensuring that their application software is energy efficient enough to operate within the strict power envelope that is provided by batteries or energy harvesting hardware. In this paper, we tackle this problem through the introduction of a distributed benchmarking middleware that rapidly and accurately quantifies the power consumption of different software configurations. Critically, our middleware operates in real-time across a distributed network of devices, allowing developers to experiment with code changes at runtime. This makes it significantly easier for developers to write applications that operate within the power constraints of batteries or energy harvesting systems. We evaluate our approach on a real world energy harvesting testbed and demonstrate that benchmarking results are accurate, with limited overhead for developers.},
booktitle = {Proceedings of the 5th Workshop on Middleware and Applications for the Internet of Things},
pages = {19–24},
numpages = {6},
keywords = {Benchmarking, Energy Harvesting, Industrial IoT Networks, Internet of Things (IoT), Self Adaptive Networks},
location = {Rennes, France},
series = {M4IoT'18}
}

@inproceedings{10.1145/3229710.3229744,
author = {Hern\'{a}ndez, Daniel and Arcas-T\'{u}nez, Francisco and Mu\~{n}oz, Andr\'{e}s and Cecilia, Jos\'{e} M.},
title = {BAUSPACE: A Scalable Infrastructure for Soft Sensors Development},
year = {2018},
isbn = {9781450365239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229710.3229744},
doi = {10.1145/3229710.3229744},
abstract = {The Internet of Things (IoT) is driving the next economic revolution where the main actors are both the data volume and the immediacy. However, the IoT world is increasingly generating vast amounts of data classified as a "dark data", since most of them are generated but never analysed. Therefore, efficient big data analysis in IoT infrastructure is becoming mandatory to transform this data deluge into meaningful information. Even after enabling this analysis, the quantitative information provided by traditional "hard" sensors is not enough to deal with some scenarios where human observations are required. These observations could be targeted through "soft sensors", where people's opinion in social networks, posts, news or comments may be analyzed to create dynamic observation resources. Combining both sources-of-information (devices and humans) automatically would provide a very powerful tool that could represent a step forward in the data understanding science. However, the development of soft sensors implies the use of many services for crawling text sources and mashup Web-based content, storage it, understanding the language or inferring information, just to mention a few. Therefore, a novel cloud-based distributed system is mandatory to be able to develop such frameworks. In this paper we introduce a work-in-progress for a distributed and modular framework to develop soft sensors in a scalable manner and transparently to the cloud provider.},
booktitle = {Workshop Proceedings of the 47th International Conference on Parallel Processing},
articleno = {22},
numpages = {4},
keywords = {Distributed Computing, Docker, Kafka, Soft Sensors, Storm, Zookeeper},
location = {Eugene, OR, USA},
series = {ICPP Workshops '18}
}

@article{10.14778/2947618.2947625,
author = {Zhao, Yiran and Li, Shen and Hu, Shaohan and Wang, Hongwei and Yao, Shuochao and Shao, Huajie and Abdelzaher, Tarek},
title = {An experimental evaluation of datacenter workloads on low-power embedded micro servers},
year = {2016},
issue_date = {May 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2947618.2947625},
doi = {10.14778/2947618.2947625},
abstract = {This paper presents a comprehensive evaluation of an ultra-low power cluster, built upon the Intel Edison based micro servers. The improved performance and high energy efficiency of micro servers have driven both academia and industry to explore the possibility of replacing conventional brawny servers with a larger swarm of embedded micro servers. Existing attempts mostly focus on mobile-class micro servers, whose capacities are similar to mobile phones. We, on the other hand, target on sensor-class micro servers, which are originally intended for uses in wearable technologies, sensor networks, and Internet-of-Things. Although sensor-class micro servers have much less capacity, they are touted for minimal power consumption (&lt; 1 Watt), which opens new possibilities of achieving higher energy efficiency in datacenter workloads. Our systematic evaluation of the Edison cluster and comparisons to conventional brawny clusters involve careful workload choosing and laborious parameter tuning, which ensures maximum server utilization and thus fair comparisons. Results show that the Edison cluster achieves up to 3.5x improvement on work-done-per-joule for web service applications and data-intensive MapReduce jobs. In terms of scalability, the Edison cluster scales linearly on the throughput of web service workloads, and also shows satisfactory scalability for MapReduce workloads despite coordination overhead.},
journal = {Proc. VLDB Endow.},
month = {may},
pages = {696–707},
numpages = {12}
}

@inproceedings{10.1109/CCGrid.2016.38,
author = {Heidari, Safiollah and Calheiros, Rodrigo N. and Buyya, Rajkumar},
title = {iGiraph: a cost-efficient framework for processing large-scale graphs on public clouds},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.38},
doi = {10.1109/CCGrid.2016.38},
abstract = {Large-scale graph analytics has gained attention during the past few years. As the world is going to be more connected by appearance of new technologies and applications such as social networks, Web portals, mobile devices, Internet of things, etc, a huge amount of data are created and stored every day in the form of graphs consisting of billions of vertices and edges. Many graph processing frameworks have been developed to process these large graphs since Google introduced its graph processing framework called Pregel in 2010. On the other hand, cloud computing which is a new paradigm of computing that overcomes restrictions of traditional problems in computing by enabling some novel technological and economical solutions such as distributed computing, elasticity and pay-as-you-go models has improved service delivery features. In this paper, we present iGiraph, a cost-efficient Pregel-like graph processing framework for processing large-scale graphs on public clouds. iGiraph uses a new dynamic re-partitioning approach based on messaging pattern to minimize the cost of resource utilization on public clouds. We also present the experimental results on the performance and cost effects of our method and compare them with basic Giraph framework. Our results validate that iGiraph remarkably decreases the cost and improves the performance by scaling the number of workers dynamically.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {301–310},
numpages = {10},
keywords = {cloud computing, cost-efficient processing, graph analytics, graph processing, partitioning, public clouds},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@article{10.1109/TNET.2018.2879607,
author = {Dong, Wei and Cao, Chenhong and Zhang, Xiaoyu and Gao, Yi},
title = {Understanding Path Reconstruction Algorithms in Multihop Wireless Networks},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2879607},
doi = {10.1109/TNET.2018.2879607},
abstract = {Low-power and multihop wireless networking is envisioned as a promising technology to achieve both energy efficiency and easy deployment for many Internet of Things IoT applications. Measuring packet-level path is crucial for managing large-scale multihop wireless networks. Packet-level path information encodes the routing path, a packet that takes through a network. The availability of packet-level path information can greatly facilitate many network management tasks. It is challenging to reconstruct packet-level paths using a small overhead, especially for large-scale networks. While there is a long list of existing path reconstruction algorithms, these algorithms focus on specific network scenarios, e.g., periodic monitoring networks or event detection networks. There lacks a unified model for systematically understanding and comparing the performance of these algorithms in different network scenarios. In this paper, we fill this gap by proposing an abstract model. Using this model, it is possible to derive a decision space for selecting the best algorithm for different networks. Furthermore, this model also guides us to devise better path reconstruction algorithms cPath$_{T}$ , cPath$_{S}$ , and cPath$_{ST}$ with respect to path reconstruction ratio. Extensive experiments demonstrate the prediction power of our model as well as the advantages of our proposed algorithms. The results show that our algorithm cPath$_{ST}$ improves a path reconstruction ratio from 94.4%, 34.3%, and 30.8% to 98.9%, 99.9%, and 60.1% on average in three network scenarios, respectively, compared with the best state-of-the-art algorithms.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {1–14},
numpages = {14}
}

@inproceedings{10.1145/3144457.3144485,
author = {Pal, Shantanu and Hitchens, Michael and Varadharajan, Vijay and Rabehaja, Tahiry},
title = {On Design of A Fine-Grained Access Control Architecture for Securing IoT-Enabled Smart Healthcare Systems},
year = {2017},
isbn = {9781450353687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144457.3144485},
doi = {10.1145/3144457.3144485},
abstract = {The Internet of Things (IoT) is facilitating the development of novel and cost-effective applications that promise to deliver efficient and improved medical facilities to patients and health organisations. This includes the use of smart 'things' as medical sensors attached to patients to deliver real-time data. However, the security of patient data is an ever-present concern in the healthcare arena. In the wider deployment of IoT-enabled smart healthcare systems one particular issue is the need to protect smart 'things' from unauthorised access. Commonly used access control approaches e.g. Attribute Based Access Control (ABAC), Role Based Access Control (RBAC) and capability based access control do not, in isolation, provide a complete solution for securing access to IoT-enabled smart healthcare devices. They may, for example, require an overly-centralised solution or an unmanageably large policy base. To address these issues we propose a novel access control architecture which improves policy management by reducing the required number of authentication policies in a large-scale healthcare system while providing fine-grained access control. We devise a hybrid access control model employing attributes, roles and capabilities. We apply attributes for role-membership assignment and in permission evaluation. Membership of roles grants capabilities. The capabilities which are issued may be parameterised based on further attributes of the user and are then used to access specific services provided by IoT 'things'. We also provide a formal specification of the model and a description of its implementation and demonstrate its application through different use-case scenarios. Evaluation results of core functionality of our architecture are provided.},
booktitle = {Proceedings of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {432–441},
numpages = {10},
keywords = {Internet of things, access control, healthcare systems, policy management, security},
location = {Melbourne, VIC, Australia},
series = {MobiQuitous 2017}
}

@inproceedings{10.1145/3345768.3355944,
author = {Bany Salameh, Haythem and Derbas, Rawan and Aloqaily, Moayad and Boukerche, Azzedine},
title = {Secure Routing in Multi-hop IoT-based Cognitive Radio Networks under Jamming Attacks},
year = {2019},
isbn = {9781450369046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345768.3355944},
doi = {10.1145/3345768.3355944},
abstract = {Integrating Cognitive Radio (CR) technology in Internet-of-Things (IoT) devices allows efficient large-scale deployment of IoT systems. Recently, research efforts are shifted toward adopting CR in IoT as a response for the spectrum scarcity problem. Unfortunately, CR Networks (CRNs) share the same security weaknesses with traditional wireless networks. CR communication is also vulnerable to jamming attacks which can significantly affect network performance, consume network resources and results in delays, that make it less suitable for IoT time-critical systems. Routing in CR-based IoT networks, in general, considered as a challenging issue. Under the jamming attack, routing becomes even more challenging. In this paper, we introduce a new jamming-aware routing and channel assignment protocol that deals with proactive jamming attacks in CR-based IoT networks without requiring extra resources. The proposed protocol attempts at improving the overall packet delivery ratio in the network while considering the primary user's activities, multi-channel fading and jamming behavior. The proposed protocol consists of three phases: route discovery, channel assignment, and path selection. The channel assignment problem along each path is formulated as an optimization problem with the objective of maximizing the end-to-end probability of success. This problem is shown to be an uni-modular problem, which can be solved in polynomial-time using linear programming techniques. Compared to reference protocols, simulation results reveal that the proposed protocol significantly improves network performance in terms of packet delivery ratio.},
booktitle = {Proceedings of the 22nd International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {323–327},
numpages = {5},
keywords = {cognitive radio networks, iot, jamming attacks, secure routing},
location = {Miami Beach, FL, USA},
series = {MSWIM '19}
}

@article{10.1145/3199523,
author = {Heidari, Safiollah and Simmhan, Yogesh and Calheiros, Rodrigo N. and Buyya, Rajkumar},
title = {Scalable Graph Processing Frameworks: A Taxonomy and Open Challenges},
year = {2018},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3199523},
doi = {10.1145/3199523},
abstract = {The world is becoming a more conjunct place and the number of data sources such as social networks, online transactions, web search engines, and mobile devices is increasing even more than had been predicted. A large percentage of this growing dataset exists in the form of linked data, more generally, graphs, and of unprecedented sizes. While today's data from social networks contain hundreds of millions of nodes connected by billions of edges, inter-connected data from globally distributed sensors that forms the Internet of Things can cause this to grow exponentially larger. Although analyzing these large graphs is critical for the companies and governments that own them, big data tools designed for text and tuple analysis such as MapReduce cannot process them efficiently. So, graph distributed processing abstractions and systems are developed to design iterative graph algorithms and process large graphs with better performance and scalability. These graph frameworks propose novel methods or extend previous methods for processing graph data. In this article, we propose a taxonomy of graph processing systems and map existing systems to this classification. This captures the diversity in programming and computation models, runtime aspects of partitioning and communication, both for in-memory and distributed frameworks. Our effort helps to highlight key distinctions in architectural approaches, and identifies gaps for future research in scalable graph systems.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {60},
numpages = {53},
keywords = {Big data, distributed systems, graph processing, large-scale graphs, parallel processing}
}

@article{10.1109/TNET.2018.2856197,
author = {Saifullah, Abusayeed and Rahman, Mahbubur and Ismail, Dali and Lu, Chenyang and Liu, Jie and Chandra, Ranveer},
title = {Low-Power Wide-Area Network Over White Spaces},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2856197},
doi = {10.1109/TNET.2018.2856197},
abstract = {As a key technology driving the Internet-of-Things, low-power wide-area networks LPWANs are evolving to overcome the range limits and scalability challenges in traditional wireless sensor networks. This paper proposes a new LPWAN architecture called sensor network over white spaces SNOW by exploiting the TV white spaces. The SNOW is the first highly scalable LPWAN over TV white spaces that enable asynchronous, bi-directional, and massively concurrent communication between numerous sensors and a base station. This is achieved through a set of novel techniques. The SNOW has a new OFDM-based physical layer that allows the base station using a single antenna-radio: 1 to send different data to different nodes concurrently and 2 to receive concurrent transmissions made by the sensor nodes asynchronously. It has a lightweight media access control protocol that: 1 efficiently implements per-transmission acknowledgments of the asynchronous transmissions by exploiting the adopted OFDM design and 2 combines CSMA/CA and location-aware spectrum allocation for mitigating hidden terminal effects, thus enhancing the flexibility of the nodes in transmitting asynchronously. We implement the SNOW in GNU radio using universal software radio peripheral devices. Experiments through deployments in three radio environments—a large metropolitan city, a rural area, and an indoor environment—as well as large-scale simulations demonstrated that the SNOW drastically enhances the scalability of a sensor network and outperforms existing techniques in terms of scalability, energy, and latency.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1893–1906},
numpages = {14}
}

@inproceedings{10.1145/3286978.3287029,
author = {Tomi\'{c}, Ivana and Chen, Po-Yu and Breza, Michael J. and McCann, Julie A.},
title = {Antilizer: Run Time Self-Healing Security for Wireless Sensor Networks},
year = {2018},
isbn = {9781450360937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286978.3287029},
doi = {10.1145/3286978.3287029},
abstract = {Wireless Sensor Network (WSN) applications range from domestic Internet of Things systems like temperature monitoring of homes to the monitoring and control of large-scale critical infrastructures. The greatest risk with the use of WSNs in critical infrastructure is their vulnerability to malicious network level attacks. Their radio communication network can be disrupted, causing them to lose or delay data which will compromise system functionality. This paper presents Antilizer, a lightweight, fully-distributed solution to enable WSNs to detect and recover from common network level attack scenarios. In Antilizer each sensor node builds a self-referenced trust model of its neighbourhood using network overhearing. The node uses the trust model to autonomously adapt its communication decisions. In the case of a network attack, a node can make neighbour collaboration routing decisions to avoid affected regions of the network. Mobile agents further bound the damage caused by attacks. These agents enable a simple notification scheme which propagates collaborative decisions from the nodes to the base station. A filtering mechanism at the base station further validates the authenticity of the information shared by mobile agents. We evaluate Antilizer in simulation against several routing attacks. Our results show that Antilizer reduces data loss down to 1% (4% on average), with operational overheads of less than 1% and provides fast network-wide convergence.},
booktitle = {Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {107–116},
numpages = {10},
keywords = {Security, Self-Healing, Trust, Wireless Sensor Networks},
location = {New York, NY, USA},
series = {MobiQuitous '18}
}

@inproceedings{10.1145/2882903.2882943,
author = {Kalyvianaki, Evangelia and Fiscato, Marco and Salonidis, Theodoros and Pietzuch, Peter},
title = {THEMIS: Fairness in Federated Stream Processing under Overload},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882943},
doi = {10.1145/2882903.2882943},
abstract = {Federated stream processing systems, which utilise nodes from multiple independent domains, can be found increasingly in multi-provider cloud deployments, internet-of-things systems, collaborative sensing applications and large-scale grid systems. To pool resources from several sites and take advantage of local processing, submitted queries are split into query fragments, which are executed collaboratively by different sites. When supporting many concurrent users, however, queries may exhaust available processing resources, thus requiring constant load shedding. Given that individual sites have autonomy over how they allocate query fragments on their nodes, it is an open challenge how to ensure global fairness on processing quality experienced by queries in a federated scenario.We describe THEMIS, a federated stream processing system for resource-starved, multi-site deployments. It executes queries in a globally fair fashion and provides users with constant feedback on the experienced processing quality for their queries. THEMIS associates stream data with its source information content (SIC), a metric that quantifies the contribution of that data towards the query result, based on the amount of source data used to generate it. We provide the BALANCE-SIC distributed load shedding algorithm that balances the SIC values of result data. Our evaluation shows that the BALANCE-SIC algorithm yields balanced SIC values across queries, as measured by Jain's Fairness Index. Our approach also incurs a low execution time overhead.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {541–553},
numpages = {13},
keywords = {approximate data processing, fairness, federated data stream processing, tuple shedding},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/2901378.2901406,
author = {Rajendran, Shanthini and Chelladurai, Suresh Rathnaraj and Aravind, Alex},
title = {An Adaptive Road Traffic Regulation with Simulation and Internet of Things},
year = {2016},
isbn = {9781450337427},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901378.2901406},
doi = {10.1145/2901378.2901406},
abstract = {Traffic congestion is a growing concern in most cities across the world. It is primarily caused by a sudden increase in the number of vehicles in a relatively small number of roads and intersections, while other roads have the capacity to accommodate more traffic. In such situations, distributing traffic to roads in a balanced way could alleviate congestion. With the help of modern technology such as Internet of Things (IoT) and simulation, road users can be encouraged to choose their route on-the-fly, by providing necessary information such as projected travel time on the next leg. In extreme situations, traffic on some critical roads could be adaptively reduced by even introducing levy. A simple solution like providing road traffic information, benefits and penalties, etc., ahead in each intersection would allow travellers to make cognizant choices and therefore could lead to a better, more efficient traffic distribution.To implement the proposed system, simulation and IoT must be brought together by a suitable communication middleware system so that they can work in synchrony. Implementing an actual IoT infrastructure and then testing the cause and effects of traffic congestion with the system in-place is a daunting task. Simulation would help us to test and validate the IoT system for functionality, performance, and scalability. In this paper, we propose a novel framework for integrating IoT and simulation using a message-oriented middleware in the context of an adaptive traffic regulation system and then demonstrate the framework with the help of a prototype implementation.},
booktitle = {Proceedings of the 2016 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {3–11},
numpages = {9},
keywords = {dynamic data-driven, internet of things, road traffic, simulation, traffic congestion, traffic regulation},
location = {Banff, Alberta, Canada},
series = {SIGSIM-PADS '16}
}

@inproceedings{10.1145/2851613.2852011,
author = {Atabekov, Amir},
title = {Internet of things-based smart classroom environment: student research abstract},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2852011},
doi = {10.1145/2851613.2852011},
abstract = {The management of classrooms, halls, offices, and public spaces and the efficient use of these resources in any organization are challenging problems. With the rise of Internet of Things (IoT), the management of these resources can be automated. Methods to automatically record activities and monitor resource usage inside classrooms in real-time are usually intricate. Previously we implemented a proof-of-concept phase of the smart chair system [1], consisting of one chair, to solve the problem of resource and classroom management at small scale. In this abstract we propose a complete system consisting of ten smart chairs, by employing IoT paradigm in order to simplify classroom management, attendance tracking and classroom interaction. Taking students attendance-monitoring systems as a motivational example, instructors have to call the names of students or the students are asked to sign the attendance sheet. The former is a time consuming process, while the latter is unreliable. The proposed smart classroom system simplifies many of these tasks by utilizing indoor localization, RFID sensors, pressure sensors, Arduino microcontrollers and cloud backend. The smart chair system can automatically check whether the chair is occupied or not by using multiple sensing technologies. Moreover, student identification can be automatically performed using RFID sensors, which can read RFID tags off student identification cards. Additionally, the tardiness or leave-early information can be analyzed by exploiting the integrated timestamps. The proposed smart classroom system will identify each student, record timestamp of arrival and departure, and will generate a dynamic seat map of students in the classroom by utilizing indoor localization.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {746–747},
numpages = {2},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2966986.2967008,
author = {Rokni, Seyed Ali and Ghasemzadeh, Hassan},
title = {Autonomous sensor-context learning in dynamic human-centered internet-of-things environments},
year = {2016},
isbn = {9781450344661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2966986.2967008},
doi = {10.1145/2966986.2967008},
abstract = {Human-centered Internet-of-Things (IoT) applications utilize computational algorithms such as machine learning and signal processing techniques to infer knowledge about important events such as physical activities and medical complications. The inference is typically based on data collected with wearable sensors or those embedded in the environment. A major obstacle in large-scale utilization of these systems is that the computational algorithms cannot be shared between users or reused in contexts different than the setting in which the training data are collected. For example, an activity recognition algorithm trained for a wrist-band sensor cannot be used on a smartphone worn on the waist. We propose an approach for automatic detection of physical sensor-contexts (e.g., on-body sensor location) without need for collecting new labeled training data. Our techniques enable system designers and end-users to share and reuse computational algorithms that are trained under different contexts and data collection settings. We develop a framework to autonomously identify sensor-context. We propose a gating function to automatically activate the most accurate computational algorithm among a set of shared expert models. Our analysis based on real data collected with human subjects while performing 12 physical activities demonstrate that the accuracy of our multi-view learning is only 7.9% less than the experimental upper bound for activity recognition using a dynamic sensor constantly migrating from one on-body location to another. We also compare our approach with several mixture-of-experts models and transfer learning techniques and demonstrate that our approach outperforms algorithms in both categories.},
booktitle = {Proceedings of the 35th International Conference on Computer-Aided Design},
articleno = {75},
numpages = {6},
location = {Austin, Texas},
series = {ICCAD '16}
}

@article{10.1145/2723159,
author = {Oren, Yossef and Keromytis, Angelos D.},
title = {Attacking the Internet Using Broadcast Digital Television},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1094-9224},
url = {https://doi.org/10.1145/2723159},
doi = {10.1145/2723159},
abstract = {In the attempt to bring modern broadband Internet features to traditional broadcast television, the Digital Video Broadcasting (DVB) consortium introduced a specification called Hybrid Broadcast-Broadband Television (HbbTV), which allows broadcast streams to include embedded HTML content that is rendered by the television. This system is already in very wide deployment in Europe and has recently been adopted as part of the American digital television standard. Our analyses of the specifications, and of real systems implementing them, show that the broadband and broadcast systems are combined insecurely. This enables a large-scale exploitation technique with a localized geographical footprint based on Radio Frequency (RF) injection, which requires a minimal budget and infrastructure and is remarkably difficult to detect. In this article, we present the attack methodology and a number of follow-on exploitation techniques that provide significant flexibility to attackers. Furthermore, we demonstrate that the technical complexity and required budget are low, making this attack practical and realistic, especially in areas with high population density: In a dense urban area, an attacker with a budget of about 450 can target more than 20,000 devices in a single attack. A unique aspect of this attack is that, in contrast to most Internet of Things/Cyber-Physical System threat scenarios, where the attack comes from the data network side and affects the physical world, our attack uses the physical broadcast network to attack the data network.},
journal = {ACM Trans. Inf. Syst. Secur.},
month = {apr},
articleno = {16},
numpages = {27},
keywords = {Smart TV, radio-frequency attacks, relay attacks}
}

@article{10.1145/3177774,
author = {Viroli, Mirko and Audrito, Giorgio and Beal, Jacob and Damiani, Ferruccio and Pianini, Danilo},
title = {Engineering Resilient Collective Adaptive Systems by Self-Stabilisation},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1049-3301},
url = {https://doi.org/10.1145/3177774},
doi = {10.1145/3177774},
abstract = {Collective adaptive systems are an emerging class of networked computational systems particularly suited for application domains such as smart cities, complex sensor networks, and the Internet of Things. These systems tend to feature large-scale, heterogeneity of communication model (including opportunistic peer-to-peer wireless interaction) and require inherent self-adaptiveness properties to address unforeseen changes in operating conditions. In this context, it is extremely difficult (if not seemingly intractable) to engineer reusable pieces of distributed behaviour to make them provably correct and smoothly composable.Building on the field calculus, a computational model (and associated toolchain) capturing the notion of aggregate network-level computation, we address this problem with an engineering methodology coupling formal theory and computer simulation. On the one hand, functional properties are addressed by identifying the largest-to-date field calculus fragment generating self-stabilising behaviour, guaranteed to eventually attain a correct and stable final state despite any transient perturbation in state or topology and including highly reusable building blocks for information spreading, aggregation, and time evolution. On the other hand, dynamical properties are addressed by simulation, empirically evaluating the different performances that can be obtained by switching between implementations of building blocks with provably equivalent functional properties. Overall, our methodology sheds light on how to identify core building blocks of collective behaviour and how to select implementations that improve system performance while leaving overall system function and resiliency properties unchanged.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {mar},
articleno = {16},
numpages = {28},
keywords = {Aggregate computing, collective adaptive systems, distributed algorithms, field calculus, self-stabilisation, simulation and modeling}
}

@article{10.14778/3324301.3324308,
author = {Cao, Lei and Yan, Yizhou and Madden, Samuel and Rundensteiner, Elke A. and Gopalsamy, Mathan},
title = {Efficient discovery of sequence outlier patterns},
year = {2019},
issue_date = {April 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3324301.3324308},
doi = {10.14778/3324301.3324308},
abstract = {Modern Internet of Things (IoT) applications generate massive amounts of time-stamped data, much of it in the form of discrete, symbolic sequences. In this work, we present a new system called TOP that de&lt;u&gt;T&lt;/u&gt;ects &lt;u&gt;O&lt;/u&gt;utlier &lt;u&gt;P&lt;/u&gt;atterns from these sequences. To solve the fundamental limitation of existing pattern mining semantics that miss outlier patterns hidden inside of larger frequent patterns, TOP offers new pattern semantics based on contextual patterns that distinguish the independent occurrence of a pattern from its occurrence as part of its super-pattern. We present efficient algorithms for the mining of this new class of contextual patterns. In particular, in contrast to the bottom-up strategy for state-of-the-art pattern mining techniques, our top-down Reduce strategy piggy backs pattern detection with the detection of the context in which a pattern occurs. Our approach achieves linear time complexity in the length of the input sequence. Effective optimization techniques such as context-driven search space pruning and inverted index-based outlier pattern detection are also proposed to further speed up contextual pattern mining. Our experimental evaluation demonstrates the effectiveness of TOP at capturing meaningful outlier patterns in several real-world IoT use cases. We also demonstrate the efficiency of TOP, showing it to be up to 2 orders of magnitude faster than adapting state-of-the-art mining to produce this new class of contextual outlier patterns, allowing us to scale outlier pattern mining to large sequence datasets.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {920–932},
numpages = {13}
}

@inproceedings{10.5555/3091125.3091184,
author = {Baarslag, Tim and Alan, Alper T. and Gomer, Richard and Alam, Muddasser and Perera, Charith and Gerding, Enrico H. and schraefel, m.c.},
title = {An Automated Negotiation Agent for Permission Management},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The digital economy is based on data sharing yet citizens have little control about how their personal data is being used. While data management during web and app-based use is already a challenge, as the Internet of Things (IoT) scales up, the number of devices accessing and requiring personal data will go beyond what a person can manually assess in terms of data access requests. Therefore, new approaches are needed for managing privacy preferences at scale and providing active consent around data sharing that can improve fidelity of operation in alignment with user intent. To address this challenge, we introduce a novel agent-based approach to negotiate the permission to exchange private data between users and services. Our agent negotiates based on learned preferences from actual users. To evaluate our agent-based approach, we developed an experimental tool to run on people's own smartphones, where users were asked to share their private, real data (e.g. photos, contacts, etc) under various conditions. The agent autonomously negotiates potential agreements for the user, which they can refine by manually continuing the negotiation. The agent learns from these interactions and updates the user model in subsequent interactions. We find that the agent is able to effectively capture the preferences and negotiate on the user's behalf but, surprisingly, does not reduce user engagement with the system. Understanding how interaction interplays with agent-based automation is a key component to successful deployment of negotiating agents in real-life settings and within the IoT context in particular.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {380–390},
numpages = {11},
keywords = {automated negotiation, mobile apps, negotiation agent, negotiation cost, partial offers, permissions, preference learning, privacy},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@article{10.1145/3151006,
author = {Taherkordi, Amir and Eliassen, Frank and Mcdonald, Michael and Horn, Geir},
title = {Context-Driven and Real-Time Provisioning of Data-Centric IoT Services in the Cloud},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3151006},
doi = {10.1145/3151006},
abstract = {The convergence of Internet of Things (IoT) and the Cloud has significantly facilitated the provision and management of services in large-scale applications, such as smart cities. With a huge number of IoT services accessible through clouds, it is very important to model and expose cloud-based IoT services in an efficient manner, promising easy and real-time delivery of cloud-based, data-centric IoT services. The existing work in this area has adopted a uniform and flat view to IoT services and their data, making it difficult to achieve the above goal. In this article, we propose a software framework, Context-driven And Real-time IoT (CARIoT) for real-time provisioning of cloud-based IoT services and their data, driven by their contextual properties. The main idea behind the proposed framework is to structure the description of data-centric IoT services and their real-time and historical data in a hierarchical form in accordance with the end-user application’s context model. CARIoT features design choices and software services to realize this service provisioning model and the supporting data structures for hierarchical IoT data access. Using this approach, end-user applications can access IoT services and subscribe to their real-time and historical data in an efficient manner at different contextual levels, e.g., from a municipal district to a street in smart city use cases. We leverage a popular cloud-based data storage platform, called Firebase, to implement the CARIoT framework and evaluate its efficiency. The evaluation results show that CARIoT’s hierarchical structure imposes no additional overhead with less data notification delay as compared to existing flat structures.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {7},
numpages = {24},
keywords = {Internet of things, cloud computing, data-centric services}
}

@inproceedings{10.1145/3339252.3339272,
author = {Pour, Morteza Safaei and Mangino, Antonio and Friday, Kurt and Rathbun, Matthias and Bou-Harb, Elias and Iqbal, Farkhund and Shaban, Khaled and Erradi, Abdelkarim},
title = {Data-driven Curation, Learning and Analysis for Inferring Evolving IoT Botnets in the Wild},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3339272},
doi = {10.1145/3339252.3339272},
abstract = {The insecurity of the Internet-of-Things (IoT) paradigm continues to wreak havoc in consumer and critical infrastructure realms. Several challenges impede addressing IoT security at large, including, the lack of IoT-centric data that can be collected, analyzed and correlated, due to the highly heterogeneous nature of such devices and their widespread deployments in Internet-wide environments. To this end, this paper explores macroscopic, passive empirical data to shed light on this evolving threat phenomena. This not only aims at classifying and inferring Internet-scale compromised IoT devices by solely observing such one-way network traffic, but also endeavors to uncover, track and report on orchestrated "in the wild" IoT botnets. Initially, to prepare the effective utilization of such data, a novel probabilistic model is designed and developed to cleanse such traffic from noise samples (i.e., misconfiguration traffic). Subsequently, several shallow and deep learning models are evaluated to ultimately design and develop a multi-window convolution neural network trained on active and passive measurements to accurately identify compromised IoT devices. Consequently, to infer orchestrated and unsolicited activities that have been generated by well-coordinated IoT botnets, hierarchical agglomerative clustering is deployed by scrutinizing a set of innovative and efficient network feature sets. By analyzing 3.6 TB of recent darknet traffic, the proposed approach uncovers a momentous 440,000 compromised IoT devices and generates evidence-based artifacts related to 350 IoT botnets. While some of these detected botnets refer to previously documented campaigns such as the Hide and Seek, Hajime and Fbot, other events illustrate evolving threats such as those with cryptojacking capabilities and those that are targeting industrial control system communication and control services.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {6},
numpages = {10},
keywords = {Internet measurements, Internet-of-Things, IoT botnets, deep learning, network security, network telescopes},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3209582.3209590,
author = {Choi, Chang-Sik and Baccelli, Fran\c{c}ois and de Veciana, Gustavo},
title = {Densification Leveraging Mobility: An IoT Architecture Based on Mesh Networking and Vehicles},
year = {2018},
isbn = {9781450357708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209582.3209590},
doi = {10.1145/3209582.3209590},
abstract = {Disruptive changes are underway in the automotive industry as large-scale platforms based on vehicular fleets are deployed to deliver ride sharing and delivery services. Such platforms can also be leveraged to deliver wireless connectivity services, e.g., large-scale connectivity for the Internet of Things (IoT). This paper examines a network architecture based on a mesh of IoT devices, roadside repositories and vehicular mobile gateways -- referred to as mesh+vehicular. We propose a system-level model to study its relative merits versus conventional infrastructure-based IoT architectures-- referred to as mesh+cellular. The model reflects the salient properties of the architectures including the key interplay among the variability in the network geometries, routing trees, wireless capacity and eventually IoT queue stability.The paper provides an initial study of the scaling of the IoT sensing capacity of the routing trees per repository and base station respectively for the two architectures: i.e., the scaling the maximum common traffic rate the trees' IoT devices can generate while maintaining the stability of its queues. We then define the harvesting capacity per mobile gateway and base station in the two architectures, i.e., the average aggregate IoT rate each can extract assuming IoT devices are limited to their sensing capacity in each tree. Perhaps surprisingly, we show that as the spatial density λs of IoT devices and corresponding density of repositories along roads scale up, the proposed mesh+vehicular architecture has a gain in its harvesting capacity of order at least λsγ/4 where γ is the wireless path loss exponent. Underlying such gains is a fundamental shift in network geometry and information flows: in mesh+cellular systems IoT data is routed toward cells' sinks (zero-dimensional objects) while in mesh+vehicular data is routed to road induced cell edges (one-dimensional objects). Detailed system-level simulations validate the obtained scaling results.},
booktitle = {Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {71–80},
numpages = {10},
keywords = {Internet-of-Things, Kelly networks, Network capacity, Queuing theory, Radial spanning trees},
location = {Los Angeles, CA, USA},
series = {Mobihoc '18}
}

@inproceedings{10.1145/3078505.3080572,
author = {Borst, Sem},
title = {Delay Scalings and Mean-Field Limits in Networked Systems},
year = {2017},
isbn = {9781450350327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078505.3080572},
doi = {10.1145/3078505.3080572},
abstract = {Load balancing mechanisms and scheduling algorithms play a critical role in achieving efficient server utilization and providing robust delay performance in a wide range of networked systems. We will review some celebrated schemes and optimality results which typically assume that detailed state information, e.g. exact knowledge of queue lengths, is available in assigning jobs to queues or allocating a shared resource among competing users. In practice, however, obtaining such state information is non-trivial, and usually involves a significant communication overhead or delay, which is particularly a concern in large-scale networked systems with massive numbers of queues. These scalability issues have prompted increasing attention for the implementation complexity of load balancing and scheduling algorithms as a crucial design criterion, besides the traditional performance metrics. In this talk we examine the delay performance in such networks for various load balancing and scheduling algorithms, in conjunction with the associated implementation overhead. In the first part of the talk we focus on a scenario with a single dispatcher where jobs arrive that need to be assigned to one of several parallel queues. In the second part of the talk we turn to a system with a single resource, e.g. a shared wireless transmission medium, which is to be allocated among several nodes. We will specifically explore the delay scaling properties in a mean-field framework where the total load and service capacity grow large in proportion. The mean-field regime not only offers analytical tractability, but is also highly relevant given the immense numbers of servers in data centers and cloud networks, and dense populations of wireless devices and sensors in Internet-of-Things (IoT) applications. Time permitting, we will also discuss the impact of the underlying network structure and a few open research challenges.},
booktitle = {Proceedings of the 2017 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems},
pages = {1},
numpages = {1},
keywords = {cloud networks, data centers, delay scalings, distributed systems, large-scale networks, load balancing, mean-field limits, medium access, resource allocation, scheduling},
location = {Urbana-Champaign, Illinois, USA},
series = {SIGMETRICS '17 Abstracts}
}

@inproceedings{10.1145/3178298.3178301,
author = {Ali, Ahmed Abdullah and El-Dessouky, Iman A. and Abdallah, Mahmoud M. and Nabih, Azza K.},
title = {The Quest for Fully Smart Autonomous Business Networks in IoT Platforms},
year = {2017},
isbn = {9781450355124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178298.3178301},
doi = {10.1145/3178298.3178301},
abstract = {The evolution of the business networks is fostering the demand for more connected devices to execute tangled and sophisticated business operations. This leverages complicated business networks space to include scalable network layers, more devices, and platforms relying on Internet of Things (IoT) solutions. Despite the fact that traditional IoT platforms usually target the technical side for IoT applications, they are not prepared enough to be easily integrated with pluggable and executable business logic or smart contracts. This makes it difficult to control a collection of shared business network resources in a standard and decentralized manner. With the rise of FinTech due to blockchain technology, it becomes possible to seamlessly engage business networks with financial digital assets. Consequently, combining IoT platforms with blockchain will drive new ways for better services consuming, transparency and products that depend on crowd-based economy [12]. In this paper, Sitechain is proposed as a new architecture to integrate IoT platforms with blockchain technology. The proposed architecture is not locked to specific IoT platforms but it can be extended to support different platforms in a standard, systematic and easy way. Sitechain is demonstrated by integrating Sitewhere and FIWARE IoT platforms with Hyperledger Fabric as a private blockchain network manager. Moreover a modeling language supported by Hyperledger composer is used to easily develop smart contracts and generate RESTful APIs, therefore any smart contract transaction events can be mapped into actions on remote devices.The system testing is performed using a large set of connected shared devices that are deployed as part of private business networks. The results are recorded in the conclusion section.},
booktitle = {Proceedings of the 3rd Africa and Middle East Conference on Software Engineering},
pages = {13–18},
numpages = {6},
keywords = {Blockchain, Consensus, Distributed Ledger, HyperLedger, Internet of Things, IoT Platforms, Smart Contracts},
location = {Cairo, Egypt},
series = {AMECSE '17}
}

@inproceedings{10.1145/3332448.3332452,
author = {Cardenas, D. Jonathan Sebastian and Hahn, Adam},
title = {IoT Threats to the Smart Grid: A Framework for Analyzing Emerging Risks},
year = {2019},
isbn = {9781450366144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332448.3332452},
doi = {10.1145/3332448.3332452},
abstract = {Internet of Things (IoT) technologies have experienced an unprecedented growth over the last decade due to their wide applicability and low overall costs. These same factors have also allowed IoT deployments to transition into industrial environments which are expected to meet high reliability requirements. These technological-base shifts have raised operational (safety) concerns among researchers, which have been further fueled by high-profile cyber incidents that have exposed vulnerabilities in field devices. Although, multiple IoT cyber security issues are being actively researched, a compelling issue is to identify threats associated with cross-domain devices. A cross-domain vulnerability is any such vulnerability that can cause other non-IT system to experience unintended consequences.Recent research has shown that physical systems, like the electrical grid might be exposed to abnormal conditions if a large set of load-controllable IoT devices are compromised [5]. Although risk reduction methodologies for bulk power components have been proposed these are focused towards SCADA-based systems which are owned, maintained, and operated by the utility under controlled environments. Whereas, IoT devices are owned by customers, spread across a wide service area, without supervised security policies.In this work, a detailed analysis of such threats is performed on large-scale IoT deployments that could allow attackers to control a large amount of aggregated power. Based on our research, future-growth of large-load controllers and smart inverters could pose threats to grid operations due to their rapid load changing capabilities, these changes could exceed steady-state or transient design limits leading to unintended consequences. Therefore utilities need to methodologically analyzing these risks. This paper proposes such a methodology, which includes risk modeling and mitigation strategies.},
booktitle = {Proceedings of the Northwest Cybersecurity Symposium},
articleno = {1},
numpages = {8},
keywords = {Cybersecurity, Distributed Energy Resources, Internet of Things},
location = {Richland, WA, USA},
series = {NCS '19}
}

@inproceedings{10.1145/3210240.3210320,
author = {Wu, Fang-Jing and Solmaz, G\"{u}rkan},
title = {CrowdEstimator: Approximating Crowd Sizes with Multi-modal Data for Internet-of-Things Services},
year = {2018},
isbn = {9781450357203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210240.3210320},
doi = {10.1145/3210240.3210320},
abstract = {Crowd mobility has been paid attention for the Internet-of-things (IoT) applications. This paper addresses the crowd estimation problem and builds an IoT service to share the crowd estimation results across different systems. The crowd estimation problem is to approximate the crowd size in a targeted area using the observed information (e.g., Wi-Fi data). This paper exploits Wi-Fi probe request packets ("Wi-Fi probes" for short) broadcasted by mobile devices to solve this problem. However, using only Wi-Fi probes to estimate the crowd size may result in inaccurate results due to various environmental uncertainties which may lead to crowd overestimation or underestimation. Moreover, the ground-truth is unavailable because the coverage of Wi-Fi signals is time-varying and invisible. This paper introduces auxiliary sensors, stereoscopic cameras, to collect the near ground-truth at a specified calibration choke point. Two calibration algorithms are proposed to solve the crowd estimation problem. The key idea is to calibrate the Wi-Fi-only crowd estimation based on the correlations between the two types of data modalities. Then, to share the calibrated results across systems required by different stakeholders, our system is integrated with the FIWARE-based IoT platform. To verify the proposed system, we have launched an indoor pilot study in the Wellington Railway Station and an outdoor pilot study in the Christchurch Re:START Mall in New Zealand. The large-scale pilot studies show that stereoscopic cameras can reach minimum accuracy of 85% and high precision detection for providing the near ground-truth. The proposed calibration algorithms reduce estimation errors by 43.68% on average compared to the Wi-Fi-only approach.},
booktitle = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {337–349},
numpages = {13},
keywords = {cyber-physical systems, human mobility, smart cities},
location = {Munich, Germany},
series = {MobiSys '18}
}

@article{10.1145/3329168,
author = {Min, Weiqing and Jiang, Shuqiang and Liu, Linhu and Rui, Yong and Jain, Ramesh},
title = {A Survey on Food Computing},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3329168},
doi = {10.1145/3329168},
abstract = {Food is essential for human life and it is fundamental to the human experience. Food-related study may support multifarious applications and services, such as guiding human behavior, improving human health, and understanding the culinary culture. With the rapid development of social networks, mobile networks, and Internet of Things (IoT), people commonly upload, share, and record food images, recipes, cooking videos, and food diaries, leading to large-scale food data. Large-scale food data offers rich knowledge about food and can help tackle many central issues of human society. Therefore, it is time to group several disparate issues related to food computing. Food computing acquires and analyzes heterogenous food data from different sources for perception, recognition, retrieval, recommendation, and monitoring of food. In food computing, computational approaches are applied to address food-related issues in medicine, biology, gastronomy, and agronomy. Both large-scale food data and recent breakthroughs in computer science are transforming the way we analyze food data. Therefore, a series of works has been conducted in the food area, targeting different food-oriented tasks and applications. However, there are very few systematic reviews that shape this area well and provide a comprehensive and in-depth summary of current efforts or detail open problems in this area. In this article, we formalize food computing and present such a comprehensive overview of various emerging concepts, methods, and tasks. We summarize key challenges and future directions ahead for food computing. This is the first comprehensive survey that targets the study of computing technology for the food area and also offers a collection of research studies and technologies to benefit researchers and practitioners working in different food-related fields.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {92},
numpages = {36},
keywords = {Food computing, food perception, food recognition, food retrieval, health, monitoring, recipe analysis, recipe recommendation, survey}
}

@article{10.1145/3224429,
author = {Borst, Sem and Zubeldia, Martin},
title = {Delay Scaling in Many-Sources Wireless Networks without Queue State Information},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3224429},
doi = {10.1145/3224429},
abstract = {We examine a canonical scenario where several wireless data sources generate sporadic delay-sensitive messages that need to be transmitted to a common access point. The access point operates in a time-slotted fashion, and can instruct the various sources in each slot with what probability to transmit a message, if they have any. When several sources transmit simultaneously, the access point can detect a collision, but is unable to infer the identities of the sources involved. While the access point can use the channel activity observations to obtain estimates of the queue states at the various sources, it does not have any explicit queue length information otherwise.We explore the achievable delay performance in a regime where the number of sources n grows large while the relative load remains fixed. This scaling is particularly pertinent in Internet of Things (IoT) scenarios, where a key challenge is to achieve low delay when the overall traffic activity is dispersed across massive numbers of highly intermittent sources.We establish that, under any medium access algorithm without queue state information, the average delay must be at least of the order of n slots when the load exceeds some threshold λ* &lt; 1$. This demonstrates that bounded delay can only be achieved if a positive fraction of the system capacity is sacrificed. Furthermore, we introduce a scalable Two-Phase algorithm for low-delay IoT applications which achieves a delay upper bounded uniformly in n when the load is below e-1, and a delay of the order of n slots when the load is between e-1 and 1. Additionally, this algorithm provides robustness against correlated source activity, which is also prevalent in IoT scenarios.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {jun},
articleno = {34},
numpages = {45},
keywords = {delay scaling, internet-of-things, medium access, performance tradeoffs, scheduling policies}
}

@inproceedings{10.1145/3213846.3213855,
author = {Khazem, Kareem and Barr, Earl T. and Hosek, Petr},
title = {Making data-driven porting decisions with Tuscan},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213855},
doi = {10.1145/3213846.3213855},
abstract = {Software typically outlives the platform that it was originally written for. To smooth the transition to new tools and platforms, programs should depend on the underlying platform as little as possible. In practice, however, software build processes are highly sensitive to their build platform, notably the implementation of the compiler and standard library. This makes it difficult to port existing, mature software to emerging platforms---web based runtimes like WebAssembly, resource-constrained environments for Internet-of-Things devices, or innovative new operating systems like Fuchsia.  We present Tuscan, a framework for conducting automatic, deterministic, reproducible tests on build systems. Tuscan is the first framework to solve the problem of reproducibly testing builds cross-platform at massive scale. We also wrote a build wrapper, Red, which hijacks builds to tolerate common failures that arise from platform dependence, allowing the test harness to discover errors later in the build. Authors of innovative platforms can use Tuscan and Red to test the extent of unportability in the software ecosystem, and to quantify the effort necessary to port legacy software.  We evaluated Tuscan by building an operating system distribution, consisting of 2,699 Red-wrapped programs, on four platforms, yielding a `catalog' of the most common portability errors. This catalog informs data-driven porting decisions and motivates changes to programs, build systems, and language standards; systematically quantifies problems that platform writers have hitherto discovered only on an ad-hoc basis; and forms the basis for a common substrate of portability fixes that developers can apply to their software.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {276–286},
numpages = {11},
keywords = {build systems, portability, toolchains},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/2976749.2978335,
author = {Ambrosin, Moreno and Conti, Mauro and Ibrahim, Ahmad and Neven, Gregory and Sadeghi, Ahmad-Reza and Schunter, Matthias},
title = {SANA: Secure and Scalable Aggregate Network Attestation},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978335},
doi = {10.1145/2976749.2978335},
abstract = {Large numbers of smart connected devices, also named as the Internet of Things (IoT), are permeating our environments (homes, factories, cars, and also our body - with wearable devices) to collect data and act on the insight derived. Ensuring software integrity (including OS, apps, and configurations) on such smart devices is then essential to guarantee both privacy and safety. A key mechanism to protect the software integrity of these devices is remote attestation: A process that allows a remote verifier to validate the integrity of the software of a device. This process usually makes use of a signed hash value of the actual device's software, generated by dedicated hardware. While individual device attestation is a well-established technique, to date integrity verification of a very large number of devices remains an open problem, due to scalability issues. In this paper, we present SANA, the first secure and scalable protocol for efficient attestation of large sets of devices that works under realistic assumptions. SANA relies on a novel signature scheme to allow anyone to publicly verify a collective attestation in constant time and space, for virtually an unlimited number of devices. We substantially improve existing swarm attestation schemes by supporting a realistic trust model where: (1) only the targeted devices are required to implement attestation; (2) compromising any device does not harm others; and (3) all aggregators can be untrusted. We implemented SANA and demonstrated its efficiency on tiny sensor devices. Furthermore, we simulated SANA at large scale, to assess its scalability. Our results show that SANA can provide efficient attestation of networks of 1,000,000 devices, in only 2.5 seconds.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {731–742},
numpages = {12},
keywords = {collective attestation, denial of service (dos), malware infestation, network protocol, remote attestation},
location = {Vienna, Austria},
series = {CCS '16}
}

@article{10.1145/3166070,
author = {Mezghani, Emna and Exposito, Ernesto and Drira, Khalil},
title = {An Autonomic Cognitive Pattern for Smart IoT-Based System Manageability: Application to Comorbidity Management},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3166070},
doi = {10.1145/3166070},
abstract = {The adoption of the Internet of Things (IoT) drastically witnesses an increase in different domains and contributes to the fast digitalization of the universe. Henceforth, next generation of IoT-based systems are set to become more complex to design and manage. Collecting real-time IoT-generated data unleashes a new wave of opportunities for business to take more precise and accurate decisions at the right time. However, a set of challenges, including the design complexity of IoT-based systems and the management of the ensuing heterogeneous big data as well as the system scalability, need to be addressed for the development of flexible smart IoT-based systems. Consequently, we proposed a set of design patterns that diminish the system design complexity through selecting the appropriate combination of patterns based on the system requirements. These patterns identify four maturity levels for the design and development of smart IoT-based systems. In this article, we are mainly dealing with the system design complexity to manage the context changeability at runtime. Thus, we delineate the autonomic cognitive management pattern, which is at the most mature level. Based on the autonomic computing, this pattern identifies a combination of management processes able to continuously detect and manage the context changes. These processes are coordinated based on cognitive mechanisms that allow the system perceiving and understanding the meaning of the received data to make business decisions, as well as dynamically discovering new processes that meet the requirements evolution at runtime. We demonstrated the use of the proposed pattern with a use case from the healthcare domain; more precisely, the patient comorbidity management based on wearables.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {8},
numpages = {17},
keywords = {IoT-based system, autonomic computing, cognitive computing, design patterns, healthcare, maturity level}
}

@inproceedings{10.1145/3321408.3321596,
author = {Zhang, Ruizhi and Tian, Chujie and Ji, Yang},
title = {An emotion recognition system based on edge computing and cloud computing applied in the kindergarten},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3321596},
doi = {10.1145/3321408.3321596},
abstract = {As much attention is paid to the safety and education of children in the kindergarten, the importance of analyzing emotions of children is realized. With the adventure of the Internet of Things, more and more visual sensors are connected to the network, which makes the emotion analysis available through the recognition of images. It is necessary to response quickly when negative emotions of children are recognized, which means the processing of images containing negative emotions is delay-sensitive. The processing of an enormous amount of images brings challenge for emotion recognition based on traditional cloud computing. Cloud computing is of high scalability and sufficient capability, but cannot meet the need of real-time detection due to the time delay of transmission and computing of large amounts of images. Emerging edge computing can help to solve the problem by offloading computing from cloud center to edge nodes. However, because the resource of edge nodes is limited, only part of the computing can be placed at the edge nodes. In our work, an emotion recognition system combined with edge computing and cloud computing is proposed to apply in the kindergarten. Due to delay-sensitive negative emotions in this case, a priority scheduling algorithm is also proposed to improve the computing effectiveness of the edge. In this way, the low latency of the edge and the high scalability of the cloud are utilized to reduce the time delay and improve the capacity of the emotion recognition system. The experiments in the real scene of the kindergarten verify that the system performs well.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {34},
numpages = {5},
keywords = {cloud computing, edge computing, emotion recognition, kindergarten, priority scheduling, time delay},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3147213.3155013,
author = {Chang, Wo},
title = {NIST Big Data Reference Architecture for Analytics and Beyond},
year = {2017},
isbn = {9781450351492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147213.3155013},
doi = {10.1145/3147213.3155013},
abstract = {Big Data is the term used to describe the deluge of data in our networked, digitized, sensor-laden, information driven world. There is a broad agreement among commercial, academic, and government leaders about the remarkable potential of "Big Data" to spark innovation, fuel commerce, and drive progress. The availability of vast data resources carries the potential to answer questions previously out of reach. However, there is also broad agreement on the ability of Big Data to overwhelm traditional approaches. Big Data architectures come in many shapes and forms ranging from academic research settings to product-oriented workflows. With massive-scale dynamic data being generate from social media, Internet of Things, Smart Cities, and others, it is critical to analyze these data in real-time and provide proactive decision. With the advancement of computer architecture in multi-cores and GPUs, and fast communication between CPUs and GPUs, parallel processing utilizes these platforms could optimize resources at a reduced time. This presentation will provide the past, current, and future activities of the NIST Big Data Public Working Group (NBD-PWG) and how the NIST Reference Architecture may address the rate at which data volumes, speeds, and complexity are growing requires new forms of computing infrastructure to enable Big Data analytics interoperability such that analytics tools can be re-usable, deployable, and operational. The focus of NBD-PWG is to form a community of interest from industry, academia, and government, with the goal of developing consensus definitions, taxonomies, secure reference architectures, and standards roadmap which would create vendor-neutral, technology and infrastructure agnostic framework. The aim is to enable Big Data stakeholders to pick-and-choose best analytics tools for their processing under the most suitable computing platforms and clusters while allowing value-additions from Big Data service providers and flow of data between the stakeholders in a cohesive and secure manner.},
booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {3},
numpages = {1},
keywords = {big data analytics, big data reference architecture, high-performance computing, many cpus/cores/gpus},
location = {Austin, Texas, USA},
series = {UCC '17}
}

@inproceedings{10.1145/3307650.3322215,
author = {Liu, Zhenhong and Yazdanbakhsh, Amir and Wang, Dong Kai and Esmaeilzadeh, Hadi and Kim, Nam Sung},
title = {AxMemo: hardware-compiler co-design for approximate code memoization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322215},
doi = {10.1145/3307650.3322215},
abstract = {Historically, continuous improvements in general-purpose processors have fueled the economic success and growth of the IT industry. However, the diminishing benefits from transistor scaling and conventional optimization techniques necessitates moving beyond common practices. Approximate computing is one such unconventional technique that has shown promise in pushing the boundaries of general-purpose processing. This paper sets out to employ approximation for processors that are commonly used in cyber-physical domains and may become building blocks of Internet of Things. To this end, we propose AxMemo to exploit the computation redundancy that stems from data similarity in the inputs of code blocks. Such input behavior is prevalent in cyber-physical systems as they deal with real-world data that naturally harbors redundancy. Therefore, in contrast to existing memoization techniques that replace costly floating-point arithmetic operations with limited number of inputs, AxMemo focuses on memoizing blocks of code with potentially many inputs. As such, AxMemo aims to replace long sequences of instructions with a few hash and lookup operations. By reducing the number of dynamic instructions, AxMemo alleviates the von Neumann and execution overheads of passing instructions through the processor pipeline altogether. The challenge AxMemo facing is to provide low-cost hashing mechanisms that can generate rather unique signature for each multi-input combination. To address this challenge, we develop a novel use of Cyclic Redundancy Checking (CRC) to hash the inputs. To increase lookup table hit rate, AxMemo employs a two-level memoization lookup, which utilizes small dedicated SRAM and spare storage in the last level cache. These solutions enable AxMemo to efficiently memoize relatively large code regions with variable input sizes and types using the same underlying hardware. Our experiment shows that AxMemo offers 2.64\texttimes{} speedup and 2.58 \texttimes{} energy reduction with mere 0.2% of quality loss averaged across ten benchmarks. These benefits come with an area overhead of just 2.1%.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {685–697},
numpages = {13},
keywords = {approximate computing, hardware-software co-design, memoization},
location = {<conf-loc>, <city>Phoenix</city>, <state>Arizona</state>, </conf-loc>},
series = {ISCA '19}
}

@article{10.1109/TNET.2018.2879979,
author = {Yu, Jihong and Gong, Wei and Liu, Jiangchuan and Chen, Lin and Wang, Kehao},
title = {On Efficient Tree-Based Tag Search in Large-Scale RFID Systems},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2879979},
doi = {10.1109/TNET.2018.2879979},
abstract = {Tag search, which is to find a particular set of tags in a radio frequency identification RFID system, is a key service in such important Internet-of-Things applications as inventory management. When the system scale is large with a massive number of tags, deterministic search can be prohibitively expensive, and probabilistic search has been advocated, seeking a balance between reliability and time efficiency. Given a failure probability $frac {1}{mathcal {O}K}$ , where $K$ is the number of tags, state-of-the-art solutions have achieved a time cost of $mathcal {O}K log K$ through multi-round hashing and verification. Further improvement, however, faces a critical bottleneck of repetitively verifying each individual target tag in each round. In this paper, we present an efficient tree-based tag search TTS that approaches $mathcal {O}K$ through batched verification. The key novelty of TTS is to smartly hash multiple tags into each internal tree node and adaptively control the node degrees. It conducts bottom–up search to verify tags group by group with the number of groups decreasing rapidly. Furthermore, we design an enhanced tag search scheme, referred to as TTS+, to overcome the negative impact of asymmetric tag set sizes on time efficiency of TTS. TTS+ first rules out partial ineligible tags with a filtering vector and feeds the shrunk tag sets into TTS. We derive the optimal hash code length and node degrees in TTS to accommodate hash collisions and the optimal filtering vector size to minimize the time cost of TTS+. The superiority of TTS and TTS+ over the state-of-the-art solution is demonstrated through both theoretical analysis and extensive simulations. Specifically, as reliability demand on scales, the time efficiency of TTS+ reaches nearly 2 times at most that of TTS.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {42–55},
numpages = {14}
}

@inproceedings{10.1145/2896387.2900336,
author = {Bounceur, Ahc\`{e}ne},
title = {CupCarbon: A New Platform for Designing and Simulating Smart-City and IoT Wireless Sensor Networks (SCI-WSN)},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900336},
doi = {10.1145/2896387.2900336},
abstract = {The proliferation of radio communication systems and the significant advances in enabling device technologies are paving towards Internet-of-Things (IoT) and opening new horizons for Smart City applications and its services. Such evolution becomes essential in order to enhance quality of urban services, to reduce costs, and to engage citizens more actively. In this context, novel simulation tools are required to prepare the future deployments of large-scale IoT infras tructure for Smart cities in the best conditions in terms of reliability, energy consumption, and cost. This keynote session presents the CupCarbon1 framework: a platform for designing smart-city and IoT Wireless Sensor Networks (SCIWSN). CupCarbon aims to provide following benefits that makes it significant from the other conventional wireless sensor network simulators.(1) provides modeling and simulation of radio propagation channel and alpha-stable noise based interferences in more realistic way,(2) takes into account the deployment environment and quantify the uncertainty of simulations,(3) allows the representation of mobile nodes and dynamic environments,(4) allows the behavioural study of a network or networks with large number of nodes in practical environments (city, mountain, etc.).The CupCarbon simulator allows it's user to design, visualize, debug and validate distributed algorithms for monitoring environmental data collections of wireless sensor network. It creates environmental scenarios such as fires, gas, mobiles, and generally within educational and scientific projects. It offers two different simulation environments. First is a multi-agent environment that enables the design of mobility scenarios and the generation of events such as fires and gas as well as the simulation of mobile nodes. Second environment represents a discrete event simulation of wireless sensor networks which also takes into account the scenario designed on the basis of the first environment.Interference models based on the impulsive nature of noise and outdoor propagation models are embedded within Cup-Carbon to provide more realistic analysis of WSNs for smart city applications. These models are associated with spatial zones according to the electromagnetic interactions.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {1},
numpages = {1},
keywords = {CupCarbon simulator, alpha-stable distribution, interference, radio propagation channel, visibility tree},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@article{10.1145/3214268,
author = {Fan, Xiaoran and Ding, Han and Li, Sugang and Sanzari, Michael and Zhang, Yanyong and Trappe, Wade and Han, Zhu and Howard, Richard E.},
title = {Energy-Ball: Wireless Power Transfer for Batteryless Internet of Things through Distributed Beamforming},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3214268},
doi = {10.1145/3214268},
abstract = {Wireless power transfer (WPT) promises to deliver energy to devices that are otherwise hard to charge or replace batteries for. This paper presents a new power transfer approach by aligning the phases of a collection of radio frequency (RF) energy chargers at the target receiver device. Our approach can ship energy over tens of meters and to mobile targets. More importantly, our approach leads to a highly asymmetric energy density distribution in the charging area: the energy density at the target receiver is much higher than the energy density at other locations. It is a departure from existing beamforming based WPT systems that have high energy along the energy beam path. Such a technology can enable a large array of batteryless Internet of Things applications and render them much more robust and long-running. Thanks to its asymmetric energy distribution, our approach potentially can be scaled up to ship higher level of energy over longer distances.In this paper, we design, prototype, and evaluate the proposed energy transfer approach, referred to as Energy-Ball. We implement an Energy-Ball testbed that consists of 17 N210 and 4 B210 Universal Software Radio Peripheral (USRP) nodes, yielding a 20 x 20 m2 energy delivery area. We conduct carefully designed experiments on the testbed. We demo that the energy density of Energy-Ball at the target spot is considerably higher than the energy density elsewhere, with the peak to average power ratio of 8.72. We show that Energy-Ball can transfer energy to any point within the area. When the receiver moves at a speed of 0.5 m/s, Energy-Ball can transfer 80% of optimal power to the mobile receiver. Further, our results also show Energy-Ball can deliver over 0.6mw RF power that enables batteryless sensors at any point across the area.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jul},
articleno = {65},
numpages = {22},
keywords = {Batteryless IoT, Distributed Beamforming, Mobile Receiver, Wireless power transfer}
}

@article{10.1145/3322497,
author = {Liu, Xuecheng and Fu, Luoyi and Wang, Jiliang and Wang, Xinbing and Chen, Guihai},
title = {Multicast Scaling of Capacity and Energy Efficiency in Heterogeneous Wireless Sensor Networks},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3322497},
doi = {10.1145/3322497},
abstract = {Motivated by the requirement of heterogeneity in the Internet of Things, we initiate the joint study of capacity and energy efficiency scaling laws in heterogeneous wireless sensor networks, and so on. The whole network is composed of n nodes scattered in a square region with side length L = nα, and there are m = nν home points {cj}j=1m, where a generic home point cj generates qj nodes independently according to a stationary and rotationally invariant kernel k(cj, ⋅). Among the n nodes, we schedule ns independent multicast sessions each consisting of k − 1 destination nodes and one source node. According to the heterogeneity of nodes’ distribution, we classify the network into two regimes: a cluster-dense regime and a cluster-sparse regime. For the cluster-dense regime, we construct single layer highway system using percolation theory and then build the multicast spanning tree for each multicast session. This scheme yields the Ω(n½+(α − ½)γ/ns√k) per-session multicast capacity. For the cluster-sparse regime, we partition the whole network plane into several layers and construct nested highway systems. The similar multicast spanning tree yields the Ω(n½−(1− ν)γ/2/ns√k) per-session multicast capacity, where γ is the power attenuation factor. Interestingly, we find that the bottleneck of multicast capacity attributes to the network region with largest node density, which provides a guideline for the deployment of sensor nodes in large-scale sensor networks. We further analyze the upper bound of multicast capacity and the per-session multicast energy efficiency. Using both synthetic networks and real-world networks (i.e., Greenorbs), we evaluate the asymptotic capacity and energy efficiency and find that the theoretical scaling laws are gracefully supported by the simulation results. To our best knowledge, this is the first work verifying the scaling laws using real-world large-scale sensor network data.},
journal = {ACM Trans. Sen. Netw.},
month = {may},
articleno = {33},
numpages = {32},
keywords = {Wireless sensor networks, capacity, energy efficiency, multicast, scaling law, spatial heterogeneity}
}

@techreport{10.5555/3316807,
author = {Amvrosiadis, George and Butt, Ali R. and Tarasov, Vasily and Zadok, Erez and Zhao, Ming and Ahmad, Irfan and Arpaci-Dusseau, Remzi H. and Chen, Feng and Chen, Yiran and Chen, Yong and Cheng, Yue and Chidambaram, Vijay and Da Silva, Dilma and Demke-Brown, Angela and Desnoyers, Peter and Flinn, Jason and He, Xubin and Jiang, Song and Kuenning, Geoff and Li, Min and Maltzahn, Carlos and Miller, Ethan L. and Mohror, Kathryn and Rangaswami, Raju and Reddy, Narasimha and Rosenthal, David and Tosun, Ali Saman and Talagala, Nisha and Varman, Peter and Vazhkudai, Sudharshan and Waldani, Avani and Zhang, Xiaodong and Zhang, Yiying and Zheng, Mai},
title = {Data Storage Research Vision 2025: Report on NSF Visioning Workshop held May 30--June 1, 2018},
year = {2018},
publisher = {National Science Foundation},
address = {USA},
abstract = {With the emergence of new computing paradigms (e.g., cloud and edge computing, big data, Internet of Things (IoT), deep learning, etc.) and new storage hardware (e.g., non-volatile memory (NVM), shingled-magnetic recording (SMR) disks, and kinetic drives, etc.), a number of open challenges and research issues need to be addressed to ensure sustained storage systems efficacy and performance. The wide variety of applications demand that the fundamental design of storage systems should be revisited to support application-specific and application-defined semantics. Existing standards and abstractions need to be reevaluated; new sustainable data representations need to be designed to support emerging applications. To take advantage of hardware advancements, new storage software designs are also necessary in order to maximize overall system efficiency and performance.Therefore, there is a urgent need for a consolidated effort to identify and establish a vision for storage systems research and comprehensive techniques that provide practical solutions to the storage issues facing the information technology community. To address this need, the National Science Foundation's (NSF) "Visioning Workshop on Data Storage Research 2025" brought together a number of storage researchers from academia, industry, national laboratories, and federal agencies to develop a collective vision for future storage research, as well as to prioritize near-term and long-term storage research and scientific investigations. In-depth discussions were carried out at the workshop along four major themes: (1) Storage for Cloud, Edge, and IoT Systems; (2) AI and Storage; (3) Rethinking Storage Systems Design; and (4) Evolution of Storage Systems with Emerging Hardware. The participants especially underscored the need for focused educational and training activities to instill storage system tools and technologies in the next generation of researchers and IT practitioners. Finally, the development of shared, scalable, and flexible community infrastructure to enable and sustain innovative storage research and verifiable evaluation was also discussed. This report presents the findings from these discussions.}
}

@inproceedings{10.1145/2808783.2808795,
author = {Yim, Kangbin and Castiglione, Aniello and Yi, Jeong Hyun and Migliardi, Mauro and You, Ilsun},
title = {Cyber Threats to Industrial Control Systems},
year = {2015},
isbn = {9781450338240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808783.2808795},
doi = {10.1145/2808783.2808795},
abstract = {Cyber attacks have been a crucial problem on the Internet for more than a couple of decades. As information and communication technologies have been applied to the field systems in the traditional industries, they also have become connected and more vulnerable to the existing cyber attack techniques. To make matters worse, a practical malicious software suite was exposed from Italian hacking team by another leakage incident, which is noted by somebody as an insider's rip. Now, we worry about social problems caused by copycats. Nevertheless, more concerns have been arisen among experts about that this tendency is getting more serious when IoT (Internet of Things) environment has been realized. In an IoT environment, every device will be massively connected to each other. We can classify the IoT devices roughly into two categories, which are leaf devices and gateways. In this environment, anonymous devices may usually participate as a authorized consumer or producer for a service in the same domain with a gateway. The gateway may take a role for some devices as a delegate for specific services. Insider's exposure problem will be more serious concern in this situation. At a user's point of view, smart-phones for wearable devices, the AVN (Audio Visual Navigation) on smart cars and HMI host for ICS (Industrial Control Systems) are considered as gateways for IoT environments. In this situation, a canonical form of large scale IoT devices should be smart cars. The smart cars are going to include more software-incorporated electric components for intelligent safety functions adopted at the vacancy of the combustion engine and are also becoming connected and remotely controllable. This means these mobile devices cannot be free from the cyber security threats, which is solicited by insider devices [Stephen:CEA]. For a recent example, commercial networked vehicles were remotely hacked and controlled without any wired connections. In this panel, we will discuss current hot issues related to the malicious software in various aspects. In the existing mobile network and the connected smart devices, Android-based mobile malware is a big trouble. Combined malware is another problem among the emerging all-connected ICS equipment. Detecting these malicious behavior is one of the new IT challenges. We will share perspective ideas related to these topics among panelists and audiences through a professional discussion.},
booktitle = {Proceedings of the 7th ACM CCS International Workshop on Managing Insider Security Threats},
pages = {79–81},
numpages = {3},
keywords = {cyber-physical systems, industrial control systems, internet of things, malware, vulnerability},
location = {Denver, Colorado, USA},
series = {MIST '15}
}

@proceedings{10.1145/2668930,
title = {ICPE '15: Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This year is the 6th International Conference on Performance Engineering - ICPE'15, which grew out of the ACM Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance Engineering Workshop (SIPEW since 2008), taking place in Austin. It is a great pleasure for us to offer an outstanding technical program, which we believe is reflecting the idea behind the ICPE - integrating theory and practice in the field of performance engineering.Overall, we received 116 submissions across all tracks. The main research track attracted 56 submissions, each paper was reviewed by at least four reviewers and we finally accepted 15 high-quality submissions as full papers and three as short papers. The Industry and Experience Track received 18 submissions, of which 7 were selected for inclusion in the program. Further, the program committee selected 9 submissions for the Vision/Work-in-Progress track. Six tutorials and a poster &amp; demo exhibition are completing the program. As in previous years, we decided to group the submissions according their topics, not the track they were submitted to. We believe that the exchange between industry and academia is fostered by presenting research and industry submissions together in the same session.The program covers all traditional ICPE topics such as software performance engineering and benchmarking with a good balance between theoretical and practical contributions. Additionally, we saw an increasing number of submissions discussing how to apply performance engineering techniques in the context of big data, cloud and Internet of things. We are very proud to have two excellent keynote speakers as part of our program: Todd Austin, from University of Michingan, USA, talking about Bridging the Moore's Law Performance Gap with Innovation Scaling andAdrian Cockcroft, from Battery Ventures, USA, talking about Cloud Native Cost Optimization.There are three co-located workshops extending the program -- the 4th International Workshop on Large-Scale Testing (LT 2015), the 1st Workshop on Performance Analysis of Big data Systems (PABS) and the 1st Workshop on Challenges in Performance Methods for Software Development (WOSP-C 2015). We are confident that the program will provide you with many new ideas and encourage the discussion and exchange between the participants.},
location = {Austin, Texas, USA}
}

@inproceedings{10.1145/3360322.3360848,
author = {Aryal, Ashrant and Becerik-Gerber, Burcin},
title = {Skin Temperature Extraction Using Facial Landmark Detection and Thermal Imaging for Comfort Assessment},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360848},
doi = {10.1145/3360322.3360848},
abstract = {Despite the large share of energy consumption, current HVAC systems in buildings fail to meet their primary purpose of maintaining comfortable indoor conditions. Current "one size fits all" approach to control the thermal conditions in an environment lead to a high degree of occupant dissatisfaction. Advancements in Internet of Things and Machine Learning have opened the possibility of deploying different sensors at a wide scale to monitor environmental and physiological information and using collected sensor data to model individual comfort requirements. Thermal imaging has recently gained interest as one of the possible ways to monitor physiological information (skin temperature) for thermal comfort assessment. Previous studies have shown that skin temperatures from different regions of the face, such as forehead, nose, cheeks and ears can provide useful information for predicting thermal sensation at an individual level. However, existing approaches to process thermal images either rely on manual temperature extraction or use methods that are less reliable in accurately identifying different facial regions. One of the major challenges of using thermal imaging for monitoring skin temperatures in actual buildings is that occupants may move relative to the camera. It is not practical to expect building occupants to be oriented facing the cameras at all times, therefore, it is important to be able to extract as much information as possible from instances where it is feasible to extract relevant information. In this paper, we describe an approach to extract skin temperature by locating specific regions of the face in thermal images. The approach involves combining data from RGB images with thermal images and leveraging facial landmark detection in RGB images. We also evaluate our approach with existing approach of face detection used in previous studies. Our study demonstrates that facial landmark detection provides a more accurate calculation of different locations in the face compared to previous studies. We show an improvement in overall quantity and quality of temperature measurements extracted from thermal images compared to previous studies. More accurate temperature measurements from thermal images can improve the accuracy of thermal imaging for modeling and predicting thermal comfort.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {71–80},
numpages = {10},
keywords = {Infrared thermography, facial landmarks, thermal comfort, thermal imaging},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@proceedings{10.1145/3194696,
title = {SEHS '18: Proceedings of the International Workshop on Software Engineering in Healthcare Systems},
year = {2018},
isbn = {9781450357340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the workshop on Software Engineering in Healthcare Systems, co-located with ICSE 2018 in Gothenburg, Sweden! This year marks the tenth anniversary of holding an event at ICSE dedicated to discussing experiences, challenges and solutions related to engineering software in healthcare applications. Given the dramatic changes associated with digitalizing healthcare at a global scale, ICSE 2008 featured an Experience Track on Software Engineering in Health Care, an event that gave birth to a subsequent series of workshops and symposia. Ten years later, software-intensive systems and systems of systems have become pivotal components of most modern healthcare organizations. Still, healthcare systems continue to undergo significant transformations due to the innovation of new technologies and paradigms such as connected medical devices, the Internet of Things (IoT), Big Data analytics and context-aware, adaptive services. At the same time, advances in health care such as personalized and omics-based medicine have further increased the need for software-based knowledge-management and decision-support. In the developing world and in low-resource settings, softwareintensive healthcare delivery has shown tremendous success in improving efficiency and access to services. Notwithstanding these successes, there have been ongoing concerns and highly publicized failures pertaining to critical quality attributes of healthcare software, particularly with respect to safety, efficiency, security, interoperability and effectiveness. A principle challenge underlying most of these concerns is that despite the important role of software in modern health service delivery, health care remains a human-intensive industry and thus must be considered a socio-technical system. Integrating software components that have been engineered and quality assured in isolation into the larger context of a system of systems often results in unforeseen and potentially negative consequences. The purpose of this workshop is to bring together software engineering researchers and health informaticians with the goal to identify the key challenges faced in current healthcare software development and discuss potential approaches on how to address these challenges. This year's workshop program has its particular focus on healthcare process engineering, the development of medical device software and infrastructure, and emerging software engineering paradigms for connected health delivery systems. Our goal is to discuss recent research innovations and challenges in these areas, and to continue developing an interdisciplinary, international community with an effective research, educational and industrial agenda for supporting software engineering in the healthcare sector.},
location = {Gothenburg, Sweden}
}

@proceedings{10.1145/2851553,
title = {ICPE '16: Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 7th ACM/SPEC International Conference on Performance Engineering (ICPE 2016) takes place in Delft in The Netherlands in March 2016. The conference grew out of the ACM Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance Engineering Workshop (SIPEW since 2008), with the goal of integrating theory and practice in the field of performance engineering. It is a great pleasure for us to offer an outstanding technical program this year, which we believe will allow researchers and practitioners to present their visions and latest innovation, and to exchange ideas within the community.Overall, we received 89 high quality submissions across all three tracks. The main Research Track attracted 57 submissions with 19 accepted (33% acceptance rate) for presentation at the conference. Among them were 16 full papers and three short papers. Each paper received at least three reviews from experienced program committee members. In the Work-In-Progress and Vision Track, six out of 15 contributions were selected. The Industry and Experience Track received 17 submissions, of which seven were selected for inclusion in the program. The accepted papers were organized into five research track sessions, two industry track sessions, and one WiP and vision track session. Three best paper candidates were also selected: two research papers and one industry paper.We are proud to have three excellent keynote speakers as part of our technical program: Bianca Schroeder from University of Toronto, Canada, presenting "Case studies from the real world: The importance of measurement and analysis in building better systems"Wilhelm Hasselbring from Kiel University, Germany, discussing "Microservices for Scalability"Angelo Corsaro, Chief Technology Officer at PrismTech, talking about "Cloudy, Foggy and Misty Internet of Things"In addition, the program includes four tutorials, a doctoral symposium, a poster and demo track, the SPEC Distinguished Dissertation Award, and three interesting workshops, including the International Workshop on Large-Scale Testing (LT), the 2nd International Workshop on Performance Analysis of Big data Systems (PABS), and the 2nd Workshop on Challenges in Performance Methods for Software Development (WOSPC).The program covers traditional ICPE topics such as software and systems performance modeling and prediction, analysis and optimization, characterization and profiling, as well as application of performance engineering theory and techniques to several practical fields, including distributed systems, cloud computing, storage, energy, big data, virtualized systems and containers.},
location = {Delft, The Netherlands}
}

@proceedings{10.1145/2859889,
title = {ICPE '16 Companion: Companion Publication for ACM/SPEC on International Conference on Performance Engineering},
year = {2016},
isbn = {9781450341479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 7th ACM/SPEC International Conference on Performance Engineering (ICPE 2016) takes place in Delft in The Netherlands in March 2016. The conference grew out of the ACM Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance Engineering Workshop (SIPEW since 2008), with the goal of integrating theory and practice in the field of performance engineering. It is a great pleasure for us to offer an outstanding technical program this year, which we believe will allow researchers and practitioners to present their visions and latest innovation, and to exchange ideas within the community.Overall, we received 89 high quality submissions across all three tracks. The main Research Track attracted 57 submissions with 19 accepted (33% acceptance rate) for presentation at the conference. Among them were 16 full papers and three short papers. Each paper received at least three reviews from experienced program committee members. In the Work-In-Progress and Vision Track, six out of 15 contributions were selected. The Industry and Experience Track received 17 submissions, of which seven were selected for inclusion in the program. The accepted papers were organized into five research track sessions, two industry track sessions, and one WiP and vision track session. Three best paper candidates were also selected: two research papers and one industry paper.We are proud to have three excellent keynote speakers as part of our technical program: Bianca Schroeder from University of Toronto, Canada, presenting "Case studies from the real world: The importance of measurement and analysis in building better systems"Wilhelm Hasselbring from Kiel University, Germany, discussing "Microservices for Scalability"Angelo Corsaro, Chief Technology Officer at PrismTech, talking about "Cloudy, Foggy and Misty Internet of Things"In addition, the program includes four tutorials, a doctoral symposium, a poster and demo track, the SPEC Distinguished Dissertation Award, and three interesting workshops, including the International Workshop on Large-Scale Testing (LT), the 2nd International Workshop on Performance Analysis of Big data Systems (PABS), and the 2nd Workshop on Challenges in Performance Methods for Software Development (WOSPC).The program covers traditional ICPE topics such as software and systems performance modeling and prediction, analysis and optimization, characterization and profiling, as well as application of performance engineering theory and techniques to several practical fields, including distributed systems, cloud computing, storage, energy, big data, virtualized systems and containers.},
location = {Delft, The Netherlands}
}

@inproceedings{10.1145/3267809.3275464,
author = {Siddique, A. B. and Eldawy, Ahmed},
title = {Experimental Evaluation of Sketching Techniques for Big Spatial Data},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3275464},
doi = {10.1145/3267809.3275464},
abstract = {Ubiquitously connected devices, e.g., Internet of Things (IoT), space telescopes, social networks, and GPS-enabled gadgets, are contributing to the perpetual and swift growth of the data. 2.5 exabytes of daily-produced data, of which 60-80% is geo-referenced. Space telescopes broadcast about 140 GB of data weekly. Availability of such large amount of data calls for new scalable query processing techniques. One of the techniques that is getting attention is sketching which summarizes the data and computes an approximate answer on the sketch. This general technique is used in partitioning [3], clustering [1], selectivity estimation [2], and visualization [4], among others. This paper introduces a sketching-based framework for big spatial data which provides four sketching methods and uses them to implement three common operations, namely, partitioning, clustering, and selectivity estimation. The framework is executed in three phases, sketching, local operation, and generalization, which can apply to a wide range of operations on big spatial data.Sampling is a widely used sketching technique, but there exist other techniques such as uniform and non-uniform histograms which are not well-studied due to two challenges. First, each sketching method has a different representation and creation parameters, e.g., sampling ratio or number of histogram cells, which make it hard to compare their performance. Second, while existing algorithms can be used as-is with samples, other sketching methods might require some tweaks to the algorithms to work. This work provides a comprehensive evaluation to understand the trade-offs in the different sketching techniques for big spatial data.In this paper, we present a three-phase sketching-based framework for big data processing. The first phase uses Spark to efficiently compute four types of data sketches, namely, sampling, uniform, non-uniform, and enhanced histograms. To make the sketching methods comparable, we define a parameter B which indicates the memory budget. Regardless of their representation, all sketching methods are designed to use up-to that memory budget. The second phase uses a single-machine to process the sketch and provide a partial answer to three popular and diverse operations, namely, partitioning, clustering, and selectivity estimation. Previous work mostly applied these techniques with sample-based sketches except for selectivity estimation which also used histograms. In this paper, we propose histogram-based spatial partitioning and K-means clustering and show that they can outperform sampling-based methods. The third phase takes the partial answer and scans all the data in parallel to generalize the answer to the entire dataset.In our experiments, we use both real and synthetic datasets of up-to 2.7 billion records and 100 GB of data. We vary the memory budget that we use for sketching and study its effect in both the execution time and quality of the results.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {522},
numpages = {1},
keywords = {Clustering, Partitioning, Selectivity Estimation, Sketching},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@inproceedings{10.1145/2897845.2901788,
author = {Vigna, Giovanni},
title = {Binary Analysis for Autonomous Hacking: Invited Abstract},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2901788},
doi = {10.1145/2897845.2901788},
abstract = {Despite the rise of interpreted languages and the World Wide Web, binary analysis has remained the focus of much research in computer security. There are several reasons for this. First, interpreted languages are either interpreted by binary programs or Just-In-Time compiled down to binary code. Second, "core" OSconstructs and performance-critical applications are still writtenin languages (usually, C or C++) that compile down to binary code. Third, the rise of the Internet of Things is powered by devices that are, in general, very resource-constrained. Without cycles to waste on interpretation or Just-In-Time compilation, the firmware of these devices tends to be written in languages (again, usually C) that compile to binary.Unfortunately, many of these languages provide few security guarantees, often leading to vulnerabilities. For example, buffer overflows stubbornly remain as one of the most common discovered software flaws despite efforts to develop technologies to mitigate such vulnerabilities. Worse, the wider class of memory corruption vulnerabilities", the vast majority of which also stem from the use of unsafe languages, make up a substantial portion of the most common vulnerabilities. This problem is not limited to software on general-purpose computing devices: remotely exploitable vulnerabilities have been discovered in devices ranging from smart locks, to pacemakers, to automobiles.However, finding vulnerabilities in binaries and generating patches that fix exploitable flaws is challenging because of the lack of high-level abstractions, such as type information and control ow constructs. Current approaches provide tools to support the manual analysis of binaries, but are far from being completely automated solutions to the vulnerability analysis of binary programs.To foster research in automated binary analysis, in October of 2013, DARPA announced the DARPA Cyber Grand Challenge (CGC). Like DARPA Grand Challenges in other fields (such as robotics and autonomous vehicles), the CGC pits teams from around the world against each other in a competition in which the participants are autonomous systems. During the CGC competition, these systems must identify, exploit, and patch vulnerabilities in binary programs, without any human in the loop. Millions of dollars in prize money were announced: the top 7 teams to complete the CGC Qualifying Event (held in June, 2015) received 750,000 USD, and the top 3 teams in the CGC Final Event (held in August, 2016) will receive 2,000,000 USD, 1,000,000 USD, and 750,000 USD, respectively.The Shellphish hacking team is one of the qualified teams. This talk presents some insights into the field of automated binary analysis exploitation and patching, gained through the participation in the CGC competition. In addition, the talk provides a discussion of the use of competitions to foster both research and education, based on the experience in designing and running a large-scale live security hacking competition (called the iCTF) for the past 13 years.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {473},
numpages = {1},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@proceedings{10.1145/3081333,
title = {MobiSys '17: Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
year = {2017},
isbn = {9781450349284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure and privilege to serve as program chairs for ACM MobiSys 2017 -- the 15th ACM International Conference on Mobile Systems, Applications, and Services. We hope you enjoy this technical material behind the conference that attracts a diverse set of attendees from both academia and industry and is a leading venue for publications and idea exchange on mobile systems. ACM MobiSys 2017 has a highly selective, single-track program featuring research related to mobile systems and applications. It is an ideal venue to address research challenges facing the design, development, deployment, use, and fundamental limits of these systems.Notes on the review process: Since its inception in 2003 MobiSys has had a single-blind review policy where the identity of the reviewers is not revealed to the authors but the reviewers know the names, affiliations and contact information of the authors. Beginning 2017, we modified this policy so that the identity of the authors was not revealed to the reviewers during the initial review of the paper. However, once preliminary outcomes were decided, identities of the authors was revealed to enable referees to ask appropriate questions, making it easier to compare the new results with the author(s) previously published work and to ensure that a true advance was being reported.Our paper review process this year was highly selective. Out of 188 submissions, the technical program committee accepted only 34 for publication and presentation as full papers, yielding an acceptance rate around 18%. Submitted papers underwent a rigorous, multi-stage review process. First, we checked all submissions for compliance, general quality, and topic match. We administratively rejected those not meeting our submission criteria. We assigned 3 reviewers to papers that survived this stage from the program committee and the external review committee. At the conclusion of this stage, those papers where none of the reviewers were enthusiastic about acceptance were rejected. We then assigned at least 2 additional reviewers from the program committee to papers that survived, thus totaling at least 5 reviews per paper. An online discussion phase then ensued, resulting in the PC recommending 60 papers for discussion at the PC meeting. The PC meeting was held in person in Sonoma CA, USA, the day after ACM HotMobile 2017. At the conclusion of the PC meeting, we accepted 34 papers to the conference. Several accepted papers were assigned shepherds to help ensure that the authors produce a final manuscript that satisfactorily addresses reviewer comments.The program: Our program this year covers an exciting set of topics including sensing using acoustic, RF, and light signals, novel communication techniques, deep learning on mobiles, mobile performance, security and privacy, and operating systems. It also includes a keynote by Pattie Maes on human augmentation i.e. how systems can actively assist people with memory, learning, decision making, communication and physical skills, a SIGMOBILE Outstanding Contribution Award talk by Norman Abramson, recognized for his pioneering work on the ALOHAnet wireless networking system, five invited talks on trends in deep learning on mobiles, virtual reality, internet of things, biomedical diagnostics using phones, and large-scale wireless testbeds, brief talks by five Test-of-Time award winners, as well as an extensive poster/demo session.},
location = {Niagara Falls, New York, USA}
}

@inproceedings{10.1145/3127479.3134761,
author = {Vernik, Gil and Factor, Michael and Kolodner, Elliot K. and Ofer, Effi and Michiardi, Pietro and Pace, Francesco},
title = {Stocator: an object store aware connector for apache spark},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3134761},
doi = {10.1145/3127479.3134761},
abstract = {Data is the natural resource of the 21st century. It is being produced at dizzying rates, e.g., for genomics, for media and entertainment, and for Internet of Things. Object storage systems such as Amazon S3, Azure Blob storage, and IBM Cloud Object Storage, are highly scalable distributed storage systems that offer high capacity, cost effective storage. But it is not enough just to store data; we also need to derive value from it. Apache Spark is the leading big data analytics processing engine combining MapReduce, SQL, streaming, and complex analytics. We present Stocator, a high performance storage connector, enabling Spark to work directly on data stored in object storage systems, while providing the same correctness guarantees as Hadoop's original storage system, HDFS.Current object storage connectors from the Hadoop community, e.g., for the S3 and Swift APIs, do not deal well with eventual consistency, which can lead to failure. These connectors assume file system semantics, which is natural given that their model of operation is based on interaction with HDFS. In particular, Spark and Hadoop achieve fault tolerance and enable speculative execution by creating temporary files, listing directories to identify these files, and then renaming them. This paradigm avoids interference between tasks doing the same work and thus writing output with the same name. However, with eventually consistent object storage, a container listing may not yet include a recently created object, and thus an object may not be renamed, leading to incomplete or incorrect results.Solutions such as EMRFS [1] from Amazon, S3mper [4] from Netflix, and S3Guard [2], attempt to overcome eventual consistency by requiring additional strongly consistent data storage. These solutions require multiple storage systems, are costly, and can introduce issues of consistency between the stores.Current object storage connectors from the Hadoop community are also notorious for their poor performance for write workloads. This, too, stems from their use of the rename operation, which is not a native object storage operation; not only is it not atomic, but it must be implemented using a costly copy operation, followed by delete. Others have tried to improve the performance of object storage connectors by eliminating rename, e.g., the Direct-ParquetOutputCommitter [5] for S3a introduced by Databricks, but have failed to preserve fault tolerance and speculation.Stocator takes advantage of object storage semantics to achieve both high performance and fault tolerance. It eliminates the rename paradigm by writing each output object to its final name. The name includes both the part number and the attempt number, so that multiple attempts to write the same part use different objects. Stocator proposes to extend an already existing success indicator object written at the end of a Spark job, to include a manifest with the names of all the objects that compose the final output; this ensures that a subsequent job will correctly read the output, without resorting to a list operation whose results may not be consistent. By leveraging the inherent atomicity of object creation and using a manifest we obtain fault tolerance and enable speculative execution; by avoiding the rename paradigm we greatly decrease the complexity of the connector and the number of operations on the object storage.We have implemented our connector and shared it in open source [3]. We have compared its performance with the S3a and Hadoop Swift connectors over a range of workloads and found that it executes many fewer operations on the object storage, in some cases as few as one thirtieth. Since the price for an object storage service typically includes charges based on the number of operations executed, this reduction in operations lowers the costs for clients in addition to reducing the load on client software. It also reduces costs and load for the object storage provider since it can serve more clients with the same amount of processing power. Stocator also substantially increases performance for Spark workloads running over object storage, especially for write intensive workloads, where it is as much as 18 times faster.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {653},
numpages = {1},
location = {Santa Clara, California},
series = {SoCC '17}
}

@inproceedings{10.1145/3331076.3331128,
author = {Dustdar, Schahram},
title = {Novel paradigms for engineering large-scale resilient IoT systems},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331128},
doi = {10.1145/3331076.3331128},
abstract = {This invited talk explores the research challenges in the domain of IoT from multiple angles and reflects on the urgently needed collective efforts from various research communities to collaborate on those. Our approach fundamentally challenges the current understanding of scientific, technological, and political paradigms in tackling the engineering of large-scale IoT systems. We discuss technical paradigms and research challenges in the domains of Cloud and Edge Computing as well as the requirements of people in such systems embedded in Smart Cities.},
booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
articleno = {1},
numpages = {1},
keywords = {IoT device, IoT security, data privacy, data security},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.1145/2768510.2768520,
author = {S., Sharad and P., Bagavathi Sivakumar and Anantha Narayanan, V.},
title = {A Novel IoT-Based Energy Management System for Large Scale Data Centers},
year = {2015},
isbn = {9781450336093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2768510.2768520},
doi = {10.1145/2768510.2768520},
abstract = {The high energy consumption in data centers is becoming a major concern because it leads to increased operating costs and also, pollution, as fuel is burnt to produce the required energy. While many techniques and methods have been proposed by various organizations and researchers to minimize the energy consumption, there has been considerably less work done in making a smart-energy management system that is capable of collecting the data available and make decisions based on the energy consumption patterns. In this work, a smart system is proposed that uses Internet of Things to gather data and a machine learning algorithm for decision making.},
booktitle = {Proceedings of the 2015 ACM Sixth International Conference on Future Energy Systems},
pages = {313–318},
numpages = {6},
keywords = {beagle bone black, cc2500, cloud, high energy consumption, iot, large data centers, wireless communication},
location = {Bangalore, India},
series = {e-Energy '15}
}

@inproceedings{10.1145/3007818.3007824,
author = {Cuzzocrea, Alfredo},
title = {Big data compression paradigms for supporting efficient and scalable data-intensive IoT frameworks},
year = {2016},
isbn = {9781450347549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3007818.3007824},
doi = {10.1145/3007818.3007824},
abstract = {In this paper we focus on big data compression paradigms within reference data-intensive IoT frameworks, which are currently recognized as one of the emerging scientific in a rich interdisciplinary field that comprises service-oriented infrastructures, Cloud computing, big data management and analytics. Basically, big data compression techniques allow to tame the complexity of big data management tasks within such frameworks, hence beneficially influencing all the other activities, perhaps delivered as services in a reference Cloud architecture. Inspired by these considerations, in this paper we provide an overview on noticeable state-of-the-art big data compression techniques, and depict future research directions on the investigated scientific topic to be considered during future years.},
booktitle = {Proceedings of the Sixth International Conference on Emerging Databases: Technologies, Applications, and Theory},
pages = {67–71},
numpages = {5},
location = {Jeju, Republic of Korea},
series = {EDB '16}
}

@inproceedings{10.1145/3290480.3290491,
author = {Jiang, Yikun and Xie, Wei and Tang, Yong},
title = {Detecting Authentication-Bypass Flaws in a Large Scale of IoT Embedded Web Servers},
year = {2018},
isbn = {9781450365673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290480.3290491},
doi = {10.1145/3290480.3290491},
abstract = {With the rapid development of network and communication technologies, everything is able to be connected to the Internet. IoT devices, which include home routers, IP cameras, wireless printers and so on, are crucial parts facilitating to build pervasive and ubiquitous networks. As the number of IoT devices around the world increases, the security issues become more and more serious.To handle with the security issues and protect the IoT devices from being compromised, the firmware of devices needs to be strengthened by discovering and repairing vulnerabilities. Current vulnerability detection tools can only help strengthening traditional software, nevertheless these tools are not practical enough for IoT device firmware, because of the peculiarity in firmware's structure and embedded device's architecture. Therefore, new vulnerability detection framework is required for analyzing IoT device firmware.This paper reviews related works on vulnerability detection in IoT firmware, proposes and implements a framework to automatically detect authentication-bypass flaws in a large scale of Linux-based firmware. The proposed framework is evaluated with a data set of 2351 firmware images from several target vendors, which is proved to be capable of performing large-scale and automated analysis on firmware, and 1 known and 10 unknown authentication-bypass flaws are found by the analysis.},
booktitle = {Proceedings of the 8th International Conference on Communication and Network Security},
pages = {56–63},
numpages = {8},
keywords = {IoT firmware, Vulnerability detection, automated, large scale},
location = {Qingdao, China},
series = {ICCNS '18}
}

@inproceedings{10.1145/2968219.2979129,
author = {Viroli, Mirko and Casadei, Roberto and Pianini, Danilo},
title = {On execution platforms for large-scale aggregate computing},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2979129},
doi = {10.1145/2968219.2979129},
abstract = {Aggregate computing is proposed as a computational model and associated toolchain to engineer adaptive large-scale situated systems, including IoT and wearable computing systems. Though originated in the context of WSN-like (peer-to-peer and fully distributed) systems, we argue it is a model that can transparently fit a variety of execution platforms (decentralised, server-mediated, cloud/fog-oriented), due to its ability of declaratively designing systems by global-level abstractions: it opens the possibility of intrinsically supporting forms of load balancing, elasticity and toleration of medium- and long-term changes of computational infrastructures. To ground the discussion, we present ongoing work in the context of scafi, a language and platform support for computational fields based on the Scala programming language and Akka actor framework.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1321–1326},
numpages = {6},
keywords = {aggregate computing, cloud computing, execution platforms, internet of things, large-scale systems},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/2810156.2812611,
author = {Malik, Adeel Mohammad and Ahlgren, Bengt and Ohlman, B\"{o}rje},
title = {NetInf Live Video Streaming for Events with Large Crowds},
year = {2015},
isbn = {9781450338554},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810156.2812611},
doi = {10.1145/2810156.2812611},
abstract = {Information Centric Networking (ICN) aims to evolve the Internet from a host-centric to a data-centric paradigm. In particular, it improves performance and resource efficiency in events with large crowds where many users in a local area want to generate and watch media content related to the event.We present the design of a live video streaming system built on the NetInf ICN architecture and how the architecture was adapted to support live streaming of media content. To evaluate the feasibility and performance of the system, extensive field tests were carried out over several days during a major sports event. Our system streams videos successfully with low delay and communication overhead compared with existing Internet streaming services. It can scale to support several thousands of simultaneous users at a time and is well-suited for events with large crowds and flash crowd scenarios.},
booktitle = {Proceedings of the 2nd ACM Conference on Information-Centric Networking},
pages = {209–210},
numpages = {2},
keywords = {caching, flash crowd, icn, live video streaming, netinf, point-to-multipoint, publish-subscribe, request aggregation, subscribe-notify},
location = {San Francisco, California, USA},
series = {ACM-ICN '15}
}

@inproceedings{10.1145/3147213.3147230,
author = {Sampaio, Lilia and Silva, F\'{a}bio and Souza, Amanda and Brito, Andrey and Felber, Pascal},
title = {Secure and Privacy-Aware Data Dissemination for Cloud-Based Applications},
year = {2017},
isbn = {9781450351492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147213.3147230},
doi = {10.1145/3147213.3147230},
abstract = {In this paper we propose a data dissemination platform that supports data security and different privacy levels even when the platform and the data are hosted by untrusted infrastructures. The proposed system aims at enabling an application ecosystem that uses off-the-shelf trusted platforms (in this case, Intel SGX), so that users may allow or disallow third parties to access the live data stream with a specific sensitivity-level. Moreover, this approach does not require users to manage the encryption keys directly. Our experiments show that such an approach is indeed practical for medium scale systems, where participants disseminate small volumes of data at a time, such as in smart grids and IoT environments.},
booktitle = {Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {47–56},
numpages = {10},
keywords = {cloud computing, data security, iot, privacy, sgx, trusted execution},
location = {Austin, Texas, USA},
series = {UCC '17}
}

@inproceedings{10.1145/2791405.2791410,
author = {Sarvabhatla, Mrudula and Reddy, M. Chandra Mouli and Vorugunti, Chandra Sekhar},
title = {A Robust and Light Weight Authentication Framework for Hadoop File System in Cloud Computing Environment},
year = {2015},
isbn = {9781450333610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791405.2791410},
doi = {10.1145/2791405.2791410},
abstract = {The advancement of web and mobile technologies results in the rapid augmentation of traditional enterprise data, IoT generated data, social media data which outcomes in peta bytes and exa bytes of structured and un structured data across clusters of servers per day. The storage, processing, analyzing and securing these big data is becoming a serious concern to large and medium enterprises. Hadoop or HDFS is a distributed file system based on cloud for storage and processing of large voluminous amounts of data across clusters of servers. Along with huge potential for dynamism for processing and scalability, HDFS also brought inherent security drawbacks like lack of authentication and authorization of remote user connecting to cluster, missing of encryption of sensitive data at communication, storage and processing levels. These existing drawbacks demands for a robust, light weight security framework for HDFS. In this context, we propose a secure and light weight remote user authentication framework for HDFS, which guarantees all the critical security requirements of a distributed file system.},
booktitle = {Proceedings of the Third International Symposium on Women in Computing and Informatics},
pages = {463–468},
numpages = {6},
keywords = {Authentication, Authorization, Big Data, Hadoop Distributed File System, Hadoop Framework},
location = {Kochi, India},
series = {WCI '15}
}

@inproceedings{10.1145/3329391,
author = {Esposito, Christian and Pop, Florin and Choi, Chang},
title = {Session details: Theme: Information systems: SFECS - Sustainability of fog/edge computing systems track},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329391},
doi = {10.1145/3329391},
abstract = {Fog/Edge Computing paradigms are widely used in enterprises to address the emerging challenges of big data analysis, because of their underlying scalable, flexible and distributed data management schemes. The data centers in the Clouds are facing great challenges on the burden of the consequent increasing the amount of data to be man- aged and the additional requirements of location awareness and low latency at the edge of network necessary by smart cites and factories. These are the reasons why a centralized model cannot be an efficient solution for generated or required data by the IoT devices in those applications and there is the progressive shift towards fog nodes and smarted edge nodes mediating between the cloud and the IoT devices. The Fog/Edge computing paradigm is a decentralized model that transfers a part of low computing data analysis from the cloud to the intermediate (fog) nodes or the edges, performing only high computing tasks in the cloud. This new approach tries to minimize the three factors that negatively compromise the effective and efficient application of the Cloud computing to smart cities and factories, or similar application domains: the network bandwidth usage, decentralization of the data processing tasks and reduced response latency for clients (IoT devices). Fog/Edge computing is a hierarchical approach where the overall infrastructure is structured in multiple layers, each responsible of offering a good coordination and data management to the nodes at the lower layer. The lowest layer is usually composed of sensors and/or actuators that measure and/or control the environment or a given business process, implemented as mobile devices that are running a sensing/controlling application. In this case, combining Sustainable computing with Fog and Edge computing represents a new approach for increasing quality-of- service and efficiency of the system, creating the capability to present temporal and geo-coded information, and increasing innovation, and co-designing sustainable future large scale distributed systems. This new paradigm appears to offer a good approach in handling the scale factor of the data size, reducing the network bandwidth usage and the response latency of the system. In order to support specifically the Fog/Edge architectures, there is a need, for instance, of location-awareness and computation placement, replication and recovery. In many cases Edge resources would be required for both computation and data storage to address the time and locality constraints. There are multiple kinds of orchestration management solutions for virtualization in this type of architecture with different characteristics and drawbacks. This results in different restrictions for application definition, scalability, availability, load balancing and so on. Also, virtualization may be needed at multiple levels in a Fog/Edge architecture as it consists of the following levels of abstraction: at the sensing level we have the IoT devices/smart things, at the Edge level there are the gateways to a first collection and the data from the IoT devices and their preliminary processing, at the Fog level we have an additional data management layer, and at the Cloud level there is the compute/storage infrastructure with applications on top. Last, but not least, the energy efficiency is particularly important at the IoT and edge level since the devices may be equipped with a limited battery, possible difficult or impossible to be charged. So, optimizing the energy consumption is a must. To address several open research is- sues regarding sustainability of future Fog/Edge systems, this track aims at solicit contributions highlighting challenges, state-of-the-art, and solutions to a set of currently unresolved key questions including - but not limited to - performance, modeling, optimization, energy-efficiency, reliability, security, privacy and techno-economic aspects of Fog/Edge systems. Through addressing these concerns while understanding their impacts and limitations, technological advancements will be channeled toward more sustainable/efficient platforms for tomorrow's ever-connected systems.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3292500.3330653,
author = {Hwang, Seong Jae and Lee, Joonseok and Varadarajan, Balakrishnan and Gordon, Ariel and Xu, Zheng and Natsev, Apostol (Paul)},
title = {Large-Scale Training Framework for Video Annotation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330653},
doi = {10.1145/3292500.3330653},
abstract = {Video is one of the richest sources of information available online but extracting deep insights from video content at internet scale is still an open problem, both in terms of depth and breadth of understanding, as well as scale. Over the last few years, the field of video understanding has made great strides due to the availability of large-scale video datasets and core advances in image, audio, and video modeling architectures. However, the state-of-the-art architectures on small scale datasets are frequently impractical to deploy at internet scale, both in terms of the ability to train such deep networks on hundreds of millions of videos, and to deploy them for inference on billions of videos. In this paper, we present a MapReduce-based training framework, which exploits both data parallelism and model parallelism to scale training of complex video models. The proposed framework uses alternating optimization and full-batch fine-tuning, and supports large Mixture-of-Experts classifiers with hundreds of thousands of mixtures, which enables a trade-off between model depth and breadth, and the ability to shift model capacity between shared (generalization) layers and per-class (specialization) layers. We demonstrate that the proposed framework is able to reach state-of-the-art performance on the largest public video datasets, YouTube-8M and Sports-1M, and can scale to 100 times larger datasets.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2394–2402},
numpages = {9},
keywords = {distributed framework, mapreduce, scalability, video annotation},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3018896.3036378,
author = {Baldoni, Gabriele and Lombardo, Alfio and Melita, Marcello and Micalizzi, Sergio and Rametta, Corrado and Vassallo, Alessandro},
title = {An emulation framework for SDN-NFV based services},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3036378},
doi = {10.1145/3018896.3036378},
abstract = {Due to the emergence of a hyper-connectivity communication paradigm, the "softwarisation" of the Internet infrastructure and of its network management framework is gaining increasing popularity. Cloud computing helps supporting this evolution, together with the emergence of Software Defined Networking (SDN) and Network Functions Virtualization (NFV). This technological paradigm allows to both move services closer to users, ensuring a lower latency in the service fruition, and support personalization of services by means of the migration of the latter towards edge nodes (in the so-called "fog computing" fashion). Actually, the development of cloud services based on the SDN-NFV paradigm has to cope with the availability for researcher and network enthusiasts to access to a full SDN testbed.Target of this paper is to present an emulation framework allowing simplification and cost reduction of network and application functions development, test and deployment. In such a way, the proposed architecture aims at supporting future Internet personal cloud services in a more scalable and sustainable way. Authors also present a proof-of-concept of the described architecture: a live video broadcasting service enabling small/medium and unusual content providers to share events with a restricted number of interested users without the need of adopting a dedicated and expensive data delivery infrastructure and/or subscribing expensive contracts with Telco.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {135},
numpages = {8},
keywords = {cloud/fog computing, network and application functions virtualization, network emulation, network orchestration, video broadcasting over SDN/NFV},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3054977.3057295,
author = {Taherkordi, Amir and Eliassen, Frank},
title = {Data-Centric IoT Services Provisioning in Fog-Cloud Computing Systems: Poster Abstract},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3057295},
doi = {10.1145/3054977.3057295},
abstract = {Fog computing is mainly proposed for IoT applications that are geospatially distributed, large-scale, and latency sensitive. This poses new research challenges in real-time and scalable provisioning of IoT services distributed across Fog-Cloud computing platforms. Data-centric IoT services, as a dominant type of IoT services in large-scale deployments, require design solutions to speed up data processing and notification, and scale up with the data volume. In this paper, we propose a service-oriented design architecture which is particularly focused on provisioning and processing data-centric IoT services over Fog-Cloud systems. In the proposed architecture, data-centric IoT services are organized in a service integrating tree structure, adhering to the hierarchical fog-based IoT computing models. A service node in the tree is empowered with features for real-time service data notification, local data processing and multi-level IoT data access. The initial results show that, along the design advantages of the proposed model, it does not impose any additional overhead as compared to state-of-the-art solutions.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {317–318},
numpages = {2},
keywords = {Data-Centric Services, Fog Computing, Internet of Things},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@article{10.1145/3199524.3199545,
author = {Cecchi, F. and Borst, S.C. and van Leeuwaarden, J.S.H. and Whiting, P.A.},
title = {Spatial Mean-Field Limits for Ultra-Dense Random-Access Networks},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3199524.3199545},
doi = {10.1145/3199524.3199545},
abstract = {Random-access algorithms such as the CSMA protocol provide a popular mechanism for distributed medium access control in wireless networks. In saturated-buffer scenarios the joint activity process in such random-access networks has a product-form stationary distribution which provides useful throughput estimates for persistent traffic flows. However, these results do not capture the relevant performance metrics in unsaturated-buffer scenarios, which in particular arise in an IoT context with highly intermittent traffic sources. Mean-field analysis has emerged as a powerful approach to obtain tractable performance estimates in such situations, and is not only mathematically convenient, but also relevant as wireless networks grow larger and denser with the emergence of IoT applications. A crucial requirement for the classical mean-field framework to apply however is that the node population can be partitioned into a finite number of classes of statistically indistinguishable nodes. The latter condition is a severe restriction since nodes typically have different locations and hence experience different interference constraints. Motivated by the above observations, we develop in the present paper a novel mean-field methodology which does not rely on any exchangeability property. Since the spatiotemporal evolution of the network can no longer be described through a finite-dimensional population process, we adopt a measure-valued state description, and prove that the latter converges to a deterministic limit as the network grows large and dense. The limit process is characterized in terms of a system of partial-differential equations, which exhibit a striking local-global-interaction and time scale separation property. Specifically, the queueing dynamics at any given node are only affected by the global network state through a single parsimonious quantity. The latter quantity corresponds to the fraction of time that no activity occurs within the interference range of that particular node in case of a certain static spatial activation measure. Extensive simulation experiments demonstrate that the solution of the partial-differential equations yields remarkably accurate approximations for the queue length distributions and delay metrics, even when the number of nodes is fairly moderate.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {mar},
pages = {123–136},
numpages = {14},
keywords = {CSMA, Mean-field limits, Measure-valued state description, Random-access networks}
}

@inproceedings{10.1145/3297858.3304013,
author = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
title = {An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud &amp; Edge Systems},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304013},
doi = {10.1145/3297858.3304013},
abstract = {Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds or thousands of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and cloud utilization.In this paper we explore the implications microservices have across the cloud system stack. We first present DeathStarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {3–18},
numpages = {16},
keywords = {acceleration, cloud computing, cluster management, datacenters, fpga, microservices, qos, serverless},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.1145/3307772.3328305,
author = {Marnerides, Angelos K. and Giotsas, Vasileios and Mursch, Troy},
title = {Identifying infected energy systems in the wild},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328305},
doi = {10.1145/3307772.3328305},
abstract = {The 2016 Mirai outbreak established an entirely new mindset in the history of large-scale Internet attacks. A plethora of Mirai-like variants have emerged in the last two years that are capable to infiltrate any type of device. In this paper we provide a 7-month retrospective analysis of Internet-connected energy systems that are infected by Mirai-like malware variants. By utilizing network measurements from several Internet vantage points, we demonstrate that a number of energy systems on a global scale were infected during the period of our observation. While past works have studied vulnerabilities and patching practises of ICS and energy systems, little information has been available on actual exploits of such vulnerabilities. Hence, we provide evidence that energy systems relying on ICS networks are often compromised by vulnerabilities in non-ICS devices (routers, servers and IoT devices) which provide foothold for lateral network attacks. Our work offers a first look in compromised energy systems by malware infections, and offers insights on the lack of proper security practices for systems that are increasingly dependent on internet services and more recently the IoT. In addition, we indicate that such systems were infected for relatively large periods, thus potentially remaining undetected by their corresponding organizational units.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {263–267},
numpages = {5},
keywords = {Energy systems, ICS, Internet measurements, IoT botnets, Mirai},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@inproceedings{10.1145/2736277.2741082,
author = {Bakshy, Eytan and Frachtenberg, Eitan},
title = {Design and Analysis of Benchmarking Experiments for Distributed Internet Services},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741082},
doi = {10.1145/2736277.2741082},
abstract = {The successful development and deployment of large-scale Internet services depends critically on performance. Even small regressions in processing time can translate directly into significant energy and user experience costs. Despite the widespread use of distributed server infrastructure (e.g., in cloud computing and Web services), there is little research on how to benchmark such systems to obtain valid and precise inferences with minimal data collection costs. Correctly A/B testing distributed Internet services can be surprisingly difficult because interdependencies between user requests (e.g., for search results, social media streams, photos) and host servers violate assumptions required by standard statistical tests. We develop statistical models of distributed Internet service performance based on data from Perflab, a production system used at Facebook which vets thousands of changes to the company's codebase each day. We show how these models can be used to understand the tradeoffs between different benchmarking routines, and what factors must be taken into account when performing statistical tests. Using simulations and empirical data from Perflab, we validate our theoretical results, and provide easy-to-implement guidelines for designing and analyzing such benchmarks.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {108–118},
numpages = {11},
keywords = {A/B testing, benchmarking, bootstrapping, cloud computing},
location = {Florence, Italy},
series = {WWW '15}
}

@inproceedings{10.1145/3321408.3321608,
author = {He, Shuqing and Wang, Haifeng and Cao, Yunpeng and Zhao, Deyu},
title = {A wide-deep event model for complex event processing in edge and cloud computing environment},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3321608},
doi = {10.1145/3321408.3321608},
abstract = {With the rapid development of smart IoT devices, it is a trend for complex event processing in large-scale IoT application to be intelligent capacity, complex event processing will involve more intelligent collaboration and evolution. However, the studies on traditional complex event processing focus more on performance and efficiency, and fewer consider intelligent collaboration and evolution. In this paper, aiming at the edge and cloud computing in large-scale IoT application, a wide-deep event model and its suitable complex event processing architecture are proposed to mainly improve intelligent collaboration and evolution. The model and architecture fully consider the feature of the edge and cloud computing, combine with deep learning and wide-deep learning and make them seamless integration, making the architecture have better intelligent collaboration and evolution.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {46},
numpages = {2},
keywords = {cloud computing, complex event processing, edge computing, wide-deep event model},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1109/CCGRID.2017.73,
author = {Wu, Dongyao and Sakr, Sherif and Zhu, Liming and Wu, Huijun},
title = {Towards Big Data Analytics across Multiple Clusters},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.73},
doi = {10.1109/CCGRID.2017.73},
abstract = {Big data are increasingly collected and stored in a highly distributed infrastructures due to the development of sensor network, cloud computing, IoT and mobile computing among many other emerging technologies. In practice, the majority of existing big-data-processing frameworks (e.g., Hadoop and Spark) are designed based on the single-cluster setup with the assumptions of centralized management and homogeneous connectivity which makes them sub-optimal and sometimes infeasible to apply for scenarios that require implementing data analytics jobs on highly distributed data sets (across racks, data centers or multi-organizations). In order to tackle this challenge, we present HDM-MC, a multi-cluster big data processing framework which is designed to enable the capability of performing large scale data analytics across multi-clusters with minimum extra overhead due to additional scheduling requirements. In this paper, we present the architecture and realization of the system. In addition, we evaluate the performance of our framework in comparison to other state-of-art single cluster big data processing frameworks.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {218–227},
numpages = {10},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@inproceedings{10.1145/2968219.2968588,
author = {Amaxilatis, Dimitrios and Lagoudianakis, Evangelos and Mylonas, Georgios and Theodoridis, Evangelos},
title = {Managing smartphone crowdsensing campaigns through the organicity smart city platform},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968588},
doi = {10.1145/2968219.2968588},
abstract = {We briefly present the design and architecture of a system that aims to simplify the process of organizing, executing and administering crowdsensing campaigns in a smart city context over smartphones volunteered by citizens. We built our system on top of an Android app substrate on the end-user level, which enables us to utilize smartphone resources. Our system allows researchers and other developers to manage and distribute their "mini" smart city applications, gather data and publish their results through the Organicity smart city platform. We believe this is the first time such a tool is paired with a large scale IoT infrastructure, to enable truly city-scale IoT and smart city experimentation.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1460–1465},
numpages = {6},
keywords = {IoT, android, campaign, crowdsensing, experiment, experimentation management, mobile, sensors, smart city, smartphone, volunteers},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3010089.3010141,
author = {Pham, Congduc},
title = {Internet-of-Thing and reasons why it is becoming a reality},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010141},
doi = {10.1145/3010089.3010141},
abstract = {It is widely accepted that the Era of IoT can potentially connect billions of sensors, devices, equipment, systems, etc. In turn, the challenge is about driving business outcomes, consumer benefits, and the creation of new value. While benefits of IoT are clearly stated for increased process efficiency through automation &amp; optimization, the deployment of such devices in a large scale is still held back by technical challenges. However, there are a number of small revolutions that are rapidly turning IoT into reality. In this presentation we will present how new contributions in the domain of hardware, communication, data storage and data-processing definitely make the IoT paradigm to happen with an unpreceding level of flexibility and cost effective implementations. The presentation will also discuss how IoT can become reality in developing countries by carefully take into account these specific contcxts when designing and developing IoT frameworks and platforms. We will illustrate with outcomes of the H2020 WAZIUP project that targets deployment of low-cost IoT in sub-Saharan Africa countries.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {1},
numpages = {1},
keywords = {Internet-of-Thing, Low-power WAN, Rural development, long-range radios},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@proceedings{10.1145/2948076,
title = {PDC '16: Proceedings of the 14th Participatory Design Conference: Short Papers, Interactive Exhibitions, Workshops - Volume 2},
year = {2016},
isbn = {9781450341363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Participatory Design is a diverse collection of principles and practices aimed at making technologies, tools, environments, businesses and social institutions more responsive to human needs. A central tenet of Participatory Design (PD) is the direct involvement of people in the co-design of things and technologies they use and live with.This second volume of the conference proceedings includes Short Papers, and the Workshops and the Interactive Exhibitions submissions.The theme for Participatory Design Conference 2016 is 'Participatory Design in an Era of Participation'. Over 25 years after the first PDC in 1990, participation and co-creation have become essential features of design and research into technology. Living in an era of participation prompts critical questions around the goals and practices of involving people in diverse aspects of developing, redesigning and using IT. The distribution and promise of information technologies cut across emerging societal challenges at various levels. Sharing economy, crowdfunding and participatory cultures create new forms of engagement that challenge traditional ideas of participation. Public engagement in radical social innovation is used to address shrinking finances to public services, which has resulted in citizen-involving projects and labs in various domains. Maker technologies, notions of hacking and shared data, are promoting civic engagement with technology innovation that changes the material and socio-economic contexts of production. At the same time, centralization of the Internet, big data and large-scale infrastructuring challenge the core democratic ideals of PD.The Participatory Design Conferences (PDC) continue to be the main gathering point of the PD community and an important venue for international discussion of the collaborative, social and political dimensions of technology innovation and use. PDC is a premier venue for presenting research on the direct involvement of people in the design, development, implementation and appropriation of information and communication technology. Held every two years since 1990, PDC brings together a multidisciplinary and international group of researchers and practitioners from multiple fields. These include, but are not limited to, Human-Computer Interaction, CSCW (computer supported cooperative work), Co-Design, Design Research, CSCL (computer supported collaborative learning), ICT4D (information and communication technology for development), design anthropology, design psychology, design Industry and the Arts. The conference has helped to broaden participatory approaches in design around a variety of arenas including information and communication technologies, work, healthcare, learning, new media and digital culture, community settings, architecture and the urban environment, visual communication, interaction design and service design to mention some of the fields involved.},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3204949,
title = {MMSys '18: Proceedings of the 9th ACM Multimedia Systems Conference},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Since its inauguration in 2011, the MMSys series has provided an opportunity for researchers to present and share their latest research in multimedia systems. MMSys covers interdisciplinary research, spanning across the following system domains in the context of multimedia data types: networking; operating systems; real-time systems; and database communities. In particular, MMSys is the home of researchers who explore: (i) complete multimedia systems that provide a new kind of multimedia experience or systems or (ii) enhancements to one or more system components for continuous media or time-dependent services.While building on the successful format of past MMSys conferences, MMSys 2018 introduces some novelties and extends previous successful initiatives. The main objectives we would like to highlight for this year are:- to promote open science by supporting the datasets and open source track, and by granting ACM reproducibility badges;- to increase diversity and facilitate inclusion by offering many travel grants to young, diverse scholars;- to increase the impact and broaden the community by incorporating top-quality workshops and forward-looking special sessions.First of all, the format of the conference has slightly changed from past years by including four prestigious workshops: Packet Video and the International Workshop on Immersive Mixed and Virtual Environment Systems (MMVE) the day before the main conference, and NOSSDAV and NetGames the day after. Secondly, we have strived to facilitate diversity and inclusion. Thanks to the generous assistance of ACM SIGMM, a large number of young scholars are supported to attend MMSys by offering two types of travel grants: one for students with an accepted article and another one for nurturing new talent in SIGMM (part of the SIGMM Funding for New Initiatives including Desk Study of Gender and Diversity within SIGMM: http://www.sigmm.org/news/sigmm_funding_for_new_initiatives).We have also continued (and expanded) initiatives that were successfully introduced in 2017. With the goal to further grow the community and to broaden the scope of the conference, we included five special sessions: human-centric internet and multimedia systems; immersive multimedia experiences; integrative computer vision and multimedia systems; IoT and smart cities; and multimedia in 5G network architectures. The aim is that in the coming years such special sessions will become core areas of interest. In addition, we have continued to support recent efforts for scientific reproducibility. A record number of MMsys papers, 31 (45% of all the accepted articles), obtained an ACM badge by making data sets and code available, which will allow other researchers to reproduce the results presented in these papers.We are delighted to see that MMSys has become a venue that is of high interest to the research community. This was expressed by a total of 115 submissions (56 for the research track and 59 combined for the special sessions). Out of the 115 submissions a total of 30 papers were accepted for publication. This represents a 26% acceptance rate. As in previous years, a high standard was applied for the review process. For the research track, each paper received a minimum of four reviews, while for the special sessions, each paper received at least three reviews. Authors also had the opportunity to rebut the reviews, and decisions were taken based on the reviews, the rebuttal, and the discussion between the reviewers. Obviously, establishing such a rigorous, double-blind review process would not have been possible without the tireless efforts of the program committee. Apart from the research track, the demo session received 20 submissions, and the open source/dataset track an impressive number of 36 submissions. This demonstrates the importance of open science to our community.The program of the conferences also includes two keynote speeches from Philip A. Chou (8i), "Holograms are the Next Video", and Nuria Oliver (Vodafone &amp; Data-Pop Alliance), "Pervasive Data for Large-scale Human Behavior Modeling"; and four overview talks from Klara Nahrstedt (University of Illinois at Urbana-Champaign) on "Distribution Systems for 3D Teleimmersive and Video 360 Content: Similarities and Differences", Jan De Cock (Netflix) on the "Evolution of Video Streams at Netflix: Streaming Higher Quality at Lower Bitrates", Rob Koenen (Tiledmedia) on "VR360: State of the Art, State of the Industry &amp; State of the Standards", and Imed Bouazizi (Samsung Research America) on "Media Distribution over Next Generation 5G Networks".We would like to thank everyone who has contributed to the conference: the authors, the keynotes and invited speakers, the steering committee, the technical program committee and the reviewers. Special thanks go to the organising committee, which has seamlessly and effectively coordinated all the aspects of the event. An amazing team! We also thank the sponsors including ACM and ACM SIGMM; the co-sponsors, including SIGCOMM, SIGMOBILE, and SIGOPS; the gold supporter Adobe, and the silver supporters YouTube, Comcast, Bitmovin, Unified Streaming, Nokia, and DASH-IF; and the host, Centrum Wiskunde &amp; Informatica (CWI) in Amsterdam. Without the generous sponsorship and support from all of them, MMsys 2018 would have not been possible.We sincerely hope that you enjoy the conference in Amsterdam, the exciting discussions, and the social event. Please beware of the bikes!},
location = {Amsterdam, Netherlands}
}

@article{10.1145/2996183,
author = {Alsaedi, Nasser and Burnap, Pete and Rana, Omer},
title = {Can We Predict a Riot? Disruptive Event Detection Using Twitter},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2996183},
doi = {10.1145/2996183},
abstract = {In recent years, there has been increased interest in real-world event detection using publicly accessible data made available through Internet technology such as Twitter, Facebook, and YouTube. In these highly interactive systems, the general public are able to post real-time reactions to “real world” events, thereby acting as social sensors of terrestrial activity. Automatically detecting and categorizing events, particularly small-scale incidents, using streamed data is a non-trivial task but would be of high value to public safety organisations such as local police, who need to respond accordingly. To address this challenge, we present an end-to-end integrated event detection framework that comprises five main components: data collection, pre-processing, classification, online clustering, and summarization. The integration between classification and clustering enables events to be detected, as well as related smaller-scale “disruptive events,” smaller incidents that threaten social safety and security or could disrupt social order. We present an evaluation of the effectiveness of detecting events using a variety of features derived from Twitter posts, namely temporal, spatial, and textual content. We evaluate our framework on a large-scale, real-world dataset from Twitter. Furthermore, we apply our event detection system to a large corpus of tweets posted during the August 2011 riots in England. We use ground-truth data based on intelligence gathered by the London Metropolitan Police Service, which provides a record of actual terrestrial events and incidents during the riots, and show that our system can perform as well as terrestrial sources, and even better in some cases.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {18},
numpages = {26},
keywords = {Social media, classification, clustering, evaluation, event detection, feature selection}
}

@inproceedings{10.1109/CCGrid.2016.37,
author = {Villamizar, Mario and Garc\'{e}s, Oscar and Ochoa, Lina and Castro, Harold and Salamanca, Lorena and Verano, Mauricio and Casallas, Rubby and Gil, Santiago and Valencia, Carlos and Zambrano, Angee and Lang, Mery},
title = {Infrastructure cost comparison of running web applications in the cloud using AWS lambda and monolithic and microservice architectures},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.37},
doi = {10.1109/CCGrid.2016.37},
abstract = {Large Internet companies like Amazon, Netflix, and LinkedIn are using the microservice architecture pattern to deploy large applications in the cloud as a set of small services that can be developed, tested, deployed, scaled, operated and upgraded independently. However, aside from gaining agility, independent development, and scalability, infrastructure costs are a major concern for companies adopting this pattern. This paper presents a cost comparison of a web application developed and deployed using the same scalable scenarios with three different approaches: 1) a monolithic architecture, 2) a microservice architecture operated by the cloud customer, and 3) a microservice architecture operated by the cloud provider. Test results show that microservices can help reduce infrastructure costs in comparison to standard monolithic architectures. Moreover, the use of services specifically designed to deploy and scale microservices reduces infrastructure costs by 70% or more. Lastly, we also describe the challenges we faced while implementing and deploying microservice applications.1},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {179–182},
numpages = {4},
keywords = {AWS lambda, Amazon web services, cloud computing, microservice architecture, microservices, scalable applications, service oriented architectures, software architecture, software engineering},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@article{10.1145/2723872.2723890,
author = {Paiva, Jo\~{a}o and Rodrigues, Lu\'{\i}s},
title = {On Data Placement in Distributed Systems},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/2723872.2723890},
doi = {10.1145/2723872.2723890},
abstract = {Data placement refers to the problem of deciding how to assign data items to nodes in a distributed system to optimize one or several of a number of performance criteria such as reducing network congestion, improving load balancing, among others. This document reports on our experience when addressing this problem in distributed systems of different scales, namely: medium size datacenter-scale and internet-scale systems.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jan},
pages = {126–130},
numpages = {5},
keywords = {Data Placement, Distributed Systems, Key-Value Storage, Load Balancing, Replication, Scalability, Storage}
}

@inproceedings{10.1145/2976749.2989057,
author = {Stock, Ben and Pellegrino, Giancarlo and Rossow, Christian and Johns, Martin and Backes, Michael},
title = {POSTER: Mapping the Landscape of Large-Scale Vulnerability Notifications},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2989057},
doi = {10.1145/2976749.2989057},
abstract = {The Internet is an ever-growing ecosystem with diverse software and hardware applications deployed in numerous countries around the globe. This heterogenous structure, however, is reduced to a homogenous means of addressing servers, i.e., their IP address. Due to this, analyzing different Internet services for vulnerabilities at scale is easy, leading to many researcher focusing on large-scale detection of many types of flaws. On the other hand, the persons responsible for the administration of said services are as heterogenous as the Internet architecture itself: be it in spoken languages or knowledge of technical details of the services. The notification of vulnerable services has long been treated as a side note in research. Recently, the community has focussed more not only the detection of flaws, but also on the notification of affected parties. These works, however, only analyze a small segment of the problem space. Hence, in this paper, we investigate the issues encountered by the previous works and provide a number of future directions for research, ultimately aiming to allow for an easier means of notifying affected parties about vulnerabilities at scale.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1787–1789},
numpages = {3},
keywords = {vulnerability notification, web vulnerabilities},
location = {Vienna, Austria},
series = {CCS '16}
}

@proceedings{10.1145/3237009,
title = {ManLang '18: Proceedings of the 15th International Conference on Managed Languages &amp; Runtimes},
year = {2018},
isbn = {9781450364249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This conference serves as a premier forum for presenting and discussing novel results in all aspects of managed programming languages and runtime systems, which function as building blocks for some of the most important computing systems around. These systems range from small-scale (embedded and real-time systems) to large-scale (cloud-computing and big-data platforms) and anything in between (mobile, IoT, and wearable applications).},
location = {Linz, Austria}
}

@inproceedings{10.1145/3184407.3184430,
author = {Gotin, Manuel and L\"{o}sch, Felix and Heinrich, Robert and Reussner, Ralf},
title = {Investigating Performance Metrics for Scaling Microservices in CloudIoT-Environments},
year = {2018},
isbn = {9781450350952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184407.3184430},
doi = {10.1145/3184407.3184430},
abstract = {A CloudIoT solution typically connects thousands of IoT things with cloud applications in order to store or process sensor data. In this environment, the cloud applications often consist of microservices which are connected to each other via message queues and must reliably handle a large number of messages produced by the IoT things. The state of a message queue in such a system can be a challenge if the rate of incoming messages continuously exceeds the rate of outgoing messages. This can lead to performance and reliability degradations due to overloaded queues and result in the unavailability of the cloud application. In this paper we present a case study to investigate which performance metrics to be used by a threshold-based auto-scaler for scaling consuming microservices of a message queue in order to prevent overloaded queues and to avoid SLA violations. We evaluate the suitability of each metric for scaling I/O-intensive and compute-intensive microservices with constant and varying characteristics, such as service time. We show, that scaling decisions based on message queue metrics are much more resilient to microservice characteristics variations. In this case, relying on the CPU utilization may result in massive overprovisioning or no scaling decision at all which could lead to an overloaded queue and SLA violations. We underline the benefits of using message queue metrics for scaling decisions instead of the more traditional CPU utilization particularly for I/O-intensive microservices due to the vulnerability to variations in the microservice characteristics.},
booktitle = {Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {157–167},
numpages = {11},
keywords = {auto-scaler, cloud computing, internet of things (IoT), message queues, microservices, performance, performance metrics, threshold-based rules},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/3055624.3075944,
author = {Hu, Yonghao and Chen, Zhaohui and Liu, Xiaojun and Huang, Fei and Jia, Jinyuan},
title = {WebTorrent based fine-grained P2P transmission of large-scale WebVR indoor scenes},
year = {2017},
isbn = {9781450349550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055624.3075944},
doi = {10.1145/3055624.3075944},
abstract = {The latency of transmitting large-scale1 WebVR scenes over mobile Internet is known as the bottleneck problem. This paper attacks this challenging problem by combining graphics based progressive transmission and networking based P2P transmission together. Different with those pure research DVE (Distributed Virtual Environment)-P2P works built on simulation platform, a novel WebVR-P2P framework is realized based on WebTorrent and WebGL. At server side, huge WebVR scenes are lightweighted by finding all repetitive components and removing redundant ones, that avoids unnecessary transmission. Furthermore, large-scale WebVR indoor scenes are divided into smaller fine-grained subspaces in terms of closeness and visibility to lower networking congestions. These two preprocessing steps are integrated to decrease less bandwidth occupation at utmost. Then, each fine-grained subspace is packaged adaptively in terms of Frustum Fill Ratio (FFR) for smooth and efficient transmission. A new WebTorrent framework is extended to transmit Web3D files and all packaged WebVR subspaces are transferred in the peer-to-peer style. At Web-end, two-thread, one for package transferring and the other for rendering, is employed asynchronously to realize online real time rendering. Finally, WebVR-P2P platform with three layer architecture is implemented based on all above key technologies, a large-scale WebVR Metro scene (about 1GB) is chosen to test for P2P transmission performance in this WebVR-P2P platform, the practical experimenting results are conducted to show the effectiveness and potentiality of our proposed solution.},
booktitle = {Proceedings of the 22nd International Conference on 3D Web Technology},
articleno = {7},
numpages = {8},
keywords = {WebRTC, WebTorrent, WebVR, fine-grained preprocessing, frustum fill ratio (FFR), lightweight preprocessing, progressive transmission},
location = {Brisbane, Queensland, Australia},
series = {Web3D '17}
}

@inproceedings{10.1145/2987443.2987482,
author = {Orsini, Chiara and King, Alistair and Giordano, Danilo and Giotsas, Vasileios and Dainotti, Alberto},
title = {BGPStream: A Software Framework for Live and Historical BGP Data Analysis},
year = {2016},
isbn = {9781450345262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987443.2987482},
doi = {10.1145/2987443.2987482},
abstract = {We present BGPStream, an open-source software framework for the analysis of both historical and real-time Border Gateway Protocol (BGP) measurement data. Although BGP is a crucial operational component of the Internet infrastructure, and is the subject of research in the areas of Internet performance, security, topology, protocols, economics, etc., there is no efficient way of processing large amounts of distributed and/or live BGP measurement data. BGPStream fills this gap, enabling efficient investigation of events, rapid prototyping, and building complex tools and large-scale monitoring applications (e.g., detection of connectivity disruptions or BGP hijacking attacks). We discuss the goals and architecture of BGPStream. We apply the components of the framework to different scenarios, and we describe the development and deployment of complex services for global Internet monitoring that we built on top of it.},
booktitle = {Proceedings of the 2016 Internet Measurement Conference},
pages = {429–444},
numpages = {16},
keywords = {bgp measurement, bgp monitoring, internet measurement, internet routing, network measurement, network monitoring, real-time monitoring},
location = {Santa Monica, California, USA},
series = {IMC '16}
}

@inproceedings{10.1145/3219819.3219861,
author = {Borisyuk, Fedor and Gordo, Albert and Sivakumar, Viswanath},
title = {Rosetta: Large Scale System for Text Detection and Recognition in Images},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219861},
doi = {10.1145/3219819.3219861},
abstract = {In this paper we present a deployed, scalable optical character recognition (OCR) system, which we call Rosetta , designed to process images uploaded daily at Facebook scale. Sharing of image content has become one of the primary ways to communicate information among internet users within social networks such as Facebook, and the understanding of such media, including its textual information, is of paramount importance to facilitate search and recommendation applications. We present modeling techniques for efficient detection and recognition of text in images and describe Rosetta 's system architecture. We perform extensive evaluation of presented technologies, explain useful practical approaches to build an OCR system at scale, and provide insightful intuitions as to why and how certain components work based on the lessons learnt during the development and deployment of the system.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {71–79},
numpages = {9},
keywords = {optical character recognition, text detection, text recognition},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3224207.3224211,
author = {Huang, Chenn-Jung and Hu, Kai-Wen and Huang, Yu-Kang},
title = {A Day-Ahead Renewables-Based Power Scheduling System for Internet of Energy},
year = {2018},
isbn = {9781450364188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3224207.3224211},
doi = {10.1145/3224207.3224211},
abstract = {The rapid development of emerging technologies and significant cost reductions offered by the utilization of solar energy and wind power have made it feasible to replace traditional power generation methods with renewable energy sources in the future. However, one thing that distinguishes renewables from currently deployed centralized power sources is that the former are categorized as intermittent energy sources. What's more, the scale of renewables is relatively small and their deployment could be described as scattered. In the recent literature, the architecture of the Internet of Energy has been proposed to replace the current smart grid in the future. However, the large volume of energy produced, the copious amounts of accompanying consumption data, and the uncertainty of the arrival of electric vehicles and the intermittence of the renewable energy will result in the short-term energy management of the IoE in the future being much more complicated than the energy management of traditional power generation systems which still rely on centralized-control. We thus propose a day-ahead power scheduling system based on the architecture of the IoE to tackle these complex energy management problems. The whole power system is divided into different geographical regions under a hierarchical framework. The microgrids first collect electricity consumption data from smart appliances used in households and data pertaining to the power generating capacity of renewable energy sources at the microgrid level. Then, the regional energy routers schedule the usage of electricity for the customers by considering the efficiency of the use of distributed renewables and the battery storage systems. Notably, a reallocation mechanism is presented in this work to allow the energy routers to allocate excess electricity generated in a microgrid to others facing power supply shortages, whereby the maximal usage of distributed renewables and a reduction of the burden on some microgrids during time periods of peak load can be simultaneously achieved. The experimental results show that the hierarchical day-ahead power scheduling system proposed in this work can mitigate the dependency on traditional power plants effectively and balance peak and off-peak period loads in an electricity market.},
booktitle = {Proceedings of the International Conference on Data Processing and Applications},
pages = {49–53},
numpages = {5},
keywords = {Internet of Energy, data mining, optimization, power scheduling, renewables},
location = {Guangdong, China},
series = {ICDPA 2018}
}

@inproceedings{10.1145/3125649.3125653,
author = {Riviere, Theo and Ayala, Hector Gutierrez and Hajek, Jeremy},
title = {The Extension and Implementation of the Autonomous Movement Framework},
year = {2017},
isbn = {9781450351201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3125649.3125653},
doi = {10.1145/3125649.3125653},
abstract = {The internet changes the way we do business, with companies like Amazon, Uber, and Google reshaping the way commerce is done delivering packages. Companies, such as Nokia, are demonstrating drone fleets being used for public safety over large scale desert areas. Our research asked, could this technology be replicated on a small scale for independent operators to use? The initial goal of this project was to design and develop a framework to control and manage drone fleets for use in search and rescue and disaster relief. We were able to design a platform and framework that integrated common off-the-shelf drones and accessible Windows computers and Android Phones to build and deploy our Autonomous Movement Framework.},
booktitle = {Proceedings of the 6th Annual Conference on Research in Information Technology},
pages = {7–10},
numpages = {4},
keywords = {3dr iris, automation, autopilot, charging pad, delivery drone, ground station, mavlink, mavproxy, mission planner, open-source, pixhawk, python, quadcopter, rechargeable batteries, uav},
location = {Rochester, New York, USA},
series = {RIIT '17}
}

@inproceedings{10.1145/3277593.3277606,
author = {Mani, Sathiya Kumaran and Durairajan, Ramakrishnan and Barford, Paul and Sommers, Joel},
title = {An architecture for IoT clock synchronization},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277606},
doi = {10.1145/3277593.3277606},
abstract = {In this paper, we describe an architecture for clock synchronization in IoT devices that is designed to be scalable, flexibly accommodate diverse hardware, and maintain tight synchronization over a range of operating conditions. We begin by examining clock drift on two standard IoT prototyping platforms. We observe clock drift on the order of seconds over relatively short time periods, as well as poor clock rate stability, each of which make standard synchronization protocols ineffective. To address this problem, we develop a synchronization system, which includes a lightweight client, a new packet exchange protocol called SPoT and a scalable reference server. We evaluate the efficacy of our system over a range of configurations, operating conditions and target platforms. We find that SPoT performs synchronization 22x and 17x more accurately than MQTT and SNTP, respectively, at high noise levels, and maintains a clock accuracy of within ~15ms at various noise levels. Finally, we report on the scalability of our server implementation through microbenchmark and wide area experiments, which show that our system can scale to support large numbers of clients efficiently.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {17},
numpages = {8},
keywords = {MQTT, SNTP, internet of things, measurement, time, wireless},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/3128128.3128129,
author = {Talei, Hanaa and Essaaidi, Mohamed and Benhaddou, Driss},
title = {Smart campus energy management system: advantages, architectures, and the impact of using cloud computing},
year = {2017},
isbn = {9781450352819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128128.3128129},
doi = {10.1145/3128128.3128129},
abstract = {A Microgrid is a subset of smart grid, a small-scale electrical system powered with renewable energy resources that can operate either in a connected or a disconnected mode to/from the main grid. Given that universities are very important electricity consumers, using an academic Microgrid will solve many problems regarding energy usage[1]. The purpose of this paper, is to describe Microgrid components with an emphasis on energy management (EMS). Given its vital role, the paper presents different architectures of an EMS and discusses how cloud computing can be incorporated to the Microgrid architecture to improve the EMS efficiency. The paper concludes with presenting results of EMS data collection using Kaa IoT platform.},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment},
pages = {1–7},
numpages = {7},
keywords = {IoT, agent, cloud computing, energy management system, microgrid},
location = {Rabat, Morocco},
series = {ICSDE '17}
}

@inproceedings{10.1145/3319535.3363204,
author = {Naor, Moni and Pinkas, Benny and Ronen, Eyal},
title = {How to (not) Share a Password: Privacy Preserving Protocols for Finding Heavy Hitters with Adversarial Behavior},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363204},
doi = {10.1145/3319535.3363204},
abstract = {Bad choices of passwords were and are a pervasive problem. Users choosing weak passwords do not only compromise themselves, but the whole ecosystem. E.g, common and default passwords in IoT devices were exploited by hackers to create botnets and mount severe attacks on large Internet services, such as the Mirai botnet DDoS attack. We present a method to help protect the Internet from such large scale attacks. Our method enables a server to identify popular passwords (heavy hitters), and publish a list of over-popular passwords that must be avoided. This filter ensures that no single password can be used to compromise a large percentage of the users. The list is dynamic and can be changed as new users are added or when current users change their passwords. We apply maliciously secure two-party computation and differential privacy to protect the users' password privacy. Our solution does not require extra hardware or cost, and is transparent to the user. Our private heavy hitters construction is secure even against a malicious coalition of devices which tries to manipulate the protocol to hide the popularity of some password that the attacker is exploiting. It also ensures differential privacy under continual observation of the blacklist as it changes over time. As a reality check we conducted three tests: computed the guarantees that the system provides wrt a few publicly available databases, ran full simulations on those databases, and implemented and analyzed a proof-of-concept on an IoT device. Our construction can also be used in other settings to privately learn heavy hitters in the presence of an active malicious adversary. E.g., learning the most popular sites accessed by the Tor network.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1369–1386},
numpages = {18},
keywords = {differential privacy, heavy hitters, malicious model, passwords, secure computation},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1145/3323873.3326924,
author = {Abdur Rahman, Md and Loukas, George and Maruf Abdullah, Syed and Abdu, Areej and Sadiqur Rahman, Syed and Hassanain, Elham and Arafa, Yasmine},
title = {Blockchain and IoT-based Secure Multimedia Retrieval System for a Massive Crowd: Sharing Economy Perspective},
year = {2019},
isbn = {9781450367653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323873.3326924},
doi = {10.1145/3323873.3326924},
abstract = {Blockchain's properties in addressing trust in highly decentralized environments can make it an enabler for novel sharing economy services. In this paper, we demonstrate the practicality of blockchain-based Secure IoT as a Service (SIoTaaS), where an IoT device can be rented from a service provider, securely and in a privacy-preserving fashion. Our framework allows the simultaneous operations of distinct providers of IoT-based sharing economy services at a large scale. Multiple parties can securely share text and multimedia in the context of location and point-of-interest sharing, perform financial transactions by hiding true identity of parties involved in various online transactions, perform user and IoT registration, transfer value transactions via Ethereum tokens between providers and consumers, as well as raw IoT data payload. This can turn smart room IoT devices, such as smart locks, light bulbs, air conditioning and fans into rentable business entities within a secure sharing economy platform. We will demonstrate such a proof of concept IoT sharing economy framework, which is specifically designed to support the temporary IoT needs of very large numbers of users, such as Hajj pilgrims concentrating for a short period of time at a single area in Saudi Arabia.},
booktitle = {Proceedings of the 2019 on International Conference on Multimedia Retrieval},
pages = {404–407},
numpages = {4},
keywords = {IoT as a service, blockchain, off-chain, sharing economy},
location = {Ottawa ON, Canada},
series = {ICMR '19}
}

@inproceedings{10.1145/3132211.3132456,
author = {Huang, Zhe and Balasubramanian, Bharath and Alsudais, Azzam and Joshi, Kaustubh},
title = {An edge-facilitated message broker for scalable device discovery: poster},
year = {2017},
isbn = {9781450350877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132211.3132456},
doi = {10.1145/3132211.3132456},
abstract = {Searching for a particular device in an ocean of devices is a perfect illustration of the idiom 'searching a needle in a haystack'. Yet the future IoT and edge computing platforms are facing an even more challenging problem because their mission-critical operations (e.g., application orchestration, device and application telemetry, inventory management) depend on their capability of identifying nodes of interest from potentially millions of service providers across the globe according to highly dynamic attributes such as geo-location information, bandwidth availability, real-time workload and so on. For example, a vehicular-based crowd sensing application that collects air quality data near an exit of a highway needs to locate cars in close proximity to the exit among millions of cars running on the road. In a business model where an enterprise offers a framework for clients to avail such edge/IoT services, we investigate the following problem: "among millions of IoT/Edge nodes, how do we locate and communicate with only those nodes that satisfy certain attributes, especially when some of these attributes change rapidly?"In this paper, we address this problem through the design of a scalable message broker based on the following novel intuition: device discovery should be a joint effort between a centrally managed enterprise-level system (high availability, low accuracy) and the fully decentralized edge (high accuracy, unpredictable availability). To elaborate, the enterprise can centrally maintain and manage the attributes of all the IoT devices. However, since millions of devices cannot constantly update their attribute information, central management has the issue of attribute staleness. Clearly the devices themselves have the most up-to-date information. However, it is not feasible for every request to be routed to million devices connected by unpredictable networks, where only some of them may possess the correct attributes. In this paper, we propose a message broker, in which requests for relatively static device attributes are handled by the centrally managed system, whereas, requests for dynamic attributes are handled by peer-to-peer networks of the edge devices containing those attributes. This combination provides a scalable solution wherein, based on client needs, we can obtain attribute values without compromising on freshness or performance.There exist several previous works that aim to tackle the device searching problem. Name-based networking solutions such as Intentional Naming System (INS) [1], Auspice [5], and global name service [3] propose to implement a centrally managed name resolution service. Devices periodically update their status information and descriptions in a push approach. While maintaining complete knowledge of every device in the network centrally makes the searching much easier, the excessive workload from millions of devices updating their status in a highly dynamic environment renders the scheme unsaleable. At the other end of the spectrum, a pull-based solution such as Geocast [2] eliminates the status update workload entirely by forwarding device searching query to the devices and relying on the devices to voluntarily identify themselves if their status and attributes match the query. However, the pull-based solutions require an attribute-aware message routing scheme such as distributed hash table [4] that knows exactly how to reach to devices that may match the query. Such design also suffers from longer query response delay caused by query forwarding, and increased security risks because they trust the devices to honestly report their identities and attributes. A better solution should be able to combine the strengths from both the push and pull design principles.Based on the targeted edge/IoT enviroment and applications, we identify the following design goals for a message broker being able to support a large scale, highly dynamic network enviroment:• Searchability. The message broker must be able to identify and access devices according to arbitrary attributes, service descriptions and queries.• Verifiability. The message broker must be able to verify attributes and descriptions with the authoritative information source.• Scalability. The message broker must be able to support a large scale deployment with minimum infrastructure cost.• Timeliness. The message broker must identify devices according to the latest attributes and device status when requested by the users.• Inclusiveness. The message broker must return a device list that contains every active devices that match the received query.• Robustness. The message broker must be resilient to service failures and high network churn.In our design, searchability allows the message broker to be expressive so that applications and devices can declare their own attribute keys and values. The message broker allows device query to contain tailor-made device searching logic so that the applications have tremendous flexibility to define how to identify the corresponding nodes of interest.Among the declared attribute keys and values, the applications also have the freedom to declare who are the authoritative source of information for each attribute. Only the authoritative information source is allowed to access and modify certain attribute field of the corresponding devices. Such verifiability can effectively prevent malicious attacks such as identity spoofing and eavesdropping.To achieve scalability, the message broker offloads the device status upload workload to the end devices. Some selected end devices will receive status update from others. By maintaining the list of such representative devices, the message broker service is capable of pulling the up-to-date device status when needed.The message broker can effectively offset the extra workload of frequently updating the dynamic attribute by limiting the scope of message exchanges. This mechanism allows the message broker to offer multi-granular attribute update channels for applications with different timeliness requirements. Static attributes such as device affiliation can be updated through a global channel while a dynamic attribute such as geo-location will be exchanged in a smaller scope. In the extreme cases where real-time varying attributes will be collected, devices are no longer exchanging/updating their attributes and status information with other devices. A communication channel will be established among the nodes of interests for pulling the status and attributes directly in an on-demand manner.In our design, the inclusiveness and robustness are achieved by strong semantics that regulate and manage the message exchanges among devices. The inclusiveness guarantees that the applications can reach every active node of interest through the message broker. It givens the applications complete view of available services and resources in the globe. Each of the components is designed and implemented as a distributed system which tolerates certain level of failures. More importantly, they are designed to be self-sustainable so that they do not depend on each other to function properly.Figure 1 shows the architecture of our proposed message broker system that we refer to as EF-broker. EF-broker mainly provides three services: (1) device discovery and inventory management (DDIM), (2) dynamic group management (DGM), and (3) communication channel orchestration engine (CCOE). The DDIM is implemented as a centrally managed, geo-distributed bookkeeping service that maintain a global view of all active devices. It serves as the rendezvous point of newly arrived devices while it also maintains the availability of devices by requiring them to update their attributes and status in a low frequency as heartbeat signals. Such global attribute update channel disseminates the status update messages to geo-distributed DDIM servers in an eventual consistent manner. DDIM is capable of answering device queries that depend on relatively static attribute values efficiently. To keep track of dynamic device attributes whose values change frequently, for each dynamic attribute key, a peer-to-peer cluster of devices that we refer to as dynamic group (DG) are created. Devices in the same DG exchange attributes and status information in a much higher frequency using gossip protocols. Because of the gossip protocols, every device maintains fresh attributes and status of other members in the same group. Representative devices are selected as the entry points of each DG. The representative nodes disseminate and maintain the membership list of the DG in a strong consistent manner in order to achieve inclusiveness. The centrally managed, geo-distributed dynamic group management (DGM) service is introduced to manage the life-cycles of a large number of DGs. It is responsible of creating, terminating, splitting, merging of DGs, as well as maintaining and repairing entry points for DGs. The DGM service provides a more fine-granular attribute update channel by forwarding device queries to the entry points of the appropriate DGs. At last, EF-broker is also capable of creating pub-sub channels among devices in the DGs in an on-demand manner so that applications can pull real-time attributes and status directly from the nodes of interests. The CCOE is introduced to manage the life-cycles of the pub-sub channels.},
booktitle = {Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
articleno = {25},
numpages = {2},
location = {San Jose, California},
series = {SEC '17}
}

@inproceedings{10.1145/3093338.3106387,
author = {B\"{o}rner, Katy and Record, Elizabeth},
title = {Macroscopes for Making Sense of Science},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3106387},
doi = {10.1145/3093338.3106387},
abstract = {Interactive data visualizations known as macroscopes have great potential as tools for exploring, understanding, and communicating science [1]. Macroscopes empower individuals to interact with data in order to better understand it from multiple perspectives. Jo\"{e}l de Rosnay [2] first introduced the concept of the macroscope in his 1979 book, The Macroscope: A New World Scientific System, where he describes macroscopes as tools "not used to make things larger or smaller, but to observe what is at once too great, too slow, and too complex for our eyes." The Cyberinfrastructure for Network Science Center, in partnership with Indiana University's Advanced Visualization Laboratory, developed a touchscreen kiosk to showcase the macroscopes in the international science mapping exhibit, Places &amp; Spaces: Mapping Science [2].The exhibit showcases best examples of visualization techniques from a range of disciplines in order to stimulate the development of new algorithms, workflows, and techniques for visualizing the fields of science, technology, and innovation. New exhibit pieces are selected with the input of an advisory board of prominent data visualization experts from both academia and industry.The 2016 collection of interactive data visualizations showcases innovative examples of how to visualize large scientific data sets with user-friendly interfaces for interacting with data, developing new questions, and discovering new insights. It includes a diverse collection of interactive visualizations that draw on multiple types of data, ranging from geo-tagged social media posts to metadata from a digital library to citation counts and ship transponder data. The first macroscope uses social media posts to map smells across 12 cities. Humans can differentiate thousands of different odors, yet city officials and urban planners deal only with the management of a few bad odors. In creating Smelly Maps [3, 4, 5, 6, 7, 8], University of Turin computer science professor Rossano1 Schifanella and Bell Labs researchers Luca Maria Aiello and Daniele Quercia teamed up to introduce a new stream of research that celebrates the complex aromas of our cities and makes it possible to use this information in urban design.To map urban smellscapes, the project team first created a lexicon of smell-related words. Then, they gathered geotagged social media posts from Flickr, Instagram, and Twitter that included smell-related words. Finally, smell information was mapped by street segment.The second macroscope provides a visual approach to the vast digital collections of the HathiTrust Digital Library, a collective "elephant's memory" or storehouse of knowledge. Run by a consortium of international research libraries, HathiTrust serves as a shared and growing repository for digital copies of more than 14 million publications that span 2000 years. Visualization software developer David Reagan, curator Lisel Record, and information scientist Katy B\"{o}rner developed this visualization to provide access to the geographic and temporal diversity of the collection using freely available metadata.Yellow circles show publication locations, with the size of the circle showing how many publications were printed in that location. Lines connect publication locations to places where that language is spoken, illustrating the connection between publication location and potential readers.The third macroscope concerns itself with collaborative scientific research activity at the world's top institutions. While it may be simple to estimate which research institutions are at the top of their game, it is hard to create a statistical model to measure and map this. Lutz Bornmann, a sociologist of science at the Max Planck Society; R\"{u}diger Mutz, a Swiss researcher in social psychology and higher education; Moritz Stefaner, an independent data visualization expert; and F\'{e}lix de Moya Aneg\'{o}n, senior researcher at SCImago, took up the challenge and created Excellence Networks [9, 10, 11].This interactive web application shows how universities and other research institutions collaborate. Institutions in the SCImago Institutions Rankings were categorized by subject area. Each institution was then mapped in relation to its collaborators. The resulting networks show how successfully---in terms of citations---an institution has collaborated with others working in the same field.Created by FleetMon, a company that provides live vessel tracking, the last macroscope animates a week of ship traffic on the seven seas as seen from space [12]. The movements of hundreds of thousands of vessels were captured using shore and satellite-based tracking data from FleetMon and its partner, Luxspace.Many cargo ships, tankers, ferries, cruise ships, yachts, and tugs carry transponders that transmit their locations. That data is then made available to amateur ship spotters and maritime businesses alike through the interactive FleetMon Explorer tool. Using the tool, one can follow the flow of jet fuel and agricultural commodities around the globe, track a fleet of cruise ships in real time, or monitor traffic at ports around the world.In sum, while much of what is phenomenal about these visualizations is hidden "under the hood," and not immediately visible to a casual audience, advanced data mining techniques and algorithms are necessary in order to create these visualizations. Advanced computing provides the engines for such macroscopes, which in turn bring the power of computational research to scientists and policymakers who can then use large scale data sets to advance their areas of research or hone their expertise. This dramatically broadens the field of people who can use large scale data sets to develop new insights and facilitates scholarship in a variety of disciplines.Advanced research computing pushes the boundaries in terms of working with large data sets and providing complicated linkages between the raw data and a meaningful interface. Additionally, insights from large scale data sets lay the foundation for advancing scientific disciplines on any number of fronts. The ability to understand and visualize large data sets makes new insights possible and inspires novel questions.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {64},
numpages = {2},
keywords = {data set, data visualization, macroscope},
location = {New Orleans, LA, USA},
series = {PEARC '17}
}

@article{10.1145/3139293,
author = {Rullo, Antonino and Midi, Daniele and Serra, Edoardo and Bertino, Elisa},
title = {Pareto Optimal Security Resource Allocation for Internet of Things},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2471-2566},
url = {https://doi.org/10.1145/3139293},
doi = {10.1145/3139293},
abstract = {In many Internet of Thing (IoT) application domains security is a critical requirement, because malicious parties can undermine the effectiveness of IoT-based systems by compromising single components and/or communication channels. Thus, a security infrastructure is needed to ensure the proper functioning of such systems even under attack. However, it is also critical that security be at a reasonable resource and energy cost. In this article, we focus on the problem of efficiently and effectively securing IoT networks by carefully allocating security resources in the network area. In particular, given a set of security resources R and a set of attacks to be faced A, our method chooses the subset of R that best addresses the attacks in A, and the set of locations where to place them, that ensure the security coverage of all IoT devices at minimum cost and energy consumption. We model our problem according to game theory and provide a Pareto-optimal solution in which the cost of the security infrastructure, its energy consumption, and the probability of a successful attack are minimized. Our experimental evaluation shows that our technique improves the system robustness in terms of packet delivery rate for different network topologies. Furthermore, we also provide a method for handling the computation of the resource allocation plan for large-scale networks scenarios, where the optimization problem may require an unreasonable amount of time to be solved. We show how our proposed method drastically reduces the computing time, while providing a reasonable approximation of the optimal solution.},
journal = {ACM Trans. Priv. Secur.},
month = {oct},
articleno = {15},
numpages = {30},
keywords = {Internet of things, clustering, pareto analysis, stochastic allocation}
}

@inproceedings{10.1145/3011141.3011148,
author = {Inomoto, Hikaru and Saiki, Sachio and Nakamura, Masahide and Matsumoto, Shinsuke},
title = {Mission-oriented large-scale environment sensing based on analogy of military system},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011148},
doi = {10.1145/3011141.3011148},
abstract = {As typically seen in Smart City, emerging technologies enable large-scale environment sensing using IoT devices deployed in a wide area. From the viewpoint of cost and efficiency, infrastructure of the large-scale environment sensing should be shared by multiple applications, with dynamically adapting the sensing behavior for different purposes. To achieve this, the infrastructure must implement a clever method that can command and control a lot of IoT devices in good order. To implement such multi-purpose large-scale environment sensing, we introduce an analogy of military system. Specifically, we propose a mission-oriented sensing with army hierarchy, where individual IoT devices and their dynamic purposes are regarded as soldiers and missions, respectively.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {414–421},
numpages = {8},
keywords = {IoT, context-aware sensing, large-scale environment sensing, mission-oriented sensing},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@inproceedings{10.1145/3277593.3277611,
author = {Giura, Paul and Jim, Trevor},
title = {Sapphire: using network gateways for IoT security},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277611},
doi = {10.1145/3277593.3277611},
abstract = {The increasing popularity of IoT devices in both residences and enterprises has widened the attack surface for network connected devices. Many popular IoT devices have unpatched vulnerabilities or default passwords and lack basic security mechanisms, making them easy prey for malware and botnets.In this paper, we share our experience of designing and using an experimental deployment of network gateways to provide IoT security, to both the IoT devices and the gateways themselves. We propose three approaches for framework design and collecting the network data, each providing different levels of visibility into IoT device behavior. Finally we present our methodology and experimental evaluation of a small-scale deployment of gateways and IoT devices for volumetric anomaly detection and IoT device identification using the data collected by the gateways behind the NAT, or in the cloud, outside the NAT.We believe that securing IoT devices can be more efficient and effective when there is more visibility into device activity and security capabilities are deployed close to the devices, in the gateway. However, a hybrid approach in which data is collected on the gateways and analyzed in the cloud can be more practical; special considerations regarding sensitive data storage and privacy guarantees have to be taken into account.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {5},
numpages = {8},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/3229710.3229740,
author = {Shaik, Shehenaz and Baskiyar, Sanjeev},
title = {Hierarchical and Autonomous Fog Architecture},
year = {2018},
isbn = {9781450365239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229710.3229740},
doi = {10.1145/3229710.3229740},
abstract = {Cloud computing paradigm stops short in its offerings towards deployment of latency-critical and bandwidth-intensive applications. Fog computing paradigm emerged as a promising solution to realize deployment of large scale IoT environments and low latency real-time services, leveraging large number of resource-constrained, heterogeneous fog nodes distributed across vast geographical areas and located closer to users and data sources, as compared to core cloud which is usually located at large data centers, far from users and IoT devices. To facilitate efficient deployment of services on the fog infrastructure, we propose Hierarchical and Autonomous Fog Architecture (HAFA) to organize heterogeneous fog nodes into a multi-layered connected hierarchy based on several parameters such as physical location, distance from IoT devices and/or users, node resource configuration, privacy and security. Fog nodes are grouped to facilitate resource pooling and local control, and groups of fog nodes are linked to facilitate disaster readiness and autonomy. HAFA helps reducing effort in finding an optimal node with required resource characteristics towards service deployment.},
booktitle = {Workshop Proceedings of the 47th International Conference on Parallel Processing},
articleno = {18},
numpages = {8},
keywords = {Architecture, Cloud Computing, Fog Computing, Infrastructure Management, Internet of Things, Smart City},
location = {Eugene, OR, USA},
series = {ICPP Workshops '18}
}

@inproceedings{10.1145/3190645.3190678,
author = {Jerkins, James A. and Stupiansky, Jillian},
title = {Mitigating IoT insecurity with inoculation epidemics},
year = {2018},
isbn = {9781450356961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190645.3190678},
doi = {10.1145/3190645.3190678},
abstract = {Compromising IoT devices to build botnets and disrupt critical infrastructure is an existential threat. Refrigerators, washing machines, DVRs, security cameras, and other consumer goods are high value targets for attackers due to inherent security weaknesses, a lack of consumer security awareness, and an absence of market forces or regulatory requirements to motivate IoT security. As a result of the deficiencies, attackers have quickly assembled large scale botnets of IoT devices to disable Internet infrastructure and deny access to dominant web properties with near impunity. IoT malware is often transmitted from host to host similar to how biological viruses spread in populations. Both biological viruses and computer malware may exhibit epidemic characteristics when spreading in populations of vulnerable hosts. Vaccines are used to stimulate resistance to biological viruses by inoculating a sufficient number of hosts in the vulnerable population to limit the spread of the biological virus and prevent epidemics. Inoculation programs may be viewed as a human instigated epidemic that spreads a vaccine in order to mitigate the damage from a biological virus. In this paper we propose a technique to create an inoculation epidemic for IoT devices using a novel variation of a SIS epidemic model and show experimental results that indicate utility of the approach.},
booktitle = {Proceedings of the ACMSE 2018 Conference},
articleno = {4},
numpages = {6},
keywords = {IoT, botnet, epidemic, inoculation},
location = {Richmond, Kentucky},
series = {ACMSE '18}
}

@inproceedings{10.1145/3078714.3078723,
author = {Mondal, Mainack and Silva, Leandro Ara\'{u}jo and Benevenuto, Fabr\'{\i}cio},
title = {A Measurement Study of Hate Speech in Social Media},
year = {2017},
isbn = {9781450347082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078714.3078723},
doi = {10.1145/3078714.3078723},
abstract = {Social media platforms provide an inexpensive communication medium that allows anyone to quickly reach millions of users. Consequently, in these platforms anyone can publish content and anyone interested in the content can obtain it, representing a transformative revolution in our society. However, this same potential of social media systems brings together an important challenge---these systems provide space for discourses that are harmful to certain groups of people. This challenge manifests itself with a number of variations, including bullying, offensive content, and hate speech. Specifically, authorities of many countries today are rapidly recognizing hate speech as a serious problem, specially because it is hard to create barriers on the Internet to prevent the dissemination of hate across countries or minorities. In this paper, we provide the first of a kind systematic large scale measurement and analysis study of hate speech in online social media. We aim to understand the abundance of hate speech in online social media, the most common hate expressions, the effect of anonymity on hate speech and the most hated groups across regions. In order to achieve our objectives, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both of these systems. Our results identify hate speech forms and unveil a set of important patterns, providing not only a broader understanding of online hate speech, but also offering directions for detection and prevention approaches.},
booktitle = {Proceedings of the 28th ACM Conference on Hypertext and Social Media},
pages = {85–94},
numpages = {10},
keywords = {anonymity, hate speech, pattern recognition, social media, twitter, whisper},
location = {Prague, Czech Republic},
series = {HT '17}
}

@inproceedings{10.1145/3300061.3355631,
author = {Casas, Pedro and Wamser, Florian and Bustamante, Fabi\'{a}n and Choffnes, David},
title = {Internet-QoE 2019: 4th Internet-QoE Workshop on QoE-based Analysis and Management of Data Communication Networks},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3355631},
doi = {10.1145/3300061.3355631},
abstract = {After three highly successful editions of the Internet-QoE workshop organized at ACM SIGCOMM 2016, ACM SIGCOMM 2017, and IEEE ICDCS 2018, the goal of the fourth edition of the Internet-QoE workshop is to scale the concepts of Quality of Experience (user satisfaction, user engagement, and behavioral analysis) out of the lab studies context and bring it to the analysis and operation of distributed systems and communication networks, giving a user-centric perspective to the research performed by the MOBICOM community. By fostering an explicit and deep integration of the end-user directly into the design, analysis and management of large-scale operational networks, we expect to reduce the gap between QoE research and its application to future network management paradigms, as well as to provide a more targeted end-user perspective to the research on distributed communication systems. The 4th edition of Internet-QoE also focuses on novel end-user services enabled by next generation technologies such as immersive media (3D, Virtual Reality and Augmented Reality), self-driving cars, intelligent manufacturing systems, Industry 4.0 and tactile Internet, 5G ultra-low-latency mobile networks, and real-time applications.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {108},
numpages = {2},
keywords = {distributed large-scale measurements, interdisciplinary workshop, quality of experience, user engagement, user experience},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inproceedings{10.1145/3129457.3129497,
author = {Ye, Kejiang},
title = {Anomaly Detection in Clouds: Challenges and Practice},
year = {2017},
isbn = {9781450349239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129457.3129497},
doi = {10.1145/3129457.3129497},
abstract = {Cloud computing is an important infrastructure for many enterprises. After 10 years of development, cloud computing has achieved a great success, and has greatly changed the economy, society, science and industries. In particular, with the rapid development of mobile Internet and big data technology, almost all of the online services and data services are built on the top of cloud computing, such as the online banking services provided by banks, the electronic services provided by the news media, the government cloud information systems provided by the government departments, the mobile services provided by the communications companies. Besides, tens of thousands of Start-ups rely on the provision of cloud computing services. Therefore, ensuring cloud reliability is very important and essential. However, the reality is that the current cloud systems are not reliable enough.On February 28th 2017, Amazon Web Services, the popular storage and hosting platform used by a huge range of companies, experienced S3 service interruption for 4 hours in the Northern Virginia (US-EAST-1) Region, and then quickly spread other online service providers who rely on the S3 service [2]. This failure caused a huge economic loss. It is because cloud computing service providers typically set a Service Level Agreement (SLA) with customers. For example, when customers require 99.99% availability, it means that 99.99% of the time must meet the requirement for 365 days per year. If the service breaks more than 0.01%, compensation is required.In fact, with the continuous development and maturity of cloud computing, a large number of traditional business systems have been deployed on the cloud platform. Cloud computing integrates existing hardware resources through virtualization technology to create a shared resource pool that enables applications to obtain computing, storage, and network resources on demand, effectively enhancing the scalability and resource utilization of traditional IT infrastructures and significantly reducing the operation cost of the traditional business systems. However, with the growing number of applications running on the cloud, the scale of cloud data center has been expanding, the current cloud computing system has become very complex, mainly reflected in: 1) Large scale. A typical data center involves more than 100,000 servers and 10,000 switches, more nodes usually mean higher probability of failure; 2) Complex application structure. Web search, e-commerce and other typical cloud program has a complex interactive behavior. For example, an Amazon page request involves interaction with hundreds of components [7], error in any one component will lead to the whole application anomalies; 3) Shared resource pattern. One of the basic features of cloud computing is resource sharing, a typical server in Google Cloud data center hosts 5 to 18 applications simultaneously, each server runs about 10.69 applications [5]. Resource competition will interfere with each other and affect application performance. The complexity of these cloud computing systems, the complexity of application interaction structure and the inherent sharing pattern of cloud platforms make cloud systems more prone to performance anomalies than traditional platforms. It can be said that anomaly is a normal state in cloud computing [3].For further analysis, resource competition, resource bottlenecks, misconfiguration, software defects, hardware failures, and external attacks can cause cloud system anomalies or failures. Performance anomaly refers to any sudden degradation of performance that deviates from the normal behavior of the system. Unlike outages that cause the system to stop running immediately, performance anomalies typically result in a decrease in system efficiency. The reasons such as misconfiguration, software defects, hardware failures, can often cause performance anomalies. For cloud computing systems, it is not enough to detect outages or other functional anomalies, because those anomalies often cause service interruption and can be resolved by simply restarting or replacing hardware. While performance anomalies caused by resource sharing and interference are more worthy of attention [4], because the performance anomalies can be eliminated before service interruption to ensure continued services.If the performance anomalies of cloud computing system are not timely handled, it may cause very serious consequences, which not only affect the business system to run normally, but also hinder the enterprise to deploy their services on cloud systems. Especially for the those latency-sensitive cloud applications, it is extremely important to eliminate performance anomalies in a timely manner. For example, Amazon found a 1% decline in sales per 100ms latency, Google found a 20% drop in traffic for every 0.5s latency in search page, and stock traders found that it would cause a loss of 400 Million dollars if their electronic trading platform lagged behind the competitors by 5 ms. Other research also shows that the average maximum time of cloud data center failure is about 100 hours, which seriously affects the experience of cloud service users. In the cloud environment, as a large number of business systems are deployed in the cloud data center, cloud data center failure will affect a large number of users, such as the previously mentioned Amazon S3 failure, resulting in serious economic losses.Thus, timely and accurate detection of the cloud computing anomalies is very important. Anomaly detection is an effective means to help cloud platform administrators monitor and analyze cloud behaviors and improve cloud reliability. It helps to identify unusual behavior of the system so that cloud platform administrators can take proactive operations before a system crash or service failure. However, due to the characteristics such as large-scale, complex and resource sharing, it is very difficult to accurately detect anomalies in cloud computing. If the anomalies can not be accurately detected, the further recovery will be out of the question. Due to the importance of the problem, current mainstream cloud computing service providers usually provide online monitoring services.Amazon developed CloudWatch [1] for its EC2 users to monitor virtual machine instance status, resource usage, network traffic, disk read and write status, etc. Google developed Dapper framework [6] to provide state monitoring for Google search engines. But those monitoring services only provide simple data presentation, lack of in-depth analysis of monitoring data (such as cross-level correlation analysis), and is not intelligent enough for anomaly reasoning (such as cross-node fault source localization). In cloud data center, as the size of detected objects is very large and interrelated, the object being detected itself is in a high dynamic environment, it is very challenging to detect anomalies in an accurate, real-time and adaptive way. The existing anomaly detection solution lacks effective discovery and reasoning of the anomaly, which leads to the inability to locate and eliminate the anomaly in time. This is also the main reason that causes the current cloud platform accident frequently.In this talk, we introduce our solution for anomaly detection in clouds.1) Anomaly detection. In order to efficiently detect the potential anomalies, we perform large-scale offline performance testing and also create an online detection method. i) Offline testing. The purpose is to find the key performance bottleneck and quantify comparison between difference hardware and software. We first propose a three-layer benchmarking methodology to fully evaluates cloud performance and then present a new benchmark suite -- Virt-B [11] -- that measures various scenarios, such as single machine virtualization, server consolidation, VM mapping, VM live migration, HPC virtual clusters and Hadoop virtual cluster. Finally, we introduce a performance testing toolkit to automate the benchmarking process. ii) Online detection. The purpose is to monitor applications in real time and quickly detect potential faults. We propose a quantile regression based online anomaly detection method and did a case study on 67 real Yahoo! anomaly traffic datasets.2) Anomaly inference. We propose a dependency graph based anomaly inference method. Dependency reflects interaction relationship and execution path, and can be used for fault localization. There are usually three methods that can be used to fetch the dependency graph: instrumentation, extract configuration files and analyze network traffic. We create light-weight agents to monitor the traffic and use sampling technique to reduce the overheads. The advantages of our solution include: supports VMs (Xen/KVM), accuracy guarantee based on probability theory, dynamic dependency construction, focuses on the PM/VM layer, and low overheads.3) Anomaly recovery. The traditional recovery methods like Checkpoint/Restart has high overheads and are not suitable for latency-sensitive applications. While we propose two solutions: cache-aware fault VM isolation and migration-based fault recovery.i) Cash-Aware Fault Isolation [8]. We first give a quantitative definition of isolation, then we propose a VM-core scheduling method to improve the fault isolation. ii) Fault Recovery based on Migration [9, 10, 12]. We propose a fault recovery method based on live migration. The main advantages include: online service, low overheads, and can be used in large-scale cloud datacenters.},
booktitle = {Proceedings of the First Workshop on Emerging Technologies for Software-Defined and Reconfigurable Hardware-Accelerated Cloud Datacenters},
articleno = {6},
numpages = {2},
location = {Xi'an, China},
series = {ETCD'17}
}

@inproceedings{10.1145/3077136.3080746,
author = {Xu, Keyang and Liu, Zhengzhong and Callan, Jamie},
title = {De-duping URLs with Sequence-to-Sequence Neural Networks},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080746},
doi = {10.1145/3077136.3080746},
abstract = {Many URLs on the Internet point to identical contents, which increase the burden of web crawlers. Techniques that detect such URLs (known as URL de-duping) can greatly save resources such as bandwidth and storage for crawlers. Traditional de-duping methods are usually limited to heavily engineered rule matching strategies.In this work, we propose a novel URL de-duping framework based on sequence-to-sequence (Seq2Seq) neural networks. A single concise translation model can take the place of thousands of explicit rules. Experiments indicate that a vanilla Seq2Seq architecture yields robust and accurate results in detecting duplicate URLs. Furthermore, we demonstrate the efficiency of this framework in the real large-scale web environment.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1157–1160},
numpages = {4},
keywords = {sequence-to-sequence neural network, url de-duplication, web crawling},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/2934583.2953981,
author = {ouyang, Jian and Qi, Wei and Wang, Yong},
title = {Extending the Moore's law by exploring new data center architecture: Invited Paper},
year = {2016},
isbn = {9781450341851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934583.2953981},
doi = {10.1145/2934583.2953981},
abstract = {In recent ten years, lots of new applications emerged, such as AI, big data and cloud. Though the workloads of these applications are very diverse, they demand huge resource of data center. In contrast, the silicon technology moves slower and slower because the Moore's law is going to the end. Consequently, the data center building from commodity hardware cannot provide enough cost-efficiency and power-efficiency. To meet the increasingly resource needs of emerging applications, the scale of data center is become much larger and larger. It consumes huge power and cost of hardware. From the business perspective, the slow development of hardware technology limits the value creation of emerging applications.We, Baidu, the largest search engine in China, have faced this challenge in several years ago. We find that the server number increases much faster than the scale of business. And this case is common for internet companies. Because the iteration of general processor becomes slower and slower. For example, Intel announced that the Tick-Tock production strategic was out of date in this early year. This problem drive us to look for new methods to boost business.From Internet Company's perspective, building new chips or new architecture based on its applications' characteristics makes sense. This method can break the limitation of commodity chips and commodity hardware. And according to academic and industry experiences, domain-specified architecture can achieve much better performance and power efficiency than general architecture. Consequently, we are exploring new architecture to extend Moore's law.In this paper, we present the works on exploring new architecture for data center. The data center resource includes storage, memory, computing and networking. Hence, we focus on these four areas. Firstly, we implemented SDF for large-scale distributed storage system. The SDF aims to low cost and high performance flash storage system. Secondly, we implemented SDA for deep learning big data. The SDA is dedicated to solve the computing bottle of emerging applications.The left paper is organized as following. The section 2 is about SDF [1]. The section 3 describes SDA for deep learning [2]. Section 4 presents SDA for big data [3]. And the last section is the conclusion.},
booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design},
pages = {148–149},
numpages = {2},
keywords = {Data center, FPGA, architecture, big data, deep learning, storage},
location = {San Francisco Airport, CA, USA},
series = {ISLPED '16}
}

@inproceedings{10.1145/3139324.3139326,
author = {Sadeghi, Ahmad-Reza},
title = {Hardware-Assisted Security: Promises, Pitfalls and Opportunities},
year = {2017},
isbn = {9781450353977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139324.3139326},
doi = {10.1145/3139324.3139326},
abstract = {Hardware security architectures and primitives are becoming increasingly important in practice providing trust anchors and trusted execution environment to protect modern IT systems, and particularly secure the insecure legacy software. Emerging applications, for instance in IoT area, increasingly involve large numbers of connected and heterogeneous device swarms and pose crucial security and privacy challenges on the underlying devices. Over the past two decades we have seen various hardware security solutions and trends in practice from Trusted Platform Modules (TPM), ARM's TrustZone, and Physically Unclonable Functions (PUFs), to very recent advances such as Intel's Software Guard Extension (SGX) and Control-Flow Enforcement technology (CET). However, despite their advantages these solutions are rarely used by third party developers, make strong trust assumptions about manufacturers, are too expensive for small constrained devices, do not easily scale, or suffer from information leakage. In this talk we will discuss the real-world impact of hardware-based security solutions, their strengths and shortcomings as well as new research directions.},
booktitle = {Proceedings of the 2017 Workshop on Attacks and Solutions in Hardware Security},
pages = {5},
numpages = {1},
keywords = {hardware assisted security, hardware security primitives, internet of things, trust anchors, trusted execution environments},
location = {Dallas, Texas, USA},
series = {ASHES '17}
}

@inproceedings{10.1145/3229574.3229583,
author = {Mukherjee, Shreyasee and Ravindran, Ravishankar and Raychaudhuri, Dipankar},
title = {A Distributed Core Network Architecture for 5G Systems and Beyond},
year = {2018},
isbn = {9781450359078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229574.3229583},
doi = {10.1145/3229574.3229583},
abstract = {This paper presents a distributed core network architecture for use in future cellular systems. The proposed architecture addresses the performance bottlenecks and latency associated with the centralized control and data gateways used in today's mobile core networks. A fully distributed architecture for the mobile core is realized through the use of identifier-based protocol extensions to IP which run on base stations (eNodeB) and routers without the need for centralized gateways. The resulting "flat" mobile core network is capable of supporting a variety of mobility and IoT services with significantly lower latency and improved throughput relative to current solutions. Specific data plane service examples including service chaining and local VOIP are given. The paper concludes with an evaluation of control and data-plane overhead for a large US-scale cellular network operator in the proposed architecture.},
booktitle = {Proceedings of the 2018 Workshop on Networking for Emerging Applications and Technologies},
pages = {33–38},
numpages = {6},
location = {Budapest, Hungary},
series = {NEAT '18}
}

@inproceedings{10.5555/3233397.3233474,
author = {Takefusa, A. and Haga, J. and Toseef, U. and Ikeda, T. and Kudoh, T. and Tanaka, J. and Pentikousis, K.},
title = {Realizing business continuity planning over FELIX infrastructure},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {FELIX federates existing Future Internet (FI) experimental facilities across continents to build a test environment for large-scale SDN experiments. The management framework developed by FELIX allows the execution of experimental network services in a distributed environment comprised of heterogeneous resources. The demonstration described in this paper showcases the implementation of the FELIX architecture over the federated experimental facilities across Japan and Europe leveraging on both the infrastructure resources and the FELIX management stack. The presented use-case also provides an important experimental scenario for data center operators who are developing Business Continuity Planning for IT services.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {416–417},
numpages = {2},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@inproceedings{10.1145/2996890.2996911,
author = {T\"{a}rneberg, William and Chandrasekaran, Vishal and Humphrey, Marty},
title = {Experiences creating a framework for smart traffic control using AWS IOT},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.2996911},
doi = {10.1145/2996890.2996911},
abstract = {Public clouds such as Amazon Web Services (AWS) and Microsoft's Azure provide excellent capabilities for scalable Web applications and Hadoop-based processing. Recent additions to public clouds to support connected devices and IoT have the potential to similarly disrupt emerging home-grown and/or proprietary approaches. While early public cloud IoT success stories have focused on smaller-scale scenarios such as connected houses, it is unclear to what extent these new public cloud mechanisms and abstractions are suitable and effective for larger-scale and/or scientific scenarios, which often have a different set of constraints or requirements. In this paper, the design and implementation of a representative cloud-based IoT infrastructure in a specific public cloud - AWS - is presented. The system created is for dynamic vehicle traffic control based on vehicle volumes/patterns and public transport punctuality. We find that constructing server-less, stateful, and data driven IoT applications in AWS that can operate in real-time is non-trivial. The primary challenges span application manageability and design, latency performance, asynchronicity, and scalability.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {63–69},
numpages = {7},
keywords = {AWS, IOT, PaaS, automatic control, cloud, connected vehicles, data analysis, data collection, public transport, smart cities, traffic, traffic signal priority},
location = {Shanghai, China},
series = {UCC '16}
}

@article{10.1145/3371934.3371956,
author = {Mathis, Matt and Mahdavi, Jamshid},
title = {Deprecating the TCP macroscopic model},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/3371934.3371956},
doi = {10.1145/3371934.3371956},
abstract = {The TCP Macroscopic Model will be completely obsolete soon. It was a closed form performance model for Van Jacobson's landmark congestion control algorithms presented at Sigcomm'88. Jacobson88 requires relatively large buffers to function as intended, while Moore's law is making them uneconomical.BBR-TCP is a break from the past, unconstrained by many of the assumptions and principles defined in Jacobson88. It already out performs Reno and CUBIC TCP over large portions of the Internet, generally without creating queues of the sort needed by earlier congestion control algorithms. It offers the potential to scale better while using less queue buffer space than existing algorithms.Because BBR-TCP is built on an entirely new set of principles, it has the potential to deprecate many things, including the Macroscopic Model. New research will be required to lay a solid foundation for an Internet built on BBR.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {nov},
pages = {63–68},
numpages = {6},
keywords = {BBR-TCP, TCP performance, performance modeling}
}

@inproceedings{10.1145/2911996.2912025,
author = {Song, Emily and Ellis, Joseph G. and Li, Hongzhi and Chang, Shih-Fu},
title = {Watching What and How Politicians Discuss Various Topics: A Large-Scale Video Analytics UI},
year = {2016},
isbn = {9781450343596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911996.2912025},
doi = {10.1145/2911996.2912025},
abstract = {Accurately gauging the political atmosphere is especially difficult in this day and age, as individuals have access to a constantly growing collection of written and audiovisual news sources. This is especially true with regards to the U.S. presidential election, as there are numerous candidates, countless stories, and opinion articles discussing the merits of each particular candidate. It is therefore challenging for people to make an accurate assessment of what each candidate represents and how they would act if they were elected into office. To address this problem, we present a large-scale dataset comprised of videos of politicians speaking organized by the topics they are speaking about, and a user interface for exploring this interesting dataset. Our interface links people and events to relevant pieces of audiovisual media, and presents the desired information in a meaningful and intuitive manner. Our approach is unique by direct linking to actual speaking by politicians about specific topics, rather than links to textual quotes only. We describe the larger underlying infrastructure, a novel automated system that crawls thousands of internet news sources and 100 television news channels daily, and automatically discovers entities and indexes the content into events and topics. We examine how our user interface provides helpful and unique insights to its users, and give an example of the type of large scale trend analysis that can be performed with our system. Our online demo can be accessed at: http://www.ee.columbia.edu/dvmm/PoliticialSpeakerDemo},
booktitle = {Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval},
pages = {401–404},
numpages = {4},
keywords = {event detection, multimedia application, person naming, topic linking, user interface, visualization platform},
location = {New York, New York, USA},
series = {ICMR '16}
}

@article{10.1145/3130979,
author = {Wu, Di and Arkhipov, Dmitri I. and Przepiorka, Thomas and Li, Yong and Guo, Bin and Liu, Qiang},
title = {From Intermittent to Ubiquitous: Enhancing Mobile Access to Online Social Networks with Opportunistic Optimization},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130979},
doi = {10.1145/3130979},
abstract = {Accessing online social networks in situations with intermittent Internet connectivity is a challenge. We have designed a context-aware mobile system to enable efficient offline access to online social media by prefetching, caching and disseminating content opportunistically when signal availability is detected. This system can measure, crowdsense and predict network characteristics, and then use these predictions of mobile network signal to schedule cellular access or device-to-device (D2D) communication. We propose several opportunistic optimization schemes to enhance controlled crowdsensing, resource constrained mobile prefetch, and D2D transmissions impacted by individual selfishness. Realistic tests and large-scale trace analysis show our system can achieve a significant improvement over existing approaches in situations where users experience intermittent cellular service or disrupted network connection.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {114},
numpages = {32},
keywords = {D2D communication, Mobile access, mobile crowdsensing, mobile social networks, opportunistic networking}
}

@inproceedings{10.1145/3308558.3313653,
author = {Shan, Huasong and Chen, Yuan and Liu, Haifeng and Zhang, Yunpeng and Xiao, Xiao and He, Xiaofeng and Li, Min and Ding, Wei},
title = {?-Diagnosis: Unsupervised and Real-time Diagnosis of Small- window Long-tail Latency in Large-scale Microservice Platforms},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313653},
doi = {10.1145/3308558.3313653},
abstract = {Microservice architectures and container technologies are broadly adopted by giant internet companies to support their web services, which typically have a strict service-level objective (SLO), tail latency, rather than average latency. However, diagnosing SLO violations, e.g., long tail latency problem, is non-trivial for large-scale web applications in shared microservice platforms due to million-level operational data and complex operational environments. We identify a new type of tail latency problem for web services, small-window long-tail latency (SWLT), which is typically aggregated during a small statistical window (e.g., 1-minute or 1-second). We observe SWLT usually occurs in a small number of containers in microservice clusters and sharply shifts among different containers at different time points. To diagnose root-causes of SWLT, we propose an unsupervised and low-cost diagnosis algorithm-?-Diagnosis, using two-sample test algorithm and ?-statistics for measuring similarity of time series to identify root-cause metrics from millions of metrics. We implement and deploy a real-time diagnosis system in our real-production microservice platforms. The evaluation using real web application datasets demonstrates that ?-Diagnosis can identify all the actual root-causes at runtime and significantly reduce the candidate problem space, outperforming other time-series distance based root-cause analysis algorithms.},
booktitle = {The World Wide Web Conference},
pages = {3215–3222},
numpages = {8},
keywords = {Root-cause analysis, tail latency, time series similarity},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3325425.3329943,
author = {Fan, Xiaoran},
title = {Facilitating the Deployment of Next Billion IoT Devices with Distributed Antenna Systems},
year = {2019},
isbn = {9781450367769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325425.3329943},
doi = {10.1145/3325425.3329943},
abstract = {Tiny IoT devices have shown their utilities in many fields. However, due to the low cost, small form factor, and inherently restricted computation resources, these IoT devices are facing many fundamental challenges such as the power issue, the communication issue, and the security issue when deployed in scale or operated in long-term period. In this paper, we discuss the feasibility of using distributed antenna systems to facilitate the deployment of IoT devices. Specifically, by coherently combining the phase of each antenna in a 3D distributed antenna system, we form an energy ball at the target location where the energy density level is significantly higher than the energy density level at any other locations. We highlight the properties of energy ball and deploy a testbed with over 20 software defined radios. Our preliminary results demonstrate that this energy ball has a great potential to be leveraged to solve many fundamental problems in IoT and enable exciting IoT applications.},
booktitle = {The ACM MobiSys 2019 on Rising Stars Forum},
pages = {1–6},
numpages = {6},
keywords = {packet collision, phased array, wireless power transfer, wireless security},
location = {Seoul, Republic of Korea},
series = {RisingStarsForum'19}
}

@article{10.1145/2854003,
author = {Tyson, Gareth and Elkhatib, Yehia and Sastry, Nishanth and Uhlig, Steve},
title = {Measurements and Analysis of a Major Adult Video Portal},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/2854003},
doi = {10.1145/2854003},
abstract = {Today, the Internet is a large multimedia delivery infrastructure, with websites such as YouTube appearing at the top of most measurement studies. However, most traffic studies have ignored an important domain: adult multimedia distribution. Whereas, traditionally, such services were provided primarily via bespoke websites, recently these have converged towards what is known as “Porn 2.0”. These services allow users to upload, view, rate, and comment on videos for free (much like YouTube). Despite their scale, we still lack even a basic understanding of their operation. This article addresses this gap by performing a large-scale study of one of the most popular Porn 2.0 websites: YouPorn. Our measurements reveal a global delivery infrastructure that we have repeatedly crawled to collect statistics (on 183k videos). We use this data to characterise the corpus, as well as to inspect popularity trends and how they relate to other features, for example, categories and ratings. To explore our discoveries further, we use a small-scale user study, highlighting key system implications.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {35},
numpages = {25},
keywords = {Adult video content, Porn 2.0, media streaming, user behaviour}
}

@inproceedings{10.1145/3292500.3330728,
author = {Ma, Xiaoyang and Zhang, Lan and Xu, Lan and Liu, Zhicheng and Chen, Ge and Xiao, Zhili and Wang, Yang and Wu, Zhengtao},
title = {Large-scale User Visits Understanding and Forecasting with Deep Spatial-Temporal Tensor Factorization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330728},
doi = {10.1145/3292500.3330728},
abstract = {Understanding and forecasting user visits is of great importance for a variety of tasks, e.g., online advertising, which is one of the most profitable business models for Internet services. Publishers sell advertising spaces in advance with user visit volume and attributes guarantees. There are usually tens of thousands of attribute combinations in an online advertising system. The key problem is how to accurately forecast the number of user visits for each attribute combination. Many traditional work characterizing temporal trends of every single time series are quite inefficient for large-scale time series. Recently, a number of models based on deep learning or matrix factorization have been proposed for high-dimensional time series forecasting. However, most of them neglect correlations among attribute combinations, or are tailored for specific applications, resulting in poor adaptability for different business scenarios.Besides, sophisticated deep learning models usually cause high time and space complexity. There is still a lack of an efficient highly scalable and adaptable solution for accurate high-dimensional time series forecasting. To address this issue, in this work, we conduct a thorough analysis on large-scale user visits data and propose a novel deep spatial-temporal tensor factorization framework, which provides a general design for high-dimensional time series forecasting. We deployed the proposed framework in Tencent online guaranteed delivery advertising system, and extensively evaluated the effectiveness and efficiency of the framework in two different large-scale application scenarios. The results show that our framework outperforms existing methods in prediction accuracy. Meanwhile, it significantly reduces the parameter number and is resistant to incomplete data with up to 20% missing values.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2403–2411},
numpages = {9},
keywords = {guaranteed delivery advertising, high-dimensional time series forecasting, user visits forecasting},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3014812.3014855,
author = {Sun, Dawei and Gao, Shang},
title = {Scalable-DSP: a high scalable distributed storage and processing system for unstructured data in big data environments},
year = {2017},
isbn = {9781450347686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014812.3014855},
doi = {10.1145/3014812.3014855},
abstract = {High scalability is very important for an Internet-scale data storage and processing system in big data era. To achieve scalability, data-relevant issues are identified: unstructured data management, cost of data storage and processing, and cross-domain data management. In this paper, a high scalable distributed storage and processing system for unstructured data is proposed and developed. The paper includes the following contributions. (1) A high scalable distributed architecture is designed. (2) A multilevel, unstructured data storage system is built. (3) A distributed data processing system is implemented to verify the scalable architecture. Experimental results conclusively demonstrate the efficiency and effectiveness of the proposed storage and processing system, which achieves higher data storage efficiency and lower data access time objectives in Internet-scale big data environments.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {41},
numpages = {5},
keywords = {big data, data processing, data storage, internet scale, scalability, unstructured data},
location = {Geelong, Australia},
series = {ACSW '17}
}

@inproceedings{10.1145/3124680.3124746,
author = {Um, Taegeon and Lee, Gyewon and Lee, Sanha and Kim, Kyungtae and Chun, Byung-Gon},
title = {Scaling Up IoT Stream Processing},
year = {2017},
isbn = {9781450351973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3124680.3124746},
doi = {10.1145/3124680.3124746},
abstract = {Users create large numbers of IoT stream queries with data streams generated from various IoT devices. Current stream processing systems such as Storm and Flink are unable to support such large numbers of IoT stream queries efficiently, as their execution models cause a flurry of cache misses while processing the events of the queries. To solve this problem, we present a new group-aware execution model, which processes the events of IoT stream queries in a way that exploits the locality of data and code references, to reduce cache misses and improve system performance. The group-aware execution model leverages the fact that users create the groups of queries according to their interests or location contexts and that queries in the same group can share the same data and codes. We realize the group-aware execution model on MIST---a new stream processing system tailored for processing many IoT stream queries efficiently---to scale up the number of IoT queries that can be processed in a machine. Our preliminary evaluation shows that our group-aware execution model increases the number of queries that can be processed within a single machine up to 3.18X compared to the Flink-based execution model.},
booktitle = {Proceedings of the 8th Asia-Pacific Workshop on Systems},
articleno = {25},
numpages = {7},
location = {Mumbai, India},
series = {APSys '17}
}

@inproceedings{10.1145/3293881.3295781,
author = {Foster, Derek and White, Laurie and Adams, Joshua and Erdil, D. Cenk and Hyman, Harvey and Kurkovsky, Stan and Sakr, Majd and Stott, Lee},
title = {Cloud computing: developing contemporary computer science curriculum for a cloud-first future},
year = {2018},
isbn = {9781450362238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293881.3295781},
doi = {10.1145/3293881.3295781},
abstract = {Cloud Computing adoption has seen significant growth over the last five years. It offers a diverse range of scalable and redundant service deployment models, including Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), Software-as-a-Service (SaaS), and Containers-as-a-Service (CaaS). These models are applied to areas such as IoT, Cyber-Physical Systems, Social Media, Data Science, Media Streaming, Ecommerce, and Health Informatics. The growth in cloud presents challenges for companies to source cloud expertise to support their business, particularly small and medium-sized enterprises with limited resources. The UK Government recently published the Digital Skills Crisis report, identifying skill-set challenges facing industry, with a shortage in cloud skills negatively impacting business. While cloud technologies have evolved at significant pace, the development of Computer Science curriculum in the further and higher education sector has lagged behind. The challenges faced in the sector includes the training of educators, institutional gaps (software and hardware policies), regulatory constraints, and access to cloud platforms. By embedding fundamental cloud skills throughout the educator and student journey, both stakeholders will be better positioned to understand and practically apply the use of appropriate cloud services, and produce graduates to support the needs of industry. This working group has carried out work to: i) assess current cloud computing curricula in CS and similar programs, ii) document industry needs for in-demand cloud skills, iii) identify issues and gaps around cloud curriculum uptake, and iv) develop solutions to meet the skill demands on core Cloud Computing topics, technical skills exercises, and modules for integration with contemporary Computer Science curricula.},
booktitle = {Proceedings Companion of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {130–147},
numpages = {18},
keywords = {Cloud Computing, Computer Science, Curriculum Development, Distributed Computing, Education},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018 Companion}
}

@inproceedings{10.1145/3300061.3343371,
author = {Sevilla, Spencer and Johnson, Matthew and Kosakanchit, Pat and Liang, Jenny and Heimerl, Kurtis},
title = {Demo: An All-in-One Community LTE Network},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3343371},
doi = {10.1145/3300061.3343371},
abstract = {We will introduce and demonstrate CoLTE, an all-in-one solution for LTE-based community networks. CoLTE is a lightweight, Internet-only LTE core network (EPC) based on OpenAirInterface. CoLTE is designed to facilitate the deployment and operation of small-scale, community owned and operated LTE networks, with a particular eye towards expanding Internet access into rural areas with limited and unreliable backhaul. CoLTE comes paired with a basic, IP-based network manager called Haulage, as well as basic locally-hosted webservices. The key differentiator of CoLTE, when compared to existing LTE solutions, is that in CoLTE the EPC is designed to be located in the field and deployed alongside a small number of cellular radios (eNodeBs), as opposed to the centralized model seen in large-scale telecom networks.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {68},
numpages = {3},
keywords = {4g, cellular networks, community networks, lte},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inproceedings{10.1145/3345645.3351106,
author = {Vassio, Luca},
title = {Human Behaviour on the Web: Evolution, Interactions and Exploitation},
year = {2019},
isbn = {9781450369039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345645.3351106},
doi = {10.1145/3345645.3351106},
abstract = {The Web has a fundamental impact on our life, and its usage is quite dynamic and heterogeneous. Moreover, the Web, and in particular Online Social Networks allow people to communicate directly with the public, bypassing filters of traditional medias. Among the others, politicians and companies are exploiting this technologies to widen their influence. In the talk I will show techniques to capture such usage evolution and analyze people interaction on the Internet. This information allows us to understand how users and web services change over time, and how someone can take advantage of these behaviours. There is a large literature about how to evaluate and influence a social network from an analytic point of view [7]. However, it is often not clear if the hypotheses in the mathematical models are valid in real cases and rarely there is enough ground-truth information in large scale experiments. In practice, we observe in the networks heuristic strategies following a trial-and-error approach and emerging behaviours. This is why I am focusing on capturing the human behaviour, directly measured in the present (and past) Web. Thanks to logs of users' traffic, and by active crawling Online Social Networks, I show how to reconstruct users' online activity and to model their behaviour, thanks also to Machine Learning approaches. We deeply understand the evolution of time spent of the Web by the users and the shifting from static pages to the usage of dynamic user-created pages and content in social networks ([4, 6, 9]). The peculiar social networks and other categories usage and evolution can be seen in [1, 4]. Still, considering a short horizon, usage is repetitive and this can exploited for identifying users even when they are not logged (behavioural fingerprints, [8]). Data from human behaviour can be used for extracting and processing social information, sometimes even without the explicit cooperation of the users, to provide new collaborative services. For example, a new service could be the recommendation of hot news that are obtained from aggregated clicks of entire communities (WeBrowse tool, proposed in [3]). Emerging behaviours of the users can also be exploited for expanding someone's influence. A clear example is the recent political debate in Instagram [5] or in WhatsApp [2]. Results suggest that profiles of politicians are able attract markedly different interactions. Moreover, a small group of very active followers can influence a large portion of the network.},
booktitle = {Proceedings of the 5th International Workshop on Social Media World Sensors},
pages = {3–4},
numpages = {2},
keywords = {influence, network dynamics, network monitoring, online social networks, politics, user behaviour},
location = {Hof, Germany},
series = {SIdEWayS'19}
}

@inproceedings{10.1145/3152688.3152694,
author = {Trystram, Jean-Baptiste},
title = {Toward software updates in IoT environments: why existing P2P systems are not enough},
year = {2017},
isbn = {9781450351997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152688.3152694},
doi = {10.1145/3152688.3152694},
abstract = {The number of connected devices is growing, as well as their embedded software complexity. Applications require management and updates. Modern software orchestrators and management systems are mostly centralised and expensive to scale to very large systems. We propose a decentralised approach to distribute software updates more efficiently to IoT devices using P2P. We seek to adapt system behaviour to IoT constraints, such as device heterogeneity, unreliable network connectivity and applications specifics.},
booktitle = {Proceedings of the 18th Doctoral Symposium of the 18th International Middleware Conference},
pages = {15–16},
numpages = {2},
location = {Las Vegas, Nevada},
series = {Middleware '17}
}

@inproceedings{10.1145/3209811.3212703,
author = {Kassem, Mohamed M. and Marina, Mahesh K. and Radunovic, Bozidar},
title = {DIY Model for Mobile Network Deployment: A Step Towards 5G for All},
year = {2018},
isbn = {9781450358163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209811.3212703},
doi = {10.1145/3209811.3212703},
abstract = {Mobile phones and innovative data oriented mobile services have the potential to bridge the digital divide in Internet access and have transformative developmental impact. However as things stand currently, economics come in the way for traditional mobile operators to reach out and provide high-end services to under-served regions. We propose a do-it-yourself (DIY) model for deploying mobile networks in such regions that is in the spirit of earlier community cellular networks but aimed at provisioning high-end (4G and beyond) mobile services. Our proposed model captures and incorporates some of the key trends underlying 5G mobile networks and look to expand their scope beyond urban areas to reach all by empowering small-scale local operators and communities to build and operate modern mobile networks themselves. We showcase a particular instance of the proposed deployment model through a trial deployment in rural UK to demonstrate its practical feasibility.},
booktitle = {Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies},
articleno = {47},
numpages = {5},
keywords = {5G mobile networks, network architecture and deployment models, rural and developing regions, universal Internet access},
location = {Menlo Park and San Jose, CA, USA},
series = {COMPASS '18}
}

@inproceedings{10.1145/3345837.3355954,
author = {Merzoug, Mohammed Amine and Mostefaoui, Ahmed and Benyahia, Abderrezak},
title = {Smart IoT Notification System for Efficient In-City Parking},
year = {2019},
isbn = {9781450369060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345837.3355954},
doi = {10.1145/3345837.3355954},
abstract = {This paper presents an intelligent distributed IoT system which was designed to automatically detect, count and notify drivers about empty parking spaces in major cities. This proposed system has two operating phases: (i) continuous detection and counting of empty spots in the monitored far-apart parking lots, and (ii) instantaneous driver notification through a lightweight MQTT publish/subscribe mechanism. Among the numerous features of this notification system is its reliance on only information collected from the preinstalled IP cameras; no other apparatus installation or maintenance such as ground sensors is required. To check the proper operation of our solution, we have implemented a small-scale version of it and assessed its performance while considering different deep learning frameworks. The obtained results confirmed the proper operation and efficiency of our system in terms of notifications. The conducted experiments have also confirmed the ease of deployment and ease of extension of this system.},
booktitle = {Proceedings of the 15th ACM International Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {37–42},
numpages = {6},
keywords = {deep learning, in-city parking, internet of things, mqtt, parking spot occupancy},
location = {Miami Beach, FL, USA},
series = {Q2SWinet'19}
}

@inproceedings{10.1145/2934872.2959050,
author = {Li, Yue and Iannone, Luigi},
title = {Performance Evaluation of Locator/Identifier Separation Protocol through RIPE Atlas},
year = {2016},
isbn = {9781450341936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934872.2959050},
doi = {10.1145/2934872.2959050},
abstract = {The emph{Locator/Identifier Separation Protocol} (LISP) introduces several benefits to the Internet architecture, yet, since it is just in the initial deployment stage, comprehensive understanding of its integration performance with legacy Internet becomes essential. We leverage on RIPE Atlas, the largest Internet measurement infrastructure, to conduct large scale measurements analysis to provide the feedback to improve LISP technology. The preliminary evaluations show that LISP generally has a reliable performance, compared with the existing Internet. From our vantage point, we observe that LISP introduces a non-negligible latency for the European and North American destinations, occasionally some extremely large delay, however, it shows a faster connection for the Asian intercontinental transmission.},
booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference},
pages = {561–562},
numpages = {2},
keywords = {LISP, RIPE Atlas, experimentation, measurement},
location = {Florianopolis, Brazil},
series = {SIGCOMM '16}
}

@inproceedings{10.1145/3162957.3163023,
author = {Ji, Youngmin and Choi, Woo Suk and Ok, Kisu and Ahn, Jooyoung and Yoo, Junjae},
title = {Space inference system for buildings using IoT},
year = {2017},
isbn = {9781450353656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3162957.3163023},
doi = {10.1145/3162957.3163023},
abstract = {Korea has continued to receive international pressure on GHG (Greenhouse Gas) reduction as it is the world's seventh-largest GHG emission source, and the government has already submitted to the UN INDCs (Intended National Determined Contributions) to reduce 37% of its GHG emission compared to a Business As Usual (BAU) in post-2020 on June 2015. There are various methods to save energy in order to fulfill this reduction obligation. The most effective methods are to increase the energy efficiency of the building, which accounts for about 30% of the existing energy consumption in Korea. Most of the existing buildings do not use 30% of the building space. To reduce this, a system such as BEMS (Building Energy Management System) is needed to grasp and monitor the environment inside the low-cost buildings. However, most BEMS systems are very expensive, and small-scale buildings are difficult to apply because of the low cost of investment. In this paper, we propose a system for inferring the energy use status of each space by collecting environmental information of existing buildings using IoT device. The IoT sensor uses the temperature, humidity, CO2 (Carbon Dioxide), and PIR (Passive Infrared) sensor to judge whether the thermal comfort of the room and the room is occupied or not, and whether the energy is efficiently consumed by the space. Finally, the intelligent system can utilize the historical data of the inferred occupant to predict the occupancy in advance for the control.},
booktitle = {Proceedings of the 3rd International Conference on Communication and Information Processing},
pages = {140–145},
numpages = {6},
keywords = {building automation system (BAS), building energy management system (BEMS), intelligence building, internet of things (IoT)},
location = {Tokyo, Japan},
series = {ICCIP '17}
}

@inproceedings{10.1145/2939672.2939812,
author = {Cao, Yue and Long, Mingsheng and Wang, Jianmin and Yang, Qiang and Yu, Philip S.},
title = {Deep Visual-Semantic Hashing for Cross-Modal Retrieval},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939812},
doi = {10.1145/2939672.2939812},
abstract = {Due to the storage and retrieval efficiency, hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which enables efficient retrieval of images in response to text queries or vice versa, has received increasing attention recently. Most existing work on cross-modal hashing does not capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the intrinsic cross-modal correspondences between visual data and natural language. DVSH is a hybrid deep architecture that constitutes a visual-semantic fusion network for learning joint embedding space of images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based on a novel combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable learning of similarity-preserving and high-quality hash codes. Extensive empirical evidence shows that our DVSH approach yields state of the art results in cross-modal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1445–1454},
numpages = {10},
keywords = {cross-modal retrieval, deep hashing, multimodal embedding},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2911451.2911486,
author = {Zhuang, Mengdie},
title = {Modelling User Search Behaviour Based on Process},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2911486},
doi = {10.1145/2911451.2911486},
abstract = {Typically, interactive information retrieval (IIR) system evaluations assess search processes and outcomes using a combination of two types of measures: 1. user perception (e.g. users? attitudes of the search experience and outcome); 2. user behaviour (e.g. time and counts of various actions including mouse and keyboard clicks). In general, we assume that they are indicative of the search outcomes (e.g. performance, opinion). However, search is a dynamic process with changing outcomes. Therefore, neither measure solely provides a holistic way of evaluating search. On one hand, user behaviour measures are only descriptive of the outcome, and are not interpretive of the process. That is to say, they lack the rationale behind why those behaviours occurred. Another problem is that some mental activities may not reflect on user behaviour [1]. The challenge with logfiles, which contain behaviour data, is the voluminous number of data points and the need to find a reliable approach to define groups or sets based on behavioural patterns. Not all users are alike and nor do they all take the same approach to search for the same things, as evidenced by the TREC, INEX and CLEF interactive tracks. On the other hand, user perception measures are acquired in such small samples that do not scale to large participant populations, and are rarely measured constantly due to the laborious and time consuming data collection methods (e.g. questionnaire, interview). Moreover, not enough emphasis is put on assessing the reliability of individual perception measures, and the wide usage of likert-type scale limits the interpretation of answers. For a holistic understanding of the search process, we need both perception and behaviour measures. I speculate that user behaviour may predict user perception, and thus we should be able to analyse large-scale files for a greater understanding of the likely human responses.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1179},
numpages = {1},
keywords = {interactive information retrieval evaluation, search process, user behaviour},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@article{10.1145/3342241,
author = {Zhang, Ya-Lin and Zhou, Jun and Zheng, Wenhao and Feng, Ji and Li, Longfei and Liu, Ziqi and Li, Ming and Zhang, Zhiqiang and Chen, Chaochao and Li, Xiaolong and Qi, Yuan (Alan) and Zhou, Zhi-Hua},
title = {Distributed Deep Forest and its Application to Automatic Detection of Cash-Out Fraud},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3342241},
doi = {10.1145/3342241},
abstract = {Internet companies are facing the need for handling large-scale machine learning applications on a daily basis and distributed implementation of machine learning algorithms which can handle extra-large-scale tasks with great performance is widely needed. Deep forest is a recently proposed deep learning framework which uses tree ensembles as its building blocks and it has achieved highly competitive results on various domains of tasks. However, it has not been tested on extremely large-scale tasks. In this work, based on our parameter server system, we developed the distributed version of deep forest. To meet the need for real-world tasks, many improvements are introduced to the original deep forest model, including MART (Multiple Additive Regression Tree) as base learners for efficiency and effectiveness consideration, the cost-based method for handling prevalent class-imbalanced data, MART based feature selection for high dimension data, and different evaluation metrics for automatically determining the cascade level. We tested the deep forest model on an extra-large-scale task, i.e., automatic detection of cash-out fraud, with more than 100 million training samples. Experimental results showed that the deep forest model has the best performance according to the evaluation metrics from different perspectives even with very little effort for parameter tuning. This model can block fraud transactions in a large amount of money each day. Even compared with the best-deployed model, the deep forest model can additionally bring a significant decrease in economic loss each day.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {sep},
articleno = {55},
numpages = {19},
keywords = {Deep forest, large-scale machine learning, parameter server}
}

@inproceedings{10.1145/3307772.3328289,
author = {Vishwanath, Arun and Hong, Yu-Heng and Blake, Charles},
title = {Experimental Evaluation of a Data Driven Cooling Optimization Framework for HVAC Control in Commercial Buildings},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328289},
doi = {10.1145/3307772.3328289},
abstract = {Heating, ventilation, and air conditioning (HVAC) is known to dominate energy consumption in commercial buildings. Increasing electricity prices worldwide is putting pressure on facility managers to reduce the energy consumption and costs associated with operating their HVAC. In this paper, we evaluate the efficacy of a data driven energy cost optimization framework for reducing HVAC cooling energy consumption via experiments conducted in a large commercial building. The major contributions of the paper are as follows. First, we develop an integer linear program (ILP) based cooling optimization framework to minimize the electricity costs incurred for cooling a building, subject to satisfying the thermal comfort of the occupants. The ILP formulation relies on data readily available from a building management system (BMS), paving the way for widespread adoption of our solution. Second, we describe the system architecture of the framework, which has been hosted on the IBM cloud platform. We outline the motivation behind implementing the solution on the cloud and highlight its key components, including the ability to use secure RESTful APIs alongside the Project Haystack open source IoT initiative to autonomously communicate with a BMS situated anywhere in the world. Third, we have deployed our framework to control the HVAC of a large office building located in northern Australia. The experiments, commenced mid Nov 2018 (and currently ongoing), carried out in two sections of the building, spanning approximately 1500 m2 and housing 100 people, demonstrate that HVAC cooling energy consumption and costs can be reduced by 20%, amounting to substantial savings in annual electricity bills, without impacting the thermal comfort of the occupants. Our data driven solution is low-cost, scalable and uses sensor data commonly logged by all BMSs, providing an effective and practical mechanism for facility managers to reduce the energy consumption of their building HVAC today.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {78–88},
numpages = {11},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@inproceedings{10.1145/3197091.3205843,
author = {Foster, Derek and White, Laurie and Adams, Joshua and Erdil, D. Cenk and Hyman, Harvey and Kurkovsky, Stan and Sakr, Majd and Stott, Lee},
title = {Cloud computing: developing contemporary computer science curriculum for a cloud-first future},
year = {2018},
isbn = {9781450357074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197091.3205843},
doi = {10.1145/3197091.3205843},
abstract = {Cloud Computing has gained significant momentum in the last five years and is regarded as a paradigm shift away from traditional 'silo' based computing. It is no longer seen as a niche area of technology, offering a diverse range of scalable and redundant service deployment models, including Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), Software-as-a-Service (SaaS), and Containers-as-a-Service (CaaS). These models are applied to areas such as IoT, Cyber-Physical Systems, Social Media, Data Science, Media Streaming, Ecommerce, and Health Informatics. The growth in cloud presents challenges for companies to source expertise that securely supports their business when migrating/deploying services to the Cloud - particularly Small-Medium-Enterprises (SME) with limited resources. The UK Government recently published the Digital Skills Crisis report, identifying skill-set challenges facing industry, with a shortage in cloud skills negatively impacting business. While cloud technologies have evolved at significant pace, the development of contemporary Computer Science curriculum in the further and higher education (HE) sector has lagged behind. The challenges faced in the sector includes the training of educators, institutional gaps (software and hardware policies), regulatory constraints, and access to cloud platforms. Collectively these challenges are significant, but not insurmountable. By embedding fundamental cloud skills throughout the educator and student journey, both stakeholders will be better positioned to understand and practically apply the use of appropriate cloud services, and produce graduates that can support the needs of industry. This working group (WG) aims to: i) assess current cloud computing curricula in CS and similar programs, ii) document industry needs for in-demand cloud skills, iii) identify issues and gaps around cloud curriculum uptake, and iv) develop solutions to meet the skill demands on core Cloud Computing topics, technical skills exercises, and modules for integration with contemporary Computer Science curricula.},
booktitle = {Proceedings of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {346–347},
numpages = {2},
keywords = {Cloud Computing, Computer Science, Curriculum Development, Distributed Computing, Education},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018}
}

@inproceedings{10.1145/3279963.3279970,
author = {Yadav, Nikhil and Keshtkar, Fazel and Schweikert, Christina and Crocetti, Giancarlo},
title = {CRADLE: an IoMT psychophysiological analytics platform},
year = {2018},
isbn = {9781450360753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279963.3279970},
doi = {10.1145/3279963.3279970},
abstract = {The effect of certain emotional stimuli on physiological indicators is helpful in the treatment of, amongst others, strokes and post-traumatic stress disorder (PTSD). Such knowledge can help with understanding the efficacy of rehabilitation procedures after serious debilitating medical conditions. With recent advances in Internet of Medical Things (IoMT) technology, solutions can be built to correlate psychophysiological signals. Furthermore, data mining and artificial intelligence applied to these signals can help in early intervention in cases where direct correlation is inconclusive. When combined with carefully designed and customized feedback mechanisms, AR can be of high value to the rehabilitation process. In this paper we present a platform built using IoMT sensors and augmented reality (AR) technology. The system is capable of creating ambient environments in AR simulating quantifiable emotional stimuli while measuring physiological variability. The CRADLE (Correlational Research Application Development Linking Emotions) platform can capture subject personalities, procure psychophysiological data from large scale studies, and in the future, perform data mining tasks to make recommendations about conducive environments for psychological wellness. Using CRADLE, this paper shows that heart rate variability (HRV) is impacted in a limited study of five subjects using AR to simulate emotional states. Effectiveness of rehabilitation tasks can hence be actively measured and modified without explicit feedback from subjects using this system.},
booktitle = {Proceedings of the Workshop on Human-Habitat for Health (H3): Human-Habitat Multimodal Interaction for Promoting Health and Well-Being in the Internet of Things Era},
articleno = {3},
numpages = {7},
keywords = {IoMT, augmented reality, context-aware health systems, mobile UI, mobile health},
location = {Boulder, Colorado},
series = {H3 '18}
}

@inproceedings{10.1145/3208806.3208829,
author = {Rumi\'{n}ski, Dariusz and Walczak, Krzysztof},
title = {An architecture for distributed semantic augmented reality services},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3208829},
doi = {10.1145/3208806.3208829},
abstract = {In this paper, we present an architecture for distributed augmented reality (AR) services. The presented architecture is based on a client-server design, which supports semantic modeling and building contextual AR presentations for a large number of users. It encompasses two approaches: SOA and CARE. The Service-Oriented Architecture enables building distributed systems that provide application functionality as services to either end-user applications or other services. In turn, CARE allows semantic modeling of AR environments, which combined with SOA enables dividing responsibility between loosely coupled semantic services distributed on the internet. We also provide the results of an experimental evaluation of the performance of a prototype based on the above-mentioned architecture. The findings are promising and demonstrate that an application of semantic web techniques can be an effective approach to implementation of large-scale contextual distributed AR services.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {18},
numpages = {9},
keywords = {AR, CARE, SARO, augmented reality, care modeler, semantic web},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/3167132.3167173,
author = {Neema, Himanshu and Potteiger, Bradley and Koutsoukos, Xenofon and Karsai, Gabor and Volgyesi, Peter and Sztipanovits, Janos},
title = {Integrated simulation testbed for security and resilience of CPS},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167173},
doi = {10.1145/3167132.3167173},
abstract = {Owing1 to an immense growth of internet-connected and learning-enabled cyber-physical systems (CPSs) [1], several new types of attack vectors have emerged. Analyzing security and resilience of these complex CPSs is difficult as it requires evaluating many subsystems and factors in an integrated manner. Integrated simulation of physical systems and communication network can provide an underlying framework for creating a reusable and configurable testbed for such analyses. Using a model-based integration approach and the IEEE High-Level Architecture (HLA) [2] based distributed simulation software; we have created a testbed for integrated evaluation of large-scale CPS systems. Our tested supports web-based collaborative metamodeling and modeling of CPS system and experiments and a cloud computing environment for executing integrated networked co-simulations. A modular and extensible cyber-attack library enables validating the CPS under a variety of configurable cyber-attacks, such as DDoS and integrity attacks. Hardware-in-the-loop simulation is also supported along with several hardware attacks. Further, a scenario modeling language allows modeling of alternative paths (Courses of Actions) that enables validating CPS under different what-if scenarios as well as conducting cyber-gaming experiments. These capabilities make our testbed well suited for analyzing security and resilience of CPS. In addition, the web-based modeling and cloud-hosted execution infrastructure enables one to exercise the entire testbed using simply a web-browser, with integrated live experimental results display.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {368–374},
numpages = {7},
keywords = {courses of action, cyber-physical systems, high-level architecture, modeling and simulation, security and resilience},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2737095.2737115,
author = {Liang, Chieh-Jan Mike and Karlsson, B\"{o}rje F. and Lane, Nicholas D. and Zhao, Feng and Zhang, Junbei and Pan, Zheyi and Li, Zhao and Yu, Yong},
title = {SIFT: building an internet of safe things},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2737115},
doi = {10.1145/2737095.2737115},
abstract = {As the number of connected devices explodes, the use scenarios of these devices and data have multiplied. Many of these scenarios, e.g., home automation, require tools beyond data visualizations, to express user intents and to ensure interactions do not cause undesired effects in the physical world. We present SIFT, a safety-centric programming platform for connected devices in IoT environments. First, to simplify programming, users express high-level intents in declarative IoT apps. The system then decides which sensor data and operations should be combined to satisfy the user requirements. Second, to ensure safety and compliance, the system verifies whether conflicts or policy violations can occur within or between apps. Through an office deployment, user studies, and trace analysis using a large-scale dataset from a commercial IoT app authoring platform, we demonstrate the power of SIFT and highlight how it leads to more robust and reliable IoT apps.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {298–309},
numpages = {12},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/3241748.3241774,
author = {Tien, Ching-Ting and Cheng, Hsu Ko and Pei-Ling, Syu},
title = {The Mediated Effect of Relationship Marketing on the Influences of Irritation Advertising in Fintech Times},
year = {2018},
isbn = {9781450364812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241748.3241774},
doi = {10.1145/3241748.3241774},
abstract = {Internet being a less cost advertisement and medium, had paved the way for tremendous and intensity of online and on time. However, while the anytime advertisement messages had been delivered, there had been limited studies on key variables such as irritation advertisement, customer complaint behavior, customer loyalty and relationship marketing. This study examines the role of irritation advertisement and compliant behavior on customer loyalty by customers in banks and the mediating role relationship marketing plays in those relationships.In this study, methods used questionnaires, including the irritation advertisement scale, compliant behavior scale, relationship marketing scale, and customer loyalty scale. A total 300 questionnaires were administered in customers of banks in Taiwan. Out of these 300 questionnaires, 265 responses were received, a return rate of 88.34%. After discarding invalid questionnaires, a total of 265 valid questionnaires, the rate of valid samples was 100%. Data had been analyzed using descriptive analysis and structural equation model. Finding that the mediated effect of relationship marketing on the relationships of irritation advertisement, compliant behavior and customer loyalty. Finally there were suggestions and managerial implications had been proposed.},
booktitle = {Proceedings of the 2018 2nd International Conference on E-Education, E-Business and E-Technology},
pages = {99–101},
numpages = {3},
keywords = {Compliant Behavior, Customer Loyalty, Irritation Advertising, Relationship Marketing},
location = {Beijing, China},
series = {ICEBT '18}
}

@article{10.1109/TNET.2018.2854795,
author = {Liu, Zhuotao and Jin, Hao and Hu, Yih-Chun and Bailey, Michael},
title = {Practical Proactive DDoS-Attack Mitigation via Endpoint-Driven In-Network Traffic Control},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2854795},
doi = {10.1109/TNET.2018.2854795},
abstract = {Volumetric attacks, which overwhelm the bandwidth of a destination, are among the most common distributed denial-of-service DDoS attacks today. Despite considerable effort made by both research and industry, our recent interviews with over 100 potential DDoS victims in over 10 industry segments indicate that today’s DDoS prevention is far from perfect. On one hand, few academical proposals have ever been deployed in the Internet; on the other hand, solutions offered by existing DDoS prevention vendors are not silver bullet to defend against the entire attack spectrum. Guided by such large-scale study of today’s DDoS defense, in this paper, we present MiddlePolice, the first readily deployable and proactive DDoS prevention mechanism. We carefully architect MiddlePolice such that it requires no changes from both the Internet core and the network stack of clients, yielding instant deployability in the current Internet architecture. Further, relying on our novel capability feedback mechanism, MiddlePolice is able to enforce destination-driven traffic control so that it guarantees to deliver victim-desired traffic regardless of the attacker strategies. We implement a prototype of MiddlePolice and demonstrate its feasibility via extensive evaluations in the Internet, hardware testbed, and large-scale simulations.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1948–1961},
numpages = {14}
}

@inproceedings{10.1145/3010079.3010084,
author = {Mohammadkhan, Ali and Ramakrishnan, K.K. and Rajan, Ashok Sunder and Maciocco, Christian},
title = {CleanG: A Clean-Slate EPC Architecture and ControlPlane Protocol for Next Generation Cellular Networks},
year = {2016},
isbn = {9781450346733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010079.3010084},
doi = {10.1145/3010079.3010084},
abstract = {Cellular networks play a dominant role in how we communicate. But, the current cellular architecture and protocols are overly complex. The 'control plane' protocol includes setting up explicit tunnels for every session and exchanging a large number of packets among the different entities (mobile device, base station, the packet gateways and mobility management) to ensure state is exchanged in a consistent manner. This limits scalability. As we evolve to having to support an increasing number of users, cell-sites (e.g., 5G) and the consequent mobility, and the incoming wave of IoT devices, a re-thinking of the architecture and control protocols is required. In this work we propose CleanG, a simplified software-based architecture for the Mobile Core Network (MCN) and a simplified control protocol for cellular networks. Network Function Virtualization enables dynamic management of capacity in the cloud to support the MCN of future cellular networks. We develop a simplified protocol that substantially reduces the number of control messages exchanged to support the various events, while retaining the current functionality expected from the network. CleanG, we believe will scale better and have lower latency.},
booktitle = {Proceedings of the 2016 ACM Workshop on Cloud-Assisted Networking},
pages = {31–36},
numpages = {6},
keywords = {cellular networks, mobile core network, network function virtualization, software defined networks},
location = {Irvine, California, USA},
series = {CAN '16}
}

@inproceedings{10.1145/3102304.3102347,
author = {Noreen, Umber and Bounceur, Ahc\`{e}ne and Clavier, Laurent},
title = {Modeling Interference for Wireless Sensor Network Simulators},
year = {2017},
isbn = {9781450348447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102304.3102347},
doi = {10.1145/3102304.3102347},
abstract = {Low power wide area networks (LPWAN) are the enabling technologies for large scale wireless sensor networks (WSNs). Effective cost, long range and energy efficiency of LPWANs make them most suitable candidates for smart city applications. These technologies offer novel communication paradigm to address discrete IoT's applications.This paper presents the integration of physical layers based on ZigBee, Wi-Fi, and LoRa into a wireless sensor network simulator CupCarbon for IoT's applications. We have restructured the operations of PHYs, so it can be flexible and scalable to exploit the system services.},
booktitle = {Proceedings of the International Conference on Future Networks and Distributed Systems},
articleno = {43},
numpages = {6},
keywords = {CupCarbon, Internet of things (IoTs), LPWAN, LoRa™, Smart cities, Wi-Fi, Wireless sensor Networks, ZiBee},
location = {Cambridge, United Kingdom},
series = {ICFNDS '17}
}

@inproceedings{10.1145/3355369.3355594,
author = {Vargas, Santiago and Goel, Utkarsh and Steiner, Moritz and Balasubramanian, Aruna},
title = {Characterizing JSON Traffic Patterns on a CDN},
year = {2019},
isbn = {9781450369480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355369.3355594},
doi = {10.1145/3355369.3355594},
abstract = {Content delivery networks serve a major fraction of the Internet traffic, and their geographically deployed infrastructure makes them a good vantage point to observe traffic access patterns. We perform a large-scale investigation to characterize Web traffic patterns observed from a major CDN infrastructure. Specifically, we discover that responses with application/json content-type form a growing majority of all HTTP requests. As a result, we seek to understand what types of devices and applications are requesting JSON objects and explore opportunities to optimize CDN delivery of JSON traffic. Our study shows that mobile applications account for at least 52% of JSON traffic on the CDN and embedded devices account for another 12% of all JSON traffic. We also find that more than 55% of JSON traffic on the CDN is uncacheable, showing that a large portion of JSON traffic on the CDN is dynamic. By further looking at patterns of periodicity in requests, we find that 6.3% of JSON traffic is periodically requested and reflects the use of (partially) autonomous software systems, IoT devices, and other kinds of machine-to-machine communication. Finally, we explore dependencies in JSON traffic through the lens of ngram models and find that these models can capture patterns between subsequent requests. We can potentially leverage this to prefetch requests, improving the cache hit ratio.},
booktitle = {Proceedings of the Internet Measurement Conference},
pages = {195–201},
numpages = {7},
keywords = {Content Delivery Networks (CDNs), JSON, Web},
location = {Amsterdam, Netherlands},
series = {IMC '19}
}

@inproceedings{10.1145/2934328.2934337,
author = {Gupta, Vani and Lee, Stephen and Shenoy, Prashant and Sitaraman, Ramesh K. and Urgaonkar, Rahul},
title = {How to cool internet-scale distributed networks on the cheap},
year = {2016},
isbn = {9781450343930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934328.2934337},
doi = {10.1145/2934328.2934337},
abstract = {Internet-scale Distributed Networks (IDNs) are large distributed systems that comprise hundreds of thousands of servers located in hundreds of data centers around the world. A canonical example of an IDN is a content delivery network (CDN) that delivers content to users from a large global deployment of servers around the world. IDNs consume significant amounts of energy to power their deployed server infrastructure, and nearly as much energy to cool that infrastructure. We study the potential benefits of using two new cooling technologies---open air cooling (OAC) and thermal energy storage (TES)---to reduce the energy usage as well as the operational and capital costs incurred by an IDN for cooling. We develop novel algorithms to incorporate both technologies into the IDN architecture and empirically evaluate their efficacy using extensive work load traces from Akamai's global CDN and global weather data from NOAA. Our results show that both technologies hold great promise for the future sustainability of Internet-scale distributed networks. Our algorithm for power management of TES is provably near-optimal, is the first to incorporate storage efficiency, and is broadly applicable to other storage devices such as batteries.},
booktitle = {Proceedings of the Seventh International Conference on Future Energy Systems},
articleno = {9},
numpages = {12},
keywords = {data centers, energy optimization, internet-scale distributed systems, load balancing, renewable energy},
location = {Waterloo, Ontario, Canada},
series = {e-Energy '16}
}

@article{10.1109/TNET.2018.2817206,
author = {Li, Wenxin and Zhou, Xiaobo and Li, Keqiu and Qi, Heng and Guo, Deke},
title = {TrafficShaper: Shaping Inter-Datacenter Traffic to Reduce the Transmission Cost},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2817206},
doi = {10.1109/TNET.2018.2817206},
abstract = {The emerging deployment of geographically distributed data centers DCs incurs a significant amount of data transfers over the Internet. Such transfers are typically charged by Internet service providers with the widely adopted $q$ th percentile charging model. In such a charging model, the time slots with top $100-q$ percent of data transmission do not affect the total transmission cost and can be viewed as “free.” This brings the opportunity to optimize the scheduling of inter-DC transfers to minimize the entire transmission cost. However, a very little work has been done to exploit those “free” time slots for scheduling inter-DC transfers. The crux is that existing work either lacks a mechanism to accumulate traffic to “free” time slots, or inevitably relies on prior knowledge of future traffic arrival patterns. In this paper, we present TrafficShaper, a new scheduler that shapes the inter-DC traffic to exploit the “free” time slots involved in the $q$ th percentile charging model, so as to reduce or even minimize the transmission cost. When shaping traffic, TrafficShaper advocates a simple principle: more traffic peaks should be scheduled in “free” time slots, while less traffic differentiation should be maintained among the remaining time slots. To this end, TrafficShaper designs a pricing-aware control framework, which makes online decisions for inter-DC transfers without requiring a prior knowledge of traffic arrivals. To verify the performance of TrafficShaper, we conduct rigorous theoretical analysis based on Lyapunov optimization techniques, large-scale trace-driven simulations, and small-scale testbed implementation. Results from rigorous mathematical analyses demonstrate that TrafficShaper can make the transmission cost arbitrarily close to the optimum value. Extensive trace-driven simulation results show that TrafficShaper can reduce the transmission cost by up to 40.23%, compared with the state-of-the-art solutions. The testbed experiments further verify that TrafficShaper can realistically reduce the transmission cost by up to 19.38%.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1193–1206},
numpages = {14}
}

@article{10.5555/2946645.3007031,
author = {Ho, Qirong and Yin, Junming and Xing, Eric P.},
title = {Latent space inference of internet-scale networks},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {The rise of Internet-scale networks, such as web graphs and social media with hundreds of millions to billions of nodes, presents new scientific opportunities, such as overlapping community detection to discover the structure of the Internet, or to analyze trends in online social behavior. However, many existing probabilistic network models are difficult or impossible to deploy at these massive scales. We propose a scalable approach for modeling and inferring latent spaces in Internet-scale networks, with an eye towards overlapping community detection as a key application. By applying a succinct representation of networks as a bag of triangular motifs, developing a parsimonious statistical model, deriving an efficient stochastic variational inference algorithm, and implementing it as a distributed cluster program via the Petuum parameter server system, we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours. Compared to other state-of-the-art probabilistic network approaches, our method is several orders of magnitude faster, with competitive or improved accuracy at overlapping community detection.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2756–2796},
numpages = {41},
keywords = {big data, distributed computation, probabilistic network models, stochastic variational inference, triangular modeling}
}

@proceedings{10.1145/2940136,
title = {Internet-QoE '16: Proceedings of the 2016 workshop on QoE-based Analysis and Management of Data Communication Networks},
year = {2016},
isbn = {9781450344258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The ACM SIGCOMM Workshop on QoE-based Analysis and Management of Data Communication Networks, Internet-QoE'16 was born as a means to cover a growing need in the conception and applicability of user-centric, QoE-based solutions to the large scale analysis and management of modern operational networks.The main goal of the Internet-QoE workshop is to scale QoE out of the traditional lab studies context and bring it to the analysis and operation of data communication networks, giving a use-centric perspective to the network measurements and analysis community. QoE remains a poorly understood domain, currently restricted to small scale studies and very far from the analysis of real networks. Especially in the industry, QoE has become a buzz word, far from its reality within the research community, and partly due to the complexity involved in deploying QoE-based network analysis and management solutions. By fostering an explicit and deep integration of the end-user directly into the design, analysis and management of large-scale operational networks, we expect to reduce the gap between QoE research and its application to future network management paradigms, as well as to provide a more targeted end-user perspective to the research on data communication networks.A secondary yet major goal of Internet-QoE is to bridge the QoE and Internet measurements research communities, which are currently loosely coupled. On the one hand, the network measurements research community is so far missing a deeper QoE know how and expertise. On the other hand, the QoE research community needs a better and more complete understanding of the functioning, structure and open problems behind large-scale operational networks. Internet-QoE brings together researchers and practitioners from the Internet measurements and analysis domain and the QoE modeling and assessment domain, as well as industry players willing to integrate QoE aspects into the DNA of their daily business, with direct applications in network dimensioning, monitoring, management, and troubleshooting among others.Internet-QoE'16 accepted 10 technical papers out of 21, high-quality submissions. The paper review process included an evaluation phase by PC members, followed by an online discussion of the top ranked papers, out of which 7 were directly accepted to appear in the program, and 3 went into a shepherding phase. The resulting program features a variety of high-quality papers focusing on different aspects of QoE and network measurements, including QoE-based network monitoring and assessment, QoE-based network management, as well as QoE modeling and analysis.Internet-QoE'16 also features three exciting keynotes from recognized researchers in the QoE and network measurements domains: (i) "Improving Web Performance", by Prof. Harsha V. Madhyastha (University of Michigan, US), (ii) "The Challenges of Measuring Internet Quality of Experience", by Dr. Renata Cruz Teixeira (INRIA Paris, France), and (iii) "On the Analysis and Modeling of Quality of Experience for Video Streaming", by Prof. Maria Papadopouli (University of Crete and FORT, Greece).},
location = {Florianopolis, Brazil}
}

@inproceedings{10.1145/3041021.3051150,
author = {Zhang, Kewei and Arablouei, Reza and Jurdak, Raja},
title = {Predicting Prevalence of Influenza-Like Illness From Geo-Tagged Tweets},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3051150},
doi = {10.1145/3041021.3051150},
abstract = {Modeling disease spread and distribution using social media data has become an increasingly popular research area. While Twitter data has recently been investigated for estimating disease spread, the extent to which it is representative of disease spread and distribution in a macro perspective is still an open question. In this paper, we focus on macro-scale modeling of influenza-like illnesses (ILI) using a large dataset containing 8,961,932 tweets from Australia collected in 2015. We first propose modifications of the state-of-the-art ILI-related tweet detection approaches to acquire a more refined dataset. We normalize the number of detected ILI-related tweets with Internet access and Twitter penetration rates in each state. Then, we establish a state-level linear regression model between the number of ILI-related tweets and the number of real influenza notifications. The Pearson correlation coefficient of the model is 0.93. Our results indicate that: 1) a strong positive linear correlation exists between the number of ILI-related tweets and the number of recorded influenza notifications at state scale; 2) Twitter data has promising ability in helping detect influenza outbreaks; 3) taking into account the population, Internet access and Twitter penetration rates in each state enhances the prevalence modeling analysis.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1327–1334},
numpages = {8},
keywords = {classification, data mining, disease modeling, public health monitoring, regression analysis, twitter},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@article{10.1109/TNET.2016.2615889,
author = {Cheng, Jie and Liu, Yaning and Ye, Qiang and Du, Hongwei and Vasilakos, Athanasios V.},
title = {DISCS: A Distributed Coordinate System Based on Robust Nonnegative Matrix Completion},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2615889},
doi = {10.1109/TNET.2016.2615889},
abstract = {Many distributed applications, such as BitTorrent, need to know the distance between each pair of network hosts in order to optimize their performance. For small-scale systems, explicit measurements can be carried out to collect the distance information. For large-scale applications, this approach does not work due to the tremendous amount of measurements that have to be completed. To tackle the scalability problem, network coordinate system NCS was proposed to solve the scalability problem by using partial measurements to predict the unknown distances. However, the existing NCS schemes suffer seriously from either low prediction precision or unsatisfactory convergence speed. In this paper, we present a novel distributed network coordinate system DISCS that utilizes a limited set of distance measurements to achieve high-precision distance prediction at a fast convergence speed. Technically, DISCS employs the innovative robust nonnegative matrix completion method to improve the prediction accuracy. Through extensive experiments based on various publicly-available data sets, we found that DISCS outperforms the state-of-the-art NCS schemes in terms of prediction precision and convergence speed, which clearly shows the high usability of DISCS in real-life Internet applications.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {934–947},
numpages = {14}
}

@inproceedings{10.1145/3178876.3186021,
author = {Dubey, Abhimanyu and Moro, Esteban and Cebrian, Manuel and Rahwan, Iyad},
title = {MemeSequencer: Sparse Matching for Embedding Image Macros},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186021},
doi = {10.1145/3178876.3186021},
abstract = {The analysis of the creation, mutation, and propagation of social media content on the Internet is an essential problem in computational social science, affecting areas ranging from marketing to political mobilization. A first step towards understanding the evolution of images online is the analysis of rapidly modifying and propagating memetic imagery or "memes". However, a pitfall in proceeding with such an investigation is the current incapability to produce a robust semantic space for such imagery, capable of understanding differences in Image Macros. In this study, we provide a first step in the systematic study of image evolution on the Internet, by proposing an algorithm based on sparse representations and deep learning to decouple various types of content in such images and produce a rich semantic embedding. We demonstrate the benefits of our approach on a variety of tasks pertaining to memes and Image Macros, such as image clustering, image retrieval, topic prediction and virality prediction, surpassing the existing methods on each. In addition to its utility on quantitative tasks, our method opens up the possibility of obtaining the first large-scale understanding of the evolution and propagation of memetic imagery.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1225–1235},
numpages = {11},
keywords = {content understanding, embeddings, feature extraction, image macros, image virality, social network analysis, sparse representation},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3053600.3053616,
author = {Loreti, Daniela and Chesani, Federico and Ciampolini, Anna and Mello, Paola},
title = {Distributed Compliance Monitoring of Business Processes over MapReduce Architectures},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053616},
doi = {10.1145/3053600.3053616},
abstract = {In the era of IoT, large volumes of event data from different sources are collected in the form of streams. As these logs need to be online processed to extract further knowledge about the underlying business process, it is becoming more and more important to give support to run-time monitoring. In particular, increasing attention has been turned to conformance checking as a way to identify when a sequence of events deviates from the expected behavior. Albeit rather straightforward on a small log file, conformance verification techniques may show poor performance when dealing with big data, making increasingly attractive the possibility to improve scalability through distributed computation. In this paper, we adopt a previously implemented framework for compliance verification (which provides a high-level logic-based notation for the monitoring specification) and we show how it can be efficiently distributed on a set of computing nodes to support scalable run-time monitoring when dealing with large volumes of event logs.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {79–84},
numpages = {6},
keywords = {business process management, conformance checking, distributed monitoring, mapreduce},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3155921.3158432,
author = {Gharakheili, Hassan Habibi and Sivaraman, Vijay},
title = {Cloud assisted home networks},
year = {2017},
isbn = {9781450354233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3155921.3158432},
doi = {10.1145/3155921.3158432},
abstract = {Managed services for the home have traditionally been shunned by Internet Service Providers (ISPs) as having high overheads and low margins. In this paper we argue that the maturing ecosystem around Software Defined Networking (SDN) changes the equation, allowing ISPs to provide cloud-based and self-managed value-add services to consumers at low cost and large scale. We first demonstrate use-cases in which SDN gives greater visibility into home network activity, enabling self-customization of Internet experience by the household, while reducing support costs for the ISP. We then outline a cloud-assisted architecture that realizes these capabilities, and detail our implementation that leverages commodity hardware and open-source software. Finally, we deploy our fully-functional system in selected households in Australia and Iran, and analyze activity data collected over a month to present insights on number and composition of connected devices, video-viewing patterns, and content preferences based on web-sites visited.},
booktitle = {Proceedings of the 2nd Workshop on Cloud-Assisted Networking},
pages = {31–36},
numpages = {6},
keywords = {SDN, cloud, home networks, visibility},
location = {Incheon, Republic of Korea},
series = {CAN '17}
}

@inproceedings{10.1145/3143361.3143375,
author = {Almeida, Mario and Finamore, Alessandro and Perino, Diego and Vallina-Rodriguez, Narseo and Varvello, Matteo},
title = {Dissecting DNS Stakeholders in Mobile Networks},
year = {2017},
isbn = {9781450354226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143361.3143375},
doi = {10.1145/3143361.3143375},
abstract = {The functioning of mobile apps involves a large number of protocols and entities, with the Domain Name System (DNS) acting as a predominant one. Despite being one of the oldest Internet systems, DNS still operates with semi-obscure interactions among its stakeholders: domain owners, network operators, operating systems, and app developers. The goal of this work is to holistically understand the dynamics of DNS in mobile traffic along with the role of each of its stakeholders. We use two complementary (anonymized) datasets: traffic logs provided by a European mobile network operator (MNO) with 19M customers, and traffic logs from 5,000 users of Lumen, a traffic monitoring app for Android. We complement such passive traffic analysis with active measurements at four European MNOs. Our study reveals that 10k domains (out of 198M) account for 87% of total network flows. The time to live (TTL) values for such domains are mostly short (&lt; 1min), despite domain-to-IPs mapping tends to change on a longer time-scale. Further, depending on the operators recursive resolver architecture, end-user devices receive even smaller TTL values leading to suboptimal effectiveness of the on-device DNS cache. Despite a number of on-device and in-network optimizations available to minimize DNS overhead, which we find corresponding to 10% of page load time (PLT) on average, we have not found wide evidence of their adoption in the wild.},
booktitle = {Proceedings of the 13th International Conference on Emerging Networking EXperiments and Technologies},
pages = {28–34},
numpages = {7},
keywords = {Mobile DNS traffic, caching, ephemeral domains, page load time (PLT), time to live (TTL)},
location = {Incheon, Republic of Korea},
series = {CoNEXT '17}
}

@inproceedings{10.1145/2818869.2818902,
author = {Chen, Ju-Chin and Liu, Chao-Feng},
title = {Visual-based Deep Learning for Clothing from Large Database},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818902},
doi = {10.1145/2818869.2818902},
abstract = {Huge benefits can be obtained by mining information from Big Data. Analyzing large volumes of consumption behavior data that are limited by conventional machine learning techniques and computational analysis becomes a critical problem as Big Data is examined. Furthermore, there is a need for powerful visual-based analytics tools when pictures have become a core content component on the Internet. Hence, in this study, we explore Deep Learning with convolutional neural networks with a goal of resolving clothing style classification and retrieval tasks. To reduce training complexity, transfer learning is incorporated by fine-tuning pre-trained models on large scale datasets. Furthermore, because the parameters are vast for any given deep net, one architecture inspired from Adaboost is designed to use multiple deep nets that are trained with a sub-dataset. Thus, the training time can be accelerated if each net is computed in one client node in a distributed computing environment. Moreover, to increase system flexibility, two architectures with multiple deep nets with two outputs are proposed for binary-class classification. Therefore, when new classes are added, no additional computation is needed for all training data. Experiments are performed to compare existing systems with hand-crafted features and conventional learning models. According to the results, the proposed system can provide significant improvements on three public clothing datasets for style classifications.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {42},
numpages = {10},
keywords = {Big Data Analytics, Clothing Image Retrieval, Convolution Neural Network, Deep Learning, Style Recognition},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@inproceedings{10.1145/3194164.3194181,
author = {Woods, Eoin},
title = {The past, present and future of technical debt: learning from the past to prepare for the future},
year = {2018},
isbn = {9781450357135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194164.3194181},
doi = {10.1145/3194164.3194181},
abstract = {While technical debt has emerged as a formal concept relatively recently [2] we have had technical debt from the earliest days of software development, it has simply evolved in nature. So what can we learn from past types of technical debt to allow us to prepare for its future forms?When we look back over recent software history, we can see five identifiable evolutions of software systems [5], each one roughly aligning with a decade.Before and through the 1980s, software systems were largely monolithic and tended to run on single computers, with software being developed as monolithic "programs". As we moved into the 1990s, distributed systems became mainstream and the standard style for an enterprise system became three-tier client server. The Internet became a mainstream technology in the late 1990s, and organisations developed Internet-connected systems, which were "always on" rather than just "online" and could support difficult and unpredictable quality properties. In the current era, we are building Internet-native systems, where "the Internet is the system". These systems are built from a combination of open source components, remote Internet connected services and custom code, and their services often form part of the Internet via publicly accessible APIs.Following current trends, it seems that the next phase of evolution will be to Intelligent-Connected systems, as artificial intelligence (machine learning in particular) becomes mainstream [1], users expect context specific assistance, and fast, reliable networks allow us to connect "things" (devices) to our systems as well as traditional computers [3].Software engineering practice evolves in response to new challenges and each era of computing has introduced new techniques and technology but each has also introduced its own types of technical debt too.In the monolithic era, the focus was structuring a single program, with "spaghetti code", poor naming and unrestricted use of the "goto" emerging as examples of the earliest types of technical debt. As we moved into the distributed era, we ended up tangling presentation and business logic code in our client/server user interfaces, while in the Internet-connected era we distorted our systems to meet performance and scalability concerns at all costs and often ended up with poorly-designed automated tests being an inflexible technical debt of their own. More recently Internet-native systems often introduce a mishmash of microservices with poorly understood choreography and diverse internal implementations, references to external APIs that became unsupported or difficult to use and public APIs with many versions, all of which have to be maintained "forever" due to callers who would not migrate to new versions.So what types of technical debt do we expect in the future?In the intelligent-connected era, amongst other things, applications will have machine learning features and we'll need large datasets to train machine learning models and provide context-specific user experiences and we'll have lots of non-computing devices connected to our systems, providing data. So we'll probably get machine learning debt [4], ML models that we can't explain, models that we can't improve because people rely on their quirks (even if wrong). We'll also have large inflexible data sets which our systems and models rely on, and we'll have unknown and unpredictable collections of "things" connecting to our services, which we can't change because other people own them.While this sounds like a daunting set of challenges, the intelligent-connected era is only just beginning, so we have not yet incurred significant amounts of technical debt. By looking to the past as our guide to the future we can be forewarned and start find solutions to our future technical debt before we have become overwhelmed by it!},
booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
pages = {61},
numpages = {1},
location = {Gothenburg, Sweden},
series = {TechDebt '18}
}

@inproceedings{10.1145/2740908.2742850,
author = {Grbovic, Mihajlo and Djuric, Nemanja and Radosavljevic, Vladan and Bhamidipati, Narayan and Hawker, Jordan and Johnson, Caleb},
title = {queryCategorizr: A Large-Scale Semi-Supervised System for Categorization of Web Search Queries},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742850},
doi = {10.1145/2740908.2742850},
abstract = {Understanding interests expressed through user's search query is a task of critical importance for many internet applications. To help identify user interests, web engines commonly utilize classification of queries into one or more pre-defined interest categories. However, majority of the queries are noisy short texts, making accurate classification a challenging task. In this demonstration, we present queryCategorizr, a novel semi-supervised learning system that embeds queries into low-dimensional vector space using a neural language model applied on search log sessions, and classifies them into general interest categories while relying on a small set of labeled queries. Empirical results on large-scale data show that queryCategorizr outperforms the current state-of-the-art approaches. In addition, we describe a Graphical User Interface (GUI) that allows users to query the system and explore classification results in an interactive manner.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {199–202},
numpages = {4},
keywords = {query categorization, query embeddings, word2vec},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@book{10.1145/2886107,
author = {Zaharia, Matei},
title = {An Architecture for Fast and General Data Processing on Large Clusters},
year = {2016},
isbn = {9781970001570},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {11},
abstract = {Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out these computations to clusters of hundreds of machines.At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too.This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing.We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective.This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, drawing of illustrations, and links for the references have been added.}
}

@inproceedings{10.1145/3300061.3345446,
author = {Sevilla, Spencer and Johnson, Matthew and Kosakanchit, Pat and Liang, Jenny and Heimerl, Kurtis},
title = {Experiences: Design, Implementation, and Deployment of CoLTE, a Community LTE Solution},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3345446},
doi = {10.1145/3300061.3345446},
abstract = {In this paper we introduce CoLTE, a solution for LTE-based community networks. CoLTE is a lightweight, Internet-only LTE core network (EPC) designed to facilitate the deployment and operation of small-scale, community owned and operated LTE networks in rural areas with limited and unreliable backhaul. The key differentiator of CoLTE, when compared to existing LTE solutions, is that in CoLTE the EPC is designed to be located in the field and deployed alongside a small number of cellular radios (eNodeBs), as opposed to the centralized model seen in large-scale telecom networks. We also provide performance results and lessons learned from a real-world CoLTE network deployed in rural Indonesia. This network has been sustainably operating for over six months, currently serves over 40 active users, and provides measured backhaul reductions of up to 45% when compared to cloud-core solutions.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {45},
numpages = {16},
keywords = {4g, cellular networks, community networks, core networks, global connectivity, lte},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inproceedings{10.1145/3018896.3018936,
author = {Essafi, Hassane and H\`{e}de, Patrick},
title = {FRAMSTIM: framework for large scale multimedia content feature extraction based on MPI one-sided communication},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018936},
doi = {10.1145/3018896.3018936},
abstract = {Every day a large number of images are made available throw social networks and different IoT embedded sensors. R&amp;D devoted to the development of applications based on visual pattern recognition has attracted a large population of researchers in both side academic and industry. Extraction of relevant features is challenging and known to be one of the key issues in many applications where the visual pattern recognition is applied (object recognition and tracking, image identification, multimedia document categorization, indexing and retrieval, deep learning based visual feature coding, video surveillance, robotic, activity recognition). Furthermore the extraction features from a big volume of image and video data is time and resources consuming. In the context of the ITEA2 project H4H/PerfCloud ( Performance in the Cloud) we have developed parallel OpenMP threads video engine search. To scale the extraction of visual features from a large volume of streaming visual content, we have developed a framework based on OpenMP and MPI one-sided communication where the computation and communication are overlapped thanks to the RDMA approach.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {41},
numpages = {6},
keywords = {MPI, RMA, concept construction, feature extraction, multimedia engine, one-sided communication, visual data characterization and representation},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3035918.3054784,
author = {\"{O}zcan, Fatma and Tian, Yuanyuan and T\"{o}z\"{u}n, Pinar},
title = {Hybrid Transactional/Analytical Processing: A Survey},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3054784},
doi = {10.1145/3035918.3054784},
abstract = {The popularity of large-scale real-time analytics applications (real-time inventory/pricing, recommendations from mobile apps, fraud detection, risk analysis, IoT, etc.) keeps rising. These applications require distributed data management systems that can handle fast concurrent transactions (OLTP) and analytics on the recent data. Some of them even need running analytical queries (OLAP) as part of transactions. Efficient processing of individual transactional and analytical requests, however, leads to different optimizations and architectural decisions while building a data management system.For the kind of data processing that requires both analytics and transactions, Gartner recently coined the term Hybrid Transactional/Analytical Processing (HTAP). Many HTAP solutions are emerging both from the industry as well as academia that target these new applications. While some of these are single system solutions, others are a looser coupling of OLTP databases or NoSQL systems with analytical big data platforms, like Spark. The goal of this tutorial is to 1-) quickly review the historical progression of OLTP and OLAP systems, 2-) discuss the driving factors for HTAP, and finally 3-) provide a deep technical analysis of existing and emerging HTAP solutions, detailing their key architectural differences and trade-offs.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1771–1775},
numpages = {5},
keywords = {analytics, htap, hybrid transaction and analytics processing, olap, oltp, transactions},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/2935663.2935676,
author = {Meng, Hongwei and Chen, Zhong and Hu, Jianbin and Guan, Zhi},
title = {Establish the Intrinsic Binding in Naming Space for Future Internet Using Combined Public Key},
year = {2016},
isbn = {9781450341813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935663.2935676},
doi = {10.1145/2935663.2935676},
abstract = {In order to enable intrinsic security without the Public Key Infrastructure (PKI) deployment, flat self-certifying addresses have been involved into the future Internet architecture (FIA) designs. In contrast to deriving a self-certifying address from hashing of a correspondent prepared public key, we build up this self-certifying relationship along the reverse path using Combined Public Key (CPK). Our design develop the chain of trust embedded in the Internet name/address registration and allocation process for domains, hosts, services and content, to establish intrinsic bindings between three different identities: user-level human-readable names, network-level routable flat identifiers and the correspondent public keys. This binding connects the accountability between real-world space and network space. The use cases of our design are also given in named data networking (NDN) and identity/locator splitting network architecture, i.e. XIA and MobilityFirst. The analysis also shows that identity authentication based on CPK is capable of resource-constrained nodes in large-scale networks without scalability tradeoffs.},
booktitle = {Proceedings of the 11th International Conference on Future Internet Technologies},
pages = {62–68},
numpages = {7},
keywords = {combined public key, future Internet architecture, identity authentication, intrinsic security, self-certifying},
location = {Nanjing, China},
series = {CFI '16}
}

@article{10.1145/2767134,
author = {Lee, Sihyung},
title = {Detection of Political Manipulation in Online Communities through Measures of Effort and Collaboration},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/2767134},
doi = {10.1145/2767134},
abstract = {Online social media allow users to interact with one another by sharing opinions, and these opinions have a critical impact on the way readers think and behave. Accordingly, an increasing number of &lt;i&gt;manipulators&lt;/i&gt; deliberately spread messages to influence the public, often in an organized manner. In particular, political manipulation—manipulation of opponents to win political advantage—can result in serious consequences: antigovernment riots can break out, leading to candidates’ defeat in an election. A few approaches have been proposed to detect such manipulation based on the level of social interaction (i.e., manipulators actively post opinions but infrequently befriend and reply to other users). However, several studies have shown that the interactions can be forged at a low cost and thus may not be effective measures of manipulation.To go one step further, we collect a dataset for real, large-scale political manipulation, which consists of opinions found on Internet forums. These opinions are divided into manipulators and nonmanipulators. Using this collection, we demonstrate that manipulators inevitably work hard, in teams, to quickly influence a large audience. With this in mind, it could be said that a high level of collaborative efforts strongly indicates manipulation. For example, a group of manipulators may jointly post numerous opinions with a consistent theme and selectively recommend the same, well-organized opinion to promote its rank. We show that the effort measures, when combined with a supervised learning algorithm, successfully identify greater than 95% of the manipulators. We believe that the proposed method will help system administrators to accurately detect manipulators in disguise, significantly decreasing the intensity of manipulation.},
journal = {ACM Trans. Web},
month = {jun},
articleno = {16},
numpages = {24},
keywords = {Online social media, machine learning, opinion manipulation, political manipulation}
}

@inproceedings{10.5555/3237383.3237504,
author = {Ciortea, Andrei and Mayer, Simon and Michahelles, Florian},
title = {Repurposing Manufacturing Lines on the Fly with Multi-agent Systems for the Web of Things},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multi-agent systems (MAS) have long been envisioned as a key enabling technology in manufacturing, but this promise is yet to be realized: the lack of proper models, architectures, tooling, and the high level of expertise required for designing and programming agent-based manufacturing systems have hindered their large-scale acceptance. The emerging Web of Things (WoT), now being standardized at the W3C and IETF, provides new research opportunities that could help MAS enter the mainstream. In this paper, we present an approach to design scalable and flexible agent-based manufacturing systems that integrates automated planning with multi-agent oriented programming for the WoT: autonomous agents synthesize production plans using semantic descriptions of Web-based artifacts and coordinate with one another via multi-agent organizations; engineers can program and repurpose the systems on the fly via an intuitive Web user interface. The systems use the Web as an application architecture (and not just as a transport layer), which facilitates the seamless integration of geographically distributed production cells. To demonstrate our approach, we implemented a prototypical production cell that uses industry-grade robots and an augmented reality interface for human workers. Together, these contributions demonstrate a means to achieve an intriguing vision for the forthcoming fourth industrial revolution: a global collective intelligence for manufacturing.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {813–822},
numpages = {10},
keywords = {industry 4.0, multi-agent oriented programming, semantic web, web of things},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3091478.3091522,
author = {Guntuku, Sharath Chandra and Lin, Weisi and Carpenter, Jordan and Ng, Wee Keong and Ungar, Lyle H. and Preo\c{t}iuc-Pietro, Daniel},
title = {Studying Personality through the Content of Posted and Liked Images on Twitter},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3091522},
doi = {10.1145/3091478.3091522},
abstract = {Interacting with images through social media has become widespread due to ubiquitous Internet access and multimedia enabled devices. Through images, users generally present their daily activities, preferences or interests. This study aims to identify the way and extent to which personality differences, measured using the Big Five model, are related to online image posting and liking. In two experiments, the larger consisting of ~1.5 million Twitter images both posted and liked by ~4,000 users, we extract interpretable semantic concepts using large-scale image content analysis and analyze differences specific of each personality trait. Predictive results show that image content can predict personality traits, and that there can be significant performance gain by fusing the signal from both posted and liked images.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {223–227},
numpages = {5},
keywords = {content analysis, images, likes, multimedia, personality computing, personality prediction, posts, twitter},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@inproceedings{10.1145/2733373.2806237,
author = {Jiang, Lu and Yu, Shoou-I and Meng, Deyu and Yang, Yi and Mitamura, Teruko and Hauptmann, Alexander G.},
title = {Fast and Accurate Content-based Semantic Search in 100M Internet Videos},
year = {2015},
isbn = {9781450334594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2733373.2806237},
doi = {10.1145/2733373.2806237},
abstract = {Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval. Existing methods index a video by the raw concept detection score that is dense and inconsistent, and thus cannot scale to "big data" that are readily available on the Internet. This paper proposes a scalable solution. The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index. The proposed adjustment model relies on a concise optimization framework with interpretations. The proposed index leverages the text-based inverted index for video retrieval. Experimental results validate the efficacy and the efficiency of the proposed method. The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance. Specifically, the proposed method (with reranking) achieves the best result on the challenging TRECVID Multimedia Event Detection (MED) zero-example task. It only takes 0.2 second on a single CPU core to search a collection of 100 million Internet videos.},
booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
pages = {49–58},
numpages = {10},
keywords = {big data, content-based retrieval, internet video search, multimedia event detection, semantic search, zero shot},
location = {Brisbane, Australia},
series = {MM '15}
}

@inproceedings{10.1145/3288599.3288620,
author = {Maheshwari, Shobhi and Lundrigan, Philip and Kasera, Sneha Kumar},
title = {Scheduling virtual wifi interfaces for high bandwidth video upstreaming using multipath TCP},
year = {2019},
isbn = {9781450360944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288599.3288620},
doi = {10.1145/3288599.3288620},
abstract = {Live video upstreaming refers to the flow of live data in the upstream direction from mobile devices to other entities across the Internet and has found use in many modern applications such as remote driving, the recent social media trend of live video broadcasting along with the traditional applications of video calling/conferencing. Combined with the high definition video capturing capabilities of modern mobile devices, live video upstreaming is creating more upstream data traffic then what present day cellular networks are equipped to support, often resulting in sub-optimal video experience, especially in remote or crowded areas with low cellular connectivity and no WiFi.We propose that instead of using its single cellular connection, a mobile device connects to multiple nearby mobile devices and splits the live video data over the cellular bandwidth of these devices using Multipath TCP protocol. The use of MPTCP, for upstreaming live video data, has largely remained unexplored especially for scenarios where WiFi connectivity is not available. We use wireless interface virtualization, offered by Linux, to enable Multipath TCP to scale and connect to a large number of cellular devices. We design and build a system that is able to assess the instantaneous bandwidth of all the connected cellular devices/hotspots and uses the set of the most capable cellular devices for splitting and forwarding the live video data. We test our system in various settings and our experiments show that our system greatly increases the bandwidth and reliability of TCP connections in most cases and in cases where there is a significant difference in the throughput across cellular hotspots, our solution is able to recognize and isolate the better performing cellular hotspots to provide a stable throughput.},
booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
pages = {1–10},
numpages = {10},
keywords = {MPTCP, access points, cellular devices/hotspots, virtual wireless interfaces},
location = {Bangalore, India},
series = {ICDCN '19}
}

@inproceedings{10.1145/3132747.3132783,
author = {Tyagi, Nirvan and Gilad, Yossi and Leung, Derek and Zaharia, Matei and Zeldovich, Nickolai},
title = {Stadium: A Distributed Metadata-Private Messaging System},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132783},
doi = {10.1145/3132747.3132783},
abstract = {Private communication over the Internet remains a challenging problem. Even if messages are encrypted, it is hard to deliver them without revealing metadata about which pairs of users are communicating. Scalable anonymity systems, such as Tor, are susceptible to traffic analysis attacks that leak metadata. In contrast, the largest-scale systems with metadata privacy require passing all messages through a small number of providers, requiring a high operational cost for each provider and limiting their deployability in practice.This paper presents Stadium, a point-to-point messaging system that provides metadata and data privacy while scaling its work efficiently across hundreds of low-cost providers operated by different organizations. Much like Vuvuzela, the current largest-scale metadata-private system, Stadium achieves its provable guarantees through differential privacy and the addition of noisy cover traffic. The key challenge in Stadium is limiting the information revealed from the many observable traffic links of a highly distributed system, without requiring an overwhelming amount of noise. To solve this challenge, Stadium introduces techniques for distributed noise generation and differentially private routing as well as a verifiable parallel mixnet design where the servers collaboratively check that others follow the protocol. We show that Stadium can scale to support 4x more users than Vuvuzela using servers that cost an order of magnitude less to operate than Vuvuzela nodes.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {423–440},
numpages = {18},
keywords = {anonymous communication, differential privacy, mixnet, verifiable shuffle},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{10.1145/3326285.3329045,
author = {Zheng, Jiaqi and Ma, Qiufang and Tian, Chen and Dai, Haipeng and Zhang, Wei and Chen, Guihai and Zhang, Gong},
title = {Orchestrating service chain deployment with plutus in next generation cellular core},
year = {2019},
isbn = {9781450367783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326285.3329045},
doi = {10.1145/3326285.3329045},
abstract = {Today's cellular core relies on a few expensive and dedicated hardware racks to connect the radio access network and the egress point to the Internet, which are geographically placed at fixed locations and use the specific routing policies. This inelastic architecture fundamentally leads to increased capital and operating expenses, poor application performance and slow evolution. The emerging paradigm of Network Function Virtualization (NFV) and Software Defined Networking (SDN) bring new opportunities for cellular networks, which makes it possible to flexibly deploy service chains on commodity servers and fine-grained control the routing policies in a centralized way.We present a two-stage optimization framework Plutus. The network-level optimization aims to minimize the service chain deployment cost, while the server-level optimization requires to determine which Virtualized Network Function (VNF) should be deployed onto which CPU core to balance the CPU processing capability. We formulate these two problems as two optimization programs and prove their hardness. Based on parallel multi-block ADMM, we propose an (O(1), O(1)) bicriteria approximation algorithm and a 2-approximation algorithm. Large-scale simulations and DPDK-based OpenNetVM platform show that Plutus can reduce the capital cost by 84% and increase the throughput by 36% on average.},
booktitle = {Proceedings of the International Symposium on Quality of Service},
articleno = {10},
numpages = {10},
keywords = {NFV, SDN, cellular networks},
location = {Phoenix, Arizona},
series = {IWQoS '19}
}

@inproceedings{10.1145/3022227.3022336,
author = {Huong, Truong Thu and Thanh, Nguyen Huu},
title = {Software defined networking-based one-packet DDoS mitigation architecture},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022336},
doi = {10.1145/3022227.3022336},
abstract = {Nowadays, Distributed Denial of Service (DDoS) attacks get the most attention since volumetric attacks saturate company's networks and associated server infrastructure. In fact, DDoS can occur weekly or daily in a network but many organizations have no systems in place to monitor DDoS traffic so as to be aware if their networks are being attacked. Within that context, we propose to develop an architecture that enables a network a capacity of monitoring traffic on the fly and flexibly applying various detection and mitigation methods in order to reduce DDoS impact on the system shortly after it has happened. We also propose a SDN One-packet DDoS Mitigation (SODM) scheme with an Openflow switch functioning as a gateway to protect the inner server infrastructure. We also analyze Internet traffic to understand its common nature during attack and normal time. Knowledge of the traffic characteristics and the way to derive attack indicators are a critical input for the detection mechanism to work. The defense solution performance is evaluated to be able to cope with DDoS in small real time-scale with an acceptable false positive rate of ~ 6%.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {110},
numpages = {7},
keywords = {CAIDA data trace, DDoS attack, DDoS mitigation, Openflow/SDN},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@inproceedings{10.1145/3230543.3230568,
author = {Nisar, Aqib and Kashaf, Aqsa and Qazi, Ihsan Ayyub and Uzmi, Zartash Afzal},
title = {Incentivizing censorship measurements via circumvention},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230568},
doi = {10.1145/3230543.3230568},
abstract = {We present C-Saw, a system that measures Internet censorship by offering data-driven censorship circumvention to users. The adaptive circumvention capability of C-Saw incentivizes users to opt-in by offering small page load times (PLTs). As users crowdsource, the measurement data gets richer, offering greater insights into censorship mechanisms over a wider region, and in turn leading to even better circumvention capabilities. C-Saw incorporates user consent in its design by measuring only those URLs that a user actually visits. Using a cross-platform implementation of C-Saw, we show that it is effective at collecting and disseminating censorship measurements, selecting circumvention approaches, and optimizing user experience. C-Saw improves the average PLT by up to 48% and 63% over Lantern and Tor, respectively. We demonstrate the feasibility of a large-scale deployment of C-Saw with a pilot study.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {533–546},
numpages = {14},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@inproceedings{10.1145/2740908.2745945,
author = {Mehrotra, Rishabh and Bhattacharya, Prasanta},
title = {Modeling the Evolution of User-generated Content on a Large Video Sharing Platform},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2745945},
doi = {10.1145/2740908.2745945},
abstract = {Video sharing and entertainment websites have rapidly grown in popularity and now constitute some of the most visited websites on the Internet. Despite the high usage and user engagement, most of recent research on online media platforms have restricted themselves to networking based social media sites like Facebook or Twitter. The current study is among the first to perform a large-scale empirical study using longitudinal video upload data from one of the largest online video sites. Unlike previous studies in the online media space that have focused exclusively on demand-side research questions, we model the supply-side of the crowd contributed video ecosystem on this platform. The modeling and subsequent prediction of video uploads is made complicated by the heterogeneity of video types (e.g. popular vs. niche video genres), and the inherent time trend effects. We identify distinct genre-clusters from our dataset and employ a self-exciting Hawkes point-process model on each of these clusters to fully specify and estimate the video upload process. Our findings show that using a relatively parsimonious point-process model, we are able to achieve higher model fit, and predict video uploads to the platform with a higher accuracy than competing models.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {365–366},
numpages = {2},
keywords = {genre clusters, hawkes process, popularity effect, self-reinforcing effect, video uploads},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3133956.3134075,
author = {Nasr, Milad and Zolfaghari, Hadi and Houmansadr, Amir},
title = {The Waterfall of Liberty: Decoy Routing Circumvention that Resists Routing Attacks},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134075},
doi = {10.1145/3133956.3134075},
abstract = {Decoy routing is an emerging approach for censorship circumvention in which circumvention is implemented with help from a number of volunteer Internet autonomous systems, called decoy ASes. Recent studies on decoy routing consider all decoy routing systems to be susceptible to a fundamental attack -- regardless of their specific designs--in which the censors re-route traffic around decoy ASes, thereby preventing censored users from using such systems. In this paper, we propose a new architecture for decoy routing that, by design, is significantly stronger to rerouting attacks compared to all previous designs. Unlike previous designs, our new architecture operates decoy routers only on the downstream traffic of the censored users; therefore we call it downstream-only decoy routing. As we demonstrate through Internet-scale BGP simulations, downstream-only decoy routing offers significantly stronger resistance to rerouting attacks, which is intuitively because a (censoring) ISP has much less control on the downstream BGP routes of its traffic.Designing a downstream-only decoy routing system is a challenging engineering problem since decoy routers do not intercept the upstream traffic of censored users. We design the first downstream-only decoy routing system, called Waterfall, by devising unique covert communication mechanisms. We also use various techniques to make our Waterfall implementation resistant to traffic analysis attacks.We believe that downstream-only decoy routing is a significant step towards making decoy routing systems practical. This is because a downstream-only decoy routing system can be deployed using a significantly smaller number of volunteer ASes, given a target resistance to rerouting attacks. For instance, we show that a Waterfall implementation with only a single decoy AS is as resistant to routing attacks (against China) as a traditional decoy system (e.g., Telex) with 53 decoy ASes.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2037–2052},
numpages = {16},
keywords = {censorship circumvention, decoy routing, internet censorship, routing attacks},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@inproceedings{10.1145/3129292.3129294,
author = {Pareek, Alok and Khaladkar, Bhushan and Sen, Rajkumar and Onat, Basar and Nadimpalli, Vijay and Agarwal, Manish and Keene, Nicholas},
title = {Striim: A streaming analytics platform for real-time business decisions},
year = {2017},
isbn = {9781450354257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129292.3129294},
doi = {10.1145/3129292.3129294},
abstract = {Real-time decisions and insights over real-time data have become the essential mantra of success for many enterprises. The real-time data is generated from a multitude of sources and they come in a streaming fashion with high volume and velocity. The data could be machine generated e.g. clickstream data, logs, sensor data from IoT devices or human generated e.g. social data, mission critical transactional data. This is causing a technological shift from storage driven architectures to event driven architectures for enterprises to be able to capture, integrate and analyze these large sets of data for real-time decision making.Striim is a novel end-to-end analytics platform that enables business users to easily develop and deploy analytical applications that can generate real-time insights over real-time streaming data; business users and developers use a SQL-like declarative language (that has been extended to include streaming semantics) to write application logic in Striim. Striim provides high-throughput, low-latency event processing on commodity hardware with a scale-out architecture. In this paper, we describe the architecture of Striim and discuss some of the key aspects of the platform (a) built-in real-time data capture including streaming change data capture from transactional databases (ii) a natively built storage and query engine that uses modern data structures like skip lists to store streaming window data and performs query optimization, planning and run-time code generation (iii) enabling application de-coupling using persisted streams.},
booktitle = {Proceedings of the International Workshop on Real-Time Business Intelligence and Analytics},
articleno = {4},
numpages = {8},
keywords = {Streaming Analytics, data capture, exactly once processing},
location = {Munich, Germany},
series = {BIRTE '17}
}

@inproceedings{10.1145/3110025.3110135,
author = {Wu, Bin and Zhang, Cuiyun and Guo, Qian},
title = {A Parallel Network Community Detection Algorithm Based on Distance Dynamics},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110135},
doi = {10.1145/3110025.3110135},
abstract = {In recent years, community detection has drawn more and more researchers' attention. With the development of Internet, the scale of network data is growing fast. It is necessary to find an effective parallel community detection algorithm for large-scale network. In this paper, we propose a novel and parallel community detection algorithm, PCDU algorithm, based on distance dynamics. We send distances information to nodes and update distances of edges constantly, based on previous values and the unified model, which is introduced to quantify different influences from nodes and edges. It ends until the distances are stable. Then we remove some special edges from the original graph and get all subgraphs, which are the community partitions. It still inherits the advantage of uncovering small communities and outliers. Experiments based on synthetic networks and real world networks, show that our algorithm execute more efficient than stand-alone version. Since it is based on the Spark platform and designed in parallelization, the algorithm is very suitable for large datasets. We also provide a novel method taking use of double summation to calculate the NMI value of community partition result and the embedded community structure. Compared with the traditional way, it is not only as accurate as the traditional way and more efficient, but also has less space complexity. Experiments show that it is suitable for evaluating community division results in large-scale network.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {819–826},
numpages = {8},
keywords = {NMI, community detection, distance dynamics, parallelization},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3243734.3243801,
author = {Barak, Assi and Hirt, Martin and Koskas, Lior and Lindell, Yehuda},
title = {An End-to-End System for Large Scale P2P MPC-as-a-Service and Low-Bandwidth MPC for Weak Participants},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243801},
doi = {10.1145/3243734.3243801},
abstract = {Protocols for secure multiparty computation enable a set of parties to compute a joint function of their inputs, while preserving privacy, correctness and more. In theory, secure computation has broad applicability and can be used to solve many of the modern concerns around utilization of data and privacy. Huge steps have been made towards this vision in the past few years, and we now have protocols that can carry out large computations extremely efficiently, especially in the setting of an honest majority. However, in practice, there are still major barriers to widely deploying secure computation, especially in a decentralized manner. In this paper, we present the first end-to-end automated system for deploying large-scale MPC protocols between end users, called MPSaaS (for MPC system-as-a-service ). Our system enables parties to pre-enroll in an upcoming MPC computation, and then participate by either running software on a VM instance (e.g., in Amazon), or by running the protocol on a mobile app, in Javascript in their browser, or even on an IoT device. Our system includes an automation system for deploying MPC protocols, an administration component for setting up an MPC computation and inviting participants, and an end-user component for running the MPC protocol in realistic end-user environments. We demonstrate our system for a specific application of running secure polls and surveys, where the secure computation is run end-to-end with each party actually running the protocol (i.e., without relying on a set of servers to run the protocol for them). This is the first such system constructed, and is a big step forward to the goal of commoditizing MPC. One of the cryptographic difficulties that arise in this type of setting is due to the fact that end users may have low bandwidth connections, making it a challenge to run an MPC protocol with high bandwidth. We therefore present a protocol based on Beerliova-Trubiniova and Hirt (TCC 2008) with many optimizations, that has very low concrete communication, and the lowest published for small fields. Our protocol is secure as long as less than a third of the parties are malicious, and is well suited for computing both arithmetic and Boolean circuits. We call our protocol HyperMPC and show that it has impressive performance. In particular, 150 parties can compute statistics---mean, standard deviation and regression---on 4,000,000 inputs (with a circuit of size 16,000,000 gates of which 6,000,000 are multiplication) in just 45 seconds, and 150 parties can compute a circuit over GF[28] (which can be used for a Boolean computation) with 1,000,000 multiplication gates and depth-20 in just 2 seconds. Although our end-to-end system can be used to run any MPC protocol (and we have incorporated numerous protocols already), we demonstrate it for our new protocol that is optimized for end-users without~high~bandwidth.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {695–712},
numpages = {18},
location = {Toronto, Canada},
series = {CCS '18}
}

@inproceedings{10.1145/3038912.3052587,
author = {Simeonovski, Milivoj and Pellegrino, Giancarlo and Rossow, Christian and Backes, Michael},
title = {Who Controls the Internet? Analyzing Global Threats using Property Graph Traversals},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052587},
doi = {10.1145/3038912.3052587},
abstract = {The Internet is built on top of intertwined network services, e.g., email, DNS, and content distribution networks operated by private or governmental organizations. Recent events have shown that these organizations may, knowingly or unknowingly, be part of global-scale security incidents including state-sponsored mass surveillance programs and large-scale DDoS attacks. For example, in March 2015 the Great Cannon attack has shown that an Internet service provider can weaponize millions of Web browsers and turn them into DDoS bots by injecting malicious JavaScript code into transiting TCP connections.While attack techniques and root cause vulnerabilities are routinely studied, we still lack models and algorithms to study the intricate dependencies between services and providers, reason on their abuse, and assess the attack impact. To close this gap, we present a technique that models services, providers, and dependencies as a property graph. Moreover, we present a taint-style propagation-based technique to query the model, and present an evaluation of our framework on the top 100k Alexa domains.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {647–656},
numpages = {10},
keywords = {(dos) denial of service attacks, cyber-attacks, property graph traversals},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/2810103.2813687,
author = {Shokri, Reza and Shmatikov, Vitaly},
title = {Privacy-Preserving Deep Learning},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813687},
doi = {10.1145/2810103.2813687},
abstract = {Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training. Massive data collection required for deep learning presents obvious privacy issues. Users' personal, highly sensitive data such as photos and voice recordings is kept indefinitely by the companies that collect it. Users can neither delete it, nor restrict the purposes for which it is used. Furthermore, centrally kept data is subject to legal subpoenas and extra-judicial surveillance. Many data owners--for example, medical institutions that may want to apply deep learning methods to clinical records--are prevented by privacy and confidentiality concerns from sharing the data and thus benefitting from large-scale deep learning.In this paper, we design, implement, and evaluate a practical system that enables multiple parties to jointly learn an accurate neural-network model for a given objective without sharing their input datasets. We exploit the fact that the optimization algorithms used in modern deep learning, namely, those based on stochastic gradient descent, can be parallelized and executed asynchronously. Our system lets participants train independently on their own datasets and selectively share small subsets of their models' key parameters during training. This offers an attractive point in the utility/privacy tradeoff space: participants preserve the privacy of their respective data while still benefitting from other participants' models and thus boosting their learning accuracy beyond what is achievable solely on their own inputs. We demonstrate the accuracy of our privacy-preserving deep learning on benchmark datasets.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1310–1321},
numpages = {12},
keywords = {deep learning, gradient descent, neural networks, privacy},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@inproceedings{10.1145/3219819.3220004,
author = {Wang, Qinyong and Yin, Hongzhi and Hu, Zhiting and Lian, Defu and Wang, Hao and Huang, Zi},
title = {Neural Memory Streaming Recommender Networks with Adversarial Training},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220004},
doi = {10.1145/3219819.3220004},
abstract = {With the increasing popularity of various social media and E-commerce platforms, large volumes of user behaviour data (e.g., user transaction data, rating and review data) are being continually generated at unprecedented and ever-increasing scales. It is more realistic and practical to study recommender systems with inputs of streaming data. User-generated streaming data presents unique properties such as temporally ordered, continuous and high-velocity, which poses tremendous new challenges for the once very successful recommendation techniques. Although a few temporal or sequential recommender models have recently been developed based on recurrent neural models, most of them can only be applied to the session-based recommendation scenario, due to their short-term memories and the limited capability of capturing users' long-term stable interests. In this paper, we propose a streaming recommender model based on neural memory networks with external memories to capture and store both long-term stable interests and short-term dynamic interests in a unified way. An adaptive negative sampling framework based on Generative Adversarial Nets (GAN) is developed to optimize our proposed streaming recommender model, which effectively overcomes the limitations of classical negative sampling approaches and improves both effectiveness and efficiency of the model parameter inference. Extensive experiments have been conducted on two large-scale recommendation datasets, and the experimental results show the superiority of our proposed streaming recommender model in the streaming recommendation scenario.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2467–2475},
numpages = {9},
keywords = {collaborative filtering, memory networks, streaming recommender systems},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.5555/3324320.3324331,
author = {Schu\ss{}, Markus and Boano, Carlo Alberto and Weber, Manuel and Schulz, Matthias and Hollick, Matthias and R\"{o}mer, Kay},
title = {JamLab-NG: Benchmarking Low-Power Wireless Protocols under Controllable and Repeatable Wi-Fi Interference},
year = {2019},
isbn = {9780994988638},
publisher = {Junction Publishing},
address = {USA},
abstract = {Evaluating the performance of low-power wireless protocols in noisy environments in a repeatable and fullyautomated way is still an open problem in our community. On the one hand, there is a lack of tools enabling the controllable and repeatable generation of interference using Wi-Fi devices. On the other hand, existing testbeds do not offer the automated generation of Wi-Fi interference on a large-scale. In this work, we present JamLab-NG, an open-source framework allowing the generation of controllable Wi-Fi interference using off-the-shelf devices such as the Raspberry Pi 3. JamLab-NG enables the fine-grained control of individual link-layer transmissions, avoiding the uncontrollable delays introduced by the network stack, the operating system, and the clear channel assessment procedure. Furthermore, JamLab-NG allows to generate repeatable Wi-Fi interference patterns by controlling radio settings such as the transmission speed and the packet length, which would otherwise be automatically adapted by the radio firmware at run-time. We use JamLab-NG to augment a testbed and embed the generation of Wi-Fi interference into its automated execution of experiments. Among others, we allow remote configuration of the interference generated by individual Wi-Fi devices, and show that they can operate in a synchronized fashion. Finally, we use the augmented testbed to benchmark the performance of state-of-the-art IoT protocols under Wi-Fi interference in a repeatable and fully-automated way.},
booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
pages = {83–94},
numpages = {12},
keywords = {Competition, Dependability, Performance, Testbeds},
location = {Beijing, China},
series = {EWSN '19}
}

@inproceedings{10.1145/3341216.3342215,
author = {Madanapalli, Sharat Chandra and Gharakheili, Hassan Habibi and Sivaraman, Vijay},
title = {Assisting Delay and Bandwidth Sensitive Applications in a Self-Driving Network},
year = {2019},
isbn = {9781450368728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341216.3342215},
doi = {10.1145/3341216.3342215},
abstract = {Packet networks are agnostic to applications, which have served to keep the Internet infrastructure simple and scalable over the past several decades. However, the best-effort model is now seen as an inhibitor to meeting user experience expectations for the diverse applications such as streaming video, gaming, browsing, and social media. Current methods for prioritization of certain application types are static, and do not react to changes in network conditions or user experience. We envisage a self-driving network that is able to continuously monitor user experience and intervenes to assist applications as and when needed. Our contributions are: (1) We propose a self-driving network architecture that directly measures, optimizes, and dynamically controls application performance. We develop a method to measure and model application state in real-time using network behavior data. (2) We apply our framework to two representative applications, video streaming and gaming, and show how the network can detect application deterioration in terms of playback buffers and ping latency respectively, and apply remedial action to improve application performance without requiring any explicit signaling.},
booktitle = {Proceedings of the 2019 Workshop on Network Meets AI &amp; ML},
pages = {64–69},
numpages = {6},
keywords = {Quality of Experience, Self-Driving Network, Sensitive Applications},
location = {Beijing, China},
series = {NetAI'19}
}

@inproceedings{10.5555/2872518.3251214,
author = {Ahlers, Dirk and Wilde, Erik and Martins, Bruno},
title = {Session details: LocWeb'16},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {It is our great pleasure to welcome you to the 6th International Workshop on Location and the Web, associated with WWW 2016. This is the sixth workshop in its series, having previously been held at WWW (LocWeb 2008), CHI (LocWeb 2009), IoT (LocWeb 2010), CIKM (LocWeb 2014), and WWW (LocWeb 2015).LocWeb continues following its main objective of bringing together a community of researchers at the intersection of location and the Web, serving as a unique venue to integrate different backgrounds and stimulating the exchange of ideas and fostering closer cooperation. LocWeb will provide a topic-specific venue where researchers from different fields, be it data mining, recommendation, search, systems, services, social media, applications, or standards, can discuss and develop the role of location. Its focus lies in Web-scale services and systems facilitating location-aware information access. We aim for a highly interactive, collaborative workshop with ample room for discussion that will explore and advance the geospatial topic.The location topic is understood as a crosscutting issue equally concerning information access, semantics and standards, and Web-scale systems and services. The workshop establishes an integrated venue where the location aspect can be discussed in depth within an interested community. LocWeb follows the main theme of Location-Aware Information Access, with subtopics related to Search, Analytics, Mobility, Apps, Services, and Systems. It is designed to reflect the multitude of fields that demand and utilize location features from an interdisciplinary perspective.The call for papers attracted submissions from Europe, the Americas, Asia, and the Middle East. The program committee reviewed and accepted the following: Venue or Track Reviewed - 5, Accepted - 3.We encourage WWW attendees to attend the three accepted paper presentations, a keynote talk, and a discussion session. The detailed programme will be available on the workshop website.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@article{10.1145/2742549,
author = {Guo, Zhen and Zhang, Zhongfei (Mark) and Xing, Eric P. and Faloutsos, Christos},
title = {Multimodal Data Mining in a Multimedia Database Based on Structured Max Margin Learning},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2742549},
doi = {10.1145/2742549},
abstract = {Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet. In this article, we exploit the relations among different modalities in a multimedia database and present a framework for general multimodal data mining problem where image annotation and image retrieval are considered as the special cases. Specifically, the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables. In addition, in order to reduce the demanding computation, we propose a new max margin structure learning approach called Enhanced Max Margin Learning (EMML) framework, which is much more efficient with a much faster convergence rate than the existing max margin learning methods, as verified through empirical evaluations. Furthermore, we apply EMML framework to develop an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale. The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database, and excels many existing multimodal data mining methods in the literature that do not scale up at all. The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {feb},
articleno = {23},
numpages = {30},
keywords = {Multimodal data mining, image annotation, image retrieval, max margin}
}

@inproceedings{10.5555/3172795.3172823,
author = {Khazaei, Hamzeh and Ravichandiran, Rajsimman and Park, Byungchul and Bannazadeh, Hadi and Tizghadam, Ali and Leon-Garcia, Alberto},
title = {Elascale: autoscaling and monitoring as a service},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Auto-scalability has become an evident feature for cloud software systems including but not limited to big data and IoT applications. Cloud application providers now are in full control over their applications' microservices and macroservices; virtual machines and containers can be provisioned or deprovisioned on demand at run-time. Elascale strives to adjust both micro/macro resources with respect to workload and changes in the internal state of the whole application stack. Elascale leverages Elasticsearch stack for collection, analysis and storage of performance metrics. Elascale then uses its default scaling engine to elastically adapt the managed application. Extendibility is guaranteed through provider, schema, plug-in and policy elements in the Elascale by which flexible scalability algorithms, including both reactive and proactive techniques, can be designed and implemented for various technologies, infrastructures and software stacks. In this paper, we present the architecture and initial implementation of Elascale; an instance will be leveraged to add auto-scalability to a generic IoT application. Due to zero dependency to the target software system, Elascale can be leveraged to provide auto-scalability and monitoring as-a-service for any type of cloud software system.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {234–240},
numpages = {7},
keywords = {auto-scalability, cloud application, containers, docker, elasticsearch, macroservices, microservices, monitoring, scalability as a service},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3357292.3357327,
author = {Widodo, Abel Kristanto and Selvina, Oktivia and Widhiyaningrum and Siregar, Olivia Ester R.},
title = {In Corporating the E-S-QUAL Scale and Importance-Performance Analysis for Assessing Electronic Service Quality},
year = {2019},
isbn = {9781450371445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357292.3357327},
doi = {10.1145/3357292.3357327},
abstract = {All of information in the world can get with using internet. Internet being important in nowdays, because with internet for access an information, find a new social media friends, and making a business like e-commerce. E-commerce is a new way to develop effective and efficient business. It can define into transfer of goods, service, and information. E-commerce necessary to assess with service quality, because it can improve existence of e-commerce. The service quality of E-commerce can identified by E-S-QUAL (Electronic Service Quality) and try it to incorporate with importance-performance analysis (IPA) model. The E-S-QUAL scale was function to determine biggest aspect of behavior customer as the feelings are aroused during electronic service encounters. The IPA model was used here to admitting what item statements to be improved to attain customer satisfaction. It can be done through to identify what item statement which are perceived as important by the customers, so it could be reduce the excessive investment spent. A case study was conducted in one of most popular e-commerce in online marketplace. From this research, privacy (PR3), system availability (SYS4), and fulfillment (FUL7) is aspect of item statement to be improved. The result of this study can provide the managers with precious insight into the item statements of service quality that reflect customers' perception.},
booktitle = {Proceedings of the 2nd International Conference on Information Management and Management Sciences},
pages = {193–197},
numpages = {5},
keywords = {Customer Satisfaction, E-S-QUAL, IPA model, Service Quality, e-commerce},
location = {Chengdu, China},
series = {IMMS '19}
}

@inproceedings{10.1145/3208806.3219741,
author = {Dobo\v{s}, Jozef and Fan, Carmen and Knapo, Pavol and Wong, Charence},
title = {Applications of web3D technology in architecture, engineering and construction},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3219741},
doi = {10.1145/3208806.3219741},
abstract = {Architecture, engineering and construction (AEC) has witnessed a boom in the use of web3D technologies over the past few years thanks to the proliferation of WebGL support in modern web browsers, and the ever increasing need for multidisciplinary collaboration across large-scale construction projects. In this summary, we present a number of AEC-specific requirements and a set of applications for interactive visualization via the Internet based on the 3D Repo open source platform. These span collaborative design, 3D change and clash detection, data mining, maps integration and health and safety training. The presented use cases were developed in collaboration with large AEC companies, namely Atkins, Balfour Beatty, Canary Wharf Contractors, Costain and Skanska in the UK.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {30},
numpages = {2},
keywords = {3D repo, architecture, construction, engineering, unity 3D, web3D},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/2789168.2789180,
author = {Baron, Lo\"{\i}c and Boubekeur, Fadwa and Klacza, Radomir and Rahman, Mohammed Yasin and Scognamiglio, Ciro and Kurose, Nina and Friedman, Timur and Fdida, Serge},
title = {Demo: OneLab: Major Computer Networking Testbeds for IoT and Wireless Experimentation},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2789180},
doi = {10.1145/2789168.2789180},
abstract = {Gathering the required measurements to produce accurate results for mobile communications and wireless networking protocols, technologies and applications, relies on the use of expensive experimental computer networking facilities. Until very recently, large-scale testbed facilities have existed in separate silos, each with its own authentication mechanisms and experiment support tools. There lacked a viable federation model that reconciled the challenges posed by how to provide a single entry point to access heterogeneous and distributed resources, and how to federate these resources that are under the control of multiple authorities. The OneLab experimental facility, which came online in 2014, realizes this model, making a set of world-class testbeds freely available to researchers through a unique credential for each user and a common set of tools. We allow users to deploy innovative experiments across our federated platforms that include the embedded object testbeds of FIT IoT-Lab, the cognitive radio testbed of FIT CorteXlab, the wireless testbeds of NITOS-Lab, and the internet overlay testbed PlanetLab Europe (PLE), which together provide thousands of nodes for experimentation. Also federated under OneLab are the FUSECO Playground, which includes cloud, M2M, SDN, and mobile broadband; w-iLab.t wireless facilities; and the Virtual Wall testbed of wired networks and applications. Our demo describes the resources offered by the OneLab platforms, and illustrates how any member of the MobiCom community can create an account and start using these platforms today to deploy experiments for mobile and wireless testing.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {199–200},
numpages = {2},
keywords = {experimental facility, heterogeneous testbed federation, myslice, slice-based federation architecture, unique credential},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/3308557.3308698,
author = {Chen, Pei Hao and Shirai, Ryo and Hashimoto, Masanori},
title = {Coverage-scalable instant tabletop positioning system with self-localizable anchor nodes},
year = {2019},
isbn = {9781450366731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308557.3308698},
doi = {10.1145/3308557.3308698},
abstract = {This demo paper presents a desktop 3D positioning system, which is implemented with geomagnetic sensors and coils for DC magnetic field generation. Our system is composed of sensor nodes for localization, which sense and transmit the magnitude of the magnetic field to a host computer, and anchor nodes, which intermittently generate DC magnetic field with the functionality as sensor nodes. An advantage of our system is that the cover range and estimation accuracy can be enhanced instantly by adding anchor nodes since the added anchor node is also automatically localized. Also, the sensor node can be implemented in a mm-scale form factor with small power consumption thanks to the geomagnetic sensor, which enables us to attach the sensor nodes to various things. Object tracking could serve as a primary application of this system, such as virtual reality (VR), interactive educational experience and rehabilitation which can benefit the human computer interaction.},
booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
pages = {57–58},
numpages = {2},
keywords = {DC artificial magnetic field, desktop positioning system application, positioning system},
location = {Marina del Ray, California},
series = {IUI '19}
}

@inproceedings{10.1145/3322795.3331464,
author = {Qin, Yubo and Simonet, Anthony and Davis, Philip E. and Nouri, Azita and Wang, Zhe and Parashar, Manish and Rodero, Ivan},
title = {Towards a Smart, Internet-Scale Cache Service for Data Intensive Scientific Applications},
year = {2019},
isbn = {9781450367585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322795.3331464},
doi = {10.1145/3322795.3331464},
abstract = {Data and services provided by shared facilities, such as large-scale observing facilities, have become important enablers of scientific insights and discoveries across many science and engineering disciplines. Ensuring satisfactory quality of service can be challenging for facilities, due to their remote locations and to the distributed nature of the instruments, observatories, and users, as well as the rapid growth of data volumes and rates. This research explores how knowledge of the facilities usage patterns, coupled with emerging cyberinfrastructures can be leveraged to improve their performance, usability, and scientific impact. We propose a framework with a smart, internet-scale cache augmented with prefetching and data placement strategies to improve data delivery performance for scientific facilities. Our evaluations, which are based on the NSF Ocean Observatories Initiative, demonstrate that our framework is able to predict user requests and reduce data movements by more than 56% across networks.},
booktitle = {Proceedings of the 10th Workshop on Scientific Cloud Computing},
pages = {11–18},
numpages = {8},
keywords = {cyberinfrastructure, data repository, distributed data sharing, distributed facilities, prefetching, virtual data collaboratory},
location = {Phoenix, AZ, USA},
series = {ScienceCloud '19}
}

@inproceedings{10.1145/3205289.3205299,
author = {Zhou, Ke and Sun, Si and Wang, Hua and Huang, Ping and He, Xubin and Lan, Rui and Li, Wenyan and Liu, Wenjie and Yang, Tianming},
title = {Demystifying Cache Policies for Photo Stores at Scale: A Tencent Case Study},
year = {2018},
isbn = {9781450357838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205289.3205299},
doi = {10.1145/3205289.3205299},
abstract = {Photo service providers are facing critical challenges of dealing with the huge amount of photo storage, typically in a magnitude of billions of photos, while ensuring national-wide or world-wide satisfactory user experiences. Distributed photo caching architecture is widely deployed to meet high performance expectations, where efficient still mysterious caching policies play essential roles. In this work, we present a comprehensive study on internet-scale photo caching algorithms in the case of QQPhoto from Tencent Inc., the largest social network service company in China. We unveil that even advanced cache algorithms can only perform at a similar level as simple baseline algorithms and there still exists a large performance gap between these cache algorithms and the theoretically optimal algorithm due to the complicated access behaviors in such a large multi-tenant environment. We then expound the behind reasons for that phenomenon via extensively investigating the characteristics of QQPhoto workloads. Finally, in order to realistically further improve QQPhoto cache efficiency, we propose to incorporate a prefetcher in the cache stack based on the observed immediacy feature that is unique to the QQPhoto workload. Evaluation results show that with appropriate prefetching we improve the cache hit ratio by up to 7.4%, while reducing the average access latency by 6.9% at a marginal cost of 4.14% backend network traffic compared to the original system that performs no prefetching.},
booktitle = {Proceedings of the 2018 International Conference on Supercomputing},
pages = {284–294},
numpages = {11},
keywords = {Caching, Distributed Storage, Performance Evaluation},
location = {Beijing, China},
series = {ICS '18}
}

@inproceedings{10.1145/2934583.2934621,
author = {Najafi, Ali and Rudell, Jacques C. and Sathe, Visvesh},
title = {Regenerative Breaking: Recovering Stored Energy from Inactive Voltage Domains for Energy-efficient Systems-on-Chip},
year = {2016},
isbn = {9781450341851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934583.2934621},
doi = {10.1145/2934583.2934621},
abstract = {Modern Systems-on-Chip(SoCs) frequently power-off individual voltage domains to save leakage power across a variety of applications, from large-scale heterogeneous computing to ultra-low power systems in IoT applications. However, the considerable energy stored within the capacitance of the powered-off domain is lost through leakage. In this paper, we present an approach to leverage existing voltage regulators to recover this energy from the disabled voltage-domain back into the supply using a low-overhead all-digital runtime control system. Simulation experiments conducted in an industrial 65nm CMOS process indicate that over 90% of the stored energy can be recovered across a range of operating system voltages from 0.4V--1V.},
booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design},
pages = {94–99},
numpages = {6},
keywords = {All-digital control, Buck converters, Energy recovery, Energy-harvesting, Heterogeneous computing, Voltage regulators},
location = {San Francisco Airport, CA, USA},
series = {ISLPED '16}
}

@article{10.1145/3151123.3151129,
author = {Elhamshary, Moustafa and Youssef, Moustafa},
title = {Towards ubiquitous indoor spatial awareness on a worldwide scale},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3151123.3151129},
doi = {10.1145/3151123.3151129},
abstract = {While a remarkable effort has been put in developing indoor spatial awareness systems, they are still isolated efforts that are tailored to specific deployments. A truly ubiquitous indoor spatial awareness system is envisioned to be deployed on a large scale worldwide, with minimum overhead, and to work with the heterogeneous IoT devices. Such a system will enable a wide set of new applications including worldwide seamless direction finding between indoor locations, anywhere anytime health monitoring, enhanced first responders' safety, and providing richer context for indoor mobile computing applications.In this paper, we describe our vision and work towards achieving ubiquitous indoor spatial awareness systems as well as the open challenges that need to be addressed to materialize this dream.},
journal = {SIGSPATIAL Special},
month = {oct},
pages = {36–43},
numpages = {8}
}

@article{10.1145/2912123,
author = {Merani, Maria Luisa and Natali, Laura},
title = {Adaptive Streaming in P2P Live Video Systems: A Distributed Rate Control Approach},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/2912123},
doi = {10.1145/2912123},
abstract = {Dynamic Adaptive Streaming over HTTP (DASH) is a recently proposed standard that offers different versions of the same media content to adapt the delivery process over the Internet to dynamic bandwidth fluctuations and different user device capabilities. The peer-to-peer (P2P) paradigm for video streaming allows us to leverage the cooperation among peers, guaranteeing the service of video requests with increased scalability and reduced cost. We propose to combine these two approaches in a P2P-DASH architecture, exploiting the potentiality of both. The new platform is made of several swarms and a different DASH representation is streamed within each of them; unlike client-server DASH architectures, where each client autonomously selects which version to download according to current network conditions and to its device resources, we put forth a new rate control strategy implemented at peer site to maintain a good viewing quality to the local user and to simultaneously guarantee the successful operation of the P2P swarms. The effectiveness of the solution is demonstrated through simulation and it indicates that the P2P-DASH platform is able to provide its users with very good performance, much more satisfying than in a conventional P2P environment where DASH is not employed. Through a comparison with a reference DASH system modeled via the Integer Linear Programming (ILP) approach, the new system is shown to outperform such reference architecture. To further validate the proposal, in terms of both robustness and scalability, system behavior is investigated in the critical condition of a flash crowd, showing that the strong upsurge of new users can be successfully revealed and gradually accommodated.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {46},
numpages = {23},
keywords = {DASH, flash-crowd, integer linear programming, peer-to-peer video streaming}
}

@inproceedings{10.1145/3357150.3357394,
author = {Carofiglio, Giovanna and Muscariello, Luca and Aug\'{e}, Jordan and Papalini, Michele and Sardara, Mauro and Compagno, Alberto},
title = {Enabling ICN in the Internet Protocol: Analysis and Evaluation of the Hybrid-ICN Architecture},
year = {2019},
isbn = {9781450369701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357150.3357394},
doi = {10.1145/3357150.3357394},
abstract = {Information-Centric Networking (ICN) embraces a family of network architectures rethinking Internet communication principles around named-data. After several years of research and the emergence of a few popular proposals, the idea to replace the Internet protocol with data-centric networking remains a subject of debate. ICN advantages have been advocated in the context of 5G networks for the support of highly mobile, multi-access/source and latency-minimal patterns of communications. However, large scale testing and insertion in operational networks are yet to happen, likely due to the lack of a clear incremental deployment strategy. In this paper, we analyze a recent proposal Hybrid-ICN (hICN), an ICN integration inside IP (rather that over/ under/ in place of) that has the ambition to trade-off no ICN architectural principles. By reusing existing packet formats, hICN brings innovation inside the IP stack, requiring minimal software upgrades and guaranteeing transparent interconnection with existing IP networks.We describe the architecture and use the open source implementation to test hICN in the open Internet to validate its short-term deployability. Further, we consider linear video streaming over mobile wireless heterogeneous networks as use case to highlight hICN advantages compared to TCP/IP counterpart.},
booktitle = {Proceedings of the 6th ACM Conference on Information-Centric Networking},
pages = {55–66},
numpages = {12},
keywords = {Future Internet architectures, ICN, IPv6},
location = {Macao, China},
series = {ICN '19}
}

@inproceedings{10.1145/2905055.2905253,
author = {Nishad, Lahar Singh and Akriti and Paliwal, Jaya and Pandey, Roli and Beniwal, Sumitra and Kumar, Sarvesh},
title = {Security, Privacy Issues and challenges In Cloud Computing: A Survey},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905253},
doi = {10.1145/2905055.2905253},
abstract = {Cloud computing is a rising paradigm for large scale infrastructures. Cloud computing is a technology that being hugely adopted by many organizations like Google, Microsoft, Facebook, Amazon so that the resources are available to multiple users at a time over the internet on demand at any place and any time. Cloud computing opens up a new world of opportunities for entrepreneurships for small company to large company, but with these opportunities there are numerous security challenges that need to be considered and addressed prior to migrating to a cloud computing. Security and privacy issues present a strong barrier for users to use Cloud Computing systems. The security problem of cloud computing is very important and it can prevent the quick development of cloud computing. We have discussed the related work done by different author in cloud security[10]. This paper discusses security issues necessities and challenges that cloud service providers (CSP)and user face in cloud atmosphere, and we also assess how security, trust and privacy issues occur in the situation of cloud computing and discuss ways in which they may be addressed.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {47},
numpages = {7},
keywords = {Cloud Service provider(CSP), Cloud computing, Infrastructure as a Service(IaaS), Platform as a Service (PaaS), Software as a Service (SaaS)},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3121050.3121077,
author = {Mehrotra, Rishabh and Bhattacharya, Prasanta},
title = {Characterizing and Predicting Supply-side Engagement on Video Sharing Platforms Using a Hawkes Process Model},
year = {2017},
isbn = {9781450344906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121050.3121077},
doi = {10.1145/3121050.3121077},
abstract = {Video sharing platforms are one of the most popular and engaging platforms on the Internet today. Despite the increasing levels of user activity on these video platforms, current research on digital platforms have largely focused on social media and networking websites like Facebook and Twitter. We depart from previous work that have focused primarily on user demands (i.e. activity of viewers), and instead focus our attention to the supply-side activities on the platform (i.e. activity of video uploaders). We perform a large-scale empirical study by leveraging longitudinal video upload data from a major online video platform, demonstrating (i) heterogeneity of video types (e.g. presence of popular vs. niche genres), and (ii) inherent seasonality effects associated with video uploads. Through our analyses, we uncover a set of informative genre-clusters and estimate a self-exciting Hawkes point-process model on each of these clusters, to fully specify and estimate the video upload process. Additionally, we disentangle potential factors that govern user engagement and determine the video upload rates, which help supplement our analysis with additional explanatory power. Our results emphasize that using a parsimonious and relatively simple point-process model, we were able to obtain a high model fit, as well as perform prediction of video upload volumes with a higher accuracy than a number of competing models. The findings from this study can benefit platform owners in better understanding how their supply-side users engage with their site over time. We also offer a robust method for performing media upload prediction that is likely to be generalizable across media platforms which demonstrate similar temporal and genre-level heterogeneity.},
booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {159–166},
numpages = {8},
keywords = {hawkes process, user engagement, video uploads},
location = {Amsterdam, The Netherlands},
series = {ICTIR '17}
}

@inproceedings{10.1145/3278532.3278545,
author = {Sarabi, Armin and Liu, Mingyan},
title = {Characterizing the Internet Host Population Using Deep Learning: A Universal and Lightweight Numerical Embedding},
year = {2018},
isbn = {9781450356190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278532.3278545},
doi = {10.1145/3278532.3278545},
abstract = {In this paper, we present a framework to characterize Internet hosts using deep learning, using Internet scan data to produce numerical and lightweight (low-dimensional) representations of hosts. To do so we first develop a novel method for extracting binary tags from structured texts, the format of the scan data. We then use a variational autoencoder, an unsupervised neural network model, to construct low-dimensional embeddings of our high-dimensional binary representations. We show that these lightweight embeddings retain most of the information in our binary representations, while drastically reducing memory and computational requirements for large-scale analysis. These embeddings are also universal, in that the process used to generate them is unsupervised and does not rely on specific applications. This universality makes the embeddings broadly applicable to a variety of learning tasks whereby they can be used as input features. We present two such examples, (1) detecting and predicting malicious hosts, and (2) unmasking hidden host attributes, and compare the trained models in their performance, speed, robustness, and interpretability. We show that our embeddings can achieve high accuracy (&gt;95%) for these learning tasks, while being fast enough to enable host-level analysis at scale.},
booktitle = {Proceedings of the Internet Measurement Conference 2018},
pages = {133–146},
numpages = {14},
keywords = {Host Embedding, Machine Learning, Network Measurement},
location = {Boston, MA, USA},
series = {IMC '18}
}

@inproceedings{10.1145/2785956.2789996,
author = {van Rijswijk-Deij, Roland and Jonker, Mattijs and Sperotto, Anna and Pras, Aiko},
title = {The Internet of Names: A DNS Big Dataset},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2789996},
doi = {10.1145/2785956.2789996},
abstract = {The Domain Name System (DNS) is part of the core infrastructure of the Internet. Tracking changes in the DNS over time provides valuable information about the evolution of the Internet's infrastructure. Until now, only one large-scale approach to perform these kinds of measurements existed, passive DNS (pDNS). While pDNS is useful for applications like tracing security incidents, it does not provide sufficient information to reliably track DNS changes over time. We use a complementary approach based on active measurements, which provides a unique, comprehensive dataset on the evolution of DNS over time. Our high-performance infrastructure performs Internet-scale active measurements, currently querying over 50% of the DNS name space on a daily basis. Our infrastructure is designed from the ground up to enable big data analysis approaches on, e.g., a Hadoop cluster. With this novel approach we aim for a quantum leap in DNS-based measurement and analysis of the Internet.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {91–92},
numpages = {2},
keywords = {DNS, active measurements, big data, internet evolution},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{10.1145/3332186.3333255,
author = {Quick, Rob and Lannom, Larry and Krenz, Marina and Luo, Yu},
title = {E-RPID PEARC 2019: The Digital Object Architecture and Enhanced Robust Persistent Identification of Data},
year = {2019},
isbn = {9781450372275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332186.3333255},
doi = {10.1145/3332186.3333255},
abstract = {The expansion of the research community's ability to collect and store data has grown much more rapidly than its ability to catalog, make accessible, and make use of data. Recent initiatives in Open Science and Open Data have attempted to address the problems of making data discoverable, accessible and reusable at internet scales. The Enhanced Robust Persistent Identification of Data (E-RPID) project's goal is to address these deficiencies and enable options for data interoperability and reusability in the current research data landscape by utilizing Persistent Identifiers (PIDs) and a kernel of state information available with PID resolution. To do this requires integrating a set of preexisting software systems along with a small set of newly developed software solutions. The combination of these software components and the core principles of making data FAIR (findable, accessible, interoperable and reusable) will allow us to use Persistent Identifiers to create an end-to-end fabric capable of realizing the Digital Object Architecture for researchers.This poster will acquaint the audience to the concepts of the Digital Object Architecture, describe the software service architecture necessary to enable this architecture, outline the existing E-RPID testbed that is available for experimental usage from the Jetstream cloud environment, and describe the diverse set of use cases already using E-RPID to enhance their data accessibility, interoperability and reusability. It will focus on how the Digital Object Architecture and E-RPID testbed would interact with XSEDE resources and how E-RPID could assist with interoperability, reusability and reproducibility of HPC workflows.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
articleno = {123},
numpages = {4},
keywords = {E-RPID, FAIR data, PID Kernel, digital object architecture, interoperability, persistent identification of data, reproducibility},
location = {Chicago, IL, USA},
series = {PEARC '19}
}

@inproceedings{10.1145/2737095.2737119,
author = {Pfammatter, Damian and Giustiniano, Domenico and Lenders, Vincent},
title = {A software-defined sensor architecture for large-scale wideband spectrum monitoring},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2737119},
doi = {10.1145/2737095.2737119},
abstract = {Today's spectrum measurements are mainly performed by governmental agencies which drive around using expensive specialized hardware. The idea of crowdsourcing spectrum monitoring has recently gained attention as an alternative way to capture the usage of wide portions of the wireless spectrum at larger geographical and time scales. To support this vision, we develop a flexible software-defined sensor architecture that enables distributed data collection in real-time over the Internet. Our sensor design builds upon low-cost commercial off-the-shelf (COTS) hardware components with a total cost per sensor device below $100. The low-cost nature of our sensor platform makes the sensing approach particularly suitable for large-scale deployments but imposes technical challenges regarding performance and quality. To circumvent the limits of our solution, we have implemented and evaluated different sensing strategies and noise reduction techniques. Our results suggest that our sensor architecture may be useful in application areas such as dynamic spectrum access in cognitive radios, detecting regions with elevated electro-smog, or simply to gain an understanding of the spectrum usage for advanced signal intelligence such as anomaly detection or policy enforcement.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {71–82},
numpages = {12},
keywords = {crowdsourcing, distributed, spectrum monitoring, wideband},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/2926676.2926685,
author = {Raghavan, Barath and Hasan, Shaddi},
title = {Macroscopically sustainable networking: on internet quines},
year = {2016},
isbn = {9781450342605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2926676.2926685},
doi = {10.1145/2926676.2926685},
abstract = {The Internet stands atop an unseen industrial system required for its continued growth, operation, and maintenance. Its scale could not have been achieved without this reliance, and its dependencies---ranging from sophisticated manufacturing facilities to limited raw materials---make it vulnerable to supply-chain disruptions, which are more likely as human society faces global ecological limits. We introduce the concept of an Internet quine, a metaphor that represents a collection of devices, protocols, manufacturing facilities, software tools, and other related components that is self-bootstrapping and capable of being used (by engineers or autonomously) to reproduce itself and all the needed components of the Internet. In this paper, we study the nature of Internet quines and discuss how they could be built. We also attempt to identify a collection of such tools and facilities, and how small and inexpensive they can be made.},
booktitle = {Proceedings of the Second Workshop on Computing within Limits},
articleno = {11},
numpages = {6},
keywords = {life-cycle analysis, sustainability},
location = {Irvine, California},
series = {LIMITS '16}
}

@article{10.1109/TNET.2019.2926320,
author = {Ghali, Cesar and Tsudik, Gene and Uzun, Ersin},
title = {In Content We Trust: Network-Layer Trust in Content-Centric Networking},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2926320},
doi = {10.1109/TNET.2019.2926320},
abstract = {Content-Centric Networking CCN, an instance of information-centric networking, is a candidate next-generation Internet architecture that emphasizes on content distribution by making it directly addressable and routable. By opportunistically caching content within the network, CCN appears to be well-suited for a large-scale content distribution and for meeting the needs of increasingly mobile and bandwidth-hungry applications that dominate today’s Internet. To provide content authentication, CCN dictates that each content object must be digitally signed by its respective producer. All entities consumers and routers must, in principle, verify the content signature before processing it. However, in practice, this poses two challenges for routers: 1 overhead due to signature verification, key retrieval, and potential certificate chain traversal; and 2 lack of trust context, i.e., determining which public keys are trusted to verify the content signature. This renders signature verification impractical in routers, opening the door for the so-called content poisoning attacks. We study the root causes of the content poisoning attacks and reach the conclusion that meaningful mitigation of content poisoning is contingent upon a network-layer trust management architecture. We propose two approaches: deterministic and probabilistic, that allow routers to detect fake aka “poisoned” content objects. The usages of each approach depend on the location and role of routers in the network, as well as their computational capabilities.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {1787–1800},
numpages = {14}
}

@inproceedings{10.1145/2857546.2857584,
author = {Miyokawa, Shohei and Tokuda, Taiki and Yamaguchi, Saneyasu},
title = {Elasticity Improvement of Cassandra},
year = {2016},
isbn = {9781450341424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857546.2857584},
doi = {10.1145/2857546.2857584},
abstract = {Load size for a service on the Internet changes remarkably every hour. Thus, it is expected for service system scales to change dynamically according to load size. KVS (key-value store) is a scalable DBMS (database management system) widely used in large-scale Internet services. In this paper, we focus on Cassandra, a popular open-source KVS implementation, and discuss methods for improving dynamic scaling performance. First, we evaluate node joining time, which is the time to complete adding a node to a running KVS system, and show that its bottleneck process is disk I/O in the existing nodes. Second, we analyze disk accesses in the bottleneck nodes and indicate that some heavily accessed files cause a large number of disk accesses. Third, we propose a method for improving elasticity, which means decreasing node adding and removing time, of Cassandra. The method reduces disk accesses significantly by keeping the heavily accessed file in the page cache. Lastly, we evaluate elasticity of our methods. Our experimental results demonstrate that the method can improve the scaling-up and scaling-down performance of Cassandra.},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
articleno = {37},
numpages = {7},
keywords = {Cassandra, Key-Value Store, page cache},
location = {Danang, Viet Nam},
series = {IMCOM '16}
}

@inproceedings{10.1145/2740908.2741982,
author = {Jung, Kyomin and Zhang, Byoung-Tak and Mitra, Prasenjit},
title = {Deep Learning for the Web},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2741982},
doi = {10.1145/2740908.2741982},
abstract = {Deep learning is a machine learning technology that automatically extracts higher-level representations from raw data by stacking multiple layers of neuron-like units. The stacking allows for extracting representations of increasingly-complex features without time-consuming, offline feature engineering. Recent success of deep learning has shown that it outperforms state-of-the-art systems in image processing, voice recognition, web search, recommendation systems, etc [1]. A lot of industrial-scale big data processing systems including IBM Watson's Jeopardy Contest 2011, Google Now, Facebook's face recognition system, and the voice recognition systems by Google and Microsoft use deep learning [2][3][6]. Deep learning has a huge potential to improve the intelligence of the web and the web service systems by efficiently and effectively mining big data on the Web[4][5]. This tutorial provides the basics of deep learning as well as its key applications. We give the motivation and underlying ideas of deep learning and describe the architectures and learning algorithms for various deep learning models. We also cover applications of deep learning for image and video processing, natural language and text data analysis, social data analytics, and wearable IoT sensor data with an emphasis in the domain of Web systems. We will deliver the key insight and understanding of these techniques, using graphical illustrations and examples that could be important in analyzing a large amount of Web data. The tutorial is prepared to attract general audience at the WWW Conference, who are interested in machine learning and big data analysis for Web data. The tutorial consists of five parts. The first part presents the basics of neural networks, and their structures. Then we explain the training algorithm via backpropagation, which is a common method of training artificial neural networks including deep neural networks. We will emphasize how each of these concepts can be used in various Web data analysis. In the second part of the tutorial, we describe the learning algorithms for deep neural networks and related ideas, such as contrastive divergence, wake-sleep algorithms, and Monte Carlo simulation. We then describe various kinds of deep architectures, including stacked autoencoders, deep belief networks [7], convolutional neural networks [8], and deep hypernetworks [9]. In the third part, we present more details of the recursive neural networks, which can learn structured tree outputs as well as vector representations for phrases and sentences. We first show how training the recursive neural network can be achieved by a modified version of the back-propagation algorithm introduced before. These modifications allow the algorithm to work on tree structures. Then we will present its applications to sentence analysis including POS tagging, and sentiment analysis. The fourth part discusses the neural networks used to generate word embeddings, such as Word2Vec [10], DSSM for deep semantic similarity [11], and object detection in images [12], such as GoogLeNet, and AlexNet. We will explain in detail the applications of these deep learning techniques in the analysis of various social network data. By this point, the audience should have a clear understanding of how to build a deep learning system for word, sentence and document level tasks. The fifth part of the tutorial will cover other application examples of deep learning. These include object segmentation and action recognition from videos [9], web data analytics, and wearable/IoT sensor data modeling for smart services.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1525–1526},
numpages = {2},
keywords = {deep learning, document analysis, natural language processing, neural network, recursive neural network, social network},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/2801948.2802000,
author = {Konstantinos, Katsantonis and Persefoni, Mitropoulou and Evangelia, Filiopoulou and Christos, Michalakelis and Mara, Nikolaidou},
title = {Cloud computing and economic growth},
year = {2015},
isbn = {9781450335515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801948.2802000},
doi = {10.1145/2801948.2802000},
abstract = {Cloud computing, is a rapidly evolving type of internet- based computing model that relies on sharing computing resources, rather than having local servers or personnel to handle them. It has already been adopted by a significant number of Small and Medium Enterprises (SMEs) as a business advantage able to improve their business environment and help them be more efficient and productive. Due to its beneficial characteristics, as flexibility of cost and scalability, cloud computing has the potential to transform the global ICT market techniques and contribute to the boost of economic growth. The provision of cloud computing services is a new and very promising business model and cloud service providers are already enjoying growing profits.This paper seeks to highlight the economic benefits of cloud computing adoption, its impact on the economic growth of a country, and to explore its diffusion using evidence from the European area. Another main objective is the demonstration of the economic benefits an SME can achieve by adopting cloud services instead of proprietary infrastructures. A case study of a new company entering the market is considered and the corresponding calculations are based on a software tool developed by our research team for the calculation of the total cost of ownership (TCO). Results, will reveal the economic benefits of the cloud and its contribution to the economic growth.},
booktitle = {Proceedings of the 19th Panhellenic Conference on Informatics},
pages = {209–214},
numpages = {6},
keywords = {ICT market, cloud computing, cloud provider, economic growth, total cost of ownership},
location = {Athens, Greece},
series = {PCI '15}
}

@inproceedings{10.1145/3308558.3313467,
author = {Salutari, Flavia and Da Hora, Diego and Dubuc, Gilles and Rossi, Dario},
title = {A Large-scale Study of Wikipedia Users' Quality of Experience},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313467},
doi = {10.1145/3308558.3313467},
abstract = {The Web is one of the most successful Internet application. Yet, the quality of Web users' experience is still largely impenetrable. Whereas Web performances are typically gathered with controlled experiments, in this work we perform a large-scale study of one of the most popular websites,namely Wikipedia, explicitly asking (a small fraction of its) users for feedback on the browsing experience. We leverage user survey responses to build a data-driven model of user satisfaction which, despite including state-of-the art quality of experience metrics, is still far from achieving accurate results, and discuss directions to move forward. Finally, we aim at making our dataset publicly available, which hopefully contributes in enriching and refining the scientific community knowledge on Web users' quality of experience (QoE).},
booktitle = {The World Wide Web Conference},
pages = {3194–3200},
numpages = {7},
keywords = {Quality Of Experience, Web Browsing QoE, Web Performance},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.14778/2824032.2824078,
author = {Pelkonen, Tuomas and Franklin, Scott and Teller, Justin and Cavallaro, Paul and Huang, Qi and Meza, Justin and Veeraraghavan, Kaushik},
title = {Gorilla: a fast, scalable, in-memory time series database},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824078},
doi = {10.14778/2824032.2824078},
abstract = {Large-scale internet services aim to remain highly available and responsive in the presence of unexpected failures. Providing this service often requires monitoring and analyzing tens of millions of measurements per second across a large number of systems, and one particularly effective solution is to store and query such measurements in a time series database (TSDB).A key challenge in the design of TSDBs is how to strike the right balance between efficiency, scalability, and reliability. In this paper we introduce Gorilla, Facebook's in-memory TSDB. Our insight is that users of monitoring systems do not place much emphasis on individual data points but rather on aggregate analysis, and recent data points are of much higher value than older points to quickly detect and diagnose the root cause of an ongoing problem. Gorilla optimizes for remaining highly available for writes and reads, even in the face of failures, at the expense of possibly dropping small amounts of data on the write path. To improve query efficiency, we aggressively leverage compression techniques such as delta-of-delta timestamps and XOR'd floating point values to reduce Gorilla's storage footprint by 10x. This allows us to store Gorilla's data in memory, reducing query latency by 73x and improving query throughput by 14x when compared to a traditional database (HBase)-backed time series data. This performance improvement has unlocked new monitoring and debugging tools, such as time series correlation search and more dense visualization tools. Gorilla also gracefully handles failures from a single-node to entire regions with little to no operational overhead.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1816–1827},
numpages = {12}
}

@inproceedings{10.1145/3354153.3354157,
author = {ElRabih, Diana},
title = {Cooperative and Distributed Intrusion Detection using BigData},
year = {2019},
isbn = {9781450372169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3354153.3354157},
doi = {10.1145/3354153.3354157},
abstract = {Internet infrastructure is vulnerable to various attacks, then security and privacy are the key issues for Internet applications. Internet requires various security solutions where the communication is secured with confidentiality, integrity, and authentication services. Therefore, the challenge of implementing secure and protected communication in the Internet network must be addressed. The Internet network is secured with encryption and authentication, but it cannot be protected and secured against cyber-attacks. Hence, an Intrusion Detection System IDS is needed. Analyzing Internet network flows, logs, and system events has been used for intrusion detection. Big Data analytics can correlate multiple information sources into a coherent view, identify anomalies and suspicious activities, and finally achieve effective and efficient intrusion detection. One solution is to have an IDS that supervises the situation for all the computers in the Internet and makes decision regarding possible attacks. This method is not effective due to large scale of Internet and high speed of Internet. This problem is resolved in this paper by proposing an approach of a distributed intrusion detection system that is based on cooperative agents (sensors) using Big Data. Then agents (sensors) in our approach work together in a distributed and cooperative manner and these agents (sensors) perform data collection and data analysis using Big Data technology to detect intrusion in the Internet.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Storage and Data Engineering},
pages = {54–58},
numpages = {5},
keywords = {BigData, Internet, Intrusion Detection},
location = {Jeju, Republic of Korea},
series = {DSDE 2019}
}

@article{10.1145/2644827,
author = {Xiong, Haoyi and Zhang, Daqing and Wang, Leye and Gibson, J. Paul and Zhu, Jie},
title = {EEMC: Enabling Energy-Efficient Mobile Crowdsensing with Anonymous Participants},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2644827},
doi = {10.1145/2644827},
abstract = {Mobile Crowdsensing (MCS) requires users to be motivated to participate. However, concerns regarding energy consumption and privacy—among other things—may compromise their willingness to join such a crowd. Our preliminary observations and analysis of common MCS applications have shown that the data transfer in MCS applications may incur significant energy consumption due to the 3G connection setup. However, if data are transferred in parallel with a traditional phone call, then such transfer can be done almost “for free”: with only an insignificant additional amount of energy required to piggy-back the data—usually incoming task assignments and outgoing sensor results—on top of the call. Here, we present an &lt;i&gt;Energy-Efficient Mobile Crowdsensing&lt;/i&gt; (EEMC) framework where task assignments and sensing results are transferred in parallel with phone calls. The main objective, and the principal contribution of this article, is an MCS task assignment scheme that guarantees that a minimum number of anonymous participants return sensor results within a specified time frame, while also minimizing the waste of energy due to redundant task assignments and considering privacy concerns of participants. Evaluations with a large-scale real-world phone call dataset show that our proposed &lt;i&gt;EEMC&lt;/i&gt; framework outperforms the baseline approaches, and it can reduce overall energy consumption in data transfer by 54--66% when compared to the 3G-based solution.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {39},
numpages = {26},
keywords = {Mobile crowdsensing, anonymous participants, energy efficiency, task assignment decision making}
}

@inproceedings{10.1145/3328529.3328551,
author = {Adelhardt, Zinaida and Markus, Stefan and Eberle, Thomas},
title = {Concepts Clarification and Differentiation between Smartphone Addiction and Compulsive Internet Use Based on Diagnostic Investigation on Two Scales},
year = {2019},
isbn = {9781450366519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328529.3328551},
doi = {10.1145/3328529.3328551},
abstract = {Smartphone addiction, Internet addiction, online gaming addiction, communication addiction disorder and social network addiction are widely discussed topics nowadays. However, there is lack of distinction between different types of these technology-based addictions. The aim of this study is to investigate whether two commonly used instruments measure two different concepts as they intend to. Therefore, we made a diagnostic analysis of the Smartphone Addiction Scale (SAS-SV) for adolescents and the Compulsive Internet Use Scale (CIUS). We surveyed 96 adolescents aged between 14 and 15 years through online questionnaire. The results of the statistical analysis show that the two scales are highly correlated (r=.98, p≤.001). By using Confirmatory Factor Analysis (CFA) we stepwise reduced both instruments to their main "loading" cores to get insights in the phenomena they measure. The reduced scales stayed highly correlated. However, a combined model based on both reduced scales implies very good model fit and high reliability. We argue that both instruments measure almost the same phenomena and propose an alternative scale based on the reduction of the SAS-SV and CIUS scales -- the Compulsory Mobile Internet Use Scale (CMIU). Further validation of this scale is needed. The main limitation of the study is its relatively small "convenience" sample. Further limitations and implications are discussed.},
booktitle = {Proceedings of the 10th International Conference on Social Media and Society},
pages = {108–116},
numpages = {9},
keywords = {Compulsive Internet Use Scale, Compulsory Mobile Internet Use Scale, Internet addiction, Smartphone Addiction Scale, Smartphone addiction, comparison of smartphone and Internet addictions},
location = {Toronto, ON, Canada},
series = {SMSociety '19}
}

@article{10.1145/2897824.2925947,
author = {Wang, Congyi and Shi, Fuhao and Xia, Shihong and Chai, Jinxiang},
title = {Realtime 3D eye gaze animation using a single RGB camera},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925947},
doi = {10.1145/2897824.2925947},
abstract = {This paper presents the first realtime 3D eye gaze capture method that simultaneously captures the coordinated movement of 3D eye gaze, head poses and facial expression deformation using a single RGB camera. Our key idea is to complement a realtime 3D facial performance capture system with an efficient 3D eye gaze tracker. We start the process by automatically detecting important 2D facial features for each frame. The detected facial features are then used to reconstruct 3D head poses and large-scale facial deformation using multi-linear expression deformation models. Next, we introduce a novel user-independent classification method for extracting iris and pupil pixels in each frame. We formulate the 3D eye gaze tracker in the Maximum A Posterior (MAP) framework, which sequentially infers the most probable state of 3D eye gaze at each frame. The eye gaze tracker could fail when eye blinking occurs. We further introduce an efficient eye close detector to improve the robustness and accuracy of the eye gaze tracker. We have tested our system on both live video streams and the Internet videos, demonstrating its accuracy and robustness under a variety of uncontrolled lighting conditions and overcoming significant differences of races, genders, shapes, poses and expressions across individuals.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {118},
numpages = {14},
keywords = {3D eye gaze tracking, facial animation and control, facial performance capture}
}

@inproceedings{10.1145/3343031.3350538,
author = {Midoglu, Cise and Zabrovskiy, Anatoliy and Alay, Ozgu and Hoelbling-Inzko, Daniel and Griwodz, Carsten and Timmerer, Christian},
title = {Docker-Based Evaluation Framework for Video Streaming QoE in Broadband Networks},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350538},
doi = {10.1145/3343031.3350538},
abstract = {Video streaming is one of the top traffic contributors in the Internet and a frequent research subject. It is expected that streaming traffic will grow 4-fold for video globally and 9-fold for mobile video between 2017 and 2022. In this paper, we present an automatized measurement framework for evaluating video streaming QoE in operational broadband networks, using headless streaming with a Docker-based client, and a server-side implementation allowing for the use of multiple video players and adaptation algorithms. Our framework allows for integration with the acsMONROE testbed and Bitmovin Analytics, which bring on the possibility to conduct large-scale measurements in different networks, including mobility scenarios, and monitor different parameters in the application, transport, network, and physical layers in real-time.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {2288–2291},
numpages = {4},
keywords = {adaptive streaming, network measurements, ott video analytics, qoe},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3139958.3139986,
author = {Rashidian, Sina and Dong, Xinyu and Avadhani, Amogh and Poddar, Prachi and Wang, Fusheng},
title = {Effective Scalable and Integrative Geocoding for Massive Address Datasets},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3139986},
doi = {10.1145/3139958.3139986},
abstract = {With increased accessibility of large scale open data, public health studies are able to take advantage of integrative spatial big data to increase the spatial resolution to community or neighborhood level. One critical information for such studies is the large number of addresses of patients, which is private and highly sensitive. Geocoding such massive private addresses poses major challenges for public health researchers. Many geocoders provide only Web APIs which require sending private addresses over the Internet, which is not feasible. Commercial geocoders require high licensing fee and often have limitations on daily usage, which becomes a major hurdle for researchers. Scalability is another major challenge for large scale address dataset. In this paper, we present EaserGeocoder, a novel open source geocoder for effectively geocoding massive address datasets. EaserGeocoder takes an integrative approach by using multiple references based on open address data sources contributed by governments or communities. It takes a machine learning approach to automatically find the best answer from candidates produced by multiple references. The system provides high scalability through parallel processing. Our comparative studies demonstrate Easer-Geocoder outperforms open source geocoders and is comparable to commercial ones in terms of both accuracy and error. It provides a cost-effective and feasible solution for large scale public health studies.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {26},
numpages = {10},
keywords = {Geocoding, Geographic Information System, Text Searching},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@article{10.1145/2876480.2876483,
author = {Shen, Bilong and Huang, Yan and Zhao, Ying},
title = {Dynamic ridesharing},
year = {2016},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/2876480.2876483},
doi = {10.1145/2876480.2876483},
abstract = {Ridesharing, which offers empty seats in a car to other passengers, is an efficient way of transportation. In this way, the utilization of seats can be improved and the number of cars used can be reduced. Ridesharing has the potential to solve the problems of congestion, pollution, high travel cost, and energy. The development of internet, smart phone, GPS allows dynamic matchings of travel requests with available cars through real-time travel planning systems. However, matching requests and cars under certain constrains in large scale remains challenging. In this paper, we formally address the problem of dynamic ridesharing and introduce the solution framework of filter and refine, under which we summarize existing state-of-the-art works. Finally, we point out possible research directions and problems needed to be solved.},
journal = {SIGSPATIAL Special},
month = {jan},
pages = {3–10},
numpages = {8}
}

@inproceedings{10.1145/3178876.3186035,
author = {Almeida, Mario and Bilal, Muhammad and Finamore, Alessandro and Leontiadis, Ilias and Grunenberger, Yan and Varvello, Matteo and Blackburn, Jeremy},
title = {CHIMP: Crowdsourcing Human Inputs for Mobile Phones},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186035},
doi = {10.1145/3178876.3186035},
abstract = {While developing mobile apps is becoming easier, testing and characterizing their behavior is still hard. On the one hand, the de facto testing tool, called "Monkey," scales well due to being based on random inputs, but fails to gather inputs useful in understanding things like user engagement and attention. On the other hand, gathering inputs and data from real users requires distributing instrumented apps, or even phones with pre-installed apps, an expensive and inherently unscaleable task. To address these limitations we present CHIMP, a system that integrates automated tools and large-scale crowdsourced inputs. CHIMP is different from previous approaches in that it runs apps in a virtualized mobile environment that thousands of users all over the world can access via a standard Web browser. CHIMP is thus able to gather the full range of real-user inputs, detailed run-time traces of apps, and network traffic. We thus describe CHIMP»s design and demonstrate the efficiency of our approach by testing thousands of apps via thousands of crowdsourced users. We calibrate CHIMP with a large-scale campaign to understand how users approach app testing tasks. Finally, we show how CHIMP can be used to improve both traditional app testing tasks, as well as more novel tasks such as building a traffic classifier on encrypted network flows.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {45–54},
numpages = {10},
keywords = {crowdsourcing, mobile, testing, virtualization},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3131365.3131383,
author = {Jonker, Mattijs and King, Alistair and Krupp, Johannes and Rossow, Christian and Sperotto, Anna and Dainotti, Alberto},
title = {Millions of targets under attack: a macroscopic characterization of the DoS ecosystem},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131383},
doi = {10.1145/3131365.3131383},
abstract = {Denial-of-Service attacks have rapidly increased in terms of frequency and intensity, steadily becoming one of the biggest threats to Internet stability and reliability. However, a rigorous comprehensive characterization of this phenomenon, and of countermeasures to mitigate the associated risks, faces many infrastructure and analytic challenges. We make progress toward this goal, by introducing and applying a new framework to enable a macroscopic characterization of attacks, attack targets, and DDoS Protection Services (DPSs). Our analysis leverages data from four independent global Internet measurement infrastructures over the last two years: backscatter traffic to a large network telescope; logs from amplification honeypots; a DNS measurement platform covering 60% of the current namespace; and a DNS-based data set focusing on DPS adoption. Our results reveal the massive scale of the DoS problem, including an eye-opening statistic that one-third of all / 24 networks recently estimated to be active on the Internet have suffered at least one DoS attack over the last two years. We also discovered that often targets are simultaneously hit by different types of attacks. In our data, Web servers were the most prominent attack target; an average of 3% of the Web sites in .com, .net, and .org were involved with attacks, daily. Finally, we shed light on factors influencing migration to a DPS.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {100–113},
numpages = {14},
keywords = {DDoS, cloud-based mitigation, reflection attacks, spoofed attacks},
location = {London, United Kingdom},
series = {IMC '17}
}

@article{10.1145/3330336,
author = {Kakhki, Arash Molavi and Jero, Samuel and Choffnes, David and Nita-Rotaru, Cristina and Mislove, Alan},
title = {Taking a long look at QUIC: an approach for rigorous evaluation of rapidly evolving transport protocols},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3330336},
doi = {10.1145/3330336},
abstract = {Google's Quick UDP Internet Connections (QUIC) protocol, which implements TCP-like properties at the application layer atop a UDP transport, is now used by the vast majority of Chrome clients accessing Google properties but has no formal state machine specification, limited analysis, and ad-hoc evaluations based on snapshots of the protocol implementation in a small number of environ-merits. Further frustrating attempts to evaluate QUIC is the fact that the protocol is under rapid development, with extensive rewriting of the protocol occurring over the scale of months, making individual studies of the protocol obsolete before publication.Given this unique scenario, there is a need for alternative techniques for understanding and evaluating QUIC when compared with previous transport-layer protocols. First, we develop an approach that allows us to conduct analysis across multiple versions of QUIC to understand how code changes impact protocol effectiveness. Next, we instrument the source code to infer QUIC's state machine from execution traces. With this model, we run QUIC in a large number of environments that include desktop and mobile, wired and wireless environments and use the state machine to understand differences in transport-and application-layer performance across multiple versions of QUIC and in different environments. QUIC generally outperforms TCP, but we also identified performance issues related to window sizes, re-ordered packets, and multiplexing large number of small objects; further, we identify that QUIC's performance diminishes on mobile devices and over cellular networks.},
journal = {Commun. ACM},
month = {jun},
pages = {86–94},
numpages = {9}
}

@inproceedings{10.1145/2858036.2858180,
author = {Kim, Han-Jong and Kim, Ju-Whan and Nam, Tek-Jin},
title = {miniStudio: Designers' Tool for Prototyping Ubicomp Space with Interactive Miniature},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858180},
doi = {10.1145/2858036.2858180},
abstract = {Recently, it has become common for designers to deal with complex and large-scale ubicomp or IoT spaces. Designers without technical implementation skills have difficulties in prototyping such spaces, especially in the early phases of design. We present miniStudio, a designers' tool for prototyping ubicomp space with proxemic interactions. It is built on designers' existing software and modeling materials (Photoshop, Lego, and paper). Interactions can be defined in Photoshop based on five spatial relations: location, distance, motion, orientation, and custom. Projection-based augmented reality was applied to miniatures in order to enable tangible interactions and dynamic representations. Hidden marker stickers and a camera-projector system enable the unobtrusive integration of digital images on the physical miniature. Through the user study with 12 designers and researchers in the ubicomp field, we found that miniStudio supported rapid prototyping of large and complex ideas with multiple connected components. Based on the tool development and the study, we discuss the implications for prototyping ubicomp environments in the early phase of the design.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {213–224},
numpages = {12},
keywords = {augmented reality, interactive miniature, photoshop, prototyping, proxemic interaction, ubiquitous computing},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3283289.3283345,
author = {Kawai, Yasuo and Kobayashi, Natsumi and Enzaka, Ayaka},
title = {Historical streetscape simulation system that reflects changes in weather, time, and seasons},
year = {2018},
isbn = {9781450360630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3283289.3283345},
doi = {10.1145/3283289.3283345},
abstract = {In this study, we developed a historical streetscape simulation system for local areas. In recent years, the loss or replacement of regional history and culture has become a pertinent issue in Japan owing to urbanization, depopulation, declining birthrate, and aging population. Based on the cultural property law, measures are being taken to conserve and use cultural properties depending on characteristics such as tangible cultural property, intangible cultural property, folk cultural property, monument, cultural landscape, and traditional building group. However, with changes in social environments, the historical culture of an area that was previously cultivated and conveyed through the long history of local residents has now become difficult to inherit. In particular, things that have not been designated as cultural properties get buried or lost in society. To mitigate this problem, this study focuses on the historical cultural landscape of local areas and develops a landscape simulation system for communicating and inheriting the same in an easy-to-understand manner. Attempts have previously been made to reproduce historical landscapes by real-time rendering with 3D computer graphics (CG) and virtual reality technology [Fukuda et al. 2015; Boeykens. 2011]. However, owing mainly to hardware limitations for drawing, these systems focused on single buildings; they did not reproduce city-level dynamics. Meanwhile, studies were conducted to achieve urban-scale reproductions [Dylla et al. 2008; Jacobson. 2005]. Because such large-scale developments are costly, only famous places are typically selected as target areas. In a previous study, we have already developed a historical landscape simulation system for the streetscape of the late Edo period. This system was installed as a permanent exhibition at regional museums, where it has been running stably. In the present study, we aim to extend the scale of this system in response to user requests and feedback.},
booktitle = {SIGGRAPH Asia 2018 Posters},
articleno = {27},
numpages = {2},
keywords = {historical, season, streetscape, time, weather},
location = {Tokyo, Japan},
series = {SA '18}
}

@article{10.14778/3137765.3137781,
author = {Zhang, Mingming and Wo, Tianyu and Xie, Tao and Lin, Xuelian and Liu, Yaxiao},
title = {CarStream: an industrial system of big data processing for internet-of-vehicles},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137781},
doi = {10.14778/3137765.3137781},
abstract = {As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet vehicles and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes of driving data. This paper shares our experiences on designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1766–1777},
numpages = {12}
}

@inproceedings{10.1145/3278532.3278535,
author = {De Vaere, Piet and B\"{u}hler, Tobias and K\"{u}hlewind, Mirja and Trammell, Brian},
title = {Three Bits Suffice: Explicit Support for Passive Measurement of Internet Latency in QUIC and TCP},
year = {2018},
isbn = {9781450356190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278532.3278535},
doi = {10.1145/3278532.3278535},
abstract = {Passive measurement is a commonly used approach for measuring round trip time (RTT), as it reduces bandwidth overhead compared to large-scale active measurements. However, passive RTT measurement is limited to transport-specific approaches, such as those that utilize Transmission Control Protocol (TCP) timestamps. Furthermore, the continuing deployment of encrypted transport protocols such as QUIC hides the information used for passive RTT measurement from the network.In this work, we introduce the latency spin signal as a lightweight, transport-independent and explicit replacement for TCP timestamps for passive latency measurement. This signal supports per-flow, single-point and single direction passive measurement of end-to-end RTT using just three bits in the transport protocol header, leveraging the existing dynamics of the vast majority of Internet-deployed transports. We show how the signal applies to measurement of both TCP and to QUIC through implementation of the signal in endpoint transport stacks. We also provide a high-performance measurement implementation for the signal using the Vector Packet Processing (VPP) framework. Evaluation on emulated networks and in an Internet testbed demonstrate the viability of the signal, and show that it is resistant to even large amounts of loss or reordering on the measured path.},
booktitle = {Proceedings of the Internet Measurement Conference 2018},
pages = {22–28},
numpages = {7},
location = {Boston, MA, USA},
series = {IMC '18}
}

@inproceedings{10.1145/2810156.2810169,
author = {Su, Kai and Bronzino, Francesco and Ramakrishnan, K. K. and Raychaudhuri, Dipankar},
title = {MFTP: A Clean-Slate Transport Protocol for the Information Centric Mobilityfirst Network},
year = {2015},
isbn = {9781450338554},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810156.2810169},
doi = {10.1145/2810156.2810169},
abstract = {This paper presents the design and evaluation of clean-slate transport layer protocols for the MobilityFirst (MF) future Internet architecture based on the concept of named objects. The MF architecture is a specific realization of the emerging class of Information Centric Networks (ICN) that are designed to support new modes of communication based on names of information objects rather than their network addresses or locators. ICN architectures including MF are characterized by the following distinctive features: (a) use of names to identify sources and sinks of information; (b) storage of information at routers within the network in order to support content caching and disconnection; (c) multicasting and anycasting as integral network services; and in the MF case (d) hop-by-hop reliability protocols between routers in the network. These properties have significant implications for transport layer protocol design since the current Internet transports (TCP and UDP) were designed for the end-to-end Internet principle which uses address based routing with minimal functionality (i.e. no storage or reliability mechanisms) within the network. Several use cases including web access, large file transfer, Machine-to-machine and multicast services are considered, leading to an identification of four basic functions needed to constitute a flexible transport protocol for ICN: (i) fragmentation and end-to-end re-sequencing; (ii) lightweight end-to-end error recovery with in-network transport proxies; (iii) optional flow and congestion control mechanisms; and (iv) scalable multicast delivery mechanisms. The design of the MobilityFirst transport protocol (MFTP) framework realizing these features in a modular and flexible manner is presented and discussed. The proposed MFTP protocol is then experimentally evaluated and compared with TCP/IP for a few representative scenarios including mobile data delivery, web content retrieval and disconnected/late binding service. The results show that significant performance gains can be achieved in each case.},
booktitle = {Proceedings of the 2nd ACM Conference on Information-Centric Networking},
pages = {127–136},
numpages = {10},
keywords = {congestion control, end-to-end reliability, flow control, future internet architecture, hop-by-hop transport, in-network storage, information centric networks, transport protocol},
location = {San Francisco, California, USA},
series = {ACM-ICN '15}
}

@article{10.1145/2983637,
author = {Miao, Wang and Min, Geyong and Wu, Yulei and Wang, Haozhe and Hu, Jia},
title = {Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2983637},
doi = {10.1145/2983637},
abstract = {Software-Defined Networking (SDN) is an emerging architecture for the next-generation Internet, providing unprecedented network programmability to handle the explosive growth of big data driven by the popularisation of smart mobile devices and the pervasiveness of content-rich multimedia applications. In order to quantitatively investigate the performance characteristics of SDN networks, several research efforts from both simulation experiments and analytical modelling have been reported in the current literature. Among those studies, analytical modelling has demonstrated its superiority in terms of cost-effectiveness in the evaluation of large-scale networks. However, for analytical tractability and simplification, existing analytical models are derived based on the unrealistic assumptions that the network traffic follows the Poisson process, which is suitable to model nonbursty text data, and the data plane of SDN is modelled by one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies have shown that, due to the features of heavy volume and high velocity, the multimedia big data generated by real-world multimedia applications reveals the bursty and correlated nature in the network transmission. With the aim of capturing such features of realistic traffic patterns and obtaining a comprehensive and deeper understanding of the performance behaviour of SDN networks, this article presents a new analytical model to investigate the performance of SDN in the presence of the bursty and correlated arrivals modelled by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance metrics in terms of the average latency and average network throughput of the SDN networks are derived based on the developed analytical model. To consider a realistic multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted to model the SDN data plane. To address the challenging problem of obtaining the key performance metrics, for example, queue-length distribution of a PQ system with a given service capacity, a versatile methodology extending the Empty Buffer Approximation (EBA) method is proposed to facilitate the decomposition of such a PQ system to two SSSQ systems. The validity of the proposed model is demonstrated through extensive simulation experiments. To illustrate its application, the developed model is then utilised to study the strategy of the network configuration and resource allocation in SDN networks.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {sep},
articleno = {77},
numpages = {19},
keywords = {Software-defined networking, multimedia big data, performance modelling and analysis, queueing decomposition, resource allocation}
}

@article{10.1145/2831347.2831351,
author = {Sarlis, Dimitrios and Papailiou, Nikolaos and Konstantinou, Ioannis and Smaragdakis, Georgios and Koziris, Nectarios},
title = {Datix: A System for Scalable Network Analytics},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/2831347.2831351},
doi = {10.1145/2831347.2831351},
abstract = {The ever-increasing Internet traffic poses challenges to network operators and administrators that have to analyze large network datasets in a timely manner to make decisions regarding network routing, dimensioning, accountability and security. Network datasets collected at large networks such as Internet Service Providers (ISPs) or Internet Exchange Points (IXPs) can be in the order of Terabytes per hour. Unfortunately, most of the current network analysis approaches are ad-hoc and centralized, and thus not scalable.In this paper, we present Datix, a fully decentralized, open-source analytics system for network traffic data that relies on smart partitioning storage schemes to support fast join algorithms and efficient execution of filtering queries. We outline the architecture and design of Datix and we present the evaluation of Datix using real traces from an operational IXP. Datix is a system that deals with an important problem in the intersection of data management and network monitoring while utilizing state-of-the-art distributed processing engines. In brief, Datix manages to efficiently answer queries within minutes compared to more than 24 hours processing when executing existing Python based code in single node setups. Datix also achieves nearly 70% speedup compared to baseline query implementations of popular big data analytics engines such as Hive and Shark.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {sep},
pages = {21–28},
numpages = {8},
keywords = {hadoop, hbase, k-d tree, map-join, mapreduce, sflow}
}

@article{10.1145/3358211,
author = {Mohanty, Ram Prasad and Gamaarachchi, Hasindu and Lambert, Andrew and Parameswaran, Sri},
title = {SWARAM: Portable Energy and Cost Efficient Embedded System for Genomic Processing},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358211},
doi = {10.1145/3358211},
abstract = {Treatment of patients using high-quality precision medicine requires a thorough understanding of the genetic composition of a patient. Ideally, the identification of unique variations in an individual’s genome is needed for specifying the necessary treatment. Variant calling workflow is a pipeline of tools, integrating state of the art software systems aimed at alignment, sorting and variant calling for the whole genome sequencing (WGS) data. This pipeline is utilized for identifying unique variations in an individual’s genome (compared to a reference genome). Currently, such a workflow is implemented on high-performance computers (with additional GPUs or FPGAs) or in cloud computers. Such systems are large, have a high cost, and rely on the internet for genome data transfer which makes the system unusable in remote locations unequipped with internet connectivity. It further raises privacy concerns due to processing being carried out in a different facility.To overcome such limitations, in this paper, for the first time, we present a cost-efficient, offline, scalable, portable, and energy-efficient computing system named SWARAM for variant calling workflow processing. The system uses novel architecture and algorithms to match against partial reference genomes to exploit smaller memory sizes which are typically available in tiny processing systems. Extensive tests on a standard benchmark data-set (NA12878 Illumina platinum genome) confirm that the time consumed for the data transfer and completing variant calling workflow on SWARAM was competitive to that of a 32-core Intel Xeon server with similar accuracy, but costs less than a fifth, and consumes less than 40% of the energy of the server system. The original scripts and code we developed for executing the variant calling workflow on SWARAM are available in the associated Github repository https://github.com/Rammohanty/swaram.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {61},
numpages = {24},
keywords = {ARM, DNA analysis, Genome, alignment, embedded system, energy efficient system, genetic analysis, portable genome analysis, variant calling}
}

@inproceedings{10.1145/3281411.3281438,
author = {Kalra, Sukrit and Sanghi, Rishabh and Dhawan, Mohan},
title = {Blockchain-based real-time cheat prevention and robustness for multi-player online games},
year = {2018},
isbn = {9781450360807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281411.3281438},
doi = {10.1145/3281411.3281438},
abstract = {The gaming industry is affected by two key issues---cheating and DDoS attacks against game servers. In this paper, we aim to present a novel yet concrete application of the blockchain technology to address the seemingly disparate problems. Our approach uses blockchain to manage definitive game state and exploits peer consensus on every player action to track modifications to tangible player assets. While a key impediment to adopting blockchain for real-time systems is its high per-operation latency, our approach leverages several optimizations to enable real-time prevention of a large class of cheats where the reported client state is inconsistent with the observed state at the server. Further, blockchain-based games leverage the robust peer-to-peer architecture to successfully defend against DDoS attacks.Our strategy enables flexibility to customize games with minimum modifications to game clients by porting server-side logic to smart contracts that execute atop peers. We evaluate our approach on a recent port of the multi-player game Doom. Our prototype can scale to client tickrates matched by modern games, and prevent cheats in &lt;150ms for 32 peers deployed across the Internet, which is well within the latency requirements for online gaming.},
booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
pages = {178–190},
numpages = {13},
keywords = {blockchain, cheat prevention, client-server, distributed denial-of-service, multiplayer online games, peer-to-peer},
location = {Heraklion, Greece},
series = {CoNEXT '18}
}

@inproceedings{10.1145/3267955.3269015,
author = {Grewe, Dennis and Marxer, Claudio and Scherb, Christopher and Wagner, Marco and Tschudin, Christian},
title = {A network stack for computation-centric vehicular networking},
year = {2018},
isbn = {9781450359597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267955.3269015},
doi = {10.1145/3267955.3269015},
abstract = {Recently, vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) connectivity transitioned from a vision of the future to reality. Applications in such environments vary from local propagation of road conditions to large-scale traffic flow control systems. In this demo, we present a network stack for the data exchange in the automotive IoT, based on the Named Function Networking (NFN) principles. In NFN, the communication model is not restricted to propagation of static data but natively supports computation-offloading to other nodes. We present solutions and report on experiments with real cars on a test course.},
booktitle = {Proceedings of the 5th ACM Conference on Information-Centric Networking},
pages = {208–209},
numpages = {2},
keywords = {connected vehicles, information-centric networking, mobile edge computing, named function networking},
location = {Boston, Massachusetts},
series = {ICN '18}
}

@inproceedings{10.1145/3313150.3313220,
author = {Nolan, Michael and McGrath, Michael J. and Spoczynski, Marcin and Healy, D\'{a}ire},
title = {Adaptive industrial IOT/CPS messaging strategies for improved edge compute utility},
year = {2019},
isbn = {9781450366984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313150.3313220},
doi = {10.1145/3313150.3313220},
abstract = {Messaging strategies for data transport play a central role in large scale IIoT/CPS systems. Research to date in these domains has focused on comparative studies of different messaging protocols and various congestion control techniques. This paper presents an experimental evaluation of the latency and throughput for different message payload strategies using the same protocol combined with a novel congestion control algorithm. The results provide key insights based on experiments carried out in a real-world IOT wireless sensor network deployment. The approach presented dynamically varies the message schema and sizes in response to the utilization of edge nodes. The goal is to achieve the highest possible data throughput that fits within the available edge based compute capacity without requiring auto-scaling of back end cloud resources.},
booktitle = {Proceedings of the Workshop on Fog Computing and the IoT},
pages = {16–20},
numpages = {5},
keywords = {back propagation, cyber physical systems (CPS), industrial internet of things (IIOT), message schema},
location = {Montreal, Quebec, Canada},
series = {IoT-Fog '19}
}

@inproceedings{10.1145/2775292.2775312,
author = {Scully, Timothy and Dobo\v{s}, Jozef and Sturm, Timo and Jung, Yvonne},
title = {3drepo.io: building the next generation Web3D repository with AngularJS and X3DOM},
year = {2015},
isbn = {9781450336475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2775292.2775312},
doi = {10.1145/2775292.2775312},
abstract = {This paper presents a novel open source web-based 3D version control system positioned directly within the context of the recent strategic plan for digitising the construction sector in the United Kingdom. The aim is to achieve reduction of cost and carbon emissions in the built environment by up to 20% simply by properly managing digital information and 3D models. Even though previous works in the field concentrated mainly on defining novel WebGL frameworks and later on the efficiency of 3D data delivery over the Internet, there is still the emerging need for a practical solution that would provide ubiquitous access to 3D assets, whether it is for large international enterprises or individual members of the general public. We have, therefore, developed a novel platform leveraging the latest open web-based technologies such as AngularJS and X3DOM in order to define an industrial-strength collaborative cloud hosting service 3drepo.io. Firstly, we introduce the work and outline the high-level system architecture as well as improvements in relation to previous work. Next, we describe database and front-end considerations with emphasis on scalability and enhanced security. Finally, we present several performance measurement experiments and a selection of real-life industrial use cases. We conclude that jQuery provides performance benefits over AngularJS when manipulating large scene graphs in web browsers.},
booktitle = {Proceedings of the 20th International Conference on 3D Web Technology},
pages = {235–243},
numpages = {9},
keywords = {3D repo, AngularJS, BIM, X3DOM, version control},
location = {Heraklion, Crete, Greece},
series = {Web3D '15}
}

@inproceedings{10.1145/3097983.3098193,
author = {Ghosh, Shalini and Das, Ariyam and Porras, Phil and Yegneswaran, Vinod and Gehani, Ashish},
title = {Automated Categorization of Onion Sites for Analyzing the Darkweb Ecosystem},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098193},
doi = {10.1145/3097983.3098193},
abstract = {Onion sites on the darkweb operate using the Tor Hidden Service (HS) protocol to shield their locations on the Internet, which (among other features) enables these sites to host malicious and illegal content while being resistant to legal action and seizure. Identifying and monitoring such illicit sites in the darkweb is of high relevance to the Computer Security and Law Enforcement communities. We have developed an automated infrastructure that crawls and indexes content from onion sites into a large-scale data repository, called LIGHTS, with over 100M pages. In this paper we describe Automated Tool for Onion Labeling (ATOL), a novel scalable analysis service developed to conduct a thematic assessment of the content of onion sites in the LIGHTS repository. ATOL has three core components -- (a) a novel keyword discovery mechanism (ATOLKeyword) which extends analyst-provided keywords for different categories by suggesting new descriptive and discriminative keywords that are relevant for the categories; (b) a classification framework (ATOLClassify) that uses the discovered keywords to map onion site content to a set of categories when sufficient labeled data is available; (c) a clustering framework (ATOLCluster) that can leverage information from multiple external heterogeneous knowledge sources, ranging from domain expertise to Bitcoin transaction data, to categorize onion content in the absence of sufficient supervised data. The paper presents empirical results of ATOL on onion datasets derived from the LIGHTS repository, and additionally benchmarks ATOL's algorithms on the publicly available 20 Newsgroups dataset to demonstrate the reproducibility of its results. On the LIGHTS dataset, ATOLClassify gives a 12% performance gain over an analyst-provided baseline, while ATOLCluster gives a 7% improvement over state-of-the-art semi-supervised clustering algorithms. We also discuss how ATOL has been deployed and externally evaluated, as part of the LIGHTS system.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1793–1802},
numpages = {10},
keywords = {classification, clustering, darkweb, keyword discovery, onion sites, semi-supervised learning},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.5555/2888619.2889045,
author = {Gra\v{c}anin, Denis and Matkovi\'{c}, Kre\v{s}imir and Wheeler, Joseph},
title = {An approach to modeling internet of things based smart built environments},
year = {2015},
isbn = {9781467397414},
publisher = {IEEE Press},
abstract = {Smart built environments enhanced with technology can improve the lives of individuals, groups, and the broader community. We describe an approach to modeling IOT-based smart built environments that uses a large-scale virtual environment where a building model is aligned with the physical space. This approach takes advantages of affordances and embodied cognition in a large physical space to model user interaction with built spaces. The built space contains 'smart objects' with embedded sensors/actuators/controllers (e.g., kitchen appliances). A 'smart object' has the corresponding virtual object in the virtual environment. A case study (FutureHAUS) demonstrates the proposed approach.},
booktitle = {Proceedings of the 2015 Winter Simulation Conference},
pages = {3208–3209},
numpages = {2},
location = {Huntington Beach, California},
series = {WSC '15}
}

@inproceedings{10.1145/3323679.3326527,
author = {Li, Zhijing and Xiao, Zhujun and Wang, Bolun and Zhao, Ben Y. and Zheng, Haitao},
title = {Scaling Deep Learning Models for Spectrum Anomaly Detection},
year = {2019},
isbn = {9781450367646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323679.3326527},
doi = {10.1145/3323679.3326527},
abstract = {Spectrum management in cellular networks is a challenging task that will only increase in difficulty as complexity grows in hardware, configurations, and new access technology (e.g. LTE for IoT devices). Wireless providers need robust and flexible tools to monitor and detect faults and misbehavior in physical spectrum usage, and to deploy them at scale. In this paper, we explore the design of such a system by building deep neural network (DNN) models1 to capture spectrum usage patterns and use them as baselines to detect spectrum usage anomalies resulting from faults and misuse. Using detailed LTE spectrum measurements, we show that the key challenge facing this design is model scalability, i.e. how to train and deploy DNN models at a large number of static and mobile observers located throughout the network. We address this challenge by building context-agnostic models for spectrum usage and applying transfer learning to minimize training time and dataset constraints. The end result is a practical DNN model that can be easily deployed on both mobile and static observers, enabling timely detection of spectrum anomalies across LTE networks.},
booktitle = {Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {291–300},
numpages = {10},
location = {Catania, Italy},
series = {Mobihoc '19}
}

@inproceedings{10.5555/3324320.3324340,
author = {Zhang, Shiyuan and Xi, Wei and Xu, Qigui and Zhao, Kun and Cai, Yuanhang},
title = {Accurate CSI Estimation to Eliminate Unnecessary Transmission for MU-MIMO Networks},
year = {2019},
isbn = {9780994988638},
publisher = {Junction Publishing},
address = {USA},
abstract = {In large scale networks with many concurrently active IoT devices, Multi-user MIMO (MU-MIMO) is an important technology to improve data transmission efficiency, due to its ability of enabling multiple clients’ concurrent transmissions. To achieve concurrent diversity gains, the network resource allocation relies on the feedback of Channel State Information (CSI) from clients. Inaccurate CSI estimation and unnecessary CSI feedback, however, heavily degrade the capacity and throughput of a MU-MIMO network, leading to superfluous energy consumption and potential channel collision of battery constrained wireless systems (EWS) in IoT. Pursuing smart CSI feedback, we present QUICK, a protocol to achieve CSI quality estimation and power reallocation based on self-adaptive beamforming before data transmission, which could improve the CSI quality and reduce bit error rate (BER) efficiently. Based on the accurate CSI estimation, QUICK introduces a mobility-aware mechanism to eliminate unnecessary CSI feedback within coherence time. QUICK is fully compatible with the current IEEE 802.11ac standard and most state-of-the-art CSI feedback strategies, and is easy to be deployed on existing WiFi systems. Our software-radio based implementation and testbed experimentation demonstrate that QUICK substantially improves the throughput of both MU-MIMO downlink and uplink by 100% to up to 5\texttimes{}.},
booktitle = {Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks},
pages = {166–177},
numpages = {12},
location = {Beijing, China},
series = {EWSN '19}
}

@inproceedings{10.1145/2742854.2742889,
author = {Wang, Jing and Zhu, Xiaoyan and Liu, Yanjun and Zhang, Jiaqi and Wu, Minhua and Zhang, Weigong and Qiu, Keni},
title = {Heterogeneous energy-efficient cache design in warehouse scale computers},
year = {2015},
isbn = {9781450333580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742854.2742889},
doi = {10.1145/2742854.2742889},
abstract = {Energy efficiency is becoming the key design concern for modern warehouse-scale computer (WSC) systems, where tens of thousands of server processors consume a significant portion of the total power. Voltage scaling is one of the most effective mechanisms to improve energy efficiency at the cost of cell failures in large cache arrays. In this paper, we leverage the observation that there exists a diverse spectrum of tolerance to cache errors in large internet services to design a heterogeneous energy-efficient cache enforced by variable-strength error-correcting codes. The operating system may use the page coloring mechanism to control mapping applications to cache regions with differential reliability.},
booktitle = {Proceedings of the 12th ACM International Conference on Computing Frontiers},
articleno = {35},
numpages = {2},
keywords = {cache design, datacenter, energy efficiency, error-correcting codes, page coloring},
location = {Ischia, Italy},
series = {CF '15}
}

@inproceedings{10.1145/2744680.2744687,
author = {Fang, Xiu Susie},
title = {Generating Actionable Knowledge from Big Data},
year = {2015},
isbn = {9781450335294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2744680.2744687},
doi = {10.1145/2744680.2744687},
abstract = {The last few years have seen a rapid increase of sheer amount of data produced and communicated over the Internet and the Web. While it is widely believed that the availability of such ``Big Data'' holds the potential to revolutionize many aspects of our modern society (e.g., intelligent transportation, environmental monitoring, and energy saving), many challenges need to be addressed before this potential can be realized. This PhD project focuses on one critical challenge, namely extracting actionable knowledge from Big Data. Tremendous efforts have been contributed on mining large-scale data on the Web and constructing comprehensive knowledge bases (KBs). However, existing knowledge extraction systems retrieve data from limited types of Web sources. In addition, data fusion approaches consider very little of the noises produced by those knowledge extraction systems. Consequently, the constructed KBs are far from being comprehensive and accurate. In this paper, we present our initial design of a framework for extracting machine-readable data with high precision and recall from four types of data sources, namely Web texts, Document Object Model (DOM) trees, existing KBs, and query stream. Confidence scores are attached to the resulting knowledge, which can be used to further improve the knowledge fusion results.},
booktitle = {Proceedings of the 2015 ACM SIGMOD on PhD Symposium},
pages = {3–8},
numpages = {6},
keywords = {dom tree, knowledge base, knowledge fusion},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15 PhD Symposium}
}

@article{10.1109/TNET.2017.2748902,
author = {Zhang, Mingwei and Li, Jun and Brooks, Scott},
title = {I-Seismograph: Observing, Measuring, and Analyzing Internet Earthquakes},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2748902},
doi = {10.1109/TNET.2017.2748902},
abstract = {Disruptive events, such as large-scale power outages, undersea cable cuts, or security attacks, could have an impact on the Internet and cause the Internet to deviate from its normal state of operation, which we also refer to as an “Internet earthquake.” As the Internet is a large, complex moving target, unfortunately little research has been done to define, observe, quantify, and analyze such impact on the Internet, whether it is during a past event period or in real time. In this paper, we devise an Internet seismograph, or I-seismograph, to fill this gap. Since routing is the most basic function of the Internet and the Border Gateway Protocol BGP is the de facto standard inter-domain routing protocol, we focus on BGP to observe, measure, and analyze the Internet earthquakes. After defining what an impact to BGP entails, we describe how I-seismograph observes and measures the impact, exemplify its usage during both old and recent disruptive events, and further validate its accuracy and convergency. Finally, we show that I-seismograph can further be used to help analyze what happened to BGP while BGP experienced an impact, including which autonomous systems AS were affected most or which AS paths or path segments surged significantly in BGP updates during an Internet earthquake.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {3411–3426},
numpages = {16}
}

@inproceedings{10.1145/2851613.2851961,
author = {Garc\'{\i}a-Valls, Marisol and Calva-Urrego, Cristian and Alonso, Alejandro and de la Puente, Juan A.},
title = {Adjusting middleware knobs to suit CPS domains},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851961},
doi = {10.1145/2851613.2851961},
abstract = {The large scale and dynamic behavior of cyber-physical systems (CPS) require great effort at the middleware level. Middleware lies at the architectural point where low-level communication and execution meet the horizontal control and orchestration accross nodes and systems. Most realtime research on distributed CPS concentrates on off-line schedulability algorithms for systems with static structure and make heavy assumptions on the network predictability, neglecting the role of the middleware layer. In this paper, we explore the usage of middleware technology to support dynamic and timely execution in CPS, with a middleware design that considers the time requirements of the distributed processes or units. We have carried out a specific implementation that validates or approach, showing a stable service time for large numbers of clients, over an actual network that uses Internet protocols for transport and routing.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {2027–2030},
numpages = {4},
keywords = {CPS, distribution, middleware, timing requirements},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3304109.3325813,
author = {Raca, Darijo and Sani, Yusuf and Sreenan, Cormac J. and Quinlan, Jason J.},
title = {DASHbed: a testbed framework for large scale empirical evaluation of real-time DASH in wireless scenarios},
year = {2019},
isbn = {9781450362979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3304109.3325813},
doi = {10.1145/3304109.3325813},
abstract = {Recent years have witnessed an explosion of multimedia traffic carried over the Internet. Video-on-demand and live streaming services are the most dominant services. To ensure growth, many streaming providers have invested considerable time and effort to keep pace with ever-increasing users' demand for better quality and stall abolition. HTTP adaptive streaming (HAS) algorithms are at the core of every major streaming provider service. Recent years have seen sustained development in HAS algorithms. Currently, to evaluate their proposed solutions, researchers need to create a framework and numerous state-of-the-art algorithms. Often, these frameworks lack flexibility and scalability, covering only a limited set of scenarios. To fill this gap, in this paper we propose DASHbed, a highly customizable real-time framework for testing HAS algorithms in a wireless environment. Due to its low memory requirement, DASHbed offers a means of running large-scale experiments with a hundred competing players. Finally, we supplement the proposed framework with a dataset consisting of results for five HAS algorithms tested in various evaluated scenarios. The dataset showcases the abilities of DASHbed and presents the adaptation metrics per segment in the generated content (such as switches, buffer-level, P. 1203.1 values, delivery rate, stall duration, etc.), which can be used as a baseline when researchers compare the output of their proposed algorithm against the state-of-the-art algorithms.},
booktitle = {Proceedings of the 10th ACM Multimedia Systems Conference},
pages = {285–290},
numpages = {6},
keywords = {DASH, HAS, HTTP adaptive streaming, dynamic adaptive streaming over HTTP, real-time streaming, testbed framework},
location = {Amherst, Massachusetts},
series = {MMSys '19}
}

@inproceedings{10.1145/3131365.3131373,
author = {Chung, Taejoong and van Rijswijk-Deij, Roland and Choffnes, David and Levin, Dave and Maggs, Bruce M. and Mislove, Alan and Wilson, Christo},
title = {Understanding the role of registrars in DNSSEC deployment},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131373},
doi = {10.1145/3131365.3131373},
abstract = {The Domain Name System (DNS) provides a scalable, flexible name resolution service. Unfortunately, its unauthenticated architecture has become the basis for many security attacks. To address this, DNS Security Extensions (DNSSEC) were introduced in 1997. DNSSEC's deployment requires support from the top-level domain (TLD) registries and registrars, as well as participation by the organization that serves as the DNS operator. Unfortunately, DNSSEC has seen poor deployment thus far: despite being proposed nearly two decades ago, only 1% of .com, .net, and .org domains are properly signed.In this paper, we investigate the underlying reasons why DNSSEC adoption has been remarkably slow. We focus on registrars, as most TLD registries already support DNSSEC and registrars often serve as DNS operators for their customers. Our study uses large-scale, longitudinal DNS measurements to study DNSSEC adoption, coupled with experiences collected by trying to deploy DNSSEC on domains we purchased from leading domain name registrars and resellers. Overall, we find that a select few registrars are responsible for the (small) DNSSEC deployment today, and that many leading registrars do not support DNSSEC at all, or require customers to take cumbersome steps to deploy DNSSEC. Further frustrating deployment, many of the mechanisms for conveying DNSSEC information to registrars are error-prone or present security vulnerabilities. Finally, we find that using DNSSEC with third-party DNS operators such as Cloudflare requires the domain owner to take a number of steps that 40% of domain owners do not complete. Having identified several operational challenges for full DNSSEC deployment, we make recommendations to improve adoption.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {369–383},
numpages = {15},
keywords = {DNS, DNS operator, DNS security extension, DNSSEC, PKI, public key infrastructure, registrar},
location = {London, United Kingdom},
series = {IMC '17}
}

@inproceedings{10.1145/2940716.2940757,
author = {Chawla, Shuchi and Hartline, Jason and Nekipelov, Denis},
title = {A/B Testing of Auctions},
year = {2016},
isbn = {9781450339360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940716.2940757},
doi = {10.1145/2940716.2940757},
abstract = {A common method in the practice of large scale auction design, e.g., in auctions placing advertisements on online media and Internet search engines, is A/B testing. In A/B testing, the auction house is running an incumbent mechanism A, and would like to determine if a novel mechanism B obtains higher revenue. This is done by splitting the traffic so that most of it goes to A and some of it, e.g., five to ten percent, goes to B. An issue with this approach is that if the bidders are unaware of which mechanism their bid will be considered in, the bid equilibrium is neither for A nor B but for a mechanism C that is a convex combination of A and B.A miscalculation sometimes performed in practice is to consider and compare average revenue from A (resp. B) from the times when A (resp. B) is run. This miscalculation is equivalent to simulating A on the bids in C and can often give the opposite conclusion; e.g., if A and B are one- and two-unit highest-bids-win winner-pays-bid auctions, respectively, then B will always appear in this miscalculation to have higher revenue. For a fixed set of bids, a winner-pays-bid mechanism's revenue is monotone in its allocation probabilities. Of course, in equilibrium, increased allocation probabilities can cause reduced revenue as bidders may lower their bids.We present an A/B testing method that applies generally to the position auction model popularized by the Varian [2007] and Edelman et al. [2007] analyses of auctions for sponsored search and now a fundamental model for the study of auction theory; e.g., see Hartline [2013]. A position auction is defined by a decreasing sequence of weights, bidders are assigned to positions in decreasing order of bids, and payments are charged. Typical payment rules are "generalized first price" and "generalized second price"; the former requires bidders to pay their weighted bid, whereas the latter requires bidders to pay the weighted bid of the next highest bidder.},
booktitle = {Proceedings of the 2016 ACM Conference on Economics and Computation},
pages = {19–20},
numpages = {2},
keywords = {auction design, inference, position auctions},
location = {Maastricht, The Netherlands},
series = {EC '16}
}

@article{10.1109/TNET.2014.2362541,
author = {Wang, Feng and Liu, Jiangchuan and Chen, Minghua and Wang, Haiyang},
title = {Migration towards cloud-assisted live media streaming},
year = {2016},
issue_date = {February 2016},
publisher = {IEEE Press},
volume = {24},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2362541},
doi = {10.1109/TNET.2014.2362541},
abstract = {Live media streaming has become one of the most popular applications over the Internet. We have witnessed the successful deployment of commercial systems with content delivery network (CDN)- or peer-to-peer-based engines. While each being effective in certain aspects, having an all-round scalable, reliable, responsive, and cost-effective solution remains an illusive goal. Moreover, today's live streaming services have become highly globalized, with subscribers from all over the world. Such a globalization makes user behaviors and demands even more diverse and dynamic, further challenging state-of-the-art system designs. The emergence of cloud computing, however, sheds new light into this dilemma. Leveraging the elastic resource provisioning from the cloud, we present Cloud-Assisted Live Media Streaming (CALMS), a generic framework that facilitates a migration to the cloud. CALMS adaptively leases and adjusts cloud server resources in a fine granularity to accommodate temporal and spatial dynamics of demands from live streaming users. We present optimal solutions to deal with cloud servers with diverse capacities and lease prices, as well as the potential latencies in initiating and terminating leases in real-world cloud platforms. Our solution well accommodates location heterogeneity, mitigating the impact from user globalization. It also enables seamless migration for existing streaming systems, e.g., peer-to-peer, and fully explores their potentials. Simulations with data traces from both cloud service providers (Amazon EC2 and SpotCloud) and a live streaming service provider (PPTV) demonstrate that CALMS effectively mitigates the overall system deployment costs and yet provides users with satisfactory streaming latency and rate.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {272–282},
numpages = {11},
keywords = {cloud computing, live media streaming, migration, user/demand globalization}
}

@article{10.1145/2870649,
author = {Han, Shuguang and Dai, Peng and Paritosh, Praveen and Huynh, David},
title = {Crowdsourcing Human Annotation on Web Page Structure: Infrastructure Design and Behavior-Based Quality Control},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2870649},
doi = {10.1145/2870649},
abstract = {Parsing the semantic structure of a web page is a key component of web information extraction. Successful extraction algorithms usually require large-scale training and evaluation datasets, which are difficult to acquire. Recently, crowdsourcing has proven to be an effective method of collecting large-scale training data in domains that do not require much domain knowledge. For more complex domains, researchers have proposed sophisticated quality control mechanisms to replicate tasks in parallel or sequential ways and then aggregate responses from multiple workers. Conventional annotation integration methods often put more trust in the workers with high historical performance; thus, they are called performance-based methods. Recently, Rzeszotarski and Kittur have demonstrated that behavioral features are also highly correlated with annotation quality in several crowdsourcing applications. In this article, we present a new crowdsourcing system, called Wernicke, to provide annotations for web information extraction. Wernicke collects a wide set of behavioral features and, based on these features, predicts annotation quality for a challenging task domain: annotating web page structure. We evaluate the effectiveness of quality control using behavioral features through a case study where 32 workers annotate 200 Q&amp;A web pages from five popular websites. In doing so, we discover several things: (1) Many behavioral features are significant predictors for crowdsourcing quality. (2) The behavioral-feature-based method outperforms performance-based methods in recall prediction, while performing equally with precision prediction. In addition, using behavioral features is less vulnerable to the cold-start problem, and the corresponding prediction model is more generalizable for predicting recall than precision for cross-website quality analysis. (3) One can effectively combine workers’ behavioral information and historical performance information to further reduce prediction errors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {56},
numpages = {25},
keywords = {Crowdsourcing, behavioral features, quality control, worker performance}
}

@inproceedings{10.5555/2772879.2773546,
author = {Segal, Avi},
title = {Adaptation and Incentive Design in Large Scale Task Systems},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {My thesis focuses on understanding and influencing the behavior and performance of users in large scale task systems. In task systems, multiple participants work independently to solve problems that are presented to them by a computer. From e-learning systems like Khan Academy and Coursera, to citizen science systems like Zooniverse and BugGuide, large scale task oriented systems are booming on the Internet and are growing in number and in size. These systems provide a gold mine of data, containing information about the ways human beings engage with tasks of various natures.In my work I tackle three challenges which are central to the design of successful task systems: (1) understanding and modeling users' behavior in large scale task systems (2) designing incentive mechanisms for influencing users' behavior and improving their performance (3) adapting interaction in task systems to the needs and strengths of individual users. My research activities combine computational models, algorithms and empirical methodologies to meet the challenges above. They are conducted in the context of two different types of large scale task domains consisting of e-learning systems and citizen science systems. I will evaluate my approach in real world environments. My preliminary results demonstrate my proposed approach by significantly outperforming the state-of-the-art methods in personalization of tasks to students in two separate e-learning datasets.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2001–2002},
numpages = {2},
keywords = {adaptation, citizen science, e-learning, incentive design, task systems},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/3210284.3210287,
author = {Chen, Chen and Tock, Yoav and Girdzijauskas, Sarunas},
title = {BeaConvey: Co-Design of Overlay and Routing for Topic-based Publish/Subscribe on Small-World Networks},
year = {2018},
isbn = {9781450357821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210284.3210287},
doi = {10.1145/3210284.3210287},
abstract = {Distributed pub/sub must make principal design choices with regards to overlay topologies and routing protocols. It is challenging to tackle both aspects together, and most existing work merely considers one. We argue the necessity to address both problems simultaneously since only the right combination of the two can deliver an efficient internet-scale pub/sub. Traditional design space spans from structured data-oblivious overlays employing greedy routing strategies all the way to unstructured data-driven overlays using naive broadcast-based routing. The two ends of the spectra come with unacceptable prices: the former often exerts considerable overhead on each node for forwarding irrelevant messages, while the latter is difficult to scale due to prohibitive latencies stemming from unbounded node degrees and network diameters.To achieve the best of both worlds, we propose BeaConvey, a distributed pub/sub system for federated environments. First, we define the small-world and interest-close overlay (SWICO) that embraces both small-world properties and pub/sub semantics. To construct a SWCIO, we devise a greedy heuristic to assign small-world identifiers and fingers in a centralized manner. Second, we develop a family of peer-to-peer pub/sub routing protocols that leverages such SWICOs.Empirical evaluation shows that BeaConvey achieves substantial improvement in routing overhead and propagation delays. For instance, the routing overhead of BeaConvey is only 20% to 40% of the state of the art. This acceleration is consistent across a variety of pub/sub workloads, and BeaConvey obtains such adaptability by optimizing both overlay and routing, which complement each other in different situations. Under one Facebook workload with a skewed distribution, 78% of the improvement is accredited to a better overlay. Under another non-skewed workload, more advanced routing contributes 95% of cost reduction.},
booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
pages = {64–75},
numpages = {12},
keywords = {overlay, pub/sub, routing, small-world, topic-connected overlay},
location = {Hamilton, New Zealand},
series = {DEBS '18}
}

@inproceedings{10.1145/3208806.3208819,
author = {Wang, Guan and Zhang, Dejia and Zhou, Kaimao and Jia, Jinyuan},
title = {Rule and reuse based lightweight modeling and real time web3D rendering of forest scenes},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3208819},
doi = {10.1145/3208806.3208819},
abstract = {How to model and visualize a large scale forest scenes in a realistic and lightweight way keeps to be an important problem in Web3D Game and VR applications. This paper presents a novel solution to real time Web3D visualization of huge forest scenes. The data size of huge forest scenes is reduced to very low level by reusing some branches repetitively, thus, which can be transmitted over mobile Internet and rendered Web3D browsers instantly. However, the visual morphology and shaping variety of huge forest scenes can still be preserved with more straightforward hierarchical rules than L-System. From one sampling tree, we can extract it rules and grow them randomly to model large scale virtual forest scenes and render them lightweightly at Web3D browsers. Experimental Results show our proposed method outperform the existing Web3D forest modeling and rendering methods in terms of data volume, shaping polymorphism, networking bandwidth and Web rendering and achieves instant Web3D visualization of huge forest.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {8},
numpages = {8},
keywords = {L-system, instance rendering, lightweight forest modeling, web3D},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/2959100.2959122,
author = {Celma, Oscar},
title = {The Exploit-Explore Dilemma in Music Recommendation},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959122},
doi = {10.1145/2959100.2959122},
abstract = {Were The Rolling Stones right when they said, "You can't always get what you want; but if you try sometime you get what you need"? Recommendation systems are the crystal ball of the Internet: predicting user intentions, making sense of big data, and delivering what people are looking for before they even know they want it. Pandora radio is best known for the Music Genome Project; the most unique and richly labeled music catalog of 1.5 million+ tracks. While this content-based approach to music recommendation is extremely effective and still used today as the foundation to the leading online radio service, Pandora has also collected more than a decade of contextual listener feedback in the form of more than 65 billion thumbs from 79M+ monthly active users who have created more than 9 billion stations. This session will look at how the interdisciplinary team at Pandora goes about making sense of these massive data sets to successfully make large scale music recommendations to our listeners.As opposed to more traditional recommender systems which need only to recommend a single item or set of items, Pandora's recommenders must provide an evolving set of sequential items, which constantly keep the experience new and exciting. In this talk I will present a dynamic ensemble learning system that combines musicological data and machine learning models to provide a truly personalized experience. This approach allows us to switch from a lean back experience (exploitation) to a more exploration mode to discover new music tailored specifically to users individual tastes. To exemplify this, I will present a recently launched product led by the research team, Thumbprint Radio.Following this session the audience will have an in-depth understanding of how Pandora uses science to determine the perfect balance of familiarity, discovery, repetition and relevance for each individual listener, measures and evaluates user satisfaction, and how our online and offline architecture stack plays a critical role in our success.},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {377},
numpages = {1},
keywords = {A/B online testing, content-based recommendation, ensemble learning, exploit-explore dilemma, machine listening, offline evaluation, thumbprint radio},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}

@inproceedings{10.1145/3097983.3098114,
author = {Malloy, Matthew and Barford, Paul and Alp, Enis Ceyhun and Koller, Jonathan and Jewell, Adria},
title = {Internet Device Graphs},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098114},
doi = {10.1145/3097983.3098114},
abstract = {Internet device graphs identify relationships between user-centric internet connected devices such as desktops, laptops, smartphones, tablets, gaming consoles, TV's, etc. The ability to create such graphs is compelling for online advertising, content customization, recommendation systems, security, and operations. We begin by describing an algorithm for generating a device graph based on IP-colocation, and then apply the algorithm to a corpus of over 2.5 trillion internet events collected over the period of six weeks in the United States. The resulting graph exhibits immense scale with greater than 7.3 billion edges (pair-wise relationships) between more than 1.2 billion nodes (devices), accounting for the vast majority of internet connected devices in the US. Next, we apply community detection algorithms to the graph resulting in a partitioning of internet devices into 100 million small communities representing physical households. We validate this partition with a unique ground truth dataset. We report on the characteristics of the graph and the communities. Lastly, we discuss the important issues of ethics and privacy that must be considered when creating and studying device graphs, and suggest further opportunities for device graph enrichment and application.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1913–1921},
numpages = {9},
keywords = {internet device graphs},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3297858.3304011,
author = {Gobieski, Graham and Lucia, Brandon and Beckmann, Nathan},
title = {Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304011},
doi = {10.1145/3297858.3304011},
abstract = {Energy-harvesting technology provides a promising platform for future IoT applications. However, since communication is very expensive in these devices, applications will require inference "beyond the edge" to avoid wasting precious energy on pointless communication. We show that application performance is highly sensitive to inference accuracy. Unfortunately, accurate inference requires large amounts of computation and memory, and energy-harvesting systems are severely resource-constrained. Moreover, energy-harvesting systems operate intermittently, suffering frequent power failures that corrupt results and impede forward progress. This paper overcomes these challenges to present the first full-scale demonstration of DNN inference on an energy-harvesting system. We design and implement SONIC, an intermittence-aware software system with specialized support for DNN inference. SONIC introduces loop continuation, a new technique that dramatically reduces the cost of guaranteeing correct intermittent execution for loop-heavy code like DNN inference. To build a complete system, we further present GENESIS, a tool that automatically compresses networks to optimally balance inference accuracy and energy, and TAILS, which exploits SIMD hardware available in some microcontrollers to improve energy efficiency. Both SONIC &amp; TAILS guarantee correct intermittent execution without any hand-tuning or performance loss across different power systems. Across three neural networks on a commercially available microcontroller, SONIC &amp; TAILS reduce inference energy by 6.9\texttimes{} and 12.2\texttimes{}, respectively, over the state-of-the-art.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {199–213},
numpages = {15},
keywords = {deep neural network (DNN) inference, energy efficiency, intermittent computing},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@article{10.1145/2788397,
author = {Zhan, Zhi-Hui and Liu, Xiao-Fang and Gong, Yue-Jiao and Zhang, Jun and Chung, Henry Shu-Hung and Li, Yun},
title = {Cloud Computing Resource Scheduling and a Survey of Its Evolutionary Approaches},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2788397},
doi = {10.1145/2788397},
abstract = {A disruptive technology fundamentally transforming the way that computing services are delivered, cloud computing offers information and communication technology users a new dimension of convenience of resources, as services via the Internet. Because cloud provides a finite pool of virtualized on-demand resources, optimally scheduling them has become an essential and rewarding topic, where a trend of using Evolutionary Computation (EC) algorithms is emerging rapidly. Through analyzing the cloud computing architecture, this survey first presents taxonomy at two levels of scheduling cloud resources. It then paints a landscape of the scheduling problem and solutions. According to the taxonomy, a comprehensive survey of state-of-the-art approaches is presented systematically. Looking forward, challenges and potential future research directions are investigated and invited, including real-time scheduling, adaptive dynamic scheduling, large-scale scheduling, multiobjective scheduling, and distributed and parallel scheduling. At the dawn of Industry 4.0, cloud computing scheduling for cyber-physical integration with the presence of big data is also discussed. Research in this area is only in its infancy, but with the rapid fusion of information and data technology, more exciting and agenda-setting topics are likely to emerge on the horizon.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {63},
numpages = {33},
keywords = {Cloud computing, ant colony optimization, evolutionary computation, genetic algorithm, particle swarm optimization, resource scheduling}
}

@inproceedings{10.1145/2957276.2957309,
author = {Christensen, Bente},
title = {Formalization and Accountability in Surgery Planning},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957309},
doi = {10.1145/2957276.2957309},
abstract = {Due to poor resource utilization in surgery performance, there is increasing interest in applying workflow systems. Notably, due to their ability to "steer" the execution of the process toward an intended goal, according to an arrangement of things, data and resources compliant with "best practice", the systems are supposed to improve surgery planning and, hence, resource utilization. This study reports from a large-scale Electronic Patient Record development project, which also included workflow support in a surgery planning module. By applying an understanding of workflow systems and their ordering and coordinative mechanisms, this study investigates the effect of such systems on interdisciplinary work in surgery planning. The study shows that interdisciplinary work is affected by workflow systems in the way that the systems "order" responsibility and sequential dependency of tasks. The collective responsibility was affected by the sequential ordering and user role constraints inherent to the system. Moreover, there was a clear redistribution of tasks as a consequence of the formalization and the accountability mechanism.},
booktitle = {Proceedings of the 2016 ACM International Conference on Supporting Group Work},
pages = {293–302},
numpages = {10},
keywords = {electronic patient record, interdisciplinary work, surgery planning, workflow systems},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@article{10.1145/3183515,
author = {Bruneau-Queyreix, Joachim and Batalla, Jordi Mongay and Lacaud, Mathias and Negru, Daniel},
title = {PMS: A Novel Scale-Adaptive and Quality-Adaptive Hybrid P2P/Multisource Solution for Live Streaming},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3183515},
doi = {10.1145/3183515},
abstract = {Single-source HTTP adaptive streaming solutions (HAS) have become the de facto solutions to deliver live video over the Internet. By avoiding video stalling events that are mainly caused by the lack of throughput at client or at server side, HAS solutions increase the end users’ quality of experience (QoE). We propose to pragmatically extend HAS with our MS-Stream solution that simultaneously utilizes several servers. MS-Stream aims at offering high QoE for live content delivery by exploiting expanded bandwidth and link diversity in distributed heterogeneous infrastructures. By leveraging end users’ connectivity capacities, we further extend the QoE and scalability capabilities of our proposal by exposing a hybrid P2P/multisource live-streaming solution (P2P/MS-Stream (PMS)), achieving trade-offs between the system’s scale and the end users’ QoE. We propose a distributed quality adaptation algorithm run by every peer, along with a local optimization method of the usage of the server infrastructure made available. Large-scale evaluations conducted with 300 peers located in France permits validating our approach and algorithms over flash crowd events and allow us to conclude that PMS can reach the optimal trade-offs between QoE and system scale.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {35},
numpages = {25},
keywords = {DASH, P2P streaming, adaptive streaming, multiple-source streaming}
}

@inproceedings{10.1145/3148055.3148077,
author = {Villalobos, J. J. and Rodero, Ivan and Parashar, Manish},
title = {An Unsupervised Approach for Online Detection and Mitigation of High-Rate DDoS Attacks Based on an In-Memory Distributed Graph Using Streaming Data and Analytics},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148077},
doi = {10.1145/3148055.3148077},
abstract = {A Distributed Denial of Service (DDoS) attack is an attempt to make an online service, a network, or even an entire organization, unavailable by saturating it with traffic from multiple sources. DDoS attacks are among the most common and most devastating threats that network defenders have to watch out for. DDoS attacks are becoming bigger, more frequent, and more sophisticated. Volumetric attacks are the most common types of DDoS attacks. A DDoS attack is considered volumetric, or high-rate, when within a short period of time it generates a large amount of packets or a high volume of traffic. High-rate attacks are well-known and have received much attention in the past decade; however, despite several detection and mitigation strategies have been designed and implemented, high-rate attacks are still halting the normal operation of information technology infrastructures across the Internet when the protection mechanisms are not able to cope with the aggregated capacity that the perpetrators have put together. With this in mind, the present paper aims to propose and test a distributed and collaborative architecture for online high-rate DDoS attack detection and mitigation based on an in-memory distributed graph data structure and unsupervised machine learning algorithms that leverage real-time streaming data and analytics. We have successfully tested our proposed mechanism using a real-world DDoS attack dataset at its original rate in pursuance of reproducing the conditions of an actual large scale attack.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {103–112},
numpages = {10},
keywords = {analytics, big data, ddos detection, ddos mitigation, distributed, machine learning},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3308560.3316586,
author = {Wang, Jianyu and Wen, Rui and Wu, Chunming and Huang, Yu and Xiong, Jian},
title = {FdGars: Fraudster Detection via Graph Convolutional Networks in Online App Review System},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316586},
doi = {10.1145/3308560.3316586},
abstract = {Online review system enables users to submit reviews about the products. However, the openness of Internet and monetary rewards for crowdsourcing tasks stimulate a large number of fraudulent users to write fake reviews and post advertisements to interfere the rank of apps. Existing methods for detecting spam reviews have been successful but they usually aims at e-commerce (e.g. Amazon, eBay) and recommendation (e.g. Yelp, Dianping) systems. Since the behaviors of fraudulent users are complexity and varying across different review platforms, existing methods are not suitable for fraudster detection in online app review system.To shed light on this question, we are among the first to analyze the intentions of fraudulent users from different review platforms and categorize them by utilizing characteristics of contents (similarity, special symbols) and behaviors (timestamps, device, login status). With a comprehensive analysis of spamming activities and relationships between normal and malicious users, we design and present FdGars, the first graph convolutional network approach for fraudster detection in online app review system. Then we evaluate FdGars on real-world large-scale dataset (with 82,542 nodes and 42,433,134 edges) from Tencent App Store. The result demonstrates that F1-score of FdGars can achieve 0.938+, which outperforms several baselines and state-of-art fraudsters detecting methods. Moreover, we deploy FdGars on Tencent Beacon Anti-Fraud Platform to show its effectiveness and scalability. To the best of our knowledge, this is the first work to use graph convolutional networks for fraudster detection in the large-scale online app review system. It is worth to mention that FdGars can uncover malicious accounts even the data lack of labels in anti-spam tasks.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {310–316},
numpages = {7},
keywords = {Fraud Detection, Graph Convolutional Networks, Online App Review System},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3300061.3343408,
author = {Zhou, Anfu and Zhang, Huanhuan and Su, Guangyuan and Wu, Leilei and Ma, Ruoxuan and Meng, Zhen and Zhang, Xinyu and Xie, Xiufeng and Ma, Huadong and Chen, Xiaojiang},
title = {Poster: Optimizing Mobile Video Telephony Using Deep Imitation Learning},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3343408},
doi = {10.1145/3300061.3343408},
abstract = {Despite the pervasive use of real-time video telephony services, their quality of experience (QoE) remains unsatisfactory, especially over the mobile Internet. We conduct a large-scale measurement campaign on appname, an operational mobile video telephony service. Our analysis shows that the application-layer video codec and transport-layer protocols remain highly uncoordinated, which represents one major reason for the low QoE. We thus propose name, a machine learning based framework to resolve the issue. We train name with the massive data traces from the measurement campaign using a custom-designed imitation learning algorithm, which enables name to learn from past experience following an expert's iterative demonstration/supervision. We have implemented and incorporated name into the appname. Our experiments show that name outperforms state-of-the-art solutions, improving video quality while reducing stalling time by multi-folds under various practical scenarios.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {105},
numpages = {3},
keywords = {bitrate adaptation, imitation learning, video telephony},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inproceedings{10.1145/2785956.2787485,
author = {Burnett, Sam and Feamster, Nick},
title = {Encore: Lightweight Measurement of Web Censorship with Cross-Origin Requests},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787485},
doi = {10.1145/2785956.2787485},
abstract = {Despite the pervasiveness of Internet censorship, we have scant data on its extent, mechanisms, and evolution. Measuring censorship is challenging: it requires continual measurement of reachability to many target sites from diverse vantage points. Amassing suitable vantage points for longitudinal measurement is difficult; existing systems have achieved only small, short-lived deployments. We observe, however, that most Internet users access content via Web browsers, and the very nature of Web site design allows browsers to make requests to domains with different origins than the main Web page. We present Encore, a system that harnesses cross-origin requests to measure Web filtering from a diverse set of vantage points without requiring users to install custom software, enabling longitudinal measurements from many vantage points. We explain how Encore induces Web clients to perform cross-origin requests that measure Web filtering, design a distributed platform for scheduling and collecting these measurements, show the feasibility of a global-scale deployment with a pilot study and an analysis of potentially censored Web content, identify several cases of filtering in six months of measurements, and discuss ethical concerns that would arise with widespread deployment.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {653–667},
numpages = {15},
keywords = {network measurement, web censorship, web security},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{10.1145/3036669.3038255,
author = {Lu, Lee-Chung},
title = {Physical Design Challenges and Innovations to Meet Power, Speed, and Area Scaling Trend},
year = {2017},
isbn = {9781450346962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3036669.3038255},
doi = {10.1145/3036669.3038255},
abstract = {In the advanced process technologies of 7nm and beyond, the semiconductor industry faces several new challenges: (1) aggressive chip area scaling with economically feasible process technology development, (2) sufficient performance enhancement of advanced small-scale technology with significantly increased wire and via resistances, (3) power density sustainability with ever shrinking chip area, and (4) advanced chip packaging integration solutions for complex SOC systems. In this presentation, novel physical design solutions of robust IP and design methodologies will be explored to solve these challenges. These innovations are made possible by the co-optimization of process technology, IP design and design flow automation. Density scaling is the most important indicator in the continuation of Moore's law. Before 10nm, chip area reduction is mainly achieved by fundamentally shrinking transistor and metal dimensions. Starting from 7nm, maintaining sufficient and economical scaling is hard to achieve through dimension decrease alone. We present two cost-effective enablers, FIN depopulation and EUV, along with their associated innovative standard cell structures and physical design flows, to realize additional area reduction beyond process dimension scaling.Achieving high performance is always a key index for CPU designs. However, the resistance of interconnects has grown significantly as the dimensions of wires and vias are scaled aggressively. We present novel physical design solutions of the via pillar approach using metal layer promotion and multiple-width configurable wires. This fully automated via pillar design flow mitigates the high resistance impact and becomes indispensable in high performance designs for advanced process technologies.Maintaining power densities while aggressively shrinking chip areas is also a critical requirement, especially for mobile and IoT applications. Lowering supply voltages is one of the most effective means to reducing power consumption, especially for FinFET devices with much lower threshold voltages than planar devices. However, process and timing variation is high even for FinFET devices operating at very low voltages. We present robust ultra-low voltage IP design solutions and the current status and issues of non-Gaussian and asymmetric variation modeling for ultra-low voltage timing signoffs. Finally, advanced chip packaging is presented as a viable solution for integration and system level scaling for complex SOC systems. Specific packaging solutions can meet different requirements of system die and package size, form factor, bandwidth, power and homogeneous or heterogeneous integration. For a silicon-proven system, quantitative advantages of advanced packaging over traditional packaging in silicon thickness, thermal dissipation and voltage drop are presented. Chip packaging integration flow and requirements will also be discussed.},
booktitle = {Proceedings of the 2017 ACM on International Symposium on Physical Design},
pages = {63},
numpages = {1},
keywords = {chip density performance, moore's law, physical design, power, process and design co-optimization},
location = {Portland, Oregon, USA},
series = {ISPD '17}
}

@inproceedings{10.1145/2935663.2935664,
author = {Zhu, Chengang and Cheng, Guang and Guo, Xiaojun and Wang, Yuxiang},
title = {RBAS: A Real-Time User Behavior Analysis System for Internet TV in Cloud Computing},
year = {2016},
isbn = {9781450341813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935663.2935664},
doi = {10.1145/2935663.2935664},
abstract = {The characteristic of Internet TV user behavior is quite essential for designers to optimize resource schedule and improve user experience. With the rapid development of Internet, both Internet TV users and STB (set top boxes) models are booming. This brings a large amount of behavior data which requires matching computing and storage resource to process. Therefore, scalable Internet TV user behavior analysis becomes more difficult. As a solution, cloud computing framework such as Hive is emerged. But limited by performance, it's not an appropriate choice for interactive analysis or real-time data exploration. In this paper, we present a real-time Internet TV user behavior analysis system with advantages of high concurrency, low latency and good transportability. Firstly, we design an event capture scheme, consisted of agents embedded in STBs and capture server clusters, to capture every manipulation performed by users. Secondly, we develop a SQL-on-Hadoop engine with distributed transactional management to decrease the response time. The engine has excellent query performance and ability to interactively query various data sources in different Hadoop formats. Lastly, we evaluate RBAS in a commercial Internet TV platform of 16 million registered users. The results show that, with a 32-node cluster, the system can effectively process 10.2 TB of behavior data every day, which is about 40x faster than original Hive-based system.},
booktitle = {Proceedings of the 11th International Conference on Future Internet Technologies},
pages = {36–42},
numpages = {7},
keywords = {Internet TV, SQL-on-Hadoop, User behavior analysis, cloud computing},
location = {Nanjing, China},
series = {CFI '16}
}

@inproceedings{10.1145/2815675.2815693,
author = {Liu, Suqi and Foster, Ian and Savage, Stefan and Voelker, Geoffrey M. and Saul, Lawrence K.},
title = {Who is .com? Learning to Parse WHOIS Records},
year = {2015},
isbn = {9781450338486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815675.2815693},
doi = {10.1145/2815675.2815693},
abstract = {WHOIS is a long-established protocol for querying information about the 280M+ registered domain names on the Internet. Unfortunately, while such records are accessible in a ``human-readable'' format, they do not follow any consistent schema and thus are challenging to analyze at scale. Existing approaches, which rely on manual crafting of parsing rules and per-registrar templates, are inherently limited in coverage and fragile to ongoing changes in data representations. In this paper, we develop a statistical model for parsing WHOIS records that learns from labeled examples. Our model is a conditional random field (CRF) with a small number of hidden states, a large number of domain-specific features, and parameters that are estimated by efficient dynamic-programming procedures for probabilistic inference. We show that this approach can achieve extremely high accuracy (well over 99%) using modest amounts of labeled training data, that it is robust to minor changes in schema, and that it can adapt to new schema variants by incorporating just a handful of additional examples. Finally, using our parser, we conduct an exhaustive survey of the registration patterns found in 102M com domains.},
booktitle = {Proceedings of the 2015 Internet Measurement Conference},
pages = {369–380},
numpages = {12},
keywords = {information extraction, machine learning, named entity recognition, whois},
location = {Tokyo, Japan},
series = {IMC '15}
}

@inproceedings{10.1145/3167996.3168002,
author = {Mahmoud, Moustafa and Hossen, Md Zakir and Barakat, Hesham and Mannan, Mohammad and Youssef, Amr},
title = {Towards a comprehensive analytical framework for smart toy privacy practices},
year = {2018},
isbn = {9781450363570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167996.3168002},
doi = {10.1145/3167996.3168002},
abstract = {Smart toys are becoming increasingly popular with children and parents alike, primarily due to the toys' dynamic nature, superior-interactivity, and apparent educational value. However, as these toys may be Internet-connected, and equipped with various sensors that can record children's everyday interactions, they can pose serious security and privacy threats to children. Indeed, in the recent years, several smart toys have been reported to be vulnerable, and some associated companies also have suffered large-scale data breaches, exposing information collected through these toys. To complement recent efforts in analyzing and quantifying security of smart toys, in this work, we propose a comprehensive analytical framework based on 17 privacy-sensitive criteria to systematically evaluate selected privacy aspects of smart toys. Our work is primarily based on publicly available (legally-binding) privacy policies and terms of use documentation, and a static analysis of companion Android apps, which are, in most cases, essential for intended functioning of the toys. We use our framework to evaluate a representative set of 11 smart toys. Our analysis highlights incomplete/lack of information about data storage practices and legal compliance, and several instances of unnecessary collection of privacy-sensitive information, and the use of over-privileged apps. The proposed framework is a step towards comparing smart toys from a privacy perspective, which can be useful to toy manufacturers, parents, regulatory bodies, and law-makers.},
booktitle = {Proceedings of the 7th Workshop on Socio-Technical Aspects in Security and Trust},
pages = {64–75},
numpages = {12},
keywords = {applications analysis, evaluation framework, privacy policy, smart toys, terms of use},
location = {Orlando, Florida, USA},
series = {STAST '17}
}

@inproceedings{10.1145/3103010.3103024,
author = {Collomosse, John},
title = {Sketched Visual Narratives for Image and Video Search},
year = {2017},
isbn = {9781450346894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3103010.3103024},
doi = {10.1145/3103010.3103024},
abstract = {The internet is transforming into a visual medium; over 80% of the internet is forecast to be visual content by 2018, and most of this content will be consumed on mobile devices featuring a touch-screen as their primary interface. Gestural interaction, such as sketch, presents an intuitive way to interact with these devices. Imagine a Google image search in which you specify your query by sketching the desired image with your finger, rather than (or in addition to) describing it with text words. Sketch offers an orthogonal perspective on visual search - enabling concise specification of appearance (via sketch) in addition to semantics (via text). In this talk, John Collomosse will present a summary of his group's work on the use of free-hand sketches for the visual search and manipulation of images and video. He will begin by describing a scalable system for sketch based search of multi-million image databases, based upon their Gradient Field HOG (GF-HOG) descriptor. He will then describe how deep learning can be used to enhance performance of the retrieval. Imagine a product catalogue in which you sketched, say an engineering part, rather than using a text or serial numbers to find it? John will then describe how scalable search of video can be similarly achieved, through the depiction of sketched visual narratives that depict not only objects but also their motion (dynamics) as a constraint to find relevant video clips. The work presented in this talk has been supported by the EPSRC and AHRC between 2012-2016.},
booktitle = {Proceedings of the 2017 ACM Symposium on Document Engineering},
pages = {95},
numpages = {1},
keywords = {gestural interaction, image search, multimedia, sketched visual narratives},
location = {Valletta, Malta},
series = {DocEng '17}
}

@inproceedings{10.5555/3233397.3233483,
author = {Albadarneh, Jafar and Talafha, Bashar and Al-Ayyoub, Mahmoud and Zaqaibeh, Belal and Al-Smadi, Mohammad and Jararweh, Yaser and Benkhelifa, Elhadj},
title = {Using big data analytics for authorship authentication of arabic tweets},
year = {2015},
isbn = {9780769556970},
publisher = {IEEE Press},
abstract = {Authorship authentication of a certain text is concerned with correctly attributing it to its author based on its contents. It is a very important problem with deep root in history as many classical texts have doubtful attributions. The information age and ubiquitous use of the Internet is further complicating this problem and adding more dimensions to it. We are interested in the modern version of this problem where the text whose authorship needs authentication is an online text found in online social networks. Specifically, we are interested in the authorship authentication of tweets. This is not the only challenging aspect we consider here. Another challenging aspect is the language of the tweets. Most current works and existing tools support English. We chose to focus on the very important, yet largely understudied, Arabic language. Finally, we add another challenging aspect to the problem at hand by addressing it at a very large scale. We present our effort to employ big data analytics to address the authorship authentication problem of Arabic tweets. We start by crawling a dataset of more than 53K tweets distributed across 20 authors. We then use preprocessing steps to clean the data and prepare it for analysis. The next step is to compute the feature vectors of each tweet. We use the Bag-Of-Words (BOW) approach and compute the weights using the Term Frequency-Inverse Document Frequency (TF-IDF). Then, we feed the dataset to a Naive Bayes classifier implemented on a parallel and distributed computing framework known as Hadoop. To the best of our knowledge, none of the previous works on authorship authentication of Arabic text addressed the unique challenges associated with (1) tweets and (2) large-scale datasets. This makes our work unique on many levels. The results show that the testing accuracy is not very high (61.6%), which is expected in the very challenging setting that we consider.},
booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
pages = {448–452},
numpages = {5},
location = {Limassol, Cyprus},
series = {UCC '15}
}

@proceedings{10.1145/3177540,
title = {ISPD '18: Proceedings of the 2018 International Symposium on Physical Design},
year = {2018},
isbn = {9781450356268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizing committee, we are delighted to welcome you to the 2018 ACM International Symposium on Physical Design (ISPD), held at Seaside, California. Continuing the great tradition established by its twenty-six predecessors, which includes a series of five ACM/SIGDA Physical Design Workshops held intermittently in 1987-1996 and twenty one editions of ISPD in the current form since 1997. The 2018 ISPD provides a premier forum to present leading-edge research results, exchange ideas, and promote research on critical areas related to the physical design of VLSI and other related systems.The regular papers in the ISPD 2018 program were selected after a rigorous, month-long, double-blind review process and a face-to-face meeting by the Technical Program Committee (TPC) members. The papers selected exhibit latest advancements in a variety of topics in physical design, including emerging challenges for current and future process technologies, FPGA architectures, placement, detailed routing, and application of machine-learning based techniques to physical design.The ISPD 2018 program is complemented by two keynote addresses, eleven invited talks and a tribute session, all of which are delivered by distinguished researchers from both industry and academia. On Monday morning, Dr. Anthony Hill, fellow of Texas Instruments, Inc., will talk about challenges and opportunities in automotive, industrial, and IoT Physical Design. In the second keynote on Tuesday, Andreas Olofsson, DARPA's Microsystems Technology Office Program Manager, will discuss the next generation of silicon compilers. A commemorative session on Tuesday afternoon will pay tribute to Professor Te Chiang Hu. His collaborators will share with us Dr. Hu's exceptional contributions to research in physical design and VLSI applications, including his influential work on trees, flows, and networks. There will be other invited talks interspersed with the presentations of the regular papers. The topics of the invited papers range from advanced FPGA applications, high-speed processor design, logic computation, machine learning in EDA, interconnect optimization, to electromigration-aware physical design.Since 2005, the ISPD has organized highly competitive contests to promote and advance research in placement, global routing, clock network synthesis, discrete gate sizing, and detailed routingdriven placement. The contest this year, organized by Cadence, focuses on detailed routing. Continuing the tradition of all the past contests, a new large-scale real-world benchmark suite for detailed routing has been specified using LEF/DEF and will be released in the ISPD website (http://www.ispd.cc). The contest evaluates the routing quality and the ability to connect all the nets of a design without design rule violations. It is expected to lead and motivate more research and contributions on the detailed routing of large integrated circuits.},
location = {Monterey, California, USA}
}

@inproceedings{10.1145/3120459.3120478,
author = {Pries-Heje, Jan and Krohn, Malene M.},
title = {The SAFe way to the agile organization},
year = {2017},
isbn = {9781450352642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3120459.3120478},
doi = {10.1145/3120459.3120478},
abstract = {How do you make the agile organization? This paper gives an answer in form of a case story from the financial software company SimCorp. In late 2015SimCorp decided to go agile on a big scale. To do so they applied the scalable agile framework SAFe. In this framework Scrum teams are working within "Agile Release Trains" providing value streams that together realize a strategic mission. At SimCorp there were more than 500 employees in 55 teams working in eight agile release trains. SimCorp has a solid track record of releasing a new version of their standard software product - SimCorp Dimension - every 6 months. In February 2017 a new version of the product was successfully released according to plan and now with an agile organization behind it. The paper tells the story from beginning to end focusing on the three things that made it possible to become an agile organization within a year. Furthermore the paper discusses what the main challenges were.},
booktitle = {Proceedings of the XP2017 Scientific Workshops},
articleno = {18},
numpages = {3},
keywords = {agile software development, large-scale software development, scaled agile framework},
location = {Cologne, Germany},
series = {XP '17}
}

@inproceedings{10.1145/2938559.2948858,
author = {Lee, Byeong-uk and Roh, Byeong-hee and Ahn, Myung Kil and Kim, Yong Hyun},
title = {Poster: Worm Propagation Model for Cyber Warfare Modeling and Simulation in Tactical Networks},
year = {2016},
isbn = {9781450344166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938559.2948858},
doi = {10.1145/2938559.2948858},
abstract = {Recently, with the development and diffusion of internet, it has also increased the damage from malicious attacks. Worm attack refers to one of the malicious attack that replicates and spreads itself to the network. Especially, it is expected that the damage by worm in the tactical network which aims at victory in the engagement is more effective than that in the typical network. It can happen that the divorce between the combat unit and its immediate superior. If the main system is infected, it would take damage critically on whole network. Therefore, it is required to research the analysis of worm attack. However, previous research premise that is cannot reflect the hierarchical structure. Also, the simulation of the large-scaled cyberattack costly and time consuming. In this paper, we propose the worm propagation model for large-scale cyberattack using OPNET Simulator.},
booktitle = {Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion},
pages = {49},
numpages = {1},
keywords = {cyber warfare, cyberattack, tactical network, worm modeling, worm propagation model, worm spreading},
location = {Singapore, Singapore},
series = {MobiSys '16 Companion}
}

@inproceedings{10.1145/3053600.3053605,
author = {Casalicchio, Emiliano and Perciballi, Vanessa},
title = {Measuring Docker Performance: What a Mess!!!},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053605},
doi = {10.1145/3053600.3053605},
abstract = {Today, a new technology is going to change the way platforms for the internet of services are designed and managed. This technology is called container (e.g. Docker and LXC). The internet of service industry is adopting the container technology both for internal usage and as commercial offering. The use of container as base technology for large-scale systems opens many challenges in the area of resource management at run-time, for example: autoscaling, optimal deployment and monitoring. Specifically, monitoring of container based systems is at the ground of any resource management solution, and it is the focus of this work. This paper explores the tools available to measure the performance of Docker from the perspective of the host operating system and of the virtualization environment, and it provides a characterization of the CPU and disk I/O overhead introduced by containers.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {11–16},
numpages = {6},
keywords = {cloud computing, container, docker, internet of service, microservices, monitoring, performance evaluation},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@inproceedings{10.1145/3123266.3123443,
author = {Nian, Fudong and Bao, Bing-Kun and Li, Teng and Xu, Changsheng},
title = {Multi-Modal Knowledge Representation Learning via Webly-Supervised Relationships Mining},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123443},
doi = {10.1145/3123266.3123443},
abstract = {Knowledge representation learning (KRL) encodes enormous structured information with entities and relations into a continuous low-dimensional semantic space. Most conventional methods solely focus on learning knowledge representation from single modality, yet neglect the complementary information from others. The more and more rich available multi-modal data on Internet also drive us to explore a novel approach for KRL in multi-modal way, and overcome the limitations of previous single-modal based methods. This paper proposes a novel multi-modal knowledge representation learning (MM-KRL) framework which attempts to handle knowledge from both textual and visual modal web data. It consists of two stages, i.e., webly-supervised multi-modal relationship mining, and bi-enhanced cross-modal knowledge representation learning. Compared with existing knowledge representation methods, our framework has several advantages: (1) It can effectively mine multi-modal knowledge with structured textual and visual relationships from web automatically. (2) It is able to learn a common knowledge space which is independent to both task and modality by the proposed Bi-enhanced Cross-modal Deep Neural Network (BC-DNN). (3) It has the ability to represent unseen multi-modal relationships by transferring the learned knowledge with isolated seen entities and relations into unseen relationships. We build a large-scale multi-modal relationship dataset (MMR-D) and the experimental results show that our framework achieves excellent performance in zero-shot multi-modal retrieval and visual relationship recognition.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {411–419},
numpages = {9},
keywords = {knowledge representation learning, multi-modal, relationship mining, webly-supervised},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3134263.3134264,
author = {Peng, Yao and Ye, Hao and Lin, Yining and Bao, Yixin and Zhao, Zhijian and Qiu, Haonan and Lu, Yao and Wang, Li and Zheng, Yingbin},
title = {Large-Scale Video Classification with Elastic Streaming Sequential Data Processing System},
year = {2017},
isbn = {9781450355377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134263.3134264},
doi = {10.1145/3134263.3134264},
abstract = {Videos are dominant on the Internet. Current systems to process large-scale videos are suboptimal due to the following reasons: (1) machine learning modules such as feature extractors and classifiers generate huge intermediate data and place heavy burden to the storage and network, and (2) task scheduling is explicit; manually configuring the machine learning modules on the cluster is tedious and inefficient. In this work, we propose Elastic Streaming Sequential data Processing system (ESSP) that supports automatic task scheduling; multiple machine learning components are automatically parallelized. Further, our system prevents extensive disc I/O by applying the in-memory dataflow scheme. Evaluation on real-world video classification datasets shows many-fold improvements.},
booktitle = {Proceedings of the Workshop on Large-Scale Video Classification Challenge},
pages = {1–7},
numpages = {7},
keywords = {elastic streaming sequential data processing system, large-scale video classification, two-stream network},
location = {Mountain View, California, USA},
series = {LSVC '17}
}

@inproceedings{10.1145/3341301.3359655,
author = {Chou, David and Xu, Tianyin and Veeraraghavan, Kaushik and Newell, Andrew and Margulis, Sonia and Xiao, Lin and Ruiz, Pol Mauri and Meza, Justin and Ha, Kiryong and Padmanabha, Shruti and Cole, Kevin and Perelman, Dmitri},
title = {Taiji: managing global user traffic for large-scale internet services at the edge},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359655},
doi = {10.1145/3341301.3359655},
abstract = {We present Taiji, a new system for managing user traffic for large-scale Internet services that accomplishes two goals: 1) balancing the utilization of data centers and 2) minimizing network latency of user requests.Taiji models edge-to-datacenter traffic routing as an assignment problem---assigning traffic objects at the edge to the data centers to satisfy service-level objectives. Taiji uses a constraint optimization solver to generate an optimal routing table that specifies the fractions of traffic each edge node will distribute to different data centers. Taiji continuously adjusts the routing table to accommodate the dynamics of user traffic and failure events that reduce capacity.Taiji leverages connections among users to selectively route traffic of highly-connected users to the same data centers based on fractions in the routing table. This routing strategy, which we term connection-aware routing, allows us to reduce query load on our backend storage by 17%.Taiji has been used in production at Facebook for more than four years and routes global traffic in a user-aware manner for several large-scale product services across dozens of edge nodes and data centers.},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {430–446},
numpages = {17},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

@inproceedings{10.1145/3267204.3267207,
author = {Hartung, Robert and Kulau, Ulf and Lichtblau, Niels and Wolf, Lars C.},
title = {A Flexible Software Framework for Real-World Experiments and Temperature-Controlled Testbeds},
year = {2018},
isbn = {9781450359306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267204.3267207},
doi = {10.1145/3267204.3267207},
abstract = {Both reliability and robustness are required for applicationsin the field of wireless sensor networks and the internet ofthings. Testbeds are a usefool tool to verify both hardwareand software concepts under realistic conditions. However,existing testbeds often are fitted to specific use cases. Thispaper presents a software framework to build testbeds forvarious applications and scenarios. The module architectureof our framework approach is presented in detail. Communi-cation is based on MQTT which fulfills both the need to beflexible, but also to scale well for larger testbeds. Key require-ments are derived from our existing use cases, an outdoorscenario and an indoor, temperature-controlled testbed. Wepresent our temperature-controlled chambers in detail andcompare their performance to existing testbeds.},
booktitle = {Proceedings of the 12th International Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; Characterization},
pages = {30–37},
numpages = {8},
keywords = {MQTT, climate chamber, real-world experiments, reliability, temperature control, testbed, testbed framework, wireless sensor networks},
location = {New Delhi, India},
series = {WiNTECH '18}
}

@inproceedings{10.1145/2675743.2771881,
author = {Stojadinovi\'{c}, Aleksandar and Stojanovi\'{c}, Nenad and Stojanovi\'{c}, Ljiljana},
title = {Dynamic monitoring for improving worker safety at the workplace: use case from a manufacturing shop floor},
year = {2015},
isbn = {9781450332866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675743.2771881},
doi = {10.1145/2675743.2771881},
abstract = {Manufacturing is one of ever more important domains for applying IoT and real-time processing. Despite a big trend in optimizing production processes based on the real-time shop floor data (e.g. improving the quality control or the resource consumption), there is a huge need in making the workspaces more ergonomic, user friendly and safe. This is especially relevant for modern manufacturing facilities which are equipped with modern cameras and other personal or environmental monitoring systems. However, the task of personal monitoring in manufacturing is a rather a challenging task from the real-time processing point of view, starting from the quality of data that can be acquired from the monitoring system, till the complexity of the situations that should be considered from the safety point of view (their detection and the reaction on them).In this paper we present a novel approach for improving worker safety at the workplace in the manufacturing shop floor. The approach is based on a dynamic and scalable real-time processing pipeline derived from Storm architecture that satisfies complex requirements for realizing this challenging task.The system has been designed and implemented for a manufacturing line in TRW, one of the largest suppliers for the automotive industry. It uses the Kinect-based monitoring system installed in the shop floor and it is part of the risk intervention and communication process, led by safety and prevention managers.In this paper we present the requirements analysis, the conceptual architecture and the implementation and initial testing.},
booktitle = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems},
pages = {205–216},
numpages = {12},
keywords = {big data, manufacturing, real-time processing, storm architecture, worker safety},
location = {Oslo, Norway},
series = {DEBS '15}
}

@article{10.1145/2936722,
author = {Gupta, Ashish and Yang, Fan and Govig, Jason and Kirsch, Adam and Chan, Kelvin and Lai, Kevin and Wu, Shuo and Dhoot, Sandeep and Kumar, Abhilash Rajesh and Agiwal, Ankur and Bhansali, Sanjay and Hong, Mingsheng and Cameron, Jamie and Siddiqi, Masood and Jones, David and Shute, Jeff and Gubarev, Andrey and Venkataraman, Shivakumar and Agrawal, Divyakant},
title = {Mesa: a geo-replicated online data warehouse for Google's advertising system},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2936722},
doi = {10.1145/2936722},
abstract = {Mesa is a highly scalable analytic data warehousing system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy a complex and challenging set of user and systems requirements, including near real-time data ingestion and retrieval, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Specifically, Mesa handles petabytes of data, processes millions of row updates per second, and serves billions of queries that fetch trillions of rows per day. Mesa is geo-replicated across multiple datacenters and provides consistent and repeatable query answers at low latency, even when an entire datacenter fails. This paper presents the Mesa system and reports the performance and scale that it achieves.},
journal = {Commun. ACM},
month = {jun},
pages = {117–125},
numpages = {9}
}

@inproceedings{10.1145/3010089.3010095,
author = {Sindhu, C. S. and Hegde, Nagaratna P.},
title = {TAF: Temporal Analysis Framework for Handling Data velocity in Healthcare Analytics},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010095},
doi = {10.1145/3010089.3010095},
abstract = {We are inundated in a flood of data today. Data is being collected at a rapid scale from variety of sources like healthcare, e-commerce, social networking and so on. Decisions which were earlier made on assumptions can now be made on the data itself. It's a well known fact that volume, variety, velocity and veracity are the challenges associated in handling Big Data. The dynamic nature of the Internet and the velocity factor pose humongous challenges in retrieving patterns from the data. Coping up with noisy data which occurs at a rapid rate is still an open challenge. We have handled the issues associated with variety and veracity. After reviewing the existing system, it was found that there is no significant research model towards addressing data velocity problem exclusively taking case study of healthcare analytics.Hence, this paper presents a novel framework TAF or Temporal Analysis Framework that mainly targets at handling the incoming speed of data and redundancies in Healthcare Analytics. The proposed system uses real-time data analysis that significantly handles the data velocity along with retention of minimal error. The study outcome was assessed to find minimal algorithm complexities compared to any system that doesn't use this approach of self-adaptable real-time data analysis.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {10},
numpages = {11},
keywords = {Big Data, Data Volume, Healthcare Analytics, Medical Data, Real-Time Analysis},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/3098603.3098606,
author = {Gao, Qingzhu and Dey, Prasenjit and Ahammad, Parvez},
title = {Perceived Performance of Top Retail Webpages In the Wild: Insights from Large-scale Crowdsourcing of Above-the-Fold QoE},
year = {2017},
isbn = {9781450350563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098603.3098606},
doi = {10.1145/3098603.3098606},
abstract = {Clearly, no one likes webpages with poor quality of experience (QoE). Being perceived as slow or fast is a key element in the overall perceived QoE of web applications. While extensive effort has been put into optimizing web applications (both in industry and academia), not a lot of work exists in characterizing what aspects of webpage loading process truly influence human end-user's perception of the Speed of a page. In this paper we present SpeedPerception1, a large-scale web performance crowdsourcing framework focused on understanding the perceived loading performance of above-the-fold (ATF) webpage content. Our end goal is to create free open-source benchmarking datasets to advance the systematic analysis of how humans perceive webpage loading process.In Phase-1 of our SpeedPerception study using Internet Retailer Top 500 (IR 500) websites [3], we found that commonly used navigation metrics such as onLoad and Time To First Byte (TTFB) fail (less than 60% match) to represent majority human perception when comparing the speed of two webpages. We present a simple 3-variable-based machine learning model that explains the majority end-user choices better (with 87 ± 2% accuracy). In addition, our results suggest that the time needed by end-users to evaluate relative perceived speed of webpage is far less than the time of its visualComplete event.},
booktitle = {Proceedings of the Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {13–18},
numpages = {6},
keywords = {Above-the-Fold, Crowdsourcing, Perceived Speed, Perceptual SpeedIndex, Quality of Experience, SpeedIndex, TTFB, Web Performance, onLoad},
location = {Los Angeles, CA, USA},
series = {Internet QoE '17}
}

@article{10.1145/3155055.3155062,
author = {Qingzhu and Dey, Prasenjit and Ahammad, Parvez},
title = {Perceived Performance of Top Retail Webpages In the Wild},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/3155055.3155062},
doi = {10.1145/3155055.3155062},
abstract = {Clearly, no one likes webpages with poor quality of experience (QoE). Being perceived as slow or fast is a key element in the overall perceived QoE of web applications. While extensive effort has been put into optimizing web applications (both in industry and academia), not a lot of work exists in characterizing what aspects of webpage loading process truly influence human end-user's perception of the emph{Speed} of a page. In this paper we present emph{SpeedPerception}, a large-scale web performance crowdsourcing framework focused on understanding the perceived loading performance of above-the-fold (ATF) webpage content. Our end goal is to create free open-source benchmarking datasets to advance the systematic analysis of how humans perceive webpage loading process.In Phase-1 of our emph{SpeedPerception} study using Internet Retailer Top 500 (IR 500) websites, we found that commonly used navigation metrics such as emph{onLoad} and emph{Time To First Byte (TTFB)} fail (less than 60% match) to represent majority human perception when comparing the speed of two webpages. We present a simple 3-variable-based machine learning model that explains the majority end-user choices better (with $87 pm 2%$ accuracy). In addition, our results suggest that the time needed by end-users to evaluate relative perceived speed of webpage is far less than the time of its emph{visualComplete} event.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {oct},
pages = {42–47},
numpages = {6},
keywords = {Above-the-Fold, Crowdsourcing, Perceived Speed, Perceptual SpeedIndex, Quality of Experience, SpeedIndex, Web Performance}
}

@inproceedings{10.1145/2815675.2815688,
author = {Li, Zhenhua and Wilson, Christo and Xu, Tianyin and Liu, Yao and Lu, Zhen and Wang, Yinlong},
title = {Offline Downloading in China: A Comparative Study},
year = {2015},
isbn = {9781450338486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815675.2815688},
doi = {10.1145/2815675.2815688},
abstract = {Although Internet access has become more ubiquitous in recent years, most users in China still suffer from low-quality connections, especially when downloading large files. To address this issue, hundreds of millions of China's users have resorted to technologies that allow for ``offline downloading'', where a proxy is employed to pre-download the user's requested file and then deliver the file at her convenience.In this paper, we examine two typical implementations of offline downloading: the cloud-based approach and the smart AP (access point) based approach. Using a large-scale dataset collected from a major cloud-based system and comprehensive benchmarks of popular smart APs, we find that the two approaches are complementary while also being subject to distinct performance bottlenecks. Driven by these results, we design and implement a proof-of-concept middleware called ODR (Offline Downloading Redirector) to help users get rid of performance bottlenecks.We feel that offline downloading has broad applicability to other areas of the world that lack broadband penetration. By deploying offline downloading technologies, coupled with our proposed ODR middleware, the Internet experiences for users in many parts of the world can be improved.},
booktitle = {Proceedings of the 2015 Internet Measurement Conference},
pages = {473–486},
numpages = {14},
keywords = {cloud storage, dtn, internet, offline downloading, smart ap},
location = {Tokyo, Japan},
series = {IMC '15}
}

@inproceedings{10.1145/3240508.3240616,
author = {Zhuang, Peiqin and Wang, Yali and Qiao, Yu},
title = {WildFish: A Large Benchmark for Fish Recognition in the Wild},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240616},
doi = {10.1145/3240508.3240616},
abstract = {Fish recognition is an important task to understand the marine ecosystem and biodiversity. It is often challenging to identify fish species in the wild, due to the following difficulties. First, most fish benchmarks are small-scale, which may limit the representation power of machine learning models. Second, the number of fish species is huge, and there may still exist unknown categories in our planet. The traditional classifiers often fail to deal with this open-set scenario. Third, certain fish species are highly-confused. It is often hard to figure out the subtle differences, only by the unconstrained images. Motivated by these facts, we introduce a large-scale WildFish benchmark for fish recognition in the wild. Specifically, we make three contributions in this paper. First, WildFish is the largest image data set for wild fish recognition, to our best knowledge. It consists of 1000 fish categories with 54,459 unconstrained images, allowing to train high-capacity models for automatic fish classification. Second, we propose a novel open-set fish classification task for realistic scenarios, and investigate the open-set deep learning framework with a number of practical designs. Third, we propose a novel fine-grained recognition task, with the guidance of pairwise textual descriptions. Via leveraging the comparison knowledge in the sentence, we design a multi-modal fish net to effectively distinguish two confused categories in a pair. Finally, we release WildFish (https://github.com/PeiqinZhuang/WildFish), in order to bring benefit to more research studies in multimedia and beyond.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1301–1309},
numpages = {9},
keywords = {deep learning, fine-grained recognition, fish classification, open-set classification, vision-text modeling},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240551,
author = {Li, Yuke},
title = {Video Forecasting with Forward-Backward-Net: Delving Deeper into Spatiotemporal Consistency},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240551},
doi = {10.1145/3240508.3240551},
abstract = {Video forecasting is an emerging topic in the computer vision field, and it is a pivotal step toward unsupervised video understanding. However, the predictions generated from the state-of-the-art methods might be far from ideal quality, due to a lack of guidance from the labeled data of correct predictions (e.g., the annotated future pose of a person). Hence, building a network for better predicting future sequences in an unsupervised manner has to be further pursued. To this end, we put forth a novel Forward-Backward-Net (FB-Net) architecture, which delves deeper into spatiotemporal consistency. It first derives the forward consistency from the raw historical observations. In contrast to mainstream video forecasting approaches, FB-Net then investigates the backward consistency from the future to the past to reinforce the predictions. The final predicted results are inferred by jointly taking both the forward and backward consistencies into account. Moreover, we embed the motion dynamics and the visual content into a single framework via the FB-Net architecture, which significantly differs from learning each component throughout the videos separately. We evaluate our FB-Net on the large-scale KTH and UCF101 datasets. The experiments show that it can introduce considerable margin improvements with respect to most recent leading studies.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {211–219},
numpages = {9},
keywords = {computer vision, deep neural network, spatiotemporal consistency, unsupervised learning, video forecasting},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3300061.3345430,
author = {Zhou, Anfu and Zhang, Huanhuan and Su, Guangyuan and Wu, Leilei and Ma, Ruoxuan and Meng, Zhen and Zhang, Xinyu and Xie, Xiufeng and Ma, Huadong and Chen, Xiaojiang},
title = {Learning to Coordinate Video Codec with Transport Protocol for Mobile Video Telephony},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3345430},
doi = {10.1145/3300061.3345430},
abstract = {Despite the pervasive use of real-time video telephony services, the users' quality of experience (QoE) remains unsatisfactory, especially over the mobile Internet. Previous work studied the problem via controlled experiments, while a systematic and in-depth investigation in the wild is still missing. To bridge the gap, we conduct a large-scale measurement campaign on appname, an operational mobile video telephony service. Our measurement logs fine-grained performance metrics over 1 million video call sessions. Our analysis shows that the application-layer video codec and transport-layer protocols remain highly uncoordinated, which represents one major reason for the low QoE. We thus propose name, a machine learning based framework to resolve the issue. Instead of blindly following the transport layer's estimation of network capacity, name reviews historical logs of both layers, and extracts high-level features of codec/network dynamics, based on which it determines the highest bitrates for forthcoming video frames without incurring congestion. To attain the ability, we train name with the aforementioned massive data traces using a custom-designed imitation learning algorithm, which enables name to learn from past experience. We have implemented and incorporated name into appname. Our experiments show that name outperforms state-of-the-art solutions, improving video quality while reducing stalling time by multi-folds under various practical scenarios.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {29},
numpages = {16},
keywords = {bitrate adaptation, imitation learning, video telephony},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inproceedings{10.1145/2934872.2959082,
author = {Fernandes, Eder Le\~{a}o and Antichi, Gianni and Castro, Ignacio and Uhlig, Steve},
title = {Horse: towards an SDN traffic dynamics simulator for large scale networks},
year = {2016},
isbn = {9781450341936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934872.2959082},
doi = {10.1145/2934872.2959082},
abstract = {The Software Defined Networking (SDN) paradigm can be successfully applied to the inter-domain ecosystem to empower network fabrics with finer grained policies and traffic engineering capabilities. However, introducing SDN at the inter-domain level might also lead to misconfigurations with potential to negatively impact on the Internet. Simulators are a popular approach to verify network behaviour and test applications before going into production. In the case of SDN, the available options do not scale for large scale networks nor high traffic loads. In this paper we propose a new simulator to foster SDN research and improve our understanding on the impact of the new use cases over the traffic flow. A simulation tool capable of efficiently reproducing large scale networks, high traffic loads, and policies, by abstracting the interactions between switches and controllers of the SDN network.},
booktitle = {Proceedings of the 2016 ACM SIGCOMM Conference},
pages = {577–578},
numpages = {2},
keywords = {Software Defined Networking; simulation},
location = {Florianopolis, Brazil},
series = {SIGCOMM '16}
}

@inproceedings{10.1145/3090354.3090378,
author = {Chaffai, Abdelmajid and Hassouni, Larbi and Anoun, Houda},
title = {E-Learning Real Time Analysis Using Large Scale Infrastructure},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090378},
doi = {10.1145/3090354.3090378},
abstract = {Real time data analytics is the ability to extract valuable information from live data. It represents a big opportunity to drive smart strategic decisions at the right time. The organizations which adopted this concept, such internet firms, have created spectacular success by embedding reactive actions powered by analytics applied to valuable data. Nowadays, many universities have adopted the distance education in their learning process. E-Learning platforms generate massive data in the form of traces of users' interactions. Face to the fast velocity of data, the traditional methods become costly to collect and analyze the events that are generated continuously. We relied on big data concept to build a large-scale infrastructure that ensures the scalable ingestion, storage of massive data from E-Learning platforms, fast analytics processing and reporting dashboard in near real time. Our infrastructure uses Hadoop and Spark frameworks in backend and data driven document tools in frontend. To validate our system that implements our propositions we have conducted an experiment on Moodle's data, and through the results we can confirm the efficiency of this solution to perform the exploratory analysis on students' data and deliver fresh reports via a live dashboard dedicated to decision makers as support to extract valuable insights in the aim to improve the quality of the E-Learning service.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {23},
numpages = {6},
keywords = {E-learning, Hadoop, Moodle, Real time analytics, Spark, scalable},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/2905055.2905353,
author = {Varudkar, Harsh},
title = {Hadoop based collaborative recommendation system},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905353},
doi = {10.1145/2905055.2905353},
abstract = {21st century is of information and internet. Internet usage is spread across whole globe. Ecommerce market and social media are generator of rapid growth of information and data. Users' view towards market is changing rapidly. One such situation recommendation systems are great tool for users to find the better product or interest based services without crawling the whole internet. Information filtering system have a subclass called recommender systems. Collaborative recommendation system is a type of recommendation system. Recommendation system are not only useful for end users but as per industry aspect it is a very useful for understanding trends and do some analytics. But the data which has to be analyzed is in very huge amount. Analyzing such amount of data may take time as well as data handling is also difficult in normal systems. Distributed environment like Hadoop will provide good scalability to generate recommendation and handle huge amount of data.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {137},
numpages = {3},
keywords = {Cloud Computing, Collaborative filtering, Recommendation system},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/2801694.2801707,
author = {Shuba, Anastasia and Le, Anh and Gjoka, Minas and Varmarken, Janus and Langhoff, Simon and Markopoulou, Athina},
title = {AntMonitor: Network Traffic Monitoring and Real-Time Prevention of Privacy Leaks in Mobile Devices},
year = {2015},
isbn = {9781450337014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801694.2801707},
doi = {10.1145/2801694.2801707},
abstract = {Mobile devices play an essential role in the Internet today, and there is an increasing interest in using them as a vantage point for network measurement from the edge. At the same time, these devices store personal, sensitive information, and there is a growing number of applications that leak it. We propose AntMonitor -- the first system of its kind that supports (i) collection of large-scale, semantic-rich network traffic in a way that respects users' privacy preferences and (ii) detection and prevention of leakage of private information in real time. The first property makes AntMonitor a powerful tool for network researchers who want to collect and analyze large-scale yet fine-grained mobile measurements. The second property can work as an incentive for using AntMonitor and contributing data for analysis. As a proof-of-concept, we have developed a prototype of AntMonitor, deployed it to monitor 9 users for 2 months, and collected and analyzed 20 GB of mobile data from 151 applications. Preliminary results show that fine-grained data collected from AntMonitor could enable application classification with higher accuracy than state-of-the-art approaches. In addition, we demonstrated that AntMonitor could help prevent several apps from leaking private information over unencrypted traffic, including phone numbers, emails, and device identifiers.},
booktitle = {Proceedings of the 2015 Workshop on Wireless of the Students, by the Students, &amp; for the Students},
pages = {25–27},
numpages = {3},
keywords = {android security, mobile network monitoring, privacy leakage detection},
location = {Paris, France},
series = {S3 '15}
}

@inproceedings{10.1145/2789168.2789170,
author = {Shuba, Anastasia and Le, Anh and Gjoka, Minas and Varmarken, Janus and Langhoff, Simon and Markopoulou, Athina},
title = {Demo: AntMonitor: A System for Mobile Traffic Monitoring and Real-Time Prevention of Privacy Leaks},
year = {2015},
isbn = {9781450336192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2789168.2789170},
doi = {10.1145/2789168.2789170},
abstract = {Mobile devices play an essential role in the Internet today, and there is an increasing interest in using them as a vantage point for network measurement from the edge. At the same time, these devices store personal, sensitive information, and there is a growing number of applications that leak it. We propose AntMonitor-- the first system of its kind that supports (i) collection of large-scale, semantic-rich network traffic in a way that respects users' privacy preferences and (ii) detection and prevention of leakage of private information in real time. The first property makes AntMonitor a powerful tool for network researchers who want to collect and analyze large-scale yet fine-grained mobile measurements. The second property can work as an incentive for using AntMonitor and contributing data for analysis. As a proof-of-concept, we have developed a prototype of AntMonitor, deployed it to monitor 9 users for 2 months, and collected and analyzed 20 GB of mobile data from 151 applications. Preliminary results show that fine-grained data collected from AntMonitor could enable application classification with higher accuracy than state-of-the-art approaches. In addition, we demonstrated that AntMonitor could help prevent several apps from leaking private information over unencrypted traffic, including phone numbers, emails, and device identifiers.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
pages = {170–172},
numpages = {3},
keywords = {android security, mobile network monitoring, privacy leakage detection},
location = {Paris, France},
series = {MobiCom '15}
}

@inproceedings{10.1145/2736277.2741627,
author = {Kulkarni, Vivek and Al-Rfou, Rami and Perozzi, Bryan and Skiena, Steven},
title = {Statistically Significant Detection of Linguistic Change},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741627},
doi = {10.1145/2736277.2741627},
abstract = {We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning and usage of words. Such linguistic shifts are especially prevalent on the Internet, where the rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis approach constructs property time series of word usage, and then uses statistically sound change point detection algorithms to identify significant linguistic shifts. We consider and analyze three approaches of increasing complexity to generate such linguistic property time series, the culmination of which uses distributional characteristics inferred from word co-occurrences. Using recently proposed deep neural language models, we first train vector representations of words for each time period. Second, we warp the vector spaces into one unified coordinate system. Finally, we construct a distance-based distributional time series for each word to track its linguistic displacement over time.We demonstrate that our approach is scalable by tracking linguistic change across years of micro-blogging using Twitter, a decade of product reviews using a corpus of movie reviews from Amazon, and a century of written books using the Google Book Ngrams. Our analysis reveals interesting patterns of language usage change commensurate with each medium.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {625–635},
numpages = {11},
keywords = {computational linguistics, web mining},
location = {Florence, Italy},
series = {WWW '15}
}

@inproceedings{10.1145/3226116.3226132,
author = {Xie, Hang and Tang, Tiffany Y.},
title = {Vector projection on lyrics and user comments for a lightweight emotion-aware chinese music recommendation system},
year = {2018},
isbn = {9781450364270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3226116.3226132},
doi = {10.1145/3226116.3226132},
abstract = {With the development of modern internet, information explodes exponentially, which makes people hard to find out what information they really want in a huge data set. In many famous music platforms in China, such as QQ Music, Wangyi Cloud Music, Xiami Music, how to recommend appropriate music to users is a very challenging issue. Most of these recommendation systems are using collaborative filtering method to recommend music. In our paper, we propose a light-weight emotion-aware approach which can analyze emotion based on the song lyric and user comments, via vector projection to decide which song can be recommended to the users in order to maximize their experiences. Two emotion lexicons had been adopted for such purposes. To validate our system, a small-scale user study has been conducted with mixed and interesting results. We offer suggestions on the adoption of each lexicon in the context of user satisfaction and use.},
booktitle = {Proceedings of the 1st International Conference on Big Data Technologies},
pages = {88–94},
numpages = {7},
keywords = {emotion-aware, lyric, recommendation system, user comment, vector projection},
location = {Hangzhou, China},
series = {ICBDT '18}
}

@inproceedings{10.1145/2994459.2994471,
author = {Gupta, Srishti and Gupta, Payas and Ahamad, Mustaque and Kumaraguru, Ponnurangam},
title = {Exploiting Phone Numbers and Cross-Application Features in Targeted Mobile Attacks},
year = {2016},
isbn = {9781450345644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994459.2994471},
doi = {10.1145/2994459.2994471},
abstract = {Smartphones have fueled a shift in the way we communicate with each other via Instant Messaging. With the convergence of Internet and telephony, new Over-The-Top (OTT) messaging applications (e.g., WhatsApp, Viber, WeChat etc.) have emerged as an important means of communication for millions of users. These applications use phone numbers as the only means of authentication and are becoming an attractive medium for attackers to deliver spam and carry out more targeted attacks. The universal reach of telephony along with its past trusted nature makes phone numbers attractive identifiers for reaching potential attack targets. In this paper, we explore the feasibility, automation, and scalability of a variety of targeted attacks that can be carried out by abusing phone numbers. These attacks can be carried out on different channels viz. OTT messaging applications, voice, e-mail, or SMS. We demonstrate a novel system that takes a phone number as an input, leverages information from applications like Truecaller and Facebook about the victim and his / her social network, checks the presence of phone number's owner (victim) on the attack channel (OTT messaging applications, voice, e-mail, or SMS), and finally targets the victim on the chosen attack channel. As a proof of concept, we enumerated through a random pool of 1.16 million phone numbers and demonstrated that targeted attacks could be crafted against the owners of 255,873 phone numbers by exploiting cross-application features. Due to the significantly increased user engagement via new mediums of communication like OTT messaging applications and ease with which phone numbers allow collection of pertinent information, there is a clear need for better protection of applications that rely on phone numbers.},
booktitle = {Proceedings of the 6th Workshop on Security and Privacy in Smartphones and Mobile Devices},
pages = {73–82},
numpages = {10},
keywords = {caller id applications, cross-application features, facebook, over-the-top messaging applications, phishing, phone numbers, security, targeted attacks, truecaller, vanity numbers, vishing},
location = {Vienna, Austria},
series = {SPSM '16}
}

@article{10.1145/2978655,
author = {Zhou, Yipeng and Chen, Liang and Jing, Mi and Zou, Shenglong and Ma, Richard Tianbai},
title = {Design, Implementation, and Measurement of a Crowdsourcing-Based Content Distribution Platform},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2978655},
doi = {10.1145/2978655},
abstract = {Content distribution, especially the distribution of video content, unavoidably consumes bandwidth resources heavily. Internet content providers invest heavily in purchasing content distribution network (CDN) services. By deploying tens of thousands of edge servers close to end users, CDN companies are able to distribute content efficiently and effectively, but at considerable cost. Thus, it is of great importance to develop a new system that distributes content at a lower cost but comparable service quality. In lieu of expensive CDN systems, we implement a crowdsourcing-based content distribution system, Thunder Crystal, by renting bandwidth for content upload/download and storage for content cache from agents. This is a large-scale system with tens of thousands of agents, whose resources significantly amplify Thunder Crystal’s content distribution capacity. The involved agents are either from ordinary Internet users or enterprises. Monetary rewards are paid to agents based on their upload traffic so as to motivate them to keep contributing resources. As far as we know, this is a novel system that has not been studied or implemented before. This article introduces the design principles and implementation details before presenting the measurement study. In summary, with the help of agent devices, Thunder Crystal is able to reduce the content distribution cost by one half and amplify the content distribution capacity by 11 to 15 times.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {nov},
articleno = {80},
numpages = {23},
keywords = {CDN, Crowdsourcing, agent, video distribution}
}

@inproceedings{10.1145/2897845.2897918,
author = {Rahbarinia, Babak and Balduzzi, Marco and Perdisci, Roberto},
title = {Real-Time Detection of Malware Downloads via Large-Scale URL-&gt;File-&gt;Machine Graph Mining},
year = {2016},
isbn = {9781450342339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897845.2897918},
doi = {10.1145/2897845.2897918},
abstract = {In this paper we propose Mastino, a novel defense system to detect malware download events. A download event is a 3-tuple that identifies the action of downloading a file from a URL that was triggered by a client (machine). Mastino utilizes global situation awareness and continuously monitors various network- and system-level events of the clients' machines across the Internet and provides real time classification of both files and URLs to the clients upon submission of a new, unknown file or URL to the system. To enable detection of the download events, Mastino builds a large download graph that captures the subtle relationships among the entities of download events, i.e. files, URLs, and machines. We implemented a prototype version of Mastino and evaluated it in a large-scale real-world deployment. Our experimental evaluation shows that Mastino can accurately classify malware download events with an average of 95.5% true positive (TP), while incurring less than 0.5% false positives (FP). In addition, we show the Mastino can classify a new download event as either benign or malware in just a fraction of a second, and is therefore suitable as a real time defense system.},
booktitle = {Proceedings of the 11th ACM on Asia Conference on Computer and Communications Security},
pages = {783–794},
numpages = {12},
keywords = {graph mining, machine learning, malware detection},
location = {Xi'an, China},
series = {ASIA CCS '16}
}

@article{10.14778/3137628.3137655,
author = {Xie, Dong and Li, Feifei and Phillips, Jeff M.},
title = {Distributed trajectory similarity search},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137655},
doi = {10.14778/3137628.3137655},
abstract = {Mobile and sensing devices have already become ubiquitous. They have made tracking moving objects an easy task. As a result, mobile applications like Uber and many IoT projects have generated massive amounts of trajectory data that can no longer be processed by a single machine efficiently. Among the typical query operations over trajectories, similarity search is a common yet expensive operator in querying trajectory data. It is useful for applications in different domains such as traffic and transportation optimizations, weather forecast and modeling, and sports analytics. It is also a fundamental operator for many important mining operations such as clustering and classification of trajectories. In this paper, we propose a distributed query framework to process trajectory similarity search over a large set of trajectories. We have implemented the proposed framework in Spark, a popular distributed data processing engine, by carefully considering different design choices. Our query framework supports both the Hausdorff distance the Fr\'{e}chet distance. Extensive experiments have demonstrated the excellent scalability and query efficiency achieved by our design, compared to other methods and design alternatives.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1478–1489},
numpages = {12}
}

@inproceedings{10.1145/3274694.3274723,
author = {Garmany, Behrad and Stoffel, Martin and Gawlik, Robert and Koppe, Philipp and Blazytko, Tim and Holz, Thorsten},
title = {Towards Automated Generation of Exploitation Primitives for Web Browsers},
year = {2018},
isbn = {9781450365697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274694.3274723},
doi = {10.1145/3274694.3274723},
abstract = {The growing dependence on software and the increasing complexity of such systems builds and feeds the attack surface for exploitable vulnerabilities. Security researchers put up a lot of effort to develop exploits and analyze existing exploits with the goal of staying ahead of the state-of-the-art in attacks and defenses. The urge for automated systems that operate at scale, speed and efficiency is therefore undeniable. Given their complexity and large user base, web browsers pose an attractive target. Due to various mitigation strategies, the exploitation of a browser vulnerability became a time consuming, multi-step task: creating a working exploit even from a crashing input is a resource-intensive task that can take a substantial amount of time to complete. In many cases, the input, which triggers a vulnerability follows a crashing path but does not enter an exploitable state.In this paper, we introduce novel methods to significantly improve and partially automate the development process for browser exploits. Our approach is based on the observation that an analyst typically performs certain manual analysis steps that can be automated. This serves the purpose to propagate the bug-induced, controlled data to a specific program location to carry out a desired action. These actions include achieving write-what-where or control over the instruction pointer primitives. These are useful to extend control over the target program and are necessities towards successful code execution, the ultimate goal of the adversary. We implemented a prototype of our approach called PrimGen. For a given browser vulnerability, it is capable of automatically crafting data objects that lead the execution to a desired action. We show in our evaluation that our approach is able to generate new and previously unknown exploitation opportunities for real-world vulnerabilities in Mozilla Firefox, Internet Explorer, and Google Chrome. Using small templates, PrimGen generates inputs that conducts specific primitives. In total, PrimGen has found 48 JavaScript inputs which conduct the desired primitives when fed into the target browsers.},
booktitle = {Proceedings of the 34th Annual Computer Security Applications Conference},
pages = {300–312},
numpages = {13},
location = {San Juan, PR, USA},
series = {ACSAC '18}
}

@inproceedings{10.1145/3183440.3194958,
author = {Osses, Felipe and M\'{a}rquez, Gast\'{o}n and Astudillo, Hern\'{a}n},
title = {Exploration of academic and industrial evidence about architectural tactics and patterns in microservices},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3194958},
doi = {10.1145/3183440.3194958},
abstract = {Microservices are quickly becoming an outstanding architectural choice in the service-oriented software industry. This approach proposes to develop each application as a collection of small services, each running on its process and inter-communicating with lightweight mechanisms. Currently, there is still no clear perspective of emerging recurrent solutions (architectural patterns) or design decisions (architectural tactics) in microservices both in industry and academia. This article describes a systematic review of the academic and industrial literature on architectural patterns and tactics proposed for microservices. The study reported: 44 architectural patterns of microservices in academia and 80 in the industry; architectural tactics related to microservices dependent on other disciplines; and it was also found that most of architectural patterns and tactics are associated to five quality attributes: scalability, flexibility, testability, performance, and elasticity. Added to that results, it was noticed that most microservices in the academic area are reported in evidence related to DevOps and IoT, but the industry is not interested in associating disciplines. Finally, a new proposal of microservices pattern taxonomy is suggested.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {256–257},
numpages = {2},
keywords = {academy, architectural patterns, architectural tactics, industry, microservices, systematic literature review, taxonomy},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3357150.3357403,
author = {Ascigil, Onur and Re\~{n}\'{e}, Sergi and Kr\'{o}l, Micha\l{} and Pavlou, George and Zhang, Lixia and Hasegawa, Toru and Koizumi, Yuki and Kita, Kentaro},
title = {Towards Peer-to-Peer Content Retrieval Markets: Enhancing IPFS with ICN},
year = {2019},
isbn = {9781450369701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357150.3357403},
doi = {10.1145/3357150.3357403},
abstract = {In the current Internet, content delivery, e.g., video-on-demand (VoD), at scale is associated with a large distributed infrastructure which requires considerable investment. Content Providers (CPs) typically resort to third-party Content Distribution Networks (CDNs) or build their own expensive content delivery infrastructure in order to cope with the peak demand and maintain sufficient quality-of-service (QoS), while Internet Service Providers (ISPs) need to overprovision their networks. In this paper we take a first step towards designing a system that uses storage space of users as CDN caches and deliver content with sufficient (i.e., CDN-like) quality while rewarding users for their resource usage as in a content retrieval marketplace. As a possible candidate for such a system, we consider recent P2P storage and delivery systems that have adopted new mechanisms such as rewarding of useful work (e.g., storage) while ensuring fairness and accountability through cryptographic proofs. In this paper, we experiment with the popular Interplanetary File System (IPFS) and investigate its performance in delivering VoD content locally within an ISP. Our findings suggest that operating IPFS (operating on top of IP) has its performance limitations and complementing it with an ICN network layer can significantly improve the delivery quality. We then propose and compare several forwarding strategies for ICN which can efficiently route requests and balance the load between peers with limited uplink resources.},
booktitle = {Proceedings of the 6th ACM Conference on Information-Centric Networking},
pages = {78–88},
numpages = {11},
location = {Macao, China},
series = {ICN '19}
}

@inproceedings{10.1145/3006299.3006311,
author = {Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\'{\i}n-Torder, Eva},
title = {Towards a comprehensive data lifecycle model for big data environments},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006311},
doi = {10.1145/3006299.3006311},
abstract = {A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {100–106},
numpages = {7},
keywords = {big data, data complexity, data lifecycle, data management, data organization, vs challenges},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/2939672.2939676,
author = {Sun, Yu and Yuan, Nicholas Jing and Wang, Yingzi and Xie, Xing and McDonald, Kieran and Zhang, Rui},
title = {Contextual Intent Tracking for Personal Assistants},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939676},
doi = {10.1145/2939672.2939676},
abstract = {A new paradigm of recommendation is emerging in intelligent personal assistants such as Apple's Siri, Google Now, and Microsoft Cortana, which recommends "the right information at the right time" and proactively helps you "get things done". This type of recommendation requires precisely tracking users' contemporaneous intent, i.e., what type of information (e.g., weather, stock prices) users currently intend to know, and what tasks (e.g., playing music, getting taxis) they intend to do. Users' intent is closely related to context, which includes both external environments such as time and location, and users' internal activities that can be sensed by personal assistants. The relationship between context and intent exhibits complicated co-occurring and sequential correlation, and contextual signals are also heterogeneous and sparse, which makes modeling the context intent relationship a challenging task. To solve the intent tracking problem, we propose the Kalman filter regularized PARAFAC2 (KP2) nowcasting model, which compactly represents the structure and co-movement of context and intent. The KP2 model utilizes collaborative capabilities among users, and learns for each user a personalized dynamic system that enables efficient nowcasting of users' intent. Extensive experiments using real-world data sets from a commercial personal assistant show that the KP2 model significantly outperforms various methods, and provides inspiring implications for deploying large-scale proactive recommendation systems in personal assistants.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {273–282},
numpages = {10},
keywords = {context-aware recommendation, intelligent personal assistant, intent tracking, multi-task learning, nowcasting, proactive triggers},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2815675.2815680,
author = {Wang, Huandong and Xu, Fengli and Li, Yong and Zhang, Pengyu and Jin, Depeng},
title = {Understanding Mobile Traffic Patterns of Large Scale Cellular Towers in Urban Environment},
year = {2015},
isbn = {9781450338486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815675.2815680},
doi = {10.1145/2815675.2815680},
abstract = {Understanding mobile traffic patterns of large scale cellular towers in urban environment is extremely valuable for Internet service providers, mobile users, and government managers of modern metropolis. This paper aims at extracting and modeling the traffic patterns of large scale towers deployed in a metropolitan city. To achieve this goal, we need to address several challenges, including lack of appropriate tools for processing large scale traffic measurement data, unknown traffic patterns, as well as handling complicated factors of urban ecology and human behaviors that affect traffic patterns. Our core contribution is a powerful model which combines three dimensional information (time, locations of towers, and traffic frequency spectrum) to extract and model the traffic patterns of thousands of cellular towers. Our empirical analysis reveals the following important observations. First, only five basic time-domain traffic patterns exist among the 9,600 cellular towers. Second, each of the extracted traffic pattern maps to one type of geographical locations related to urban ecology, including residential area, business district, transport, entertainment, and comprehensive area. Third, our frequency domain traffic spectrum analysis suggests that the traffic of any tower among the 9,600 can be constructed using a linear combination of four primary components corresponding to human activity behaviors. We believe that the proposed traffic patterns extraction and modeling methodology, combined with the empirical analysis on the mobile traffic, pave the way toward a deep understanding of the traffic patterns of large scale cellular towers in modern metropolis.},
booktitle = {Proceedings of the 2015 Internet Measurement Conference},
pages = {225–238},
numpages = {14},
keywords = {geographical location, measurement study, mobile data traffic, traffic patterns},
location = {Tokyo, Japan},
series = {IMC '15}
}

@article{10.1109/TNET.2016.2623950,
author = {Xu, Fengli and Li, Yong and Wang, Huandong and Zhang, Pengyu and Jin, Depeng},
title = {Understanding Mobile Traffic Patterns of Large Scale Cellular Towers in Urban Environment},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2623950},
doi = {10.1109/TNET.2016.2623950},
abstract = {Understanding mobile traffic patterns of large scale cellular towers in urban environment is extremely valuable for Internet service providers, mobile users, and government managers of modern metropolis. This paper aims at extracting and modeling the traffic patterns of large scale towers deployed in a metropolitan city. To achieve this goal, we need to address several challenges, including lack of appropriate tools for processing large scale traffic measurement data, unknown traffic patterns, as well as handling complicated factors of urban ecology and human behaviors that affect traffic patterns. Our core contribution is a powerful model which combines three dimensional information time, locations of towers, and traffic frequency spectrum to extract and model the traffic patterns of thousands of cellular towers. Our empirical analysis reveals the following important observations. First, only five basic time-domain traffic patterns exist among the 9600 cellular towers. Second, each of the extracted traffic pattern maps to one type of geographical locations related to urban ecology, including residential area, business district, transport, entertainment, and comprehensive area. Third, our frequency-domain traffic spectrum analysis suggests that the traffic of any tower among 9600 can be constructed using a linear combination of four primary components corresponding to human activity behaviors. We believe that the proposed traffic patterns extraction and modeling methodology, combined with the empirical analysis on the mobile traffic, pave the way toward a deep understanding of the traffic patterns of large scale cellular towers in modern metropolis.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {1147–1161},
numpages = {15}
}

@inproceedings{10.1145/3131365.3131401,
author = {Amann, Johanna and Gasser, Oliver and Scheitle, Quirin and Brent, Lexi and Carle, Georg and Holz, Ralph},
title = {Mission accomplished? HTTPS security after diginotar},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131401},
doi = {10.1145/3131365.3131401},
abstract = {Driven by CA compromises and the risk of man-in-the-middle attacks, new security features have been added to TLS, HTTPS, and the web PKI over the past five years. These include Certificate Transparency (CT), for making the CA system auditable; HSTS and HPKP headers, to harden the HTTPS posture of a domain; the DNS-based extensions CAA and TLSA, for control over certificate issuance and pinning; and SCSV, for protocol downgrade protection.This paper presents the first large scale investigation of these improvements to the HTTPS ecosystem, explicitly accounting for their combined usage. In addition to collecting passive measurements at the Internet uplinks of large University networks on three continents, we perform the largest domain-based active Internet scan to date, covering 193M domains. Furthermore, we track the long-term deployment history of new TLS security features by leveraging passive observations dating back to 2012.We find that while deployment of new security features has picked up in general, only SCSV (49M domains) and CT (7M domains) have gained enough momentum to improve the overall security of HTTPS. Features with higher complexity, such as HPKP, are deployed scarcely and often incorrectly. Our empirical findings are placed in the context of risk, deployment effort, and benefit of these new technologies, and actionable steps for improvement are proposed. We cross-correlate use of features and find some techniques with significant correlation in deployment. We support reproducible research and publicly release data and code.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {325–340},
numpages = {16},
keywords = {CAA, CT, HPKP, HSTS, HTTPS, PKI, SCSV, TLS, X.509},
location = {London, United Kingdom},
series = {IMC '17}
}

@inproceedings{10.1145/2856636.2876471,
author = {Zodik, Gabi},
title = {Cognitive and Contextual Enterprise Mobile Computing: Invited Keynote Talk},
year = {2016},
isbn = {9781450340182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856636.2876471},
doi = {10.1145/2856636.2876471},
abstract = {The second wave of change presented by the age of mobility, wearables, and IoT focuses on how organizations and enterprises, from a wide variety of commercial areas and industries, will use and leverage the new technologies available. Businesses and industries that don't change with the times will simply cease to exist.Applications need to be powered by cognitive and contextual technologies to support real-time proactive decisions. These decisions will be based on the mobile context of a specific user or group of users, incorporating location, time of day, current user task, and more. Driven by the huge amounts of data produced by mobile and wearables devices, and influenced by privacy concerns, the next wave in computing will need to exploit data and computing at the edge of the network. Future mobile apps will have to be cognitive to 'understand' user intentions based on all the available interactions and unstructured data.Mobile applications are becoming increasingly ubiquitous, going beyond what end users can easily comprehend. Essentially, for both business-to-client (B2C) and business-to-business (B2B) apps, only about 30% of the development efforts appear in the interface of the mobile app. For example, areas such as the collaborative nature of the software or the shortened development cycle and time-to-market are not apparent to end users. The other 70% of the effort invested is dedicated to integrating the applications with back-office systems and developing those aspects of the application that operate behind the scenes.An important, yet often complex, part of the solution and mobile app takes place far from the public eye-in the back-office environment. It is there that various aspects of customer relationship management must be addressed: tracking usage data, pushing out messaging as needed, distributing apps to employees within the enterprise, and handling the wide variety of operational and management tasks-often involving the collection and monitoring of data from sensors and wearable devices. All this must be carried out while addressing security concerns that range from verifying user identities, to data protection, to blocking attempted breaches of the organization, and activation of malicious code. Of course, these tasks must be augmented by a systematic approach and vigilant maintenance of user privacy.The first wave of the mobile revolution focused on development platforms, run-time platforms, deployment, activation, and management tools for multi-platform environments, including comprehensive mobile device management (MDM). To realize the full potential of this revolution, we must capitalize on information about the context within which mobile devices are used. With both employees and customers, this context could be a simple piece of information such as the user location or time of use, the hour of the day, or the day of the week. The context could also be represented by more complex data, such as the amount of time used, type of activity performed, or user preferences. Further insight could include the relationship history with the user and the user's behavior as part of that relationship, as well as a long list of variables to be considered in various scenarios. Today, with the new wave of wearables, the definition of context is being further extended to include environmental factors such as temperature, weather, or pollution, as well as personal factors such as heart rate, movement, or even clothing worn.In both B2E and B2C situations, a context-dependent approach, based on the appropriate context for each specific user, offers a superior tool for working with both employees and clients alike. This mode of operation does not start and end with the individual user. Rather, it takes into account the people surrounding the user, the events taking place nearby, appliances or equipment activated, the user's daily schedule, as well as other, more general information, such as the environment and weather.Developing enterprise-wide, context-dependent, mobile solutions is still a complex challenge. A system of real added-value services must be developed, as well as a comprehensive architecture. These four-tier architectures comprise end-user devices like wearables and smartphones, connected to systems of engagement (SoEs), and systems of record (SoRs). All this is needed to enable data analytics and collection in the context where it is created. The data collected will allow further interaction with employees or customers, analytics, and follow-up actions based on the results of that analysis. We also need to ensure end-to-end (E2E) security across these four tiers, and to keep the data and application contexts in sync. These are just some of the challenges being addressed by IBM Research.As an example, these technologies could be deployed in the retail space, especially in brick-and-mortar stores. Identifying a customer entering a store, detecting her location among the aisles, and cross-referencing that data with the customer's transaction history, could lead to special offers tailor-made for that specific customer or suggestions relevant to her purchasing process. This technology enables real-world implementation of metrics, analytics, and other tools familiar to us from the online realm. We can now measure visits to physical stores in the same way we measure web page hits: analyze time spent in the store, the areas visited by the customer, and the results of those visits. In this way, we can also identify shoppers wandering around the store and understand when they are having trouble finding the product they want to purchase. We can also gain insight into the standard traffic patterns of shoppers and how they navigate a store's floors and departments. We might even consider redesigning the store layout to take advantage of this insight to enhance sales.In healthcare, the context can refer to insight extracted from data received from sensors on the patient, from either his mobile device or wearable technology, and information about the patient's environment and location at that moment in time. This data can help determine if any assistance is required. For example, if a patient is discharged from the hospital for continued at-home care, doctors can continue to remotely monitor his condition via a system of sensors and analytic tools that interpret the sensor readings.This approach can also be applied to the area of safety. Scientists at IBM Research are developing a platform that collects and analyzes data from wearable technology to protect the safety of employees working in construction, heavy industry, manufacturing, or out in the field. This solution can serve as a real-time warning system by analyzing information gathered from wearable sensors embedded in personal protective equipment, such as smart safety helmets and protective vests, and in the workers' individual smartphones. These sensors can continuously monitor a worker's pulse rate, movements, body temperature, and hydration level, as well as environmental factors such as noise level, and other parameters. The system can provide immediate alerts to the worker about any dangers in the work environment to prevent possible injury. It can also be used to prevent accidents before they happen or detect accidents once they occur. For example, with sophisticated algorithms, we can detect if a worker falls based on a sudden difference in elevations detected by an accelerometer, and then send an alert to notify her peers and supervisor or call for help. Monitoring can also help ensure safety in areas where continuous exposure to heat or dangerous materials must be limited based on regulated time periods.Mobile technologies can also help manage events with massive numbers of participants, such as professional soccer games, music festivals, and even large-scale public demonstrations, by sending alerts concerning long and growing lines or specific high-traffic areas. These technologies can be used to detect accidents typical of large-scale gatherings, send warnings about overcrowding, and alert the event organizers. In the same way, they can alleviate parking problems or guide public transportation operators- all via analysis and predictive analytics.IBM Research - Haifa is currently involved in multiple activities as part of IBM's MobileFirst initiative. Haifa researchers have a special expertise in time- and location-based intelligent applications, including visual maps that display activity contexts and predictive analytics systems for mobile data and users. In another area, IBM researchers in Haifa are developing new cognitive services driven from the unique data available on mobile and wearable devices. Looking to the future, the IBM Research team is further advancing the integration of wearable technology, augmented reality systems, and biometric tools for mobile user identity validation.Managing contextual data and analyzing the interaction between the different kinds of data presents fascinating challenges for the development of next-generation programming. For example, we need to rethink when and where data processing and computations should occur: Is it best to leave them at the user-device level, or perhaps they should be moved to the back-office systems, servers, and/or the cloud infrastructures with which the user device is connected? New-age applications are becoming more and more distributed. They operate on a wide range of devices, such as wearable technologies, use a variety of sensors, and depend on cloud-based systems.As a result, a new distributed programming paradigm is emerging to meet the needs of these use-cases and real-time scenarios. This paradigm needs to deal with massive amounts of devices, sensors, and data in business systems, and must be able to shift computation from the cloud to the edge, based on context in close to real-time. By processing data at the edge of the network, close to where the interactions and processing are happening, we can help reduce latency and offer new opportunities for improved privacy and security.Despite all these interactions, data collection, and the analytic insights based upon them-we cannot forget the issues of privacy. Without a proper and reliable solution that offers more control over what personal data is shared and how it is used, people will refrain from sharing information. Such sharing is necessary for developing and understanding the context in which people are carrying out various actions, and to offer them tools and services to enhance their actions.In the not-so-distant future, we anticipate the appearance of ad-hoc networks for wearable technology systems that will interact with one another to further expand the scope and value of available context-dependent data.},
booktitle = {Proceedings of the 9th India Software Engineering Conference},
pages = {11–12},
numpages = {2},
location = {Goa, India},
series = {ISEC '16}
}

@inproceedings{10.1145/3168836.3168841,
author = {Pedreschi, Dino},
title = {Social network analytics, data science ethics &amp; privacy-preserving analytics},
year = {2017},
isbn = {9781450373159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168836.3168841},
doi = {10.1145/3168836.3168841},
abstract = {GoalsOver the past decade there has been a growing public fascination with the complex "connectedness" of modern society. This connectedness is found in many contexts: in the rapid growth of the Internet and the Web, in the ease with which global communication now takes place, and in the ability of news and information as well as epidemics and financial crises to spread around the world with surprising speed and intensity. These are phenomena that involve networks and the aggregate behavior of groups of people; they are based on the links that connect us and the ways in which each of our decisions can have subtle consequences for the outcomes of everyone else.This crash course is an introduction to the analysis of complex networks, made possible by the availability of big data, with a special focus on the social network and its structure and function. Drawing on ideas from computing and information science, complex systems, mathematic and statistical modelling, economics and sociology, this lecture sketchily describes the emerging field of study that is growing at the interface of all these areas, addressing fundamental questions about how the social, economic, and technological worlds are connected.Syllabus•Big graph data and social, information, biological and technological networks•The architecture of complexity and how real networks differ from random networks: node degree and long tails, social distance and small worlds, clustering and triadic closure. Comparing real networks and random graphs. The main models of network science: small world and preferential attachment.•Strong and weak ties, community structure and long-range bridges. Robustness of networks to failures and attacks. Cascades and spreading. Network models for diffusion and epidemics. The strength of weak ties for the diffusion of information. The strength of strong ties for the diffusion of innovation.•Practical network analytics with Cytoscape and Gephi. Simulation of network processes with NetLogo.Reference TextbooksDavid Easley, Jon Kleinberg: Networks, Crowds, and Markets (2010)http://www.cs.cornell.edu/home/kleinber/networks-book/Albert-Laszlo Barabasi. Network Science (2016)http://barabasi.com/book/network-scienceNetwork Analytics SoftwareVisual Analytics: Cytoscape http://www.cytoscape.org/Gephi http://gephi.github.io/Network Simulation: NetLogohttps://ccl.northwestern.edu/netlogo/Data science ethics &amp; privacy-preserving analyticsData science created unprecedented opportunities but also new risks. Data science techniques might expose sensitive traits of individuals and invade their privacy, this information could be used to discriminate people based on their presumed characteristics, or profiles. Sophisticated data driven machine learning algorithms yield classification and prediction models of behavioral traits of individuals, such as credit score, insurance risk, health status, personal preferences and orientations, on the basis of personal data disseminated in the digital environment by users, with orsometimes without their awareness. Such automated decision-making systems are often "black boxes", mapping user's features into a class label or a ranking value without exposing the reasons .This is worrying not only for the lack of transparency, which undermines the trust of stakeholders, but also for possible social biases and prejudices hidden in the training data and learned by the algorithms, which may bring to discriminatory decisions or unfair actions. Gartner says that, within 2018, half of business ethics violations will occur through improper use of Big Data analytics .Often, the achievements of data science are the result of re-interpreting available data for analysis goals that differ from the original reasons motivating data collection. Examples include mobile phone call records, originally collected by telecom operators for billing and operations, used for accurate and timely demography and human mobility analysis at country orregional scale. This re-purposing of data clearly shows the importance of legal compliance and data ethics technologies and safeguards to protect privacy and anonymity, secure data, engage users, avoid discrimination and misuse, account for transparency and fair use - to the purpose of seizing the opportunities of data science while controlling the associated risks. This is the focus of my lecture.Syllabus• Fairness, Accountability, Confidentiality, Accuracy: the ethical challenges of data science • Privacy-preserving data mining • Privacy-by-design and data-driven risk assessment • Democratizing data science: centralised vs. user-centric analytics • Personal data analytics, collective awareness • Algorithmic bias and ethical challenges of machine learning • Discrimination-aware data mining},
booktitle = {1st Europe Summer School: Data Science},
articleno = {5},
location = {Athens, Greece},
series = {SummerSchool '17}
}

@inproceedings{10.1145/3145574.3145588,
author = {Blandin, Nicole and Colglazier, Carl and O'Hare, John and Brenner, Paul},
title = {Parallel Python for Agent-Based Modeling at a Global Scale},
year = {2017},
isbn = {9781450352697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3145574.3145588},
doi = {10.1145/3145574.3145588},
abstract = {We introduce a global scale parallel python modeling platform alongside an example global migration model. Our goals focus on improving social scientist access to computationally robust infrastructure, which enhances a scientist's capability to model more complex macro-scale global effects or larger numbers of micro-scale agents and behaviors. We built the model based on a subset of known social and economic factors to test initial computational scalability which we have kept linear with respect to agent population. The Jupyter simulation platform and Python programming language allow for a familiar developer and user interface via a standard internet browser with the computation performed remotely on high performance server hardware. We have successfully scaled to seven billion agents on a multi-core large memory server. The platform has an open license and we are working to enhance modularity for support of new global scale social models.},
booktitle = {Proceedings of the 2017 International Conference of The Computational Social Science Society of the Americas},
articleno = {10},
numpages = {7},
keywords = {Jupyter, Python, agent-based modelling, computational social science, global open simulator, human migration, parallel computing},
location = {Santa Fe, NM, USA},
series = {CSS 2017}
}

@book{10.1145/3277669,
author = {Jacobson, Ivar and Lawson, Harold "Bud" and Ng, Pan-Wei and McMahon, Paul E. and Goedicke, Michael},
title = {The Essentials of Modern Software Engineering: Free the Practices from the Method Prisons!},
year = {2019},
isbn = {9781947487277},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
abstract = {The first course in software engineering is the most critical. Education must start from an understanding of the heart of software development, from familiar ground that is common to all software development endeavors. This book is an in-depth introduction to software engineering that uses a systematic, universal kernel to teach the essential elements of all software engineering methods.This kernel, Essence, is a vocabulary for defining methods and practices. Essence was envisioned and originally created by Ivar Jacobson and his colleagues, developed by Software Engineering Method and Theory (SEMAT) and approved by The Object Management Group (OMG) as a standard in 2014. Essence is a practiceindependent framework for thinking and reasoning about the practices we have and the practices we need. Essence establishes a shared and standard understanding of what is at the heart of software development. Essence is agnostic to any particular method, lifecycle independent, programming language independent, concise, scalable, extensible, and formally specified. Essence frees the practices from their method prisons.The first part of the book describes Essence, the essential elements to work with, the essential things to do and the essential competencies you need when developing software. The other three parts describe more and more advanced use cases of Essence. Using real but manageable examples, it covers the fundamentals of Essence and the innovative use of serious games to support software engineering. It also explains how current practices such as user stories, use cases, Scrum, and microservices can be described using Essence, and illustrates how their activities can be represented using the Essence notions of cards and checklists. The fourth part of the book offers a vision how Essence can be scaled to support large, complex systems engineering.Essence is supported by an ecosystem developed and maintained by a community of experienced people worldwide. From this ecosystem, professors and students can select what they need and create their own way of working, thus learning how to create ONE way of working that matches the particular situation and needs.}
}

@inproceedings{10.1145/3130265.3130318,
author = {Yokota, Minato and Saso, Kaoru and Hara-Azumi, Yuko},
title = {One-instruction set computer-based multicore processors for energy-efficient streaming data processing},
year = {2017},
isbn = {9781450354189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3130265.3130318},
doi = {10.1145/3130265.3130318},
abstract = {For architecture designs, flexibility of application-dependent optimization for better performance and energy-efficiency and productivity enhanced by application-independent versatility and reusability are both crucial but contradicting issues. In the IoT era, due to a more stringent energy constraint and more application diversity, such issues are becoming more difficult to satisfy. Even recent embedded processors prioritize the design-productivity over flexibility, leading to a lot of energy waste in unused resources for some applications. This paper proposes novel multicore processors to address the above two issues. Our processors are composed of application-independent tiny cores and application-dependent optimizable inter-core communications, which efficiently execute applications on a large amount of streaming data, in a pipeline manner. In this work, we utilize one of the simplest RISC processors, One-Instruction Set Computer (OISC), as a core. Our evaluation demonstrates that our processors outperform an existing RISC processor in terms of performance (throughput) and energy-efficiency, while having sufficient scalability, for two different types of applications.},
booktitle = {Proceedings of the 28th International Symposium on Rapid System Prototyping: Shortening the Path from Specification to Prototype},
pages = {71–77},
numpages = {7},
keywords = {OISC, multicore processor, one-instruction set computer, stream-data processing},
location = {Seoul, South Korea},
series = {RSP '17}
}

@inproceedings{10.1145/3230543.3230549,
author = {Dhamdhere, Amogh and Clark, David D. and Gamero-Garrido, Alexander and Luckie, Matthew and Mok, Ricky K. P. and Akiwate, Gautam and Gogia, Kabir and Bajpai, Vaibhav and Snoeren, Alex C. and Claffy, Kc},
title = {Inferring persistent interdomain congestion},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230549},
doi = {10.1145/3230543.3230549},
abstract = {There is significant interest in the technical and policy communities regarding the extent, scope, and consumer harm of persistent interdomain congestion. We provide empirical grounding for discussions of interdomain congestion by developing a system and method to measure congestion on thousands of interdomain links without direct access to them. We implement a system based on the Time Series Latency Probes (TSLP) technique that identifies links with evidence of recurring congestion suggestive of an under-provisioned link. We deploy our system at 86 vantage points worldwide and show that congestion inferred using our lightweight TSLP method correlates with other metrics of interconnection performance impairment. We use our method to study interdomain links of eight large U.S. broadband access providers from March 2016 to December 2017, and validate our inferences against ground-truth traffic statistics from two of the providers. For the period of time over which we gathered measurements, we did not find evidence of widespread endemic congestion on interdomain links between access ISPs and directly connected transit and content providers, although some such links exhibited recurring congestion patterns. We describe limitations, open challenges, and a path toward the use of this method for large-scale third-party monitoring of the Internet interconnection ecosystem.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {1–15},
numpages = {15},
keywords = {internet congestion, internet topology, performance},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@inproceedings{10.1145/2872518.2888599,
author = {Jiang, Lu},
title = {Web-scale Multimedia Search for Internet Video Content},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2888599},
doi = {10.1145/2872518.2888599},
abstract = {The World Wide Web has been witnessing an explosion of video content. Video data are becoming one of the most valuable sources to assess insights and information. However, existing video search methods are still based on text matching (text-to-text search), and could fail for the huge volumes of videos that have little relevant metadata or no metadata at all. In this paper, we propose an accurate, efficient and scalable semantic search method for Internet videos that allows for intelligent and flexible search schemes over the video content (text-to-video search and text&amp;video-to-video search). To achieve this ambitious goal, we propose several novel methods to improve accuracy and efficiency. The extensive experiments demonstrate that the proposed methods are able to surpass state-of-the-art accuracy and efficiency on multiple datasets. Based on the proposed methods, we implement E-Lamp Lite, the first of its kind large-scale semantic search engine for Internet videos. According to National Institute of Standards and Technology (NIST), it achieved the best accuracy in the TRECVID Multimedia Event Detection (MED) 2013, 2014 and 2015, one of the most representative task for content-based video search. To the best of our knowledge, E-Lamp Lite is the first content-based semantic search system that is capable of indexing and searching a collection of 100 million videos.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {311–316},
numpages = {6},
keywords = {big data, content-based retrieval, multimedia event detection, video content analysis, web search},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/3018896.3066908,
author = {Bounceur, Ahc\`{e}ne},
title = {From smart-city and IoT simulation to big data generation},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3066908},
doi = {10.1145/3018896.3066908},
abstract = {Our world is digitized everyday and increasingly. In 2020, it is expected that over 70% of the population will live in or around cities. To guarantee a good quality of life, it is necessary to ensure fast and reliable services in all areas, in particular those which are mainly based on the use of connected objects. This is one of the cornerstones of a smart city project. It will make possible to provide close to real-time the remote monitoring of sick patients, the monitoring of the environment in order to know its evolution over time and to anticipate developments that can be harmful to health and the environment itself, and to accurately analyze the signals transmitted by the on-board sensors.To further develop domains such as eHealth or the monitoring of other networks in the context of Smart Cities, fast and reliable design tools are needed. Their objectives are to study the realizability of such networks, their behavior in terms of energy consumption, safety, cost and other reliability parameters.This keynote aims to present a new platform called CupCarbon that allows to design systems of connected objects mainly representing sensors and to prepare future deployments of large-scale IoT infrastructures for Smart cities in optimal conditions. This kind of platforms will be a part of systems in the world that will participate in the generation of Big Data.1},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {3},
numpages = {1},
keywords = {alpha-stable distribution, cupcarbon simulation, interference, radio propagation channel, visibility tree},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.5555/2959355.2959417,
author = {Gnawali, Omprakash and Moss, David and Shirkalin, Dmitry and Clark, Russ and Jones, Brian and Eason, William},
title = {Scaling IoT device APIs and analytics: poster abstract},
year = {2016},
isbn = {9781509008025},
publisher = {IEEE Press},
abstract = {Many IoT applications consist of two types of actions: interaction with the device, which can be sensors or actuators, and interaction with the data, for example, to reveal insights. In this poster, we introduce a software stack that provides these functionalities in a scalable manner. The API for device interaction is designed with generality in mind so that widest possible array of devices are supported and in large numbers. The analytics framework, called Composer, is designed to allow user code to be easily integrated into data analytics. We present the design, describe the implementation and deployment, and present some evaluation results. We share the performance data from a live deployment with tens of thousands of active users to demonstrate the scalability of the design.},
booktitle = {Proceedings of the 15th International Conference on Information Processing in Sensor Networks},
articleno = {62},
numpages = {2},
location = {Vienna, Austria},
series = {IPSN '16}
}

@inproceedings{10.1145/3365871.3365901,
author = {Bienz, Simon and Ciortea, Andrei and Mayer, Simon and Gandon, Fabien and Corby, Olivier},
title = {Escaping the Streetlight Effect: Semantic Hypermedia Search Enhances Autonomous Behavior in the Web of Things},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365901},
doi = {10.1145/3365871.3365901},
abstract = {The integration of systems of autonomous agents in Web of Things (WoT) environments is a promising approach to provide and distribute intelligence in world-wide pervasive systems. A central problem then is to enable autonomous agents to discover heterogeneous resources in large-scale, dynamic WoT environments. This is true in particular if an environment relies on open-standards and evolves rapidly requiring agents to adapt their behavior to achieve their goals. To this end, we developed a search engine for the WoT that allows autonomous agents to perform approximate search queries in order to find relevant resources in their environment in (weak) real time. The search engine crawls dynamic WoT environments to discover and index device metadata described with the W3C WoT Thing Description, and exposes a SPARQL endpoint that agents can use for approximate search. To demonstrate the feasibility of our approach, we implemented a prototype application for the maintenance of industrial robots in world-wide manufacturing systems. The prototype demonstrates that our semantic hypermedia search engine enhances the flexibility and agility of autonomous agents in the WoT.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {28},
numpages = {8},
keywords = {Autonomous Agents, Hypermedia, Search, Semantic Web, Web of Things},
location = {Bilbao, Spain},
series = {IoT '19}
}

@inproceedings{10.1145/3319535.3345650,
author = {Wails, Ryan and Johnson, Aaron and Starin, Daniel and Yerukhimovich, Arkady and Gordon, S. Dov},
title = {Stormy: Statistics in Tor by Measuring Securely},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3345650},
doi = {10.1145/3319535.3345650},
abstract = {Tor is a tool for Internet privacy with millions of daily users. The Tor system benefits in many ways from information gathered about the operation of its network. Measurements guide operators in diagnosing problems, direct the efforts of developers, educate users about the level of privacy they obtain, and inform policymakers about Tor's impact. However, data collection and reporting can degrade user privacy, contradicting Tor's goals. Existing approaches to measuring Tor have limited capabilities and security weaknesses. We present Stormy, a general-purpose, privacy-preserving measurement system that overcomes these limitations. Stormy uses secure multiparty computation (MPC) to compute any function of the observations made by Tor relays, while keeping those observations secret. Stormy makes use of existing efficient MPC protocols that are secure in the malicious model, and in addition it includes a novel input-sharing protocol that is secure, efficient, and fault tolerant. The protocol is non-interactive, which is consistent with how relays currently submit measurements, and it allows the relays to go offline after input submission, even while ensuring that an honest relay will not have its input excluded or modified. The input-sharing protocol is compatible with MPC protocols computing on authenticated values and may be of independent interest. We show how Stormy can be deployed in two realistic models: (1) run primarily by a small set of dedicated authorities, or (2) run decentralized across the relays in the Tor network. Stormy scales efficiently to Tor's thousands of relays, tolerates network churn, and provides security depending only on either Tor's existing trust assumption that at least one authority is honest (in the first model) or the existing assumption that a large fraction of relay bandwidth is honest (in the second model). We demonstrate how to use the system to compute two broadly-applicable statistics: the median of relay inputs and the cardinality of set-union across relays. We implement Stormy and experimentally evaluate system performance. When Stormy is run among authorities we can perform 151 median computations or 533 set-union cardinalities over 7,000 relay inputs in a single day. When run among the relays themselves, Stormy can perform 36 median computations or 134 set union cardinalities per day. Thus, both deployments enable non-trivial analytics to be securely computed in the Tor network.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {615–632},
numpages = {18},
keywords = {Tor, cryptographic protocols, secure multi-party computation},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1145/2710021,
author = {Wang, Hua and Nie, Feiping and Huang, Heng},
title = {Large-Scale Cross-Language Web Page Classification via Dual Knowledge Transfer Using Fast Nonnegative Matrix Trifactorization},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2710021},
doi = {10.1145/2710021},
abstract = {With the rapid growth of modern technologies, Internet has reached almost every corner of the world. As a result, it becomes more and more important to manage and mine information contained in Web pages in different languages. Traditional supervised learning methods usually require a large amount of training data to obtain accurate and robust classification models. However, labeled Web pages did not increase as fast as the growth of Internet. The lack of sufficient training Web pages in many languages, especially for those in uncommonly used languages, makes it a challenge for traditional classification algorithms to achieve satisfactory performance. To address this, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. In addition, we also observe that the associations between word clusters and Web page classes are another type of reliable carriers to transfer knowledge across languages. With these recognitions, in this article we propose a novel joint nonnegative matrix trifactorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification. Our approach transfers knowledge from the auxiliary language, in which abundant labeled Web pages are available, to the target languages, in which we want to classify Web pages, through two different paths: word cluster approximation and the associations between word clusters and Web page classes. With the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. In order to deal with the large-scale real world data, we further develop the proposed DKT approach by constraining the factor matrices of NMTF to be cluster indicator matrices. Due to the nature of cluster indicator matrices, we can decouple the proposed optimization objective and the resulted subproblems are of much smaller sizes involving much less matrix multiplications, which make our new approach much more computationally efficient. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results have demonstrated the effectiveness of our approach that are consistent with our theoretical analyses.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {1},
numpages = {29},
keywords = {Cross-language classification, cluster indicator matrix, knowledge transfer, large-scale data, nonnegative matrix trifactorization}
}

@inproceedings{10.1145/3173225.3173246,
author = {Anderson, Zann and Jones, Michael and Seppi, Kevin},
title = {W.O.U.S.: Widgets of Unusual Size},
year = {2018},
isbn = {9781450355681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173225.3173246},
doi = {10.1145/3173225.3173246},
abstract = {Recent work in tangible interfaces, including widget sets like .NET Gadgeteer and Phidgets, has enabled prototyping of rich physical interaction at a handheld or tabletop scale. But it remains unclear how participants respond to physical widgets at larger scales. What kinds of interaction would larger widgets enable, and what kinds of systems - if any - can or should be built with them? We built unusually-sized widgets, or "mega-widgets" in order to explore this territory. We present the results of two iterations of building mega-widgets and accompanying user studies designed to help understand participants» reactions to mega-widgets and probe possible applications. Responses indicated, among other things, a correlation between widget size and the perceived size or importance of what it might control. Mega-widgets were also perceived as increasing the precision of user input control and providing a fun and playful element. We hope that knowledge gained from this exploratory work can help lay groundwork for further exploration of widgets at larger scales.},
booktitle = {Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {221–230},
numpages = {10},
keywords = {interfaces, large-format interaction, tangible user interfaces, widgets},
location = {Stockholm, Sweden},
series = {TEI '18}
}

@inproceedings{10.1145/2830772.2830779,
author = {Vamanan, Balajee and Sohail, Hamza Bin and Hasan, Jahangir and Vijaykumar, T. N.},
title = {TimeTrader: exploiting latency tail to save datacenter energy for online search},
year = {2015},
isbn = {9781450340342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2830772.2830779},
doi = {10.1145/2830772.2830779},
abstract = {Online Search (OLS) is a key component of many popular Internet services. Datacenters running OLS consume significant amounts of energy. However, reducing their energy is challenging due to their tight response time requirements. A key aspect of OLS is that each user query goes to all or many of the nodes in the cluster, so that the overall time budget is dictated by the tail of the replies' latency distribution; replies see latency variations both in the network and compute. Previous work proposes to achieve load-proportional energy by slowing down the computation at lower datacenter loads based directly on response times (i.e., at lower loads, the proposal exploits the average slack in the time budget provisioned for the peak load). In contrast, we propose TimeTrader to reduce energy by exploiting the latency slack in the sub-critical replies which arrive before the deadline (e.g., 80% of replies are 3-4x faster than the tail). This slack is present at all loads and subsumes the previous work's load-related slack. While the previous work shifts the leaves' response time distribution to consume the slack at lower loads, TimeTrader reshapes the distribution at all loads by slowing down individual sub-critical nodes without increasing missed deadlines. TimeTrader exploits slack in both the network and compute budgets. Further, TimeTrader leverages Earliest Deadline First scheduling to largely decouple critical requests from the queuing delays of sub-critical requests which can then be slowed down without hurting critical requests. A combination of real-system measurements and at-scale simulations shows that without adding to missed deadlines, TimeTrader saves 15% and 40% energy at 90% and 30% loading, respectively, in a datacenter with 512 nodes, whereas previous work saves 0% and 30%. Further, as a proof-of-concept, we build a small-scale real implementation to evaluate TimeTrader and show 10-30% energy savings.},
booktitle = {Proceedings of the 48th International Symposium on Microarchitecture},
pages = {585–597},
numpages = {13},
keywords = {datacenter, incast, latency tail, online data-intensive (OLDI) applications, online search (OLS)},
location = {Waikiki, Hawaii},
series = {MICRO-48}
}

@inproceedings{10.1145/3297663.3310303,
author = {Seybold, Daniel and Keppler, Moritz and Gr\"{u}ndler, Daniel and Domaschka, J\"{o}rg},
title = {Mowgli: Finding Your Way in the DBMS Jungle},
year = {2019},
isbn = {9781450362399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297663.3310303},
doi = {10.1145/3297663.3310303},
abstract = {Big Data and IoT applications require highly-scalable database management system (DBMS), preferably operated in the cloud to ensure scalability also on the resource level. As the number of existing distributed DBMS is extensive, the selection and operation of a distributed DBMS in the cloud is a challenging task. While DBMS benchmarking is a supportive approach, existing frameworks do not cope with the runtime constraints of distributed DBMS and the volatility of cloud environments. Hence, DBMS evaluation frameworks need to consider DBMS runtime and cloud resource constraints to enable portable and reproducible results. In this paper we present Mowgli, a novel evaluation framework that enables the evaluation of non-functional DBMS features in correlation with DBMS runtime and cloud resource constraints. Mowgli fully automates the execution of cloud and DBMS agnostic evaluation scenarios, including DBMS cluster adaptations. The evaluation of Mowgli is based on two IoT-driven scenarios, comprising the DBMSs Apache Cassandra and Couchbase, nine DBMS runtime configurations, two cloud providers with two different storage backends. Mowgli automates the execution of the resulting 102 evaluation scenarios, verifying its support for portable and reproducible DBMS evaluations. The results provide extensive insights into the DBMS scalability and the impact of different cloud resources. The significance of the results is validated by the correlation with existing DBMS evaluation results.},
booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {321–332},
numpages = {12},
keywords = {benchmarking, cloud, distributed database, nosql, scalability},
location = {Mumbai, India},
series = {ICPE '19}
}

@inproceedings{10.1145/3052973.3055159,
author = {Kohls, Katharina Siobhan and Poepper, Christina},
title = {Traffic Analysis Attacks in Anonymity Networks},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3055159},
doi = {10.1145/3052973.3055159},
abstract = {With more than 1.7 million daily users, Tor is a large-scale anonymity network that helps people to protect their identities in the Internet. Tor provides low-latency transmissions that can serve a wide range of applications including web browsing, which renders it an easily accessible tool for a large user base. Unfortunately, its wide adoption makes Tor a valuable target for de-anonymization attacks. Recent work proved that powerful traffic analysis attacks exist which enable an adversary to relate traffic streams in the network and identify users and accessed contents. One open research question in the field of anonymity networks therefore addresses efficient countermeasures to the class of traffic analysis attacks. Defensive techniques must improve the security features of existing networks while still providing an acceptable performance that can maintain the wide acceptance of a system. The proposed work presents an analysis of mixing strategies as a countermeasure to traffic analysis attacks in Tor. First simulation results indicate the security gains and performance impairments of three main mixing strategies.},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {917–919},
numpages = {3},
keywords = {TOR, anonymity, traffic analysis},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@inproceedings{10.1145/3330204.3330235,
author = {Ferreira, Tiago Moraes and Costella, Fernando Luiz and Zanetti, Alisson Borges and da Silva, Silvano Elias and Zanatta, Alexandre Lazaretti and De Marchi, Ana Carolina Bertoletti},
title = {CrowdRec: A prototype recomendation system for crowdsourcing platforms using Google Venture Design: Google Venture Design Sprint},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330235},
doi = {10.1145/3330204.3330235},
abstract = {Collective intelligence is an interdisciplinary topic that has been explored by different areas of knowledge, like information systems, being a common practice of knowledge exchange through new forms of organization and flexible coordination in real time. Crowdsourcing emerges in this context as an act of externalizing, through the Internet, a task traditionally done internally in the organization for an undefined (and often large) group of people. However, one of the challenges found in this model is how users select and interact with available tasks for execution. Therefore, this work presents a prototype task recommendation system tool, with user experience-oriented design (UX), called CrowdRec, executed through the Google Ventures Design Studio method in conjunction with the Quant-UX tool. To evaluate the results, the TAM analysis with a five-point Likert scale was used. The results show positive interactions in the prototype.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {26},
numpages = {8},
keywords = {Human-Computer Interface, Prototype, Sprint Design, System of Recommendation},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/3025453.3026018,
author = {Chandrasekharan, Eshwar and Samory, Mattia and Srinivasan, Anirudh and Gilbert, Eric},
title = {The Bag of Communities: Identifying Abusive Behavior Online with Preexisting Internet Data},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026018},
doi = {10.1145/3025453.3026018},
abstract = {Since its earliest days, harassment and abuse have plagued the Internet. Recent research has focused on in-domain methods to detect abusive content and faces several challenges, most notably the need to obtain large training corpora. In this paper, we introduce a novel computational approach to address this problem called Bag of Communities (BoC)---a technique that leverages large-scale, preexisting data from other Internet communities. We then apply BoC toward identifying abusive behavior within a major Internet community. Specifically, we compute a post's similarity to 9 other communities from 4chan, Reddit, Voat and MetaFilter. We show that a BoC model can be used on communities "off the shelf" with roughly 75% accuracy---no training examples are needed from the target community. A dynamic BoC model achieves 91.18% accuracy after seeing 100,000 human-moderated posts, and uniformly outperforms in-domain methods. Using this conceptual and empirical work, we argue that the BoC approach may allow communities to deal with a range of common problems, like abusive behavior, faster and with fewer engineering resources.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {3175–3187},
numpages = {13},
keywords = {abusive behavior, machine learning, moderation, online communities, social computing},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2809563.2809603,
author = {Lamprecht, Daniel and Geigl, Florian and Karas, Tomas and Walk, Simon and Helic, Denis and Strohmaier, Markus},
title = {Improving recommender system navigability through diversification: a case study of IMDb},
year = {2015},
isbn = {9781450337212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809563.2809603},
doi = {10.1145/2809563.2809603},
abstract = {The Internet Movie Database (IMDb) is the world's largest collection of facts about movies and features large-scale recommendation systems connecting hundreds of thousands of items. In the past, the principal evaluation criterion for such recommender systems has been the rating accuracy prediction for recommendations within the immediate one-hop-neighborhood. Apart from a few isolated studies, the evaluation methodology for recommender systems has so far lacked approaches that quantify and measure the exposure to novel content while navigating a recommender system. As such, little is known about the support for navigation and browsing as methods to explore, browse and discover novel items within these systems. In this article, we study the navigability of IMDb's recommender systems over multiple hops. To this end, we analyze the recommendation networks of IMDb with a two-level approach: First, we study reachability in terms of components, path lengths and a bow-tie analysis. Second, we simulate practical browsing scenarios based on greedy decentralized search. Our results show that the IMDb recommendation networks are not very well-suited for navigation scenarios. To mitigate this, we apply a method for diversifying recommendations by specifically selecting recommendations which improve connectivity but do not compromise relevance. We demonstrate that this leads to improved reachability and navigability in both recommender systems. Our work underlines the importance of navigability and reachability as evaluation dimension of a large movie recommender system and shows up ways to increase navigational diversity.},
booktitle = {Proceedings of the 15th International Conference on Knowledge Technologies and Data-Driven Business},
articleno = {21},
numpages = {8},
keywords = {IMDb, diversification, navigation, recommender systems},
location = {Graz, Austria},
series = {i-KNOW '15}
}

@inproceedings{10.1145/3199902.3199913,
author = {Reynders, Brecht and Wang, Qing and Pollin, Sofie},
title = {A LoRaWAN module for ns-3: implementation and evaluation},
year = {2018},
isbn = {9781450364133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3199902.3199913},
doi = {10.1145/3199902.3199913},
abstract = {Long Range (LoRa) communication has been proposed to connect massive numbers of devices in large areas. LoRa itself is a physical layer technique. Together with the MAC layer solution LoRaWAN to realize IoT, they have attracted increasing attention from both industry and academia. In this work, we present our implemented LoRaWAN module for ns-3, to help boost the research in this rising area. Our implementation is compliant with the class A of the LoRaWAN 1.0 specification. It is highly configurable and thus can be easily used to exploit the impact of different parameters on LoRaWAN's performance. Our implemented flexible backbone architecture also allows for the easy integration of new protocols to improve the network performance. In the past two years, we have used this module to develop new protocols and algorithms to improve LoRaWAN's performance, in terms of reliability, capture effect, scalability, power consumption, among others. Our research outcomes have demonstrated the usefulness, flexibility, and configurability of the proposed LoRaWAN ns-3 module. We have made the source code of our module publicly available1.},
booktitle = {Proceedings of the 2018 Workshop on Ns-3},
pages = {61–68},
numpages = {8},
keywords = {LoRa, LoRaWAN, evaluation, flexible, implementation, ns-3 model},
location = {Surathkal, India},
series = {WNS3 '18}
}

@inproceedings{10.1145/3239576.3239606,
author = {Zhang, Hao and Lu, Minyan and Gu, Tingyang},
title = {A Petri-Net Based Reliability Prediction Method for SOA Software},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239606},
doi = {10.1145/3239576.3239606},
abstract = {In order to satisfy the requirement of the use of large-scale, complex distributed software system, Service Oriented Architecture has aroused wide concern. How to build a reliable SOA software system has become one of the important issues in this research area. As for SOA software, reliability prediction can be used for providing reliability information at the early stage of development to help developers in improving the quality of software. In current researches of SOA reliability prediction, on the one hand, researchers mainly focus on only one aspect of software system like service processes or service states, neglecting a comprehensive view of both static structure and dynamic behavior. On the other hand, the participation of users is not fully considered and modeled. In our previous research, we proposed a SOA software architecture extended modeling method considering reliability information. In this paper, we continued our research and a SOA software reliability prediction method is proposed. The proposed approach is based on our previous modeling method, and a model transformation approach is used to build the Petri-net based reliability analytic model. Then the reliability prediction result is obtained by Petri-net simulation. The proposed software prediction method can provide reliability information at the early stage of software development, which is helpful for ensuring the reliability requirement and the quality of software at the end of the development.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {165–172},
numpages = {8},
keywords = {Model transformation, Petri nets, Reliability prediction, Service-Oriented Architecture},
location = {Chengdu, China},
series = {ICAIP '18}
}

@article{10.1145/3359285,
author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
title = {Estimating Attention Flow in Online Video Networks},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359285},
doi = {10.1145/3359285},
abstract = {Online videos have shown tremendous increase in Internet traffic. Most video hosting sites implement recommender systems, which connect the videos into a directed network and conceptually act as a source of pathways for users to navigate. At present, little is known about how human attention is allocated over such large-scale networks, and about the impacts of the recommender systems. In this paper, we first construct the Vevo network -- a YouTube video network with 60,740 music videos interconnected by the recommendation links, and we collect their associated viewing dynamics. This results in a total of 310 million views every day over a period of 9 weeks. Next, we present large-scale measurements that connect the structure of the recommendation network and the video attention dynamics. We use the bow-tie structure to characterize the Vevo network and we find that its core component (23.1% of the videos), which occupies most of the attention (82.6% of the views), is made out of videos that are mainly recommended among themselves. This is indicative of the links between video recommendation and the inequality of attention allocation. Finally, we address the task of estimating the attention flow in the video recommendation network. We propose a model that accounts for the network effects for predicting video popularity, and we show it consistently outperforms the baselines. This model also identifies a group of artists gaining attention because of the recommendation network. Altogether, our observations and our models provide a new set of tools to better understand the impacts of recommender systems on collective social attention.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {183},
numpages = {25},
keywords = {empirical measurement, network effects, online attention, popularity prediction, recommender system, youtube}
}

@article{10.5555/3207692.3207702,
author = {Yu, Peng and Williams, Brian and Fang, Cheng and Cui, Jing and Haslum, Patrik},
title = {Resolving over-constrained temporal problems with uncertainty through conflict-directed relaxation},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Over-subscription, that is, being assigned too many things to do, is commonly encountered in temporal scheduling problems. As human beings, we often want to do more than we can actually do, and underestimate how long it takes to perform each task. Decision makers can benefit from aids that identify when these failure situations are likely, the root causes of these failures, and resolutions to these failures.In this paper, we present a decision assistant that helps users resolve oversubscribed temporal problems. The system works like an experienced advisor that can quickly identify the cause of failure underlying temporal problems and compute resolutions. The core of the decision assistant is the Best-first Con ict-Directed Relaxation (BCDR) algorithm, which can detect conflicting sets of constraints within temporal problems, and computes continuous relaxations for them that weaken constraints to the minimum extent, instead of removing them completely. BCDR is an extension to the Con ict-Directed A* algorithm, first developed in the model-based reasoning community to compute most likely system diagnoses or reconfigurations. It generalizes the discrete conflicts and relaxations, to hybrid conflicts and relaxations, which denote minimal inconsistencies and minimal relaxations to both discrete and continuous relaxable constraints. In addition, BCDR is capable of handling temporal uncertainty, expressed as either set-bounded or probabilistic durations, and can compute preferred trade-offs between the risk of violating a schedule requirement, versus the loss of utility by weakening those requirements.BCDR has been applied to several decision support applications in different domains, including deep-sea exploration, urban travel planning and transit system management. It has demonstrated its effectiveness in helping users resolve over-subscribed scheduling problems and evaluate the robustness of existing solutions. In our benchmark experiments, BCDR has also demonstrated its efficiency on solving large-scale scheduling problems in the aforementioned domains. Thanks to its conflict-driven approach for computing relaxations, BCDR achieves one to two orders of magnitude improvements on runtime performance when compared to state-of-the-art numerical solvers.},
journal = {J. Artif. Int. Res.},
month = {sep},
pages = {425–490},
numpages = {66}
}

@inproceedings{10.1145/2745844.2745898,
author = {Gupta, Vani and Lee, Stephen and Shenoy, Prashant and Sitaraman, Ramesh and Urgaonkar, Rahul},
title = {Towards Cooling Internet-Scale Distributed Networks on the Cheap},
year = {2015},
isbn = {9781450334860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745844.2745898},
doi = {10.1145/2745844.2745898},
abstract = {Internet-scale Distributed Networks (IDNs) are large distributed systems that comprise hundreds of thousands of servers located around the world. IDNs consume significant amounts of energy to power their deployed server infrastructure, and nearly as much energy to cool that infrastructure. We study the potential benefits of using renewable open air cooling (OAC) in an IDN. Our results show that by using OAC, a global IDN can extract 51% cooling energy reducing during summers and a 92% reduction in the winter.},
booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {469–470},
numpages = {2},
keywords = {energy, internet-scale distributed systems, load balancing},
location = {Portland, Oregon, USA},
series = {SIGMETRICS '15}
}

@inproceedings{10.1145/3372318.3372322,
author = {Zamiri-Gourabi, Mohammad-Reza and Qalaei, Ali Razmjoo and Azad, Babak Amin},
title = {Gas what? I can see your GasPots. Studying the fingerprintability of ICS honeypots in the wild},
year = {2019},
isbn = {9781450377195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372318.3372322},
doi = {10.1145/3372318.3372322},
abstract = {Internet connectivity of electronic devices has brought us the ease of centralized management and these days more and more devices are connected to this globally accessible network. At the same time, this landscape has opened new doors for malicious actors. While internet connectivity is a built-in feature for desktop and mobile devices, Industrial Control Systems (ICS) lag behind. Traditionally, ICS networks have been air-gapped and as a result, many ICS devices are not well-equipped to be connected to the internet. Absence of proper authentication and other security mechanisms is commonly observed on these devices.In response to the new threats of connected ICS systems, various ICS honeypots have been developed during the past decade. These honeypots are used to collect information on the attack landscape of ICS systems. In this research, we show that ICS honeypots should be designed more carefully and existing honeypots can fairly easily be fingerprinted by the attackers.We systematically study the categories of often overlooked behaviors that make ICS honeypots fingerprintable. Moreover, to demonstrate the impact of these flaws, we perform a large scale analysis over the internet to detect GasPot honeypots that emulate automatic tank gauges (ATG). We were able to find 17 existing honeypot instances which is more than the number of discovered GasPots by Shodan. Finally, we released our ICS honeypot scanner and our ATG honeypot which provides full protocol support and fixes the existing flaws within GasPot that makes it detectable.},
booktitle = {Proceedings of the Fifth Annual Industrial Control System Security (ICSS) Workshop},
pages = {30–37},
numpages = {8},
keywords = {ICS, SCADA, honeypot, honeypot detection, honeypot fingerprinting, security},
location = {San Juan, PR, USA},
series = {ICSS}
}

@inproceedings{10.1145/3196398.3196402,
author = {Ott, Jordan and Atchison, Abigail and Harnack, Paul and Bergh, Adrienne and Linstead, Erik},
title = {A deep learning approach to identifying source code in images and video},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196402},
doi = {10.1145/3196398.3196402},
abstract = {While substantial progress has been made in mining code on an Internet scale, efforts to date have been overwhelmingly focused on data sets where source code is represented natively as text. Large volumes of source code available online and embedded in technical videos have remained largely unexplored, due in part to the complexity of extraction when code is represented with images. Existing approaches to code extraction and indexing in this environment rely heavily on computationally intense optical character recognition. To improve the ease and efficiency of identifying this embedded code, as well as identifying similar code examples, we develop a deep learning solution based on convolutional neural networks and autoencoders. Focusing on Java for proof of concept, our technique is able to identify the presence of typeset and handwritten source code in thousands of video images with 85.6%-98.6% accuracy based on syntactic and contextual features learned through deep architectures. When combined with traditional approaches, this provides a more scalable basis for video indexing that can be incorporated into existing software search and mining tools.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {376–386},
numpages = {11},
keywords = {convolutional neural networks, deep learning, programming tutorials, video mining},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3232755.3232759,
author = {Singh, Rachee and Ghobadi, Manya and Foerster, Klaus-Tycho and Filer, Mark and Gill, Phillipa},
title = {Beyond Binary Failures in Networks},
year = {2018},
isbn = {9781450355858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232755.3232759},
doi = {10.1145/3232755.3232759},
abstract = {Fiber optic cables are the workhorses of today's Internet services, but they are an expensive resource and require significant monetary investment. Their importance has driven a conservative deployment approach with redundancy baked into multiple layers of the network under the assumption that links have a constant reliability status and operate at a fixed capacity. In this work, we take an unconventional approach and argue that link failures should not be always considered binary events; this approach enables the foundation of a framework for network links with dynamic capacity and reliability. We investigated this idea by conducting the first ever large-scale study of operational optical signals, analyzing over 2,000 channels in a wide-area network for a period of three years. Our analysis uncovered several findings that enable cross-layer optimizations and smart algorithms to improve traffic engineering, increase capacity, and reduce cost. For instance, we show the capacity of over 90% of wide-area links can be augmented by at least 50 Gbps, leading to an overall capacity gain of more than 100 Tbps. This means we get higher capacity and better availability using the same links. Based on work published at [1] Rachee Singh, Monia Ghobadi, Klaus-Tycho Foerster, Mark Filer, and Phillipa Gill. 2017. Run, Walk, Crawl: Towards Dynamic Link Capacities. In Proceedings of the 16th ACM Workshop on Hot Topics in Networks (HotNets-XVI). ACM Press, Palo Alto, CA, 143--149. DOI:https://doi.org/10.1145/3152434.3152451},
booktitle = {Proceedings of the Applied Networking Research Workshop},
pages = {8},
numpages = {1},
location = {Montreal, QC, Canada},
series = {ANRW '18}
}

@inproceedings{10.1145/3053600.3053645,
author = {Avritzer, Alberto},
title = {Performance Assessment of High-availability Systems using Markov Chains},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053645},
doi = {10.1145/3053600.3053645},
abstract = {As our society evolves, more and more aspects of our daily life depend on large-scale infrastructures such as software intensive computer infrastructures, rails and road networks, gas networks, water networks, power networks, and telecommunication networks, including the internet, wired and wireless telephony. Critical infrastructures are everywhere and they are becoming increasingly more interconnected and interdependent. Open source software repositories (e.g. Sonatype Nexus) have become central to these critical infrastructures, as they are used to support continuous system integration in several critical domains such as telecom, banking, airlines and government. In this keynote, we present an approach for Survivability Evaluation of Critical infrastructures and its application in a DevOps environment. We present examples of application to Water, Gas, Power, and Computer infrastructures. This work is the fruit of open global research collaboration with many colleagues in several Universities and research Labs.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {209},
numpages = {1},
keywords = {high-availability, open source development, survivability modeling},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1109/TNET.2013.2296331,
author = {Wang, Jiliang and Dong, Wei and Cao, Zhichao and Liu, Yunhao},
title = {On the delay performance in a large-scale wireless sensor network: measurement, analysis, and implications},
year = {2015},
issue_date = {February 2015},
publisher = {IEEE Press},
volume = {23},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2013.2296331},
doi = {10.1109/TNET.2013.2296331},
abstract = {We present a comprehensive delay performance measurement and analysis in a large-scale wireless sensor network. We build a lightweight delay measurement system and present a robust method to calculate the per-packet delay. We show that the method can identify incorrect delays and recover them with a bounded error. Through analysis of delay and other system metrics, we seek to answer the following fundamental questions: What are the spatial and temporal characteristics of delay performance in a real network? What are the most important impacting factors, and is there any practical model to capture those factors? What are the implications to protocol designs? In this paper, we identify important factors from the data trace and show that the important factors are not necessarily the same with those in the Internet. Furthermore, we propose a delay model to capture those factors. We revisit several prevalent protocol designs such as Collection Tree Protocol, opportunistic routing, and Dynamic Switching-based Forwarding and show that our model and analysis are useful to practical protocol designs.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {186–197},
numpages = {12},
keywords = {delay measurement, impacting factor, large-scale, wireless sensor networks}
}

@inproceedings{10.1145/2699026.2699110,
author = {Pattuk, Erman and Kantarcioglu, Murat and Ulusoy, Huseyin},
title = {BigGate: Access Control Framework for Outsourced Key-Value Stores},
year = {2015},
isbn = {9781450331913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2699026.2699110},
doi = {10.1145/2699026.2699110},
abstract = {Due to its scalable design, key-value stores have become the backbone of many large-scale Internet companies that need to cope with millions of transactions every day. It is also an attractive cloud outsourcing technology: driven by economical benefits, many major companies like Amazon, Google, and Microsoft provide key-value storage services to their customers. However, customers are reluctant to utilize such services due to security and privacy concerns. Outsourced sensitive key-value data (e.g., social security numbers as keys, and health reports as value) may be stolen by third-party adversaries and/or malicious insiders. Furthermore, an institution, who is utilizing key-value storage services, may naturally desire to have access control mechanisms among its departments or users, while leaking as little information as possible to the cloud provider to preserve data privacy. We believe that addressing these security and privacy concerns are crucial in further adoption of key-value storage services. In this paper, we present a novel system, BigGate, that provides secure outsourcing and efficient processing of encrypted key-value data, and enforces access control policies. We formally prove the security of our system, and by carefully implemented empirical analysis, show that the overhead induced by sysname can be as low as 2%.},
booktitle = {Proceedings of the 5th ACM Conference on Data and Application Security and Privacy},
pages = {171–182},
numpages = {12},
keywords = {access control, cloud computing, key-value stores, outsourcing, searchable encryption, security and privacy},
location = {San Antonio, Texas, USA},
series = {CODASPY '15}
}

@inproceedings{10.1145/3212734.3212800,
author = {Gotsman, Alexey},
title = {Tutorial: Consistency Choices in Modern Distributed Systems},
year = {2018},
isbn = {9781450357951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3212734.3212800},
doi = {10.1145/3212734.3212800},
abstract = {Distributed systems underlying large-scale Internet services often guarantee immediate availability and tolerate network failures at the expense of providing only weak data consistency guarantees. This is compensated for by new programming constructs, such as replicated data types (aka CRDTs) and novel forms of transactions. Navigating the spectrum of possible consistency models and programming constructs is far from trivial. This tutorial surveys recent developments that help in this: formal definitions of consistency model semantics and methods for reasoning about how the weakness of consistency models affects the correctness of applications using them.},
booktitle = {Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing},
pages = {491},
numpages = {1},
location = {Egham, United Kingdom},
series = {PODC '18}
}

@inproceedings{10.1145/2713579.2713584,
author = {Caudle, Kyle and Karlsson, Christer and Pyeatt, Larry D.},
title = {Using Density Estimation to Detect Computer Intrusions},
year = {2015},
isbn = {9781450333412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713579.2713584},
doi = {10.1145/2713579.2713584},
abstract = {Density estimation can be used to make sense of data collected by large scale systems. An estimate of the underlying probability density function can be used to characterize normal network operating conditions. In this paper, we present a recursive method for constructing and updating an estimate of the non-stationary high dimensional probability density function using parallel programming. Once we have characterized standard operating conditions we perform real time checks for changes. We demonstrate the effectiveness of the approach via the use of simulated data as well as data from Internet header packets.},
booktitle = {Proceedings of the 2015 ACM International Workshop on International Workshop on Security and Privacy Analytics},
pages = {43–48},
numpages = {6},
keywords = {data streams, density estimation, parallel programming, wavelets},
location = {San Antonio, Texas, USA},
series = {IWSPA '15}
}

@inproceedings{10.1145/2998476.2998480,
author = {Rani, Asma and Goyal, Navneet and Gadia, Shashi K.},
title = {Efficient Multi-depth Querying on Provenance of Relational Queries Using Graph Database},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998480},
doi = {10.1145/2998476.2998480},
abstract = {Data Provenance is the history associated with that data. It constitutes the origin, creation, processing, and archiving of data. In today's Internet era, it has gained significant importance for database analytics. Most of the provenance models store provenance information in relational databases for further querying and analysis. Although, querying of provenance in Relational Databases is very efficient for small data sets, it becomes inefficient as the provenance data grows and traversal depth of provenance query increases. This is mainly due to increase in number of join operations to search the entire provenance data. Graph Databases provide an alternative to RDBMSs for storing and analyzing provenance data as it can scale to billions of nodes and at the same time traverse thousands of relationships efficiently. In this paper, we propose efficient multi-depth querying of provenance data using graph databases. The proposed solution allows efficient querying of provenance of current as well as historical queries. A comparison between relational and graph databases is presented for varying provenance data size and traversal depths. Graph databases are found to scale well with increasing depth of provenance queries, whereas in relational databases the querying time increases exponentially.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {11–20},
numpages = {10},
keywords = {DPHQ, Data Provenance, Graph Database, Neo4j, Provenance Querying, Query Inversion, Relational Database, TPC-H, ZILD, improved DPHQ},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/3308560.3316541,
author = {Mei, Qiaozhu},
title = {Decoding the New World Language: Analyzing the Popularity, Roles, and Utility of Emojis},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316541},
doi = {10.1145/3308560.3316541},
abstract = {Emojis have quickly become a universal language that is used by worldwide users, for everyday tasks, across language barriers, and in different apps and platforms. The prevalence of emojis has quickly attracted great attentions from various research communities such as natural language processing, Web mining, ubiquitous computing, and human-computer interaction, as well as other disciplines including social science, arts, psychology, and linguistics.This talk summarizes the recent efforts made by my research group and our collaborators on analyzing large-scale emoji data. The usage of emojis by worldwide users presents interesting commonality as well as divergence. In our analysis of emoji usage by millions of smartphone users in 212 countries, we show that the different preferences and usage of emojis provide rich signals for understanding the cultural differences of Internet users, which correlate with the Hofstede’s cultural dimensions [4].Emojis play different roles when used alongside text. Through jointly learning the embeddings and topological structures of words and emojis, we reveal that emojis present both complementary and supplementary relations to words. Based on the structural properties of emojis in the semantic spaces, we are able to untangle several factors behind the popularity of emojis [1].This talk also highlights the utility of emojis. In general, emojis have been used by Internet users as text supplements to describe objects and situations, express sentiments, or express humor and sarcasm; they are also used as communication tools to attract attention, adjust tones, or establish personal relationships. The benefit of using emojis goes beyond these intentions. In particular, we show that including emojis in the description of an issue report on GitHub results in the issue being responded to by more users and resolved sooner.Large-scale emoji data can also be utilized by AI systems to improve the quality of Web mining services. In particular, a smart machine learning system can infer the latent topics, sentiments, and even demographic information of users based on how they use emojis online. Our analysis reveals a considerable difference between female and male users of emojis, which is big enough for a machine learning algorithm to accurately predict the gender of a user. In Web services that are customized for gender groups, gender inference models built upon emojis can complement those based on text or behavioral traces with fewer privacy concerns [2].Emojis can be also used as an instrument to bridge Web mining tasks across language barriers, especially to transfer sentiment knowledge from a language with rich training labels (e.g., English) to languages that have been difficult for advanced natural language processing tasks [3]. Through this bridge, developers of AI systems and Web services are able to reduce the inequality in the quality of services received by the international users that has been caused by the imbalance of available human annotations in different languages.In general, emojis have evolved from visual ideograms to a brand-new world language in the era of AI and a new Web. The popularity, roles, and utility of emojis have all gone beyond people’s original intentions, which have created a huge opportunity for future research that calls for joint efforts from multiple disciplines.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {417–418},
numpages = {2},
keywords = {emojis, natural language processing, text mining},
location = {San Francisco, USA},
series = {WWW '19}
}

@article{10.1109/TNET.2017.2675450,
author = {Fontugne, Romain and Abry, Patrice and Fukuda, Kensuke and Veitch, Darryl and Cho, Kenjiro and Borgnat, Pierre and Wendt, Herwig},
title = {Scaling in Internet Traffic: A 14 Year and 3 Day Longitudinal Study, With Multiscale Analyses and Random Projections},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2675450},
doi = {10.1109/TNET.2017.2675450},
abstract = {In the mid 1990s, it was shown that the statistics of aggregated time series from Internet traffic departed from those of traditional short range-dependent models, and were instead characterized by asymptotic self-similarity. Following this seminal contribution, over the years, many studies have investigated the existence and form of scaling in Internet traffic. This contribution first aims at presenting a methodology, combining multiscale analysis wavelet and wavelet leaders and random projections or sketches, permitting a precise, efficient and robust characterization of scaling, which is capable of seeing through non-stationary anomalies. Second, we apply the methodology to a data set spanning an unusually long period: 14 years, from the MAWI traffic archive, thereby allowing an in-depth longitudinal analysis of the form, nature, and evolutions of scaling in Internet traffic, as well as network mechanisms producing them. We also study a separate three-day long trace to obtain complementary insight into intra-day behavior. We find that a biscaling two ranges of independent scaling phenomena regime is systematically observed: long-range dependence over the large scales, and multifractallike scaling over the fine scales. We quantify the actual scaling ranges precisely, verify to high accuracy the expected relationship between the long range dependent parameter and the heavy tail parameter of the flow size distribution, and relate fine scale multifractal scaling to typical IP packet inter-arrival and to round-trip time distributions.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {2152–2165},
numpages = {14}
}

@inproceedings{10.1145/3123878.3131988,
author = {Sperotto, Anna and van der Toorn, Olivier and van Rijswijk-Deij, Roland},
title = {TIDE: Threat Identification Using Active DNS Measurements},
year = {2017},
isbn = {9781450350570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123878.3131988},
doi = {10.1145/3123878.3131988},
abstract = {The Domain Name System contains a wealth of information about the security, stability and health of the Internet. Most research that leverages the DNS for detection of malicious activities does so by using passive measurements. The limitation of this approach, however, is that it is effective only once an attack is ongoing. In this paper, we explore a different approach. We advocate the use of active DNS measurements for pro-active (i.e., before the actual attack) identification of domains set up for malicious use. Our research makes uses of data from the OpenINTEL large-scale active DNS measurement platform, which, since February 2015, collects daily snapshots of currently more than 60% of the DNS namespace. We illustrate the potential of our approach by showing preliminary results in three case studies, namely snowshoe spam, denial of service attacks and a case of targeted phishing known as CEO fraud.},
booktitle = {Proceedings of the SIGCOMM Posters and Demos},
pages = {65–67},
numpages = {3},
keywords = {DNS, active measurements, network security},
location = {Los Angeles, CA, USA},
series = {SIGCOMM Posters and Demos '17}
}

@inproceedings{10.1145/2783258.2788627,
author = {Grbovic, Mihajlo and Radosavljevic, Vladan and Djuric, Nemanja and Bhamidipati, Narayan and Savla, Jaikit and Bhagwan, Varun and Sharp, Doug},
title = {E-commerce in Your Inbox: Product Recommendations at Scale},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2788627},
doi = {10.1145/2783258.2788627},
abstract = {In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1809–1818},
numpages = {10},
keywords = {audience modeling, computational advertising, data mining},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/3097983.3098120,
author = {Pan, Lujia and Zhang, Jianfeng and Lee, Patrick P.C. and Cheng, Hong and He, Cheng and He, Caifeng and Zhang, Keli},
title = {An Intelligent Customer Care Assistant System for Large-Scale Cellular Network Diagnosis},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098120},
doi = {10.1145/3097983.3098120},
abstract = {With the advent of cellular network technologies, mobile Internet access becomes the norm in everyday life. In the meantime, the complaints made by subscribers about unsatisfactory cellular network access also become increasingly frequent. From a network operator's perspective, achieving accurate and timely cellular network diagnosis about the causes of the complaints is critical for both improving subscriber-perceived experience and maintaining network robustness. We present the Intelligent Customer Care Assistant (ICCA), a distributed fault classification system that exploits a data-driven approach to perform large-scale cellular network diagnosis. ICCA takes massive network data as input, and realizes both offline model training and online feature computation to distinguish between user and network faults in real time. ICCA is currently deployed in a metropolitan LTE network in China that is serving around 50 million subscribers. We show via evaluation that ICCA achieves high classification accuracy (85.3%) and fast query response time (less than 2.3 seconds). We also report our experiences learned from the deployment.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1951–1959},
numpages = {9},
keywords = {cellular network diagnosis, fault classification, sequential pattern mining},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/2808797.2809311,
author = {Doran, Derek and Yelne, Samir and Massari, Luisa and Calzarossa, Maria-Carla and Jackson, LaTrelle and Moriarty, Glen},
title = {Stay Awhile and Listen: User Interactions in a Crowdsourced Platform Offering Emotional Support},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2809311},
doi = {10.1145/2808797.2809311},
abstract = {Internet and online-based social systems are rising as the dominant mode of communication in society. However, the public or semi-private environment under which most online communications operate under do not make them suitable channels for speaking with others about personal or emotional problems. This has led to the emergence of online platforms for emotional support offering free, anonymous, and confidential conversations with live listeners. Yet very little is known about the way these platforms are utilized, and if their features and design foster strong user engagement. This paper explores the utilization and the interaction features of hundreds of thousands of users on 7 Cups of Tea, a leading online platform offering online emotional support. It dissects the user's activity levels, the patterns by which they engage in conversation with each other, and uses machine learning methods to find factors promoting engagement. The study may be the first to measure activities and interactions in a large-scale online social system that fosters peer-to-peer emotional support.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {667–674},
numpages = {8},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/2889160.2889192,
author = {Kothari, Suresh and Tamrawi, Ahmed and Sauceda, Jerem\'{\i}as and Mathews, Jon},
title = {Let's verify Linux: accelerated learning of analytical reasoning through automation and collaboration},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889192},
doi = {10.1145/2889160.2889192},
abstract = {We describe our experiences in the classroom using the internet to collaboratively verify a significant safety and security property across the entire Linux kernel. With 66,609 instances to check across three versions of Linux, the naive approach of simply dividing up the code and assigning it to students does not scale, and does little to educate. However, by teaching and applying analytical reasoning, the instances can be categorized effectively, the problems of scale can be managed, and students can collaborate and compete with one another to achieve an unprecedented level of verification.We refer to our approach as Evidence-Enabled Collaborative Verification (EECV). A key aspect of this approach is the use of visual software models, which provide mathematically rigorous and critical evidence for verification. The visual models make analytical reasoning interactive, interesting and applicable to large software.Visual models are generated automatically using a tool we have developed called L-SAP [14]. This tool generates an Instance Verification Kit (IVK) for each instance, which contains all of the verification evidence for the instance. The L-SAP tool is implemented on a software graph database platform called Atlas [6]. This platform comes with a powerful query language and interactive visualization to build and apply visual models for software verification.The course project is based on three recent versions of the Linux operating system with altogether 37 MLOC and 66,609 verification instances. The instances are accessible through a website [2] for students to collaborate and compete. The Atlas platform, the L-SAP tool, the structured labs for the project, and the lecture slides are available upon request for academic use.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {394–403},
numpages = {10},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3190619.3190641,
author = {Leonard, Leslie and Glodek, William},
title = {HACSAW: a trusted framework for cyber situational awareness},
year = {2018},
isbn = {9781450364553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190619.3190641},
doi = {10.1145/3190619.3190641},
abstract = {The HPC Architecture for Cyber Situational Awareness (HACSAW) was established by the Department of Defense (DoD) High Performance Computing Modernization Program (HPCMP) to combine a rich computational environment with operationally relevant data to perform cutting-edge cybersecurity research that will increase HPCMP's current and predictive understanding of cyberspace on the Defense Research and Engineering Network (DREN). The data repository created by this unique environment includes the collection of unclassified data sources from the edge of the network (i.e., Internet Access Points) down to the host-level, across more than one hundred (100) different DoD enclaves. Through the application of high performance computing (HPC) resources, HACSAW explores novel and innovative analytical capabilities based on a comprehensive cybersecurity dataset. The integration of HPC within the cyber workflow provides an opportunity for fusion and assessments of disparate data streams and real-time analysis using data science algorithms and machine learning (both structured and unstructured data). Our approach is designed to ultimately leverage HPC resources to significantly reduce the time to respond to changes in the cyber environment from days to minutes.Understanding the operational status of information systems, the missions (friendly and adversary) being pursued, and the threats and vulnerabilities that impact them is essential for effective mission accomplishment. This understanding is referred to as Cyberspace Situational Awareness (Cyber SA). Today's decision makers require meaningful Cyber SA to safeguard sensitive data, sustain fundamental operations, and protect national infrastructure [2]. The need and responsibility of Cyber SA spans multiple organizations within the DoD, across the entire government and in the private sector.The lack of relevant and recent real-world network enterprise data has hampered many cybersecurity research efforts to develop and validate algorithms or methods under realistic conditions. HACSAW has reduced this technical barrier with a development environment that provides computational and data-rich information to researchers to test, develop, model, measure and refine data-driven analytics. This environment is the proving ground for novel ideas, algorithms and approaches that are suitable for large scale execution in a dedicated HPC environment. Currently, HACSAW as an aggregation of over one (1) petabyte of DREN data to include network-based monitoring and intrusion detection results, web content filtering, vulnerability scanning, firewall, sensor health, etc. Context is applied to each cyber event through the use of custom enrichments that provide downstream analytical processes with information that may be useful in determining the nature of the event.During this talk, we will discuss HPCMP's initial approach to addressing Cyber SA through a Call for Proposals (CFP) to the data science, cyber, and HPC communities. Selected collaborators will receive funding for a one-year effort that demonstrates potential for integration into DREN's Cyber SA operational environment and aligns with identified Mission Essential Tasks (METs). METs will ensure decision makers have the understanding necessary to make effective decisions. Such tasks include monitoring, detection, alerting, cyber threat analysis, cyber risk and event analysis, and sharing and collaboration. Initial and future contributions in the areas of modeling and simulation [4], clustering [3] and deep learning [1] are anticipated and results will be shared at a later date.},
booktitle = {Proceedings of the 5th Annual Symposium and Bootcamp on Hot Topics in the Science of Security},
articleno = {12},
numpages = {1},
location = {Raleigh, North Carolina},
series = {HoTSoS '18}
}

@inproceedings{10.1145/2882903.2904444,
author = {Manoharan, Gokul Nath Babu and Ellner, Stephan and Schnaitter, Karl and Chegu, Sridatta and Estrella-Balderrama, Alejandro and Gudmundson, Stephan and Gupta, Apurv and Handy, Ben and Samwel, Bart and Whipkey, Chad and Aharkava, Larysa and Apte, Himani and Gangahar, Nitin and Xu, Jun and Venkataraman, Shivakumar and Agrawal, Divyakant and Ullman, Jeffrey D.},
title = {Shasta: Interactive Reporting At Scale},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2904444},
doi = {10.1145/2882903.2904444},
abstract = {We describe Shasta, a middleware system built at Google to support interactive reporting in complex user-facing applications related to Google's Internet advertising business. Shasta targets applications with challenging requirements: First, user query latencies must be low. Second, underlying transactional data stores have complex "read-unfriendly" schemas, placing significant transformation logic between stored data and the read-only views that Shasta exposes to its clients. This transformation logic must be expressed in a way that scales to large and agile engineering teams. Finally, Shasta targets applications with strong data freshness requirements, making it challenging to precompute query results using common techniques such as ETL pipelines or materialized views. Instead, online queries must go all the way from primary storage to user-facing views, resulting in complex queries joining 50 or more tables.Designed as a layer on top of Google's F1 RDBMS and Mesa data warehouse, Shasta combines language and system techniques to meet these requirements. To help with expressing complex view specifications, we developed a query language called RVL, with support for modularized view templates that can be dynamically compiled into SQL. To execute these SQL queries with low latency at scale, we leveraged and extended F1's distributed query engine with facilities such as safe execution of C++ and Java UDFs. To reduce latency and increase read parallelism, we extended F1 storage with a distributed read-only in-memory cache. The system we describe is in production at Google, powering critical applications used by advertisers and internal sales teams. Shasta has significantly improved system scalability and software engineering efficiency compared to the middleware solutions it replaced.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1393–1404},
numpages = {12},
keywords = {SQL generation, caching, heterogeneous data, middleware, user-defined functions},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3279755.3279759,
author = {Sliper, Sivert T. and Balsamo, Domenico and Weddell, Alex S. and Merrett, Geoff V.},
title = {Enabling intermittent computing on high-performance out-of-order processors},
year = {2018},
isbn = {9781450360470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3279755.3279759},
doi = {10.1145/3279755.3279759},
abstract = {Intermittent computing is a new paradigm enabling battery-less computing devices to be powered directly from energy harvesting, enabling IoT devices that are free from the cost, size and lifetime constraints of batteries. To cope with frequent power interruptions, intermittent computing systems save computational progress before power is lost, and restore it when power returns. Recent research in power-neutral operation of multiprocessor system-on-chips (MPSoCs), where performance scaling is used to instantaneously match power consumption with supply, motivates the need for intermittent computing on high-performance systems. Existing works provide solutions for microcontrollers, but with the increased complexity of high-performance SoCs, new challenges such as hierarchical memory and dependence on large existing libraries emerge. In this paper, we provide a taxonomy of published intermittent computing methods and identify the most suitable method for high-performance SoCs. The chosen method is then implemented and experimentally validated on an Arm A9 out-of-order application processor. Results show that state can be saved/restored correctly in 8.6 ms for a minimal bare-metal application, which is an order of magnitude faster than the platform's hardware boot time.},
booktitle = {Proceedings of the 6th International Workshop on Energy Harvesting &amp; Energy-Neutral Sensing Systems},
pages = {19–25},
numpages = {7},
keywords = {battery-less computing, intermittent computing, internet of things, power-neutral computing},
location = {Shenzhen, China},
series = {ENSsys '18}
}

@inproceedings{10.1145/2742647.2742670,
author = {Nikravesh, Ashkan and Yao, Hongyi and Xu, Shichang and Choffnes, David and Mao, Z. Morley},
title = {Mobilyzer: An Open Platform for Controllable Mobile Network Measurements},
year = {2015},
isbn = {9781450334945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2742647.2742670},
doi = {10.1145/2742647.2742670},
abstract = {Mobile Internet availability, performance and reliability have remained stubbornly opaque since the rise of cellular data access. Conducting network measurements can give us insight into user-perceived network conditions, but doing so requires careful consideration of device state and efficient use of scarce resources. Existing approaches address these concerns in ad-hoc ways.In this work we propose Mobilyzer, a platform for conducting mobile network measurement experiments in a principled manner. Our system is designed around three key principles: network measurements from mobile devices require tightly controlled access to the network interface to provide isolation; these measurements can be performed efficiently using a global view of available device resources and experiments; and distributing the platform as a library to existing apps provides the incentives and low barrier to adoption necessary for large-scale deployments. We describe our current design and implementation, and illustrate how it provides measurement isolation for applications, efficiently manages measurement experiments and enables a new class of experiments for the mobile environment.},
booktitle = {Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {389–404},
numpages = {16},
keywords = {cellular networks, measurement tool, mobile web, network performance, video},
location = {Florence, Italy},
series = {MobiSys '15}
}

@inproceedings{10.1145/2684464.2684474,
author = {Sarma, Atish Das and Molla, Anisur Rahaman and Pandurangan, Gopal},
title = {Distributed Computation of Sparse Cuts via Random Walks},
year = {2015},
isbn = {9781450329286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684464.2684474},
doi = {10.1145/2684464.2684474},
abstract = {A sparse cut of a graph is a partition of the vertices into two disjoint subsets such that the ratio of the number of edges across the two subsets divided by the sum of degrees of vertices in the smaller side is minimum. Finding sparse cuts is an important tool in analyzing large-scale distributed networks such as the Internet and Peer-to-Peer networks, as well as large-scale graphs such as the web graph, online social communities, and VLSI circuits. Sparse cuts are useful in graph clustering and partitioning among numerous other applications. In distributed communication networks, they are useful for topology maintenance and for designing better search and routing algorithms.In this paper, we focus on developing a fast distributed algorithm for computing sparse cuts in networks. Given an undirected n-node network G with conductance φ, the goal is to find a cut set whose conductance is close to φ. We present a distributed algorithm that finds a cut set with sparsity \~{O}(√φ) (\~{O} hides polylog n factors). Our algorithm works in the CONGEST distributed computing model and outputs a cut of conductance at most \~{O} (√φ) with high probability, in \~{O}(1/b(1/φ + n)log2) rounds, where b is balance of the cut of given conductance. In particular, to find a sparse cut of constant balance, our algorithm takes O((1/φ + n)log2 n) rounds. Our algorithm can also be used to output a local cluster, i.e., a subset of vertices near a given source node, and whose conductance is within a quadratic factor of the best possible cluster around the specified node. Our distributed algorithm can work without knowledge of the optimal φ value (with only a log n factor slowdown) and hence can be used to find approximate conductance values both globally and with respect to a given source node. Our algorithm uses random walks as a key subroutine and is fully decentralized and uses lightweight local computations.We also give a lower bound on the time needed for any distributed algorithm to compute any non-trivial sparse cut --- any distributed approximation algorithm (for any nontrivial approximation ratio) for computing sparsest cut will take Ω (√n + D) rounds, where D is the diameter of the graph.Our algorithm can be used to find sparse cuts (and their conductance values) and to identify well-connected clusters and critical edges in distributed networks. This in turn can be helpful in the design, analysis, and maintenance of topologically-aware networks.},
booktitle = {Proceedings of the 16th International Conference on Distributed Computing and Networking},
articleno = {6},
numpages = {10},
keywords = {Conductance, Distributed Algorithm, Random Walks, Sparse Cut},
location = {Goa, India},
series = {ICDCN '15}
}

@inproceedings{10.1145/3123266.3123278,
author = {Yao, Hantao and Zhang, Shiliang and Zhang, Yongdong and Li, Jintao and Tian, Qi},
title = {One-Shot Fine-Grained Instance Retrieval},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123278},
doi = {10.1145/3123266.3123278},
abstract = {Fine-Grained Visual Categorization (FGVC) has achieved significant progress recently. However, the number of fine-grained species could be huge and dynamically increasing in real scenarios, making it difficult to recognize unseen objects under the current FGVC framework. This raises an open issue to perform large-scale fine-grained identification without a complete training set. Aiming to conquer this issue, we propose a retrieval task named One-Shot Fine-Grained Instance Retrieval (OSFGIR). "One-Shot" denotes the ability of identifying unseen objects through a fine-grained retrieval task assisted with an incomplete auxiliary training set. This paper first presents the detailed description to OSFGIR task and our collected OSFGIR-378K dataset. Next, we propose the Convolutional and Normalization Networks (CN-Nets) learned on the auxiliary dataset to generate a concise and discriminative representation. Finally, we present a coarse-to-fine retrieval framework consisting of three components, i.e., coarse retrieval, fine-grained retrieval, and query expansion, respectively. The framework progressively retrieves images with similar semantics, and performs fine-grained identification. Experiments show our OSFGIR framework achieves significantly better accuracy and efficiency than existing FGVC and image retrieval methods, thus could be a better solution for large-scale fine-grained object identification.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {342–350},
numpages = {9},
keywords = {cn-nets, cnn, fine-grained visual categorization, one-shot fine-grained instance retrieval, osfgir-378k},
location = {Mountain View, California, USA},
series = {MM '17}
}

@article{10.1007/s00165-017-0447-x,
author = {Pedersen, Jan B. and Welch, Peter H.},
title = {The symbiosis of concurrency and verification: teaching and case studies},
year = {2018},
issue_date = {Mar 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {2},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-017-0447-x},
doi = {10.1007/s00165-017-0447-x},
abstract = {Concurrency is beginning to be accepted as a core knowledge area in the undergraduate CS curriculum—no longer isolated, for example, as a support mechanism in a module on operating systems or reserved as an advanced discipline for later study. Formal verification of system properties is often considered a difficult subject area, requiring significant mathematical knowledge and generally restricted to smaller systems employing sequential logic only. This paper presents materials, methods and experiences of teaching concurrency and verification as a unified subject, as early as possible in the curriculum, so that they become fundamental elements of our software engineering tool kit—to be used together every day as a matter of course. Concurrency and verification should live in symbiosis. Verification is essential for concurrent systems as testing becomes especially inadequate in the face of complex non-deterministic (and, therefore, hard to repeat) behaviours. Concurrency should simplify the expression of most scales and forms of computer system by reflecting the concurrency of the worlds in which they operate (and, therefore, have to model); simplified expression leads to simplified reasoning and, hence, verification. Our approach lets these skills be developed without requiring students to be trained in the underlying formal mathematics. Instead, we build on the work of those who have engineered that necessary mathematics into the concurrency models we use (CSP, π -calculus), the model checker (FDR) that lets us explore and verify those systems, and the programming languages/libraries (occam-π, Go, JCSP, ProcessJ) that let us design and build efficient executable systems within these models. This paper introduces a workflow methodology for the development and verification of concurrent systems; it also presents and reflects on two open-ended case studies, using this workflow, developed at the authors’ two universities. Concerns analysed include safety (don’t do bad things), liveness (do good things) and low probability deadlock (that testing fails to discover). The necessary technical background is given to make this paper self-contained and its work simple to reproduce and extend.},
journal = {Form. Asp. Comput.},
month = {mar},
pages = {239–277},
numpages = {39},
keywords = {Process-orientation, Concurrency, Deadlock, Event ordering, Liveness, Verification, Occam-π, CSP}
}

@inproceedings{10.1145/2851613.2851763,
author = {Petroni, Fabio and Querzoni, Leonardo and Beraldi, Roberto and Paolucci, Mario},
title = {Exploiting user feedback for online filtering in event-based systems},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851763},
doi = {10.1145/2851613.2851763},
abstract = {Modern large-scale internet applications, like the ubiquitous social networks, represent today a fundamental source of information for millions of users. The larger is the user base, the more difficult it is to control the quality of data that is spread from producers to consumers. This can easily hamper the usability of such systems as the amount of low quality data received by consumers grows uncontrolled. In this paper we propose a novel solution to automatically filter new data injected in event-based systems with the aim of delivering to consumers only content they are actually interested in. Filtering is executed at run-time by first profiling both producers and consumers, and then matching their profiles as new data is produced.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {2021–2026},
numpages = {6},
keywords = {event filtering, event-based systems, publish/subscribe},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3278532.3278563,
author = {Richter, Philipp and Padmanabhan, Ramakrishna and Spring, Neil and Berger, Arthur and Clark, David},
title = {Advancing the Art of Internet Edge Outage Detection},
year = {2018},
isbn = {9781450356190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278532.3278563},
doi = {10.1145/3278532.3278563},
abstract = {Measuring reliability of edge networks in the Internet is difficult due to the size and heterogeneity of networks, the rarity of outages, and the difficulty of finding vantage points that can accurately capture such events at scale. In this paper, we use logs from a major CDN, detailing hourly request counts from address blocks. We discovered that in many edge address blocks, devices, collectively, contact the CDN every hour over weeks and months. We establish that a sudden temporary absence of these requests indicates a loss of Internet connectivity of those address blocks, events we call disruptions.We develop a disruption detection technique and present broad and detailed statistics on 1.5M disruption events over the course of a year. Our approach reveals that disruptions do not necessarily reflect actual service outages, but can be the result of prefix migrations. Major natural disasters are clearly represented in our data as expected; however, a large share of detected disruptions correlate well with planned human intervention during scheduled maintenance intervals, and are thus unlikely to be caused by external factors. Cross-evaluating our results we find that current state-of-the-art active outage detection over-estimates the occurrence of disruptions in some address blocks. Our observations of disruptions, service outages, and different causes for such events yield implications for the design of outage detection systems, as well as for policymakers seeking to establish reporting requirements for Internet services.},
booktitle = {Proceedings of the Internet Measurement Conference 2018},
pages = {350–363},
numpages = {14},
keywords = {Internet outages, Internet reliability},
location = {Boston, MA, USA},
series = {IMC '18}
}

@inproceedings{10.1145/3243082.3243085,
author = {de Sousa, Peron Rezende and Lage, Marcos and de Arag\~{a}o Rocha, Antonio A.},
title = {Future Internet and Scalability Techniques in Mobile Crowdsourcing},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3243085},
doi = {10.1145/3243082.3243085},
abstract = {In this paper we present a new architecture for mobile crowdsourcing systems which leverages the infrastructure of services widely scalable. We successfully developed a proof of concept and discussed an alternative architecture that uses direct communication between devices to eliminate the additional financial contribution needed in the solutions developed with elastic/cloud computing. We also presented a new incentive mechanism and evaluated its scalability with up to 1500 simultaneous accesses. Our results show that it is capable of serving one of the largest crowdsourcing systems on the internet.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {77–84},
numpages = {8},
keywords = {Crowdsourcing, Future Internet, Scalability},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/2740908.2742000,
author = {Hall, Rob and Attenberg, Josh},
title = {Fast and Accurate Maximum Inner Product Recommendations on Map-Reduce},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742000},
doi = {10.1145/2740908.2742000},
abstract = {Personalization has become a predominant theme in online advertising; the internet allows advertisers to target only those users with the greatest chances of engagement, maximizing the probability of success and user happiness. However, a na{"i}ve approach to matching users with their most suitable content scales proportionally to the product of the cardinalities of the user and content sets. For advertisers with large portfolios, this quickly becomes intractable. In this work, we address this more general {em top-$k$ personalization} problem, giving a scalable method to produce recommendations based on personalization models where the affinity between a user and an item is captured by an inner product (i.e., most matrix factorization models). We first transform the problem into finding the $k$-nearest neighbors among the items for each user, then approximate the solution via a method which is particularly suited for use on a map-reduce cluster. We empirically show that our method is between 1 and 2 orders of magnitude faster than previous work, while maintaining excellent approximation quality. Additionally, we provide an open-source implementation of our proposed method, this implementation is used in production at Etsy for a number of large scale personalization systems, and is the same code as used in the experiments below.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1263–1268},
numpages = {6},
keywords = {map-reduce, maximum inner product search, recommender systems},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@article{10.1145/2967502,
author = {Gao, Yue and Zhang, Hanwang and Zhao, Xibin and Yan, Shuicheng},
title = {Event Classification in Microblogs via Social Tracking},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2967502},
doi = {10.1145/2967502},
abstract = {Social media websites have become important information sharing platforms. The rapid development of social media platforms has led to increasingly large-scale social media data, which has shown remarkable societal and marketing values. There are needs to extract important events in live social media streams. However, microblogs event classification is challenging due to two facts, i.e., the short/conversational nature and the incompatible meanings between the text and the corresponding image in social posts, and the rapidly evolving contents. In this article, we propose to conduct event classification via deep learning and social tracking. First, we introduce a Multi-modal Multi-instance Deep Network (M2DN) for microblogs classification, which is able to handle the weakly labeled microblogs data oriented from the incompatible meanings inside microblogs. Besides predicting each microblogs as predefined events, we propose to employ social tracking to extract social-related auxiliary information to enrich the testing samples. We extract a set of candidate-relevant microblogs in a short time window by using social connections, such as related users and geographical locations. All these selected microblogs and the testing data are formulated in a Markov Random Field model. The inference on the Markov Random Field is conducted to update the classification results of the testing microblogs. This method is evaluated on the Brand-Social-Net dataset for classification of 20 events. Experimental results and comparison with the state of the arts show that the proposed method can achieve better performance for the event classification task.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
articleno = {35},
numpages = {14},
keywords = {Event classification, Markov Random Field (MRF), multi-instance, multi-modal, social tracking}
}

@inproceedings{10.1145/3200947.3201043,
author = {Boussis, Dimitris and Dritsas, Elias and Kanavos, Andreas and Sioutas, Spyros and Tzimas, Giannis and Verykios, Vassilios S.},
title = {MapReduce Implementations for Privacy Preserving Record Linkage},
year = {2018},
isbn = {9781450364331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3200947.3201043},
doi = {10.1145/3200947.3201043},
abstract = {Over the last decade, the vast explosion of Internet data has fueled the development of Big Data management systems and technologies. The huge amount of data in combination with the need for records linkage under privacy perspective, has led us to current study. To this direction, we describe Privacy Preserving Record Linkage problem based on Bloom Filter encoding techniques which both maintain users' security and permit similarity control. Moreover, we extended our study to the HLSH/FPS private indexing technique and briefly describe four implementations in the MapReduce distributed environment that is capable of processing large scale data. We also conducted experimental evaluation of these four versions in order to evaluate them in terms of job execution time, memory and disk usage.},
booktitle = {Proceedings of the 10th Hellenic Conference on Artificial Intelligence},
articleno = {16},
numpages = {4},
keywords = {Bloom Filters, Hadoop, MapReduce, Privacy Preserving Record Linkage},
location = {Patras, Greece},
series = {SETN '18}
}

@inproceedings{10.1145/3083187.3084020,
author = {Li, Qiang and Feng, Xuan and Wang, Haining and Sun, Limin},
title = {Automatically Discovering Surveillance Devices in the Cyberspace},
year = {2017},
isbn = {9781450350020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083187.3084020},
doi = {10.1145/3083187.3084020},
abstract = {Surveillance devices with IP addresses are accessible on the Internet and play a crucial role in monitoring physical worlds. Discovering surveillance devices is a prerequisite for ensuring high availability, reliability, and security of these devices. However, today's device search depends on keywords of packet head fields, and keyword collection is done manually, which requires enormous human efforts and induces inevitable human errors. The difficulty of keeping keywords complete and updated has severely impeded an accurate and large-scale device discovery. To address this problem, we propose to automatically generate device fingerprints based on webpages embedded in surveillance devices. We use natural language processing to extract the content of webpages and machine learning to build a classification model. We achieve real-time and non-intrusive web crawling by leveraging network scanning technology. We implement a prototype of our proposed discovery system and evaluate its effectiveness through real-world experiments. The experimental results show that those automatically generated fingerprints yield very high accuracy of 99% precision and 96% recall. We also deploy the prototype system on Amazon EC2 and search surveillance devices in the whole IPv4 space (nearly 4 billion). The number of devices we found is almost 1.6 million, about twice as many as those using commercial search engines.},
booktitle = {Proceedings of the 8th ACM on Multimedia Systems Conference},
pages = {331–342},
numpages = {12},
keywords = {Automatic device discovery, Network measurement, Surveillance device},
location = {Taipei, Taiwan},
series = {MMSys'17}
}

@inproceedings{10.1145/3106237.3117764,
author = {Bu, Wenqi and Xue, Minhui and Xu, Lihua and Zhou, Yajin and Tang, Zhushou and Xie, Tao},
title = {When program analysis meets mobile security: an industrial study of misusing Android internet sockets},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117764},
doi = {10.1145/3106237.3117764},
abstract = {Despite recent progress in program analysis techniques to identify vulnerabilities in Android apps, significant challenges still remain for applying these techniques to large-scale industrial environments. Modern software-security providers, such as Qihoo 360 and Pwnzen (two leading companies in China), are often required to process more than 10 million mobile apps at each run. In this work, we focus on effectively and efficiently identifying vulnerable usage of Internet sockets in an industrial setting. To achieve this goal, we propose a practical hybrid approach that enables lightweight yet precise detection in the industrial setting. In particular, we integrate the process of categorizing potential vulnerable apps with analysis techniques, to reduce the inevitable human inspection effort. We categorize potential vulnerable apps based on characteristics of vulnerability signatures, to reduce the burden on static analysis. We flexibly integrate static and dynamic analyses for apps in each identified family, to refine the family signatures and hence target on precise detection. We implement our approach in a practical system and deploy the system on the Pwnzen platform. By using the system, we identify and report potential vulnerabilities of 24 vulnerable apps (falling into 3 vulnerability families) to their developers, and some of these reported vulnerabilities are previously unknown. The apps of each vulnerability family in total have over 50 million downloads. We also propose countermeasures and highlight promising directions for technology transfer.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {842–847},
numpages = {6},
keywords = {Android security, Internet sockets, Vulnerability analysis},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3174243.3174262,
author = {Stitt, Greg and Gupta, Abhay and Emas, Madison N. and Wilson, David and Baylis, Austin},
title = {Scalable Window Generation for the Intel Broadwell+Arria 10 and High-Bandwidth FPGA Systems},
year = {2018},
isbn = {9781450356145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3174243.3174262},
doi = {10.1145/3174243.3174262},
abstract = {Emerging FPGA systems are providing higher external memory bandwidth to compete with GPU performance. However, because FPGAs often achieve parallelism through deep pipelines, traditional FPGA design strategies do not necessarily scale well to large amounts of replicated pipelines that can take advantage of higher bandwidth. We show that sliding-window applications, an important subset of digital signal processing, demonstrate this scalability problem. We introduce a window generator architecture that enables replication to over 330 GB/s, which is an 8.7x improvement over previous work. We evaluate the window generator on the Intel Broadwell+Arria10 system for 2D convolution and show that for traditional convolution (one filter per image), our approach outperforms a 12-core Xeon Broadwell E5 by 81x and a high-end Nvidia P6000 GPU by an order of magnitude for most input sizes, while improving energy by 15.7x. For convolutional neural nets (CNNs), we show that although the GPU and Xeon typically outperform existing FPGA systems, projected performances of the window generator running on FPGAs with sufficient bandwidth can outperform high-end GPUs for many common CNN parameters.},
booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {173–182},
numpages = {10},
keywords = {convolution, fpga, neural networks},
location = {Monterey, CALIFORNIA, USA},
series = {FPGA '18}
}

@inproceedings{10.1145/3213344.3213346,
author = {Vasilakis, Nikos and Goel, Pranjal and Demoulin, Henri Maxime and Smith, Jonathan M.},
title = {The Web as a Distributed Computing Platform},
year = {2018},
isbn = {9781450358378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213344.3213346},
doi = {10.1145/3213344.3213346},
abstract = {Perceived as a vast, interconnected graph of content, the reality of the web is very different. Immense computational resources are used to deliver this content and associated services. An even larger pool of computing power is comprised by edge user devices. This latent potential has gone unused. Ar~frames the web as a distributed computing platform, unifying processing and storage infrastructure with a core programming model and a common set of browser-provided services. By exposing the inherent capacities to programmers, a far more powerful capability has been unleashed, that of the Internet as a distributed computing system. We have implemented a prototype system that, while modest in scale, fully illustrates what can be realized.},
booktitle = {Proceedings of the 1st International Workshop on Edge Systems, Analytics and Networking},
pages = {7–12},
numpages = {6},
keywords = {Distribution, Internet, JavaScript, Web},
location = {Munich, Germany},
series = {EdgeSys'18}
}

@inproceedings{10.1145/3326365.3326424,
author = {Faroqi, Md Gofran},
title = {The Role of Telecentre in Developing Entrepreneurship: a Case Study on Union Digital Centres in Bangladesh},
year = {2019},
isbn = {9781450366441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326365.3326424},
doi = {10.1145/3326365.3326424},
abstract = {This paper evaluates the role of a telecentre project- the Union Digital Centres (UDC) in Bangladesh- in developing entrepreneurship among rural youths under Public-Private Partnership. Contrary to engaging qualified operators in terms of efficiency and investment the UDC has employed local youths who are small scale investors, home grown and amateur in skill. Many operators earn very low and some run off the project leading to closure of centres. In this context understanding the issue of entrepreneurship attains significance for the sustainability of the model. Entrepreneurship is defined in terms of operator's income, his/her satisfaction on income and the level of investment. The study hypothesizes that factors responsible for entrepreneurship development include ICT and internet, services, clients' turnout, stakeholders' involvement and experience of the operator. Based on an internet survey and interview of operators and a review of literature the study explores features of UDC facilitated entrepreneurship. It describes stakeholders' involvement in ensuring inputs and their roles in promoting entrepreneurship. An application of a structural equation model supports the hypotheses validating that operator's own initiative synergized with support from the government influence the development of entrepreneurship among operators. The results specify that the UDC bears potential to develop entrepreneurship among operators given they use a variety of equipment and better quality internet, provide a range of services to a greater volume of clients, receive stakeholder's support and demonstrate computer and managerial skills. The government's role in implementation, human resource management and monitoring of the project influences the process. The study also finds that the project experiences a range of challenges from inadequate involvement of partners. Since deficiency in any partner's role can affect the mission of entrepreneurship development, the author suggests that the government must take the lead role in engaging relevant partners to convert the UDC as an enterprise.},
booktitle = {Proceedings of the 12th International Conference on Theory and Practice of Electronic Governance},
pages = {446–456},
numpages = {11},
keywords = {Digital Bangladesh, Entrepreneur, ICT, Income, Investment, Public-Private Partnership, Sustainability},
location = {Melbourne, VIC, Australia},
series = {ICEGOV '19}
}

@inproceedings{10.1145/2987443.2987460,
author = {Malloy, Matthew and McNamara, Mark and Cahn, Aaron and Barford, Paul},
title = {Ad Blockers: Global Prevalence and Impact},
year = {2016},
isbn = {9781450345262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987443.2987460},
doi = {10.1145/2987443.2987460},
abstract = {Ad blockers are a formidable threat to the vitality of the online advertising eco-system. Understanding their prevalence and impact is challenging due to the massive scale and diversity of the eco-system. In this paper, we utilize unique data gathering assets to assess the prevalence and impact of ad blockers from an Internet-wide perspective. Our study is based on (i) a 2 million person world-wide user panel that provides ground truth for ad blocker installations and (ii) telemetry from large number of publisher web pages and ads served to publishers. We describe a novel method for assessing the prevalence of ad blocker installations that is based on Mixture Proportion Estimation. We apply this method to nearly 2 trillion web transactions collected over the period of 1 month (February 2016), to derive ad blocker prevalence estimates for desktop systems in diverse geographic areas and for diverse demographic groups. Next, using deployment estimates we consider the impact of ad blockers on users and on publisher sites. Specifically, we report on the reduction of ads shown to users with ad blockers installed and show that even though a user may have an ad blocker installed, they are still exposed to a significant number of ads. We also characterize the impact of ad blockers across different categories of publisher sites including those that may be participating in whitelisting.},
booktitle = {Proceedings of the 2016 Internet Measurement Conference},
pages = {119–125},
numpages = {7},
keywords = {ad blockers, empirical measurement, mixture proportion estimation},
location = {Santa Monica, California, USA},
series = {IMC '16}
}

@inproceedings{10.1145/2785989.2785990,
author = {Fossati, Thomas and Gurbani, Vijay K. and Kolesnikov, Vladimir},
title = {Love All, Trust Few: on Trusting Intermediaries in HTTP},
year = {2015},
isbn = {9781450335409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785989.2785990},
doi = {10.1145/2785989.2785990},
abstract = {Recent pervasive monitoring of Internet traffic has resulted in an effort to protect all communications by using Transport Layer Security (TLS) to thwart malicious third parties. We argue that such large-scale use of TLS may potentially disrupt many useful network-based services provided by middleboxes such as content caching, web acceleration, anti-malware scanning and traffic shaping when faced with congestion. As the use of Internet grows to include devices with varying resources and capabilities, and access networks with differing link characteristics, the prevalent two-party TLS model may prove restrictive. We present EFGH, a pluggable TLS extension that allows a trusted third-party to be introduced in the two-party model without affecting the underlying end-to-end security of the channel. The extension stresses the end-to-end trust relationship integrity by allowing selective exposure of the exchanged data to trusted middleboxes.},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Hot Topics in Middleboxes and Network Function Virtualization},
pages = {1–6},
numpages = {6},
keywords = {EFGH, HTTP, HTTPs, TLS, middlebox, trusted proxy},
location = {London, United Kingdom},
series = {HotMiddlebox '15}
}

@inproceedings{10.1145/3054977.3057294,
author = {Francis, Jonathan and Oltramari, Alessandro and Munir, Sirajum and Shelton, Charles and Rowe, Anthony},
title = {Context Intelligence in Pervasive Environments: Poster Abstract},
year = {2017},
isbn = {9781450349666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3054977.3057294},
doi = {10.1145/3054977.3057294},
abstract = {Intelligent personalization systems are becoming increasingly reliant on contextually-relevant devices and services, such as those available within modern IoT deployments. An IoT context may emerge---or become pervasive---when the intelligent system generates knowledge from dialogue-based interactions with the end-user; the context is strengthened even further by incorporating state representations about the environment (e.g., generated from wireless sensor data) into the knowledge graph. This is crucial for pervasive applications like digital assistance in IoT, where context-aware systems need to adapt quickly: activities like leaving work home-bound, driving to the grocery store, arriving at home, and walking the dog, for example, can occur in a relatively short period of time--- during which an intelligent assistant must be able to support user requests in a consistent and coherent manner. Given that computational ontologies can serve as semantic models for heterogeneous data, they are becoming increasingly viable for reasoning across different IoT contexts. This involves: (a) federation and dynamic pruning of multiple modular ontologies, ideally, to comprehensively capture only the knowledge that will facilitate execution of a multi-context task; (b) fast consistency-checking and ontology-based inferences, aided by rules-based execution environments that can evaluate/transform ambient wireless sensor network (WSN) data, in real-time; and (c) run-time execution of ontology-based control procedures, through rule-engine actuation commands sent across the WSN. Only by realizing these functionalities may intelligent systems be capable of reasoning over device properties, system states, and user activities, while appropriately delegating commands to other intelligent agents or other relevant IoT services. In this poster, we illustrate how a multi-context knowledge base can be structured on the basis of modular ontologies and integrated with a distributed rules-based inference engine in multiple smart-building environments, in order to enable scalable contextual reasoning for intelligent assistance. Preliminary results are also discussed. This work is conducted through the partnership of Bosch Research Pittsburgh and Carnegie Mellon University (CMU), and is in partial satisfaction of CMU's Bosch Energy Research Network (BERN) grant, awarded for developments in intelligent building solutions. The approach we describe is also partially based on the Ubiquitous Personal Assistant (UPA) project, Bosch Research's largest research initiative worldwide.},
booktitle = {Proceedings of the Second International Conference on Internet-of-Things Design and Implementation},
pages = {315–316},
numpages = {2},
keywords = {Ontologies, Rules Based Engines, Semantic Sensor Networks},
location = {Pittsburgh, PA, USA},
series = {IoTDI '17}
}

@inproceedings{10.1145/3175684.3175726,
author = {Kashef, Rasha and Niranjan, Akshat},
title = {Handling Large-Scale Data using Two-Tier Hierarchical Super-Peer P2P Network},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175726},
doi = {10.1145/3175684.3175726},
abstract = {In the ever-expanding world of IoT, data has not only increased in volume and velocity but has also moved from residing in centralized nodes to distributed nodes across multiple locations. Traditional data clustering technologies, based on centralized operations, cannot be scaled to efficiently manage Big Data, thus creating a need for clustering in distributed environments. To address the problem of modularity, flexibility, and scalability, a dynamic hierarchical two-tier architecture and model for cooperative clustering in distributed super-peer P2P network is presented in this paper. The proposed model is called Distributed Cooperative Clustering in super-peer P2P networks (DCCP2P). It involves a hierarchy of two layers of P2P neighborhoods. In the first layer, peers in each neighborhood are responsible for building local cooperative sub-clusters from the local data. Each node sends a summarized view of local data to its super-peer in a form of sub-cluster's centroids extracted from the local cooperative clustering, minimizing the exchange of information between nodes and their super-peers. In the next layer, sub-clusters are merged at each super-peer and at the root of the hierarchy, where one global clustering can be derived. The distributed cooperative approach finds globally optimized clusters and achieves significant improvement in global clustering solutions without the cost of centralized clustering.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {52–56},
numpages = {5},
keywords = {Decentralized Clustering, Speedup, Super-Peer P2P Network},
location = {London, United Kingdom},
series = {BDIOT '17}
}

@inproceedings{10.1145/3143361.3143362,
author = {Chiesa, Marco and Demmler, Daniel and Canini, Marco and Schapira, Michael and Schneider, Thomas},
title = {SIXPACK: Securing Internet eXchange Points Against Curious onlooKers},
year = {2017},
isbn = {9781450354226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143361.3143362},
doi = {10.1145/3143361.3143362},
abstract = {Internet eXchange Points (IXPs) play an ever-growing role in Internet inter-connection. To facilitate the exchange of routes amongst their members, IXPs provide Route Server (RS) services to dispatch the routes according to each member's peering policies. Nowadays, to make use of RSes, these policies must be disclosed to the IXP. This poses fundamental questions regarding the privacy guarantees of route-computation on confidential business information. Indeed, as evidenced by interaction with IXP administrators and a survey of network operators, this state of affairs raises privacy concerns among network administrators and even deters some networks from subscribing to RS services. We design Sixpack1, an RS service that leverages Secure Multi-Party Computation (SMPC) to keep peering policies confidential, while extending, the functionalities of today's RSes. As SMPC is notoriously heavy in terms of communication and computation, our design and implementation of Sixpack aims at moving computation outside of the SMPC without compromising the privacy guarantees. We assess the effectiveness and scalability of our system by evaluating a prototype implementation using traces of data from one of the largest IXPs in the world. Our evaluation results indicate that Sixpack can scale to support privacy-preserving route-computation, even at IXPs with many hundreds of member networks.},
booktitle = {Proceedings of the 13th International Conference on Emerging Networking EXperiments and Technologies},
pages = {120–133},
numpages = {14},
keywords = {Internet eXchange Points, Secure Multi Party Computation, interdomain routing, privacy-preserving routing},
location = {Incheon, Republic of Korea},
series = {CoNEXT '17}
}

@inproceedings{10.1145/3234804.3234811,
author = {Wang, Wentao and He, Dongzhi},
title = {Click-through Rate Estimates based on Deep Learning},
year = {2018},
isbn = {9781450364737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234804.3234811},
doi = {10.1145/3234804.3234811},
abstract = {Internet advertising has become a major source of income for Internet companies, among which the prediction of ad click-through rate is the most important task. The accuracy of ad click-through rate can directly generate revenue for the company. At present, the mainstream methods such as Baidu and Google are linear models with a lot of artificial features, which are more and more unsustainable. Because a lot of manual features consume a lot of manpower, their benefits are declining. Linear models cannot learn the nonlinear relationship between features. In this paper, we propose a method for forecasting click rate of advertisements based on deep learning, which can make full use of large-scale sparse data and learn non-linear features, and further analyze the role of different features in predicting ad click through rate. The experimental results on the KDD Cup 2012 Track2 validate that the proposed method can improve the predictive performance of search ads, with an AUC value of 0.771.},
booktitle = {Proceedings of the 2018 2nd International Conference on Deep Learning Technologies},
pages = {12–15},
numpages = {4},
keywords = {CTR, DNN, FM, deep learning},
location = {Chongqing, China},
series = {ICDLT '18}
}

@inproceedings{10.1145/2733373.2807410,
author = {Ooi, Beng Chin and Tan, Kian-Lee and Wang, Sheng and Wang, Wei and Cai, Qingchao and Chen, Gang and Gao, Jinyang and Luo, Zhaojing and Tung, Anthony K.H. and Wang, Yuan and Xie, Zhongle and Zhang, Meihui and Zheng, Kaiping},
title = {SINGA: A Distributed Deep Learning Platform},
year = {2015},
isbn = {9781450334594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2733373.2807410},
doi = {10.1145/2733373.2807410},
abstract = {Deep learning has shown outstanding performance in various machine learning tasks. However, the deep complex model structure and massive training data make it expensive to train. In this paper, we present a distributed deep learning system, called SINGA, for training big models over large datasets. An intuitive programming model based on the layer abstraction is provided, which supports a variety of popular deep learning models. SINGA architecture supports both synchronous and asynchronous training frameworks. Hybrid training frameworks can also be customized to achieve good scalability. SINGA provides different neural net partitioning schemes for training large models. SINGA is an Apache Incubator project released under Apache License 2.},
booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
pages = {685–688},
numpages = {4},
keywords = {deep learning, distributed training},
location = {Brisbane, Australia},
series = {MM '15}
}

@inproceedings{10.1145/3131365.3131406,
author = {Kline, Jeff and Barford, Paul and Cahn, Aaron and Sommers, Joel},
title = {On the structure and characteristics of user agent string},
year = {2017},
isbn = {9781450351188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131365.3131406},
doi = {10.1145/3131365.3131406},
abstract = {User agent (UA) strings transmitted during HTTP transactions convey client system configuration details to ensure that content returned by a server is appropriate for the requesting host. As such, analysis of UA strings and their structure offers a unique perspective on active client systems in the Internet and when tracked longitudinally, offers a perspective on the nature of system and configuration dynamics. In this paper, we describe our study of UA string characteristics. Our work is based on analyzing a unique corpus of over 1B UA strings collected over a period of 2 years by comScore. We begin by analyzing the general characteristics of UA strings, focusing on the most prevalent strings and dynamic behaviors. We identify the top 10 most popular User Agents, which account for 26% of total daily volume. These strings describe the expected instances of popular platforms such as Microsoft, Apple and Google. We then report on the characteristics of low-volume UA strings, which has important implications for unique device identification. We show that this class of user agent generates the overwhelming majority of traffic, with between 2M and 10M instances observed each day. We show that the distribution of UA strings has temporal dependence and we show the distribution measured depends on the type of content served. Finally, we report on two large-scale UA anomalies characterized by web browsers sending false and misleading UAs in their web requests.},
booktitle = {Proceedings of the 2017 Internet Measurement Conference},
pages = {184–190},
numpages = {7},
keywords = {character entropy matrix, internet measurement, user agent strings},
location = {London, United Kingdom},
series = {IMC '17}
}

@article{10.1145/2927964.2927977,
author = {Tong, Da and Prasanna, Viktor},
title = {High Throughput Sketch Based Online Heavy Hitter Detection on FPGA},
year = {2016},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5964},
url = {https://doi.org/10.1145/2927964.2927977},
doi = {10.1145/2927964.2927977},
abstract = {In the context of networking, a heavy hitter is an entity in a data stream whose amount of activity (such as bandwidth consumption or number of connections) is higher than a given threshold. Detecting heavy hitters is a critical task for network management and security in the Internet and data centers. Data streams in modern network usually contain millions of entities, such as traffic flows or IP domains. It is challenging to detect heavy hitters at a high throughput while supporting such a large number of entities. I this work, we propose a high throughput online heavy hitter detector based on the Count-min sketch algorithm on FPGA. We propose a high throughput hash computation architecture, optimize the Count-min sketch for hardwarebased heavy hitter detection and use forwarding to deal with data hazards. The post place-and-route results of our architecture on a state-of-the-art FPGA shows high throughput and scalability. Our architecture achieves a throughput of 114 Gbps while supporting a typical 1 M concurrent entities. It sustains 100+ Gbps throughput while supporting various number of concurrent entities, stream sizes and accuracy requirements. Our implementation demonstrates improved performance compared with other sketch acceleration techniques on various platforms using similar sketch configurations.},
journal = {SIGARCH Comput. Archit. News},
month = {apr},
pages = {70–75},
numpages = {6},
keywords = {Heavy hitter detection, Sketch data structure, Streaming algorithm}
}

@inproceedings{10.1145/3331184.3331230,
author = {Ren, Kan and Qin, Jiarui and Fang, Yuchen and Zhang, Weinan and Zheng, Lei and Bian, Weijie and Zhou, Guorui and Xu, Jian and Yu, Yong and Zhu, Xiaoqiang and Gai, Kun},
title = {Lifelong Sequential Modeling with Personalized Memorization for User Response Prediction},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331230},
doi = {10.1145/3331184.3331230},
abstract = {User response prediction, which models the user preference w.r.t. the presented items, plays a key role in online services. With two-decade rapid development, nowadays the cumulated user behavior sequences on mature Internet service platforms have become extremely long since the user's first registration. Each user not only has intrinsic tastes, but also keeps changing her personal interests during lifetime. Hence, it is challenging to handle such lifelong sequential modeling for each individual user. Existing methodologies for sequential modeling are only capable of dealing with relatively recent user behaviors, which leaves huge space for modeling long-term especially lifelong sequential patterns to facilitate user modeling. Moreover, one user's behavior may be accounted for various previous behaviors within her whole online activity history, i.e., long-term dependency with multi-scale sequential patterns. In order to tackle these challenges, in this paper, we propose a Hierarchical Periodic Memory Network for lifelong sequential modeling with personalized memorization of sequential patterns for each user. The model also adopts a hierarchical and periodical updating mechanism to capture multi-scale sequential patterns of user interests while supporting the evolving user behavior logs. The experimental results over three large-scale real-world datasets have demonstrated the advantages of our proposed model with significant improvement in user response prediction performance against the state-of-the-arts.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {565–574},
numpages = {10},
keywords = {click-through rate prediction, lifelong sequential modeling, memory network, user modeling, user response prediction},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.5555/3001647.3001660,
author = {Joseph, Vinay and Borst, Sem and Reiman, Martin I.},
title = {Optimal rate allocation for video streaming in wireless networks with user dynamics},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {1063-6692},
abstract = {We consider the problem of optimal rate allocation and admission control for adaptive video streaming sessions in wireless networks with user dynamics. The central aim is to achieve an optimal tradeoff between several key objectives: maximizing the average rate utility per user, minimizing the temporal rate variability, and maximizing the number of users supported. We derive sample path upper bounds for the long-term net utility rate in terms of either a linear program or a concave optimization problem, depending on whether the admissible rate set is discrete or continuous. We then show that the upper bounds are asymptotically achievable in large-scale systems by policies which either deny access to a user or assign it a fixed rate for its entire session, without relying on any advance knowledge of the duration. Moreover, the asymptotically optimal policies exhibit a specific structure, which allow them to be characterized through just a single variable, and have the further property that the induced offered load is unity. We exploit the latter insights to devise parsimonious online algorithms for learning and tracking the optimal rate assignments and establish the convergence of these algorithms. Extensive simulation experiments demonstrate that the proposed algorithms perform well, even in relatively small-scale systems.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {820–835},
numpages = {16},
keywords = {channel allocation, multimedia communication, quality of service, wireless networks}
}

@inproceedings{10.1145/3298689.3346996,
author = {Yi, Xinyang and Yang, Ji and Hong, Lichan and Cheng, Derek Zhiyuan and Heldt, Lukasz and Kumthekar, Aditee and Zhao, Zhe and Wei, Li and Chi, Ed},
title = {Sampling-bias-corrected neural modeling for large corpus item recommendations},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346996},
doi = {10.1145/3298689.3346996},
abstract = {Many recommendation systems retrieve and score items from a very large corpus. A common recipe to handle data sparsity and power-law item distribution is to learn item representations from its content features. Apart from many content-aware systems based on matrix factorization, we consider a modeling framework using two-tower neural net, with one of the towers (item tower) encoding a wide variety of item content features. A general recipe of training such two-tower models is to optimize loss functions calculated from in-batch negatives, which are items sampled from a random mini-batch. However, in-batch loss is subject to sampling biases, potentially hurting model performance, particularly in the case of highly skewed distribution. In this paper, we present a novel algorithm for estimating item frequency from streaming data. Through theoretical analysis and simulation, we show that the proposed algorithm can work without requiring fixed item vocabulary, and is capable of producing unbiased estimation and being adaptive to item distribution change. We then apply the sampling-bias-corrected modeling approach to build a large scale neural retrieval system for YouTube recommendations. The system is deployed to retrieve personalized suggestions from a corpus with tens of millions of videos. We demonstrate the effectiveness of sampling-bias correction through offline experiments on two real-world datasets. We also conduct live A/B testings to show that the neural retrieval system leads to improved recommendation quality for YouTube.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {269–277},
numpages = {9},
keywords = {information retrieval, neural networks, recommender systems},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{10.1145/3110025.3119403,
author = {Wu, Bin and Tong, Xuesong and Guo, Qian},
title = {CES: A System for Community Evaluation},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3119403},
doi = {10.1145/3110025.3119403},
abstract = {With the development of Internet, we are gradually entering the era of big data. The size of network is increasing and the structure of network is becoming more complex. Community is a unique network structure with great research value. The task of community analysis goes through two separate phases: first, detection of meaningful community structure from a network, and second, evaluation of the appropriateness of the detected community structure. With the popularity of network research, many community detection algorithms emerged which can be grouped in categories, based on different criteria. In order to applicate community detection algorithms in real-world network analysis, we need to measure the performance of the algorithm. The performance depends on two points, that is, whether the algorithm can give the result of community division in an acceptable time, and whether the algorithm can reveal the community structure of the network with high quality. In recent years, systems used to analyze network and detect community are mushrooming. However, existing systems rarely have evaluation function, either providing social network analysis or providing data analysis service. We need a tool to evaluate community detection algorithms. In response to the challenge, the Community Evaluation System (CES) is proposed to meet the demands of community detection algorithms' evaluation. CES can evaluate community detection algorithms with multiple metrics. It uses B/S mode, integrates Spark, Yarn and HDFS technology to support the operation of large-scale data, and experiments prove that it is effective.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {597–600},
numpages = {4},
keywords = {Community detection, Community detection metric, Evaluation system, Parallel system framework},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3307772.3331019,
author = {Cheung, Chung Ming and Kuppannagari, Sanmukh Rao and Kannan, Rajgopal and Prasanna, Viktor K.},
title = {Towards Improved Real-Time Observability of Behind-Meter PhotoVoltaic Systems: A Data-Driven Approach},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3331019},
doi = {10.1145/3307772.3331019},
abstract = {The stochasticity in energy supply introduced by increased PV penetration in grid distribution systems can lead to frequent and sudden supply-demand imbalances, thereby causing voltage and frequency fluctuations. Enhanced observability, in real-time, of grid attributes such as real and reactive power flows, power injections/consumption and voltages is becoming increasingly essential to ensure smooth grid operation under these new large-scale PV integration scenarios. Two significant challenges prevent grid operators from achieving this objective: 1) Computational complexity of quasi-static time-series simulations, and 2) Partial observability of behind-meter grid assets. In this work, we take a step towards this objective by enhancing the observability of smart inverter equipped PV systems located behind AMI meters. Smart inverters convert the DC output of PV systems into real and reactive power, the ratio of which is governed by a power factor setting. This in turn is governed by the real-time voltage at its terminal, as per a pre-set volt/var curve. While it is possible to infer the power factor setting - thereby, real and reactive power injection of the PV system - at each time interval by quasi-static time series simulations, such methods are not scalable to large grid sizes. We propose to reduce the reliance on simulations by developing a deep neural network based data-driven model which predicts the time dependent power factors of the PV systems in the grid. The model uses a small amount of simulated data to learn the dependence of power factor on 1) load and apparent PV power injection (root mean square of real and reactive power) data, and 2) AMI net-load data (for BTM PV assets). Our results show that our model can predict power factors with mean absolute error of less than 6.2% under full observability scenario and under 8.6% under the partial observability scenario. Thus, our data-driven model can reduce the amount of simulations required to compute real and reactive injections from PV systems in the distribution system with or without complete information.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {447–455},
numpages = {9},
keywords = {Data-Driven Approach, Disaggregation, Neural Networks, Real-Time Observability},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@article{10.1145/3329119,
author = {Fan, Ching-Ling and Lo, Wen-Chih and Pai, Yu-Tung and Hsu, Cheng-Hsin},
title = {A Survey on 360° Video Streaming: Acquisition, Transmission, and Display},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3329119},
doi = {10.1145/3329119},
abstract = {Head-mounted displays and 360° videos have become increasingly more popular, delivering a more immersive viewing experience to end users. Streaming 360° videos over the best-effort Internet, however, faces tremendous challenges, because of the high resolution and the short response time requirements. This survey presents the current literature related to 360° video streaming. We start with 360° video streaming systems built for real experiments to investigate the practicality and efficiency of 360° video streaming. We then present the video and viewer datasets, which may be used to drive large-scale simulations and experiments. Different optimization tools in various stages of the 360° video streaming pipeline are discussed in detail. We also present various applications enabled by 360° video streaming. In the appendices, we review the off-the-shelf hardware available at the time of writing and the open research problems.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {71},
numpages = {36},
keywords = {360° videos, Virtual reality, video streaming}
}

@inproceedings{10.1145/2756509.2756514,
author = {Newton, Ben and Aikat, Jay and Jeffay, Kevin},
title = {Simulating large-scale airborne networks with ns-3},
year = {2015},
isbn = {9781450333757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2756509.2756514},
doi = {10.1145/2756509.2756514},
abstract = {Large-scale airborne networks, which connect airborne nodes with high-bandwidth communication links are being actively pursued commercially. We propose utilizing thousands of operational passenger and cargo aircraft as the principal components of an airborne network which could provide high-speed Internet to passengers on-board and on the ground. To simulate such a network we have augmented the ns-3 network simulator with a model for ingesting and processing aircraft position information, a steerable directional antenna model, a wireless point-to-point channel and associated net devices, and a distributed topology control application to manage the topology of the mesh network. We describe our implementation of these models and some tools for visualizing airborne networks. Using a simulation of a large airborne network, covering the United States, we perform experiments to evaluate the effectiveness of using the Optimized Link State Routing Protocol (OLSR) to route network traffic. Our simulations lead us to conclude that OLSR is likely not a good fit for our envisioned network.},
booktitle = {Proceedings of the 2015 Workshop on Ns-3},
pages = {32–39},
numpages = {8},
keywords = {OLSR, airborne networks, ns-3, steerable directional antennas, topology control},
location = {Barcelona, Spain},
series = {WNS3 '15}
}

@inproceedings{10.1145/2671188.2749414,
author = {Witbrock, Michael J.},
title = {Dense Models from Videos: Can YouTube be the Font of All Knowledge Bases?},
year = {2015},
isbn = {9781450332743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2671188.2749414},
doi = {10.1145/2671188.2749414},
abstract = {Many recent advances in computer science have been driven by the convergent availability of large numbers of data and of fast machines on which to analyze them. This availability has enabled us to acquire implicit partial models of the underlying generators for the data and apply those models to tasks such as translation, transcription, and image captioning. To date, though, few if any of these models have been dense, in the sense of thoroughly modelling some aspect of the world in way that can facilitate any relevant task. Dense models should support:a) Prediction: What might happen next in this situation, or what might be true in the vicinity?b) Interpolation: What may have happened between these situations? What might be located between these things?c) Causal reasoning: Why did this happen?d) Purpose reasoning: What is this configuration of things for? For what purpose is that happening?e) Task performance: The model should be able to aid (e.g.) a robot performing a domain task.f) Explanation: The model should be at a level that supports communication.In short, a dense model is the sort of model - including both implicit and explicit components - humans form about aspects of their worlds: aspects like meetings, plants, lawnmowers, rivers and kitchens. These models support pretty-much any kind of relevant reasoning.These are also the sorts of models that builders of large-scale "commonsense" knowledge bases have been working to construct. But, to date, although some such knowledge bases support particular instances of each kind of reasoning task, they do not approach doing so comprehensively, even within quite narrow domains. Although some work is being done on automating KB construction, this generally aims at breadth, rather than density.Similarly, although machine vision and NLP researchers have long discussed the potential use of background knowledge in scene and text understanding, demonstrating that utility in any general way has been hampered by the vast incompleteness of available KBs.The time is ripe for a 5-10 year AI challenge problem in production of dense models directly from data. As a particular example, kitchens are somewhat limited in complexity, from a human point of view, and are densely modelled by most humans; we are not frequently surprised by what we find in a kitchen, or by what happens there. And we are not lacking for data; there are more than 6 million YouTube hits for "kitchen", around 5 million for cooking. If each was a mere 1 minute long, this represents 22 years of kitchen video. Dull perhaps, but also, presumably, enough grist for building a very dense model.The proposed challenge is this: to have computers automatically build, from just the vast amount of video found on the web, a sufficiently dense local world model to enable that video to be thoroughly understood for prediction, interpolation, explanation and other tasks.},
booktitle = {Proceedings of the 5th ACM on International Conference on Multimedia Retrieval},
pages = {1},
numpages = {1},
keywords = {knowledge bases, machine vision, model induction, online video},
location = {Shanghai, China},
series = {ICMR '15}
}

@proceedings{10.5555/3018843,
title = {IA^3 '16: Proceedings of the Sixth Workshop on Irregular Applications: Architectures and Algorithms},
year = {2016},
isbn = {9781509038671},
publisher = {IEEE Press},
abstract = {IA3 seeks to explore the challenges posed by irregular behaviors in applications. These occur in many subject matters. They have a significant degree of latent parallelism, which however is difficult to exploit due to their complex behavior. Current high performance architectures rely on data locality and regular computation to reduce access latencies, and often do not cope well with the requirements of these applications. Furthermore, irregular applications are difficult to scale on current supercomputing machines, due to their limits with fine-grained synchronization and small data transfers. The solutions needed to address irregular applications challenges can only come by considering the problem from all perspectives: from micro- to systemarchitectures, from compilers to languages, from libraries to runtimes, from algorithm design to data characteristics. Only collaborative efforts among researchers with different expertise, including end users, domain experts, and computer scientists, can lead to significant breakthroughs. This workshop brings together scientists with these different backgrounds to discuss, define, and design methods and technologies for efficiently supporting irregular applications on current and future systems.It has been extremely interesting to experience how the area of Irregular Applications has evolved in these years. The number of fields touched by issues of irregular behaviors has greatly expanded, as the community realized that graph-like representations are convenient in many other fields beside analysis of large networks. Computer Aided Design, healthcare, finance, government and e-government, and many others all benefit form semi-structured or not rigidly structured representations. Data streaming and data persistence are new upcoming challenges that pertain irregular applications. Sparse matrices and sparse linear algebra, used both for iterative solvers and to implement graph algorithms, have become an area of intense interest. Workflows that integrate both irregular and regular data structures, such as attributed graphs that couple graph views with tables for attributes, require new solutions and approaches at all levels to perform efficiently in both conditions. Emerging machine learning approaches exploits sparse data structures and deep nets that require further refining of techniques to mitigate issues of irregular behaviors. Novel architectural ideas, born from research for post-Moore law systems, have introduced new paradigms that appear very promising for better supporting irregular workloads and, in general, the upcoming generation of data analytics, inferencing, and discovering applications.},
location = {Salt Lake City, Utah}
}

@inproceedings{10.1145/3323716.3323734,
author = {Chen, Cheng-Chen and Chen, Li-Hsuan and Guo, Meng-Han},
title = {Integration of building interface and smart sensor network to control indoor air pollution through internet of things},
year = {2019},
isbn = {9781450361040},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323716.3323734},
doi = {10.1145/3323716.3323734},
abstract = {The interior environment of Taiwan has been affected by interior air pollution for a long time. It would affect our health and comfort directly, and it also affects our efficiency of work and study. Especially for Formaldehyde, VOCs and particle etc., those contaminations would be changed by the environmental factor easily. This study is focusing on analyzing the relationship between contamination resources and removable contamination, then we can get to understand the connection between cleansing airs and different kinds of interface management of construction. This study uses the IoT connecting system and full-scale space to analyze different factors indoor in order to collect big data. It can clarify the most influential factor for the intervening variable of different management of construction project throughout the statistics of the big data, and make the result to be one of the determent analyzing assessment mode. The study result builds the "SIPETs" throughout the "platform of smart sensing and data analyzing". It can monitor the condition of contamination distributing indoor, and control the air flowing, air cleansing, energy saving and peristome shifting etc., and maintain the benefit of interior comfort and saving energy. Building this system helps us improve those existing buildings to adjust itself for facing climate changing. And it also can protect people who live inside and maintain comfort and reduce the value at risk.},
booktitle = {Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications},
pages = {15–20},
numpages = {6},
keywords = {IOT, indoor air pollution, indoor and outdoor ratios, smart sensor network},
location = {Osaka, Japan},
series = {IEEA '19}
}

@article{10.1109/TCBB.2018.2828305,
author = {Ghoshal, Asish and Zhang, Jinyi and Roth, Michael A. and Xia, Kevin Muyuan and Grama, Ananth Y. and Chaterji, Somali},
title = {A Distributed Classifier for MicroRNA Target Prediction with Validation Through TCGA Expression Data},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {15},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2828305},
doi = {10.1109/TCBB.2018.2828305},
abstract = {Background: MicroRNAs miRNAs are approximately 22-nucleotide long regulatory RNA that mediate RNA interference by binding to cognate mRNA target regions. Here, we present a distributed kernel SVM-based binary classification scheme to predict miRNA targets. It captures the spatial profile of miRNA-mRNA interactions via smooth B-spline curves. This is accomplished separately for various input features, such as thermodynamic and sequence-based features. Further, we use a principled approach to uniformly model both canonical and non-canonical seed matches, using a novel seed enrichment metric. Finally, we verify our miRNA-mRNA pairings using an Elastic Net-based regression model on TCGA expression data for four cancer types to estimate the miRNAs that together regulate any given mRNA. Results: We present a suite of algorithms for miRNA target prediction, under the banner Avishkar, with superior prediction performance over the competition. Specifically, our final kernel SVM model, with an Apache Spark backend, achieves an average true positive rate TPR of more than 75 percent, when keeping the false positive rate of 20 percent, for non-canonical human miRNA target sites. This is an improvement of over 150 percent in the TPR for non-canonical sites, over the best-in-class algorithm. We are able to achieve such superior performance by representing the thermodynamic and sequence profiles of miRNA-mRNA interaction as curves, devising a novel seed enrichment metric, and learning an ensemble of miRNA family-specific kernel SVM classifiers. We provide an easy-to-use system for large-scale interactive analysis and prediction of miRNA targets. All operations in our system, namely candidate set generation, feature generation and transformation, training, prediction, and computing performance metrics are fully distributed and are scalable. Conclusions: We have developed an efficient SVM-based model for miRNA target prediction using recent CLIP-seq data, demonstrating superior performance, evaluated using ROC curves for different species human or mouse, or different target types canonical or non-canonical. We analyzed the agreement between the target pairings using CLIP-seq data and using expression data from four cancer types. To the best of our knowledge, we provide the first distributed framework for miRNA target prediction based on Apache Hadoop and Spark. Availability: All source code and sample data are publicly available at https://bitbucket.org/cellsandmachines/avishkar. Our scalable implementation of kernel SVM using Apache Spark, which can be used to solve large-scale non-linear binary classification problems, is available at https://bitbucket.org/cellsandmachines/kernelsvmspark.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {1037–1051},
numpages = {15}
}

@inproceedings{10.1145/2482540.2482609,
author = {Engelberg, Roee and Fabrikant, Alex and Schapira, Michael and Wajc, David},
title = {Best-response dynamics out of sync: complexity and characterization},
year = {2018},
isbn = {9781450319621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2482540.2482609},
doi = {10.1145/2482540.2482609},
abstract = {In many computational and economic models of multi-agent interaction, each participant repeatedly "best-responds" to the others' actions. Game theory research on the prominent "best-response dynamics" model typically relies on the premise that the interaction between agents is somehow synchronized. However, in many real-life settings, e.g., internet protocols and large-scale markets, the interaction between participants is asynchronous. We tackle the following important questions: (1) When are best-response dynamics guaranteed to converge to an equilibrium even under asynchrony? (2) What is the (computational and communication) complexity of verifying guaranteed convergence? We show that, in general, verifying guaranteed convergence is intractable. In fact, our main negative result establishes that this task is undecidable. We exhibit, in contrast, positive results for several environments of interest, including complete, computationally-tractable, characterizations of convergent systems. We discuss the algorithmic implications of our results, which extend beyond best-response dynamics to applications such as asynchronous Boolean circuits.},
booktitle = {Proceedings of the Fourteenth ACM Conference on Electronic Commerce},
pages = {379–396},
numpages = {18},
keywords = {asynchronous models, best response dynamics, complexity, convergence, game theory},
location = {Philadelphia, Pennsylvania, USA},
series = {EC '13}
}

@article{10.1145/3230485,
author = {McLaughlin, Adam and Bader, David A.},
title = {Accelerating GPU betweenness centrality},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3230485},
doi = {10.1145/3230485},
abstract = {Graphs that model social networks, numerical simulations, and the structure of the Internet are enormous and cannot be manually inspected. A popular metric used to analyze these networks is Betweenness Centrality (BC), which has applications in community detection, power grid contingency analysis, and the study of the human brain. However, these analyses come with a high computational cost that prevents the examination of large graphs of interest.Recently, the use of Graphics Processing Units (GPUs) has been promising for efficient processing of unstructured data sets. Prior GPU implementations of BC suffer from large local data structures and inefficient graph traversals that limit scalability and performance. Here we present a hybrid GPU implementation that provides good performance on graphs of arbitrary structure rather than just scale-free graphs as was done previously. Our methods achieve up to 13\texttimes{} speedup on high-diameter graphs and an average of 2.71\texttimes{} speedup overall compared to the best existing GPU algorithm. We also observe near linear speedup when running BC on 192 GPUs.},
journal = {Commun. ACM},
month = {jul},
pages = {85–92},
numpages = {8}
}

@inproceedings{10.5555/2830689.2830693,
author = {Esmaeilzadeh, Hadi},
title = {Approximate acceleration: a path through the era of dark silicon and big data},
year = {2015},
isbn = {9781467383202},
publisher = {IEEE Press},
abstract = {Performance is the raw material for computing. For more than 40 years, consistent and exponential improvement in transistor scaling coupled with continuous advances in general-purpose processor design has exponentially reduced its cost. However, as we enter the dark silicon era (as we projected in our study [1, 2] and others corroborate [3]), the benefits from transistor scaling are diminishing and the current paradigm of processor design significantly falls short of the traditional cadence of performance improvements due to power limitations. Performance has hit the power wall. These shortcomings can drastically curtail the industry's ability to continuously deliver new capabilities, breaking the backbone of its economic ecosystem. These challenges have coincided with the big data revolution. The rate of data generation is increasing at an overwhelming rate that is beyond the capabilities of current computing systems to match. Expert analyses show that the zettabyte barrier was cracked in 2010. In 2011, the amount of information generated surpassed 1.8 zettabytes (trillion gigabytes). By 2020, consumers will generate 50x this staggering figure [4]. While data generation is quadrupling each year, modern processors have seen a performance improvement of roughly 10--15% every two years. Worse yet, the long-standing memory sub-system bottlenecks--long access latency, bounded communication bandwidth, and limited capacity--leave little hope of managing this explosion of data through traditional incremental improvements. Moreover, many IoT objects (wearable devices/environmental sensors) must operate on hard-to-replace batteries or harvest energy from intermittent ambient sources. The application timeliness and the limited capacity of the wireless link compound the challenges arising from the power constraints. Given the scale of the problem, transformative research is essential across many domains, including computing, communication, and even control.},
booktitle = {Proceedings of the 2015 International Conference on Compilers, Architecture and Synthesis for Embedded Systems},
pages = {31–32},
numpages = {2},
location = {Amsterdam, The Netherlands},
series = {CASES '15}
}

@inproceedings{10.1145/2834050.2834102,
author = {W\"{a}hlisch, Matthias and Schmidt, Robert and Schmidt, Thomas C. and Maennel, Olaf and Uhlig, Steve and Tyson, Gareth},
title = {RiPKI: The Tragic Story of RPKI Deployment in the Web Ecosystem},
year = {2015},
isbn = {9781450340472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2834050.2834102},
doi = {10.1145/2834050.2834102},
abstract = {Web content delivery is one of the most important services on the Internet. Access to websites is typically secured via TLS. However, this security model does not account for prefix hijacking on the network layer, which may lead to traffic blackholing or transparent interception. Thus, to achieve comprehensive security and service availability, additional protective mechanisms are necessary such as the RPKI, a recently deployed Resource Public Key Infrastructure to prevent hijacking of traffic by networks. This paper argues two positions. First, that modern web hosting practices make route protection challenging due to the propensity to spread servers across many different networks, often with unpredictable client redirection strategies; and, second, that we need a better understanding why protection mechanisms are not deployed. To initiate this, we empirically explore the relationship between web hosting infrastructure and RPKI deployment. Perversely, we find that less popular websites are more likely to be secured than the prominent sites. Worryingly, we find many large-scale CDNs do not support RPKI, thus making their customers vulnerable. This leads us to explore business reasons why operators are hesitant to deploy RPKI, which may help to guide future research on improving Internet security.},
booktitle = {Proceedings of the 14th ACM Workshop on Hot Topics in Networks},
articleno = {11},
numpages = {7},
keywords = {BGP, CDN, RPKI, deployment, hosting infrastructure, secure inter-domain routing},
location = {Philadelphia, PA, USA},
series = {HotNets-XIV}
}

@inproceedings{10.1145/2785956.2787500,
author = {Chen, Fangfei and Sitaraman, Ramesh K. and Torres, Marcelo},
title = {End-User Mapping: Next Generation Request Routing for Content Delivery},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787500},
doi = {10.1145/2785956.2787500},
abstract = {Content Delivery Networks (CDNs) deliver much of the world's web, video, and application content on the Internet today. A key component of a CDN is the mapping system that uses the DNS protocol to route each client's request to a ``proximal'' server that serves the requested content. While traditional mapping systems identify a client using the IP of its name server, we describe our experience in building and rolling-out a novel system called end-user mapping that identifies the client directly by using a prefix of the client's IP address. Using measurements from Akamai's production network during the roll-out, we show that end-user mapping provides significant performance benefits for clients who use public resolvers, including an eight-fold decrease in mapping distance, a two-fold decrease in RTT and content download time, and a 30% improvement in the time-to-first byte. We also quantify the scaling challenges in implementing end-user mapping such as the 8-fold increase in DNS queries. Finally, we show that a CDN with a larger number of deployment locations is likely to benefit more from end-user mapping than a CDN with a smaller number of deployments.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {167–181},
numpages = {15},
keywords = {DNS, akamai, content delivery networks, load balancing, name servers, network measurement, request routing, server assignment, web performance},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{10.1145/3167132.3167146,
author = {Makki, Majid and Van Landuyt, Dimitri and Lagaisse, Bert and Joosen, Wouter and Hofstede, Nick},
title = {Transparent IO access control for application-level tenant isolation},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167146},
doi = {10.1145/3167132.3167146},
abstract = {The economy-of-scale benefits of multi-tenancy are most compelling at the application level, as this deployment model allows optimally sharing a single application instance and its runtime resources between multiple customer organizations. However, this requires, among other things, controlling and isolating access of tenants to IO resources (e.g. storage/network) at the application level. In this paper, we present an application-level middleware which transparently enforces tenant isolation vis-\`{a}-vis access to IO resources. The solution is useful for preventing unauthorized access to IO resources especially when access to IO resources is parameterized by overly complex user inputs and occur in numerous places of a large and complex code-base, e.g. in legacy applications.The transparent nature of isolation enforcement is achieved by extending and customizing the platform security capabilities of modern programming languages. The alternative approach, i.e. requiring application developers to implement tenant isolation explicitly, is concomitant to inevitable human errors and oversight. Our implementation is evaluated using a prototype application that is representative of realistic requirements of an industry-level SaaS provider. In order to show the reduced risk of human error, we deliver an assessment of the required development effort for enabling multi-tenancy and compare it to the baseline of implementing tenant isolation manually. In addition, our in-depth performance evaluation yields an average relative runtime overhead of 4.47% which demonstrates the suitability of the middleware for real-world use-cases.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {143–150},
numpages = {8},
keywords = {multi-tenancy, software-as-a-service, tenant isolation},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2938559.2948834,
author = {Thang, Dang Duy and Nam, Le Hoai and Khoi, Nguyen Tan},
title = {Poster: Developing an Intrusion Detection System for Cloud Computing},
year = {2016},
isbn = {9781450344166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938559.2948834},
doi = {10.1145/2938559.2948834},
abstract = {Intrusion Detection System (IDS) has been used widely in network systems to detect malicious behaviors which can harm system or computers. However, in practical implementations, an IDS is a complex work and needs investing in the new infrastructure or in training new personnel. Therefore, it is inconsistent with the users who has a small network system. In this paper, we introduce an IDSCloud model as a solution to implementing a network IDS based on cloud computing. The IDSCloud provides network IDS as a service over the internet which can be simple in deployment, maintenance, scalability without investing in the new infrastructure.},
booktitle = {Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion},
pages = {20},
numpages = {1},
keywords = {cloud computing, intrusion detection},
location = {Singapore, Singapore},
series = {MobiSys '16 Companion}
}

@inproceedings{10.1145/3307334.3328636,
author = {Kang, Hangil and Baek, Duin and Ryoo, Jihoon},
title = {Saliency based 360° Video Contents Encoding for Streaming Service (poster)},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3328636},
doi = {10.1145/3307334.3328636},
abstract = {Video streaming service has been essential to the Internet ecosystem since a majority of Internet contents is consumed via streaming ser-vices, such as Netflix and Youtube [3]. Moreover, contents providers started to upload 4K and even 8K videos on streaming service to satisfy users' demand for higher quality of streaming service in accordance with the recent refinement on display technology.However, the available bandwidth in most of the developed countries can barely support single full HD contents streaming service [1]. Such bandwidth shortage intensifies in 360 video streaming service even at a larger scale, as 360 video contents providers started to upload 4K or 8K videos at the higher frame rate(60FPS or 120FPS). No current ISP (Internet service provider) can sustain the bandwidth needed for such scale of video contents [1]. To overcome the aforementioned network limitations, many re-searchers and engineers suggested ingenious mechanisms to reduce contents size. One of the mechanisms is viewport-only streaming service that streams a partial region in each video frame that fits in the exact viewport in an HMD device (e.g., HTC Vive, OculusVR, Google Daydream VR, and Samsung GearVR) in real-time [6].Assuming the size of the viewport is set to 90 degree, the bandwidth required for the viewport-only streaming service can be approximately reduced by an eighth of the original 360 video. Although the viewport-only streaming service can reduce con-tents size significantly, it is infeasible yet due to the existing streaming network latency. Even with the state-of-the-art content delivery network (CDN), viewport-only streaming service cannot satisfy the 10ms latency requirement in the standard Internet as demonstrated in other interactive multimedia systems [4]. Consequently, viewers can suffer discontinuity of streaming contents even in the highly optimized streaming service [8]. Thus, the research trend moved to the viewport adaptive stream-ing service that buffers segments of contents where a viewport Such reduction of contents size is achieved by the contents compression process that provides the original resolution in the Field ofView(FoV) while compromising the video quality outside the FoV. Despite the reduction of contents size and delivery of navigable 360 video contents, viewport adaptive streaming service cannot accommodate viewers' head movement within buffered video frames,especially when a viewer's focal point deviates from the viewport within the buffered frames. In this case, viewers can experience video distortions or degradation of resolutions [8]. Therefore, anew encoding mechanism that can handle viewers' deviation from the viewport while reducing contents size is required for viewers'immersive experience.To answer the requirements, we propose a saliency-based view-port adaptive streaming service, SALI360 that focuses on improving viewers' quality of perception in 360 video contents. To achieve high perception quality, we first adopt visual saliency model [9]to predict fixation regions in 360 video contents. Then we render the fixation regions on top of the geometry based encoded regions.Specifically, SALI360 encodes the peripheral regions in lower resolution to reduce the contents size, and encodes the fixation regions in higher resolution to increase the quality of perception.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {582–583},
numpages = {2},
keywords = {360 video, saliency, viewport adaptive streaming service, virtual reality},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1145/3320326.3320400,
author = {Mitrevski, Pece and Mitrevski, Filip and Gusev, Marjan},
title = {A Decade Time-Lapse of Cloud Performance and Dependability Modeling: Performability Evaluation Framework},
year = {2019},
isbn = {9781450366458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320326.3320400},
doi = {10.1145/3320326.3320400},
abstract = {A plethora of works has been published over the past decade, contributing in invaluable ways to the area of cloud performance and dependability modeling. As Cloud Computing became a new emerging technology aimed at large-scale resource sharing and service-oriented computing, their attention was spread over capacity planning, joint optimization of cost and capacity, analysis of failure characteristics of servers, failure and recovery behaviors, cloud service reliability, service availability and provisioning response delays, availability of a large-scale clouds with multiple classes of server pools, migration of physical machines among pools, cloud service performance, QoS measurements, simultaneous analysis of performance, availability, and power consumption, as well as end-to-end performability analysis of a cloud service. In this paper, we present a decade time-lapse of such research efforts, with a focus on the use of a class of Stochastic Petri Nets with reward structures as an integrated part of modeling, known as Stochastic Reward Nets. In addition, we propose a modeling framework for performability evaluation of a cloud service, which reveals considerable potential for further research in this area.},
booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems &amp; Security},
articleno = {66},
numpages = {6},
keywords = {Cloud Computing, Hierarchical Composition, Performability, Performance, Service Reliability, Stochastic Petri Nets, System Availability},
location = {Rabat, Morocco},
series = {NISS '19}
}

@inproceedings{10.1145/3175731.3176178,
author = {Hachem, Jamal El and Chiprianov, Vanea and Babar, Ali and Aniorte, Philippe},
title = {Towards methodological support for secure architectures of software-intensive systems-of-systems},
year = {2016},
isbn = {9781450363990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175731.3176178},
doi = {10.1145/3175731.3176178},
abstract = {In our hyper-connected world, the complexity, interactions and security of the multiple complex software systems give rise to a particular growing interest in Systems-of-Systems (SoS) engineering. Basically, SoS are intended to achieve performance, behavior or global goals that none of their Constituent Systems (CSs) is able to achieve independently. Some examples of SoS application domains are defense and national security, business information systems, Internet-of-Everything (IoE) and smart cities. Whereas there have been several attempts to define SoS and many designations were used to describe these complex systems composed of distributed independent CSs which interact to realize a common goal, there is yet no shared agreement on their definition [9]. However, Jamshidi's [8] definition is one of the most popular: "SoS are large-scale, distributed, concurrent systems comprised of complex systems". Many other researchers characterized SoS by their main features, particularly Mair specifies the following five essential characteristics referred to by the acronym OMGEE [14][12]: Operational and Managerial independence of the CSs, Geog -raphic distribution, Evolutionary development and Emergent behavior. Several other concepts could describe SoS such as: global mission, belonging, autonomy, connectivity and diversity [14].},
booktitle = {Proceedings of the International Colloquium on Software-Intensive Systems-of-Systems at 10th European Conference on Software Architecture},
articleno = {9},
numpages = {6},
location = {Copenhagen, Denmark},
series = {SiSoS@ECSA '16}
}

@inproceedings{10.1145/3020078.3021746,
author = {Fang, Xin and Ioannidis, Stratis and Leeser, Miriam},
title = {Secure Function Evaluation Using an FPGA Overlay Architecture},
year = {2017},
isbn = {9781450343541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3020078.3021746},
doi = {10.1145/3020078.3021746},
abstract = {Secure Function Evaluation (SFE) has received considerable attention recently due to the massive collection and mining of personal data over the Internet, but large computational costs still render it impractical. In this paper, we leverage hardware acceleration to tackle the scalability and efficiency challenges inherent in SFE. To that end, we propose a generic, reconfigurable implementation of SFE as a coarse-grained FPGA overlay architecture. Contrary to tailored approaches that are tied to the execution of a specific SFE structure, and require full reprogramming of an FPGA with each new execution, our design allows repurposing an FPGA to evaluate different SFE tasks without the need for reprogramming. Our implementation shows orders of magnitude improvement over a software package for evaluating garbled circuits, and demonstrates that the circuit being evaluated can change with almost no overhead.},
booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {257–266},
numpages = {10},
keywords = {FPGA, garbled circuits, reconfigurable logic applications, secure function evaluation},
location = {Monterey, California, USA},
series = {FPGA '17}
}

@inproceedings{10.1145/3292500.3330947,
author = {Matsubara, Yasuko and Sakurai, Yasushi},
title = {Dynamic Modeling and Forecasting of Time-evolving Data Streams},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330947},
doi = {10.1145/3292500.3330947},
abstract = {Given a large, semi-infinite collection of co-evolving data sequences (e.g., IoT/sensor streams), which contains multiple distinct dynamic time-series patterns, our aim is to incrementally monitor current dynamic patterns and forecast future behavior. We present an intuitive model, namely OrbitMap, which provides a good summary of time-series evolution in streams. We also propose a scalable and effective algorithm for fitting and forecasting time-series data streams. Our method is designed as a dynamic, interactive and flexible system, and is based on latent non-linear differential equations. Our proposed method has the following advantages: (a) It is effective: it captures important time-evolving patterns in data streams and enables real-time, long-range forecasting; (b) It is general: our model is general and practical and can be applied to various types of time-evolving data streams; (c) It is scalable: our algorithm does not depend on data size, and thus is applicable to very large sequences. Extensive experiments on real datasets demonstrate that OrbitMap makes long-range forecasts, and consistently outperforms the best existing state-of-the-art methods as regards accuracy and execution speed.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {458–468},
numpages = {11},
keywords = {data stream, forecasting, non-linear systems, time series},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/2940136.2940141,
author = {Stohr, Denny and Fr\"{o}mmgen, Alexander and Fornoff, Jan and Zink, Michael and Buchmann, Alejandro and Effelsberg, Wolfgang},
title = {QoE Analysis of DASH Cross-Layer Dependencies by Extensive Network Emulation},
year = {2016},
isbn = {9781450344258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940136.2940141},
doi = {10.1145/2940136.2940141},
abstract = {With the rising importance of video streaming in the Internet, dynamic adaptive streaming over HTTP (DASH) has been established as a key technology for video delivery. Yet, variable network conditions often result in a limited quality of experience (QoE)—with the interrelation of cross-layer network factors and DASH mechanisms widely unexplored. To understand the complex dependencies between DASH configurations and network conditions, we propose a systematic extensive large-scale emulation approach with state-of- the-art QoE metrics. Using this approach with a real DASH player in Mininet, we emulated more than 10, 000 combinations of static and dynamic network conditions and DASH configurations to derive their QoE. The obtained results show that no single DASH configuration provides the highest achievable QoE. Depending on the network conditions, specific combinations of the TCP congestion control, segment sizes and the DASH adaptation algorithm provide higher QoE—showing the possibility of performance improvements in practice. Furthermore, the ex- tensive emulations show a linear relation between delay, loss and QoE that is mostly independent of bandwidth.},
booktitle = {Proceedings of the 2016 Workshop on QoE-Based Analysis and Management of Data Communication Networks},
pages = {25–30},
numpages = {6},
keywords = {Cross-Layer, DASH, Network Emulation},
location = {Florianopolis, Brazil},
series = {Internet-QoE '16}
}

@inproceedings{10.1145/2933267.2933427,
author = {Ollesch, Julius},
title = {Adaptive steering of cyber-physical systems with atomic complex event processing services: doctoral symposium},
year = {2016},
isbn = {9781450340212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933267.2933427},
doi = {10.1145/2933267.2933427},
abstract = {Given the advent of cyber-physical systems (CPS), event-based control paradigms such as complex event processing (CEP) are vital enablers for adaptive analytical control mechanisms. CPS are becoming a high-profile research topic as they are key to disruptive digital innovations such as autonomous driving, industrial internet, smart grid and ambient assisted living. However, organizational and technological scalability of today's CEP approaches is limited by their monolithic architectures. This leads to the research idea for atomic CEP entities and the hypothesis that a network of small event-based control services is better suited for CPS development and operation than current centralised approaches. In addition, the paper summarizes preliminary results of the presented doctoral work and outlines questions for future research as well as an evaluation plan.},
booktitle = {Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
pages = {402–405},
numpages = {4},
keywords = {architecture, complex event processing, cyber-physical systems, internet of things, microservices, real-time analytics, sensor networks, spatio-temporal processing, web services},
location = {Irvine, California},
series = {DEBS '16}
}

@inproceedings{10.1145/2790798.2790821,
author = {Sirisutthidecha, Suthee and Maichalernnukul, Kiattisak},
title = {Reliable Virtual Channels over VPN for Cloud},
year = {2015},
isbn = {9781450334198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790798.2790821},
doi = {10.1145/2790798.2790821},
abstract = {Cloud computing and virtualization are the novel deployments of large-scale computing systems over the Internet. Customers get their computation and storage from the cloud computing providers and can pay for the service on demand. However, the security of communication between a client and cloud is highly concerned and increasingly becoming important. Virtual private network (VPN) is the mostly used technology to ensure the security of network communications. However, there are many factors which results in VPN failure, e.g., poor network, software/hardware problem of VPN gateway. In either case, users will loss a connection and stream communication from the cloud although there are a backup VPN gateway and a backup cloud server. Especially, when users use transmission control protocol (TCP) applications, all the TCP connections will be broken. This paper proposes an improved virtual private network (iVPN) which can continuously serve users who would like to use TCP applications on a cloud site. The users do not have to modify their existing applications or operating systems in order to use iVPN over an existing VPN communication. Moreover, the existing VPN gateway and cloud server can still operate as before.},
booktitle = {Proceedings of the Eighth International C* Conference on Computer Science &amp; Software Engineering},
pages = {133–137},
numpages = {5},
keywords = {Cloud Computing, Network Virtualization, Overlay Networks},
location = {Yokohama, Japan},
series = {C3S2E '15}
}

@inproceedings{10.1145/3308558.3313433,
author = {Li, Minne and Qin, Zhiwei and Jiao, Yan and Yang, Yaodong and Wang, Jun and Wang, Chenxi and Wu, Guobin and Ye, Jieping},
title = {Efficient Ridesharing Order Dispatching with Mean Field Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313433},
doi = {10.1145/3308558.3313433},
abstract = {A fundamental question in any peer-to-peer ridesharing system is how to, both effectively and efficiently, dispatch user's ride requests to the right driver in real time. Traditional rule-based solutions usually work on a simplified problem setting, which requires a sophisticated hand-crafted weight design for either centralized authority control or decentralized multi-agent scheduling systems. Although recent approaches have used reinforcement learning to provide centralized combinatorial optimization algorithms with informative weight values, their single-agent setting can hardly model the complex interactions between drivers and orders. In this paper, we address the order dispatching problem using multi-agent reinforcement learning (MARL), which follows the distributed nature of the peer-to-peer ridesharing problem and possesses the ability to capture the stochastic demand-supply dynamics in large-scale ridesharing scenarios. Being more reliable than centralized approaches, our proposed MARL solutions could also support fully distributed execution through recent advances in the Internet of Vehicles (IoV) and the Vehicle-to-Network (V2N). Furthermore, we adopt the mean field approximation to simplify the local interactions by taking an average action among neighborhoods. The mean field approximation is capable of globally capturing dynamic demand-supply variations by propagating many local interactions between agents and the environment. Our extensive experiments have shown the significant improvements of MARL order dispatching algorithms over several strong baselines on the accumulated driver income (ADI), and order response rate measures. Besides, the simulated experiments with real data have also justified that our solution can alleviate the supply-demand gap during the rush hours, thus possessing the capability of reducing traffic congestion.},
booktitle = {The World Wide Web Conference},
pages = {983–994},
numpages = {12},
keywords = {Mean Field Reinforcement Learning, Multi-Agent Reinforcement Learning, Order Dispatching},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3297280.3297650,
author = {Duncan, Siobhan},
title = {Taking stigmergy out of the lab and into the field: student research abstract},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297650},
doi = {10.1145/3297280.3297650},
abstract = {Swarm robotics is a biologically inspired field of applied AI, which takes inspiration from the macro intelligence displayed by swarms of relatively simple animals which are capable of self-organising to achieve tasks far beyond the capability of any single agent. However swarm robotics research continues to be confined to simulation and laboratory based experiments as there are still significant challenges preventing large scale industrial applications.In this abstract, we outline work towards tackling one of these main challenges - distributed coordination of agents in the swarm. This project utilises stigmergy, coordination through information in the environment, to coordinate a swarm of robots searching an unknown environment. We propose to model stigmergy through a virtual environment stored in the cloud, which robots can communicate with over the internet. This project consists of 3 stages: first a simulation stage nearing completion; next a table-top robotics stage where a cloud service will be developed in a labroratory setting, with e-puck and a tracking and positioning setup; and finally where field robots will be used with this previously developed cloud service to investigate the use of 'Stigmergy as a Service' in real field robotic systems.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {961–964},
numpages = {4},
keywords = {cloud robotics, internet of things, multi-agent systems, stigmergy, swarm robotics},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3237191,
author = {Park, Ha-Myung and Silvestri, Francesco and Pagh, Rasmus and Chung, Chin-Wan and Myaeng, Sung-Hyon and Kang, U},
title = {Enumerating Trillion Subgraphs On Distributed Systems},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3237191},
doi = {10.1145/3237191},
abstract = {How can we find patterns from an enormous graph with billions of vertices and edges? The subgraph enumeration, which is to find patterns from a graph, is an important task for graph data analysis with many applications, including analyzing the social network evolution, measuring the significance of motifs in biological networks, observing the dynamics of Internet, and so on. Especially, the triangle enumeration, a special case of the subgraph enumeration, where the pattern is a triangle, has many applications such as identifying suspicious users in social networks, detecting web spams, and finding communities. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suffer from the massive shuffled data resulting in a very long processing time.In this article, we propose scalable methods for enumerating trillion subgraphs on distributed systems. We first propose PTE (Pre-partitioned Triangle Enumeration), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuffled data, total work, and network read. We also propose PSE (Pre-partitioned Subgraph Enumeration), a generalized version of PTE for enumerating subgraphs that match an arbitrary query graph. Experimental results show that PTE provides 79 times faster performance than recent distributed algorithms on real-world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges. Furthermore, PSE successfully enumerates 265 trillion clique subgraphs with 4 vertices from a subdomain hyperlink network, showing 47 times faster performance than the state of the art distributed subgraph enumeration algorithm.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {oct},
articleno = {71},
numpages = {30},
keywords = {Triangle enumeration, big data, distributed algorithm, graph algorithm, network analysis, scalable algorithm, subgraph enumeration}
}

@inproceedings{10.1145/3093241.3093287,
author = {Abozeid, Amr and Farouk, Hesham and ElDahshan, Kamal},
title = {Scalable Video Summarization: A Comparative Study},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093287},
doi = {10.1145/3093241.3093287},
abstract = {The amount of videos over the internet and media storage systems has dramatically increased. This poses challenges in video content understanding and management. A video is a complex and resource consuming media. In addition, efficient use of video data requires the data to be understood and accessed without having to watch it entirely. For those reasons, video summarization (VS) has been a hot topic of recent researches. VS is the process of creating a compact representation that can provide the user with concise information about the video content. VS helps in efficient storage, quick browsing, and retrieval of video data maintaining its main features. In video codec and streaming contexts, Scalable Video Coding (SVC) enables dynamic adaptation based on network conditions and device capabilities. This paper reviews the recent work on scalable video summarization (SVS) and discusses its role in current research directions.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {215–219},
numpages = {5},
keywords = {Scalable video coding, Video summarization, video content, video processing},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@inproceedings{10.1145/2898445.2898458,
author = {Wu, Ye},
title = {Giano: Toward Large Scale Access Security Management in Private Cloud},
year = {2016},
isbn = {9781450342858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2898445.2898458},
doi = {10.1145/2898445.2898458},
abstract = {Access control related problems and solutions are commonly taken for granted as the No.1 enterprise security concern. Today, as more and more companies shift their business to cloud, while blurring the boundary of information exchange and management, this inevitably introduces serious technical challenges and difficulties to the realm of access security management for loss of cognitive central control in some degree, thereby increasingly receiving more attentions and investigations on their original basis. Big internet companies such as Google, Facebook in the US., and Baidu in China, their IDCs consist of huge amount of physical servers and millions of virtual machines or containers, usually deemed as a profound private cloud. At meantime, doing daily jobs, hundreds and thousands of employees (most of them are engineers with different roles) demand to access multi-categorical resources in IDC frequently. For example, SREs may need to remotely logon production servers to configure environment or rectify system mistakes; RDs might logon by certain account associated to machines, initiating services to fulfill development work. Moreover, IDC is the place where tremendous applications are running dynamically and endlessly, among which they exchange information one another by accessing to data storage and computing services probably across domains. As a consequence, a very complicated topology based on accessing relationships is emerged due to interactions among massive people-devices-services. In order to solve such large-scale distributed access control centered problems, this apparently leads to a line of security technologies needing to be considered, including identity management, authentication methods, authorization models, auditing and reporting, regulatory compliance, tracing and forensic, domain isolation, intrusion detections, and even more the administration toolkits for security evaluation criteria.Toward designing and implementing this desired type of comprehensive security platform, while simultaneously circumventing relative reliable, scalable and performance issues in engineering, as is highly concerned in industrial-level products, it is really a daunting task if without developing appropriate abstraction on targets and innovative applicable theorem in depth, for reducing complexity and unifying mechanisms. In this talk, we just present such a real-world existing system developed by our team, namely Giano which embraces most aforementioned security techniques, already widely used for Baidu IDC operating management and integrated into many business products. Some important related theoretical work such as delegation logic, attribute-based authentication, proof-carrying authorization, et al. are about to be illustrated, with the focus on their applications in practice.},
booktitle = {Proceedings of the 4th ACM International Workshop on Security in Cloud Computing},
pages = {1},
numpages = {1},
keywords = {access control, enterprise security, identity management, private cloud},
location = {Xi'an, China},
series = {SCC '16}
}

@proceedings{10.1145/3030207,
title = {ICPE '17: Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 8th ACM/SPEC International Conference on Performance Engineering (ICPE), taking place in L'Aquila, Italy, in April 22-26, 2017. The goal of ICPE is to integrate theory and practice in the field of performance engineering by providing a forum for sharing ideas and experiences between industry and academia. ICPE grew out of the ACM Workshop on Software Performance (WOSP Est. 1998) and the SPEC International Performance Engineering Workshop (SIPEW Est. 2008). It is a great pleasure to introduce the exciting program for this year's conference in which researchers and practitioners present their latest research, newest innovations, and vision for the future of performance engineering.We received 83 high quality submissions across the Research, Industry/Experience and Work-Inprogress/ Vision tracks. The Research Track attracted 65 submissions with 24 papers (22 full, 2 short) accepted for presentation. In the Work-In-Progress/Vision Track 14 out of 25 contributions were selected and the Industry/Experience track attracted 18 submissions of which 5 were accepted for presentation. Each paper received at least three reviews from program committee members. Four best paper award candidates were also selected. The best paper is to be announced during the ICPE 2017 social event, after all four papers are presented at the conference.We are excited to also present three keynote talks as part of the technical program. Micro-Benchmarking Considered Harmful; When the Whole is Faster or Slower Than the Sum of its Parts, by Thomas Wuerthinger (Oracle Labs)Performance is Also a Matter of Where You Live, by Francesco Quaglia (University of Rome La Sapienza)Autonomic storage management at scale, by Arif Merchant (Google)In addition, the program includes five tutorials, a poster and demo track, the SPEC Distinguished Dissertation Award, and eight interesting workshops on Autonomous Control for Performance and Reliability Trade-offs in Internet of Services (ACPROSS), on Performance Analysis of Big Data Systems (PABS), on Challenges in Performance Methods for Software Development (WOSP-C), on Energy-aware Simulation (ENERGY-SIM), on Load Testing and Benchmarking of Software Systems (LTB), on Monitoring in Large-Scale Software Systems (MoLS), on Education and Practice of Performance Engineering (WEPPE), and on Quality-aware DevOps (QUDOS).The program covers traditional ICPE topics such as design for performance and problem diagnosis, online performance management, analytic models, empirical studies, model building, and benchmarking, as well as application of performance engineering theory and techniques to several practical fields, including distributed systems, cloud computing, storage, energy, big data, virtualized systems, and hardware.},
location = {L'Aquila, Italy}
}

@inproceedings{10.1145/2783258.2785467,
author = {Durrant-Whyte, Hugh},
title = {Data, Knowledge and Discovery: Machine Learning meets Natural Science},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2785467},
doi = {10.1145/2783258.2785467},
abstract = {Increasingly it is data, vast amounts of data, that drives scientific discovery. At the heart of this so-called "fourth paradigm of science" is the rapid development of large scale statistical data fusion and machine learning methods. While these developments in "big data" methods are largely driven by commercial applications such as internet search or customer modelling, the opportunity for applying these to scientific discovery is huge. This talk will describe a number of applied machine learning projects addressing real-world inference problems in physical, life and social science areas. In particular, I will describe a major Science and Industry Endowment Fund (SIEF) project, in collaboration with the NICTA and Macquarie University, looking to apply machine learning techniques to discovery in the natural sciences. This talk will look at the key methods in machine learning that are being applied to the discovery process, especially in areas like geology, ecology and biological discovery.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {7},
numpages = {1},
keywords = {data science},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@proceedings{10.1145/3053600,
title = {ICPE '17 Companion: Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 8th ACM/SPEC International Conference on Performance Engineering (ICPE), taking place in L'Aquila, Italy, in April 22-26, 2017. The goal of ICPE is to integrate theory and practice in the field of performance engineering by providing a forum for sharing ideas and experiences between industry and academia. ICPE grew out of the ACM Workshop on Software Performance (WOSP Est. 1998) and the SPEC International Performance Engineering Workshop (SIPEW Est. 2008). It is a great pleasure to introduce the exciting program for this year's conference in which researchers and practitioners present their latest research, newest innovations, and vision for the future of performance engineering.We received 83 high quality submissions across the Research, Industry/Experience and Work-Inprogress/ Vision tracks. The Research Track attracted 65 submissions with 24 papers (22 full, 2 short) accepted for presentation. In the Work-In-Progress/Vision Track 14 out of 25 contributions were selected and the Industry/Experience track attracted 18 submissions of which 5 were accepted for presentation. Each paper received at least three reviews from program committee members. Four best paper award candidates were also selected. The best paper is to be announced during the ICPE 2017 social event, after all four papers are presented at the conference.We are excited to also present three keynote talks as part of the technical program. Micro-Benchmarking Considered Harmful; When the Whole is Faster or Slower Than the Sum of its Parts, by Thomas Wuerthinger (Oracle Labs)Performance is Also a Matter of Where You Live, by Francesco Quaglia (University of Rome La Sapienza)Autonomic storage management at scale, by Arif Merchant (Google)In addition, the program includes five tutorials, a poster and demo track, the SPEC Distinguished Dissertation Award, and eight interesting workshops on Autonomous Control for Performance and Reliability Trade-offs in Internet of Services (ACPROSS), on Performance Analysis of Big Data Systems (PABS), on Challenges in Performance Methods for Software Development (WOSP-C), on Energy-aware Simulation (ENERGY-SIM), on Load Testing and Benchmarking of Software Systems (LTB), on Monitoring in Large-Scale Software Systems (MoLS), on Education and Practice of Performance Engineering (WEPPE), and on Quality-aware DevOps (QUDOS).The program covers traditional ICPE topics such as design for performance and problem diagnosis, online performance management, analytic models, empirical studies, model building, and benchmarking, as well as application of performance engineering theory and techniques to several practical fields, including distributed systems, cloud computing, storage, energy, big data, virtualized systems, and hardware.},
location = {L'Aquila, Italy}
}

@inproceedings{10.1145/3290480.3290498,
author = {Kong, Lingjing and Huang, Guowei and Zhou, Ying and Ye, Jianfeng},
title = {Fast Abnormal Identification for Large Scale Internet Traffic},
year = {2018},
isbn = {9781450365673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290480.3290498},
doi = {10.1145/3290480.3290498},
abstract = {Traffic classification and identification is the key part in network security industry. It can identify the types of the Internet traffic, and detect abnormal ones based on the features of network flows. Nowadays, due tothenon-transparency and complexity of the packets, machine learning methods are widely adopted to identify the abnormal traffic. As a classic supervised learning algorithm, SVM performed well in traffic identification, including the speed of training and predicting as well as the accuracy. However, with the amount of network traffic being larger, stand-alone SVM cannot meet the requirements and be difficult to deal with the large scale of network traffic. So, in this paper, we used parallel computing on spark to accelerate and fast deal with model training and predicting. At last, the comparison of training time, prediction time and accuracy with stand-alone SVM and SVM on spark will be given. Besides, the analysis in detail will be also presented.},
booktitle = {Proceedings of the 8th International Conference on Communication and Network Security},
pages = {117–120},
numpages = {4},
keywords = {Abnormalidentification, Spark, Support vector},
location = {Qingdao, China},
series = {ICCNS '18}
}

@inproceedings{10.1145/2808797.2808902,
author = {Popiel, Adrian and Kazienko, Przemys\l{}aw and Kajdanowicz, Tomasz},
title = {MuNeG: The Framework for Multilayer Network Generator},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2808902},
doi = {10.1145/2808797.2808902},
abstract = {It is a common problem that cost of extracting data for network analysis could be very high. Also sometimes in the Internet is it hard to find graph with desired features such as node degree or clustering level. Because of that graph generators can than be very helpful. In the past bunch of models of such generators was developed: random graphs, small worlds and scale free networks. All of these generators were developed to quickly and efficiently create networks with desired parameters. However all of this models produce single layer graphs. Domain of multiplexes or multilayer graphs has not already been so deeply analysed, also because it is hard to collect multilayer data among real datasets or there is hard to define what kind of information layers exactly should represent. Proposed MuNeG --- Multilayer Network Generator can produce, based on set of input parameters, multiplex networks - networks where each node has its counterpart in each layer. The carried out experiments proved that MuNeG graphs have different network and social parameters depends on input values. This feature gives user a very handful tool to generate multiplex networks on purpose of social network or complex network analysis. Generator features, input parameters and their influence on so called graph theory measures such as: node degree, average shortest path, diameter or clustering are described in the following article.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1316–1323},
numpages = {8},
keywords = {Collective Classification, Complex Networks, Network Generation, Synthethic Dataset},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3300061.3300123,
author = {Wang, Jing and Zheng, Yufan and Ni, Yunzhe and Xu, Chenren and Qian, Feng and Li, Wangyang and Jiang, Wantong and Cheng, Yihua and Cheng, Zhuo and Li, Yuanjie and Xie, Xiufeng and Sun, Yi and Wang, Zhongfeng},
title = {An Active-Passive Measurement Study of TCP Performance over LTE on High-speed Rails},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3300123},
doi = {10.1145/3300061.3300123},
abstract = {High-speed rail (HSR) systems potentially provide a more efficient way of door-to-door transportation than airplane. However, they also pose unprecedented challenges in delivering seamless Internet service for on-board passengers. In this paper, we conduct a large-scale active-passive measurement study of TCP performance over LTE on HSR. Our measurement targets the HSR routes in China operating at above 300 km/h. We performed extensive data collection through both controlled setting and passive monitoring, obtaining 1732.9 GB data collected over 135719 km of trips. Leveraging such a unique dataset, we measure important performance metrics such as TCP goodput, latency, loss rate, as well as key characteristics of TCP flows, application breakdown, and users' behaviors. We further quantitatively study the impact of frequent cellular handover on HSR networking performance, and conduct in-depth examination of the performance of two widely deployed transport-layer protocols: TCP CUBIC and TCP BBR. Our findings reveal the performance of today's commercial HSR networks "in the wild'', as well as identify several performance inefficiencies, which motivate us to design a simple yet effective congestion control algorithm based on BBR to further boost the throughput by up to 36.5%. They together highlight the need to develop dedicated protocol mechanisms that are friendly to extreme mobility.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {18},
numpages = {16},
keywords = {bbr, cubic, handover, high mobility, high-speed rails, lte, measurement, tcp},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inproceedings{10.1145/2817675.2817678,
author = {Liu, Xiaojun and Xie, Ning and Jia, Jinyuan},
title = {WebVis_BIM: real time web3D visualization of big BIM data},
year = {2015},
isbn = {9781450339407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2817675.2817678},
doi = {10.1145/2817675.2817678},
abstract = {With the rapid development of Building Information Modeling (BIM) and web3D technology, interactive visualization of big BIM data is very essential in industry. In this paper, Industry Foundation Classes (IFC) format is specially analyzed and we have found that it is usually big volume, complicated compositions and loose organizations, it is almost impossible to visualize big BIM data in real-time on Web browsers. To realize this challenging task, (1) big IFC data is lightweighted effectively via semantic cues and voxelization shape descriptor by removing unnecessary data redundancies; (2) each big building product is divided into exterior product and interior product; (3) incremental Frustum of Interest (FOI) based scene management is proposed for progressive loading big BIM scenes through Internet to realize real time rendering large scale BIM scene on Web browsers. The experimental results show our proposed method cannot only reduce the redundancies of big BIM data greatly, but also do visibility culling of complex BIM scenes exactly and efficiently. We successfully realize real-time visualization of complex BIM scenes on Web browsers.},
booktitle = {Proceedings of the 14th ACM SIGGRAPH International Conference on Virtual Reality Continuum and Its Applications in Industry},
pages = {43–50},
numpages = {8},
keywords = {BIM, IFC, lightweight, scene management, semantics},
location = {Kobe, Japan},
series = {VRCAI '15}
}

@inproceedings{10.1145/2799979.2799995,
author = {Stepanova, Taiana and Pechenkin, Alexander and Lavrova, Daria},
title = {Ontology-based big data approach to automated penetration testing of large-scale heterogeneous systems},
year = {2015},
isbn = {9781450334532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2799979.2799995},
doi = {10.1145/2799979.2799995},
abstract = {Global corporations and government organizations are nowadays represented in cyberspace in the form of numerous large-scale heterogeneous information systems, which implement corresponding business, technological and other types of processes. This extends the set of security analysis tasks, stated for these infrastructures, and tangles already existing tasks. This paper addresses the challenge of increasing penetration testing automation level through the adoption of semi-automatic knowledge extraction from the huge amounts of heterogeneous regularly updated data. The proposed solution is based on the novel penetration testing ontology, which gives a holistic view on the results of security analysis. Designed ontology is evaluated within the penetration testing framework prototype and binds together the conceptual (process) abstraction level, addressed by security experts, and technical abstraction level, employed in modern security analysis tools and methods.},
booktitle = {Proceedings of the 8th International Conference on Security of Information and Networks},
pages = {142–149},
numpages = {8},
keywords = {big data, large-scale systems, ontology, penetration testing},
location = {Sochi, Russia},
series = {SIN '15}
}

@inproceedings{10.1145/3264844.3271507,
author = {Das, Sajal K.},
title = {Chant or Starve: New Opportunities in Next Generation Challenged Networks},
year = {2018},
isbn = {9781450359269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264844.3271507},
doi = {10.1145/3264844.3271507},
abstract = {Challenged Networks (CHANTS) have made significant strides in more classical, often resource-starved, application domains such as disaster response, search and rescue operations, rural and opportunistic communications, vehicular and inter-planetary networks, etc. An intriguing question is: Do CHANTS have the potential to become a more mainstream technology and support innovative applications in everyday settings involving large-scale networks, such as IoTs, cyber-physical systems, crowd sensing, mobile cloud computing, smart and connected communities, to name a few? What are the most important technical challenges behind realization of the true potential of next generation CHANTS? What are the unique research opportunities? This talk will identify some of the underlying challenges as well as provide novel solutions in this direction.},
booktitle = {Proceedings of the 13th Workshop on Challenged Networks},
pages = {1},
numpages = {1},
location = {New Delhi, India},
series = {CHANTS '18}
}

@inproceedings{10.1145/3132847.3132903,
author = {Gollapudi, Sreenivas and Kumar, Ravi and Panigrahy, Debmalya and Panigrahy, Rina},
title = {Partitioning Orders in Online Shopping Services},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132903},
doi = {10.1145/3132847.3132903},
abstract = {The rapid growth of the Internet has led to the widespread use of newer and richer models of online shopping and delivery services. The race to efficient large scale on-demand delivery has transformed such services into complex networks of shoppers (typically working in the stores), stores, and consumers. The efficiency of processing orders in stores is critical to the profitability of the business model. Motivated by this setting, we consider the following problem: given a set of shopping orders each consisting of a few items, how to best partition the orders among a given number of shoppers working for an online shopping service? Formulating this as an optimization problem, we propose a family of simple and efficient algorithms that admit natural constraints such as number of items a shopper can process in this setting. In addition to showing provable guarantees for the algorithms, we also demonstrate their efficiency in practice on real-world data, outperforming strong baselines.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1319–1328},
numpages = {10},
keywords = {e-commerce, order partitioning, vehicle routing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@proceedings{10.1145/2912152,
title = {DIDC '16: Proceedings of the ACM International Workshop on Data-Intensive Distributed Computing},
year = {2016},
isbn = {9781450343527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the Sixth International Workshop on Data-intensive Distributed Computing (DIDC 2016), which is held in conjunction with the International ACM Symposium on High Performance Distributed Computing (HPDC 2016).The data needs of scientific as well as commercial applications from a diverse range of fields have been increasing exponentially over the recent years. Digital data generated from various sources such as scientific instruments, sensors, internet transactions, email, video and click streams can be large, diverse, longitudinal and distributed which poses new challenges and requirements for offline and real time processing where extraction of meaningful information can open novel application areas and lead to new breakthroughs. This data deluge and the increase in the demand for large-scale data processing has necessitated collaboration and sharing of data collections among the world's leading education, research, and industrial institutions and use of distributed resources owned by collaborating parties. In a widely distributed environment, data is often not locally accessible and has thus to be remotely retrieved and stored. While traditional distributed systems work well for computation that requires limited data handling, they may fail in unexpected ways when the computation accesses, creates, and moves large amounts of data especially over wide-area networks. Further, data accessed and created is often poorly described, lacking both metadata and provenance. Scientists, researchers, and application developers are often forced to solve basic data-handling issues, such as physically locating data, how to access it, and/or how to move it to visualization and/or compute resources for further analysis. Although many efforts have been made to develop new programming paradigms and models that can handle the data needs of the application automatically, the results are far from being optimized.DIDC focuses on the challenges imposed by data-intensive applications on distributed systems, and on the different state-of-the-art solutions proposed to overcome these challenges. It brings together the collaborative and distributed computing community and the data management community in an effort to generate productive conversations on the planning, management, and scheduling of data handling tasks and data storage resourcesThis year's workshop continues with the tradition of gathering distinguished speakers and providing a diverse program with a variety of topics ranging from data staging and indexing models for data-intensive applications to high-performance genomics and Cloud scheduling.},
location = {Kyoto, Japan}
}

@inproceedings{10.1145/3343031.3351007,
author = {Ding, Keyan and Ma, Kede and Wang, Shiqi},
title = {Intrinsic Image Popularity Assessment},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3351007},
doi = {10.1145/3343031.3351007},
abstract = {The goal of research in automatic image popularity assessment (IPA) is to develop computational models that can accurately predict the potential of a social image to go viral on the Internet. Here, we aim to single out the contribution of visual content to image popularity, ie, intrinsic image popularity. Specifically, we first describe a probabilistic method to generate massive popularity-discriminable image pairs, based on which the first large-scale image database for intrinsic IPA (I$^2$PA) is established. We then develop computational models for I$^2$PA based on deep neural networks, optimizing for ranking consistency with millions of popularity-discriminable image pairs. Experiments on Instagram and other social platforms demonstrate that the optimized model performs favorably against existing methods, exhibits reasonable generalizability on different databases, and even surpasses human-level performance on Instagram. In addition, we conduct a psychophysical experiment to analyze various aspects of human behavior in I$^2$PA.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1979–1987},
numpages = {9},
keywords = {deep neural networks, human behavior analysis., intrinsic image popularity, learning-to-rank},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3167132.3167217,
author = {NAAS, Mohammed Islam and Lemarchand, Laurent and Boukhobza, Jalil and Raipin, Philippe},
title = {A graph partitioning-based heuristic for runtime IoT data placement strategies in a fog infrastructure},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167217},
doi = {10.1145/3167132.3167217},
abstract = {Fog computing is a dense, heterogeneous and geographically distributed infrastructure. With the rise of IoT applications, objects may generate large amounts of data that may be processed at different locations of the Fog infrastructure. Data placement strategies have been designed to investigate the best storage location for data in order to reduce its access time for different IoT services spread over the infrastructure. Unfortunately, due to the large number of Fog nodes and the amount of data to be managed, placing data in such infrastructure is an NP-Hard problem. In this paper, we propose a divide and conquer heuristic for data placement strategies in Fog infrastructures. Our idea consists in dividing the original data placement problem into several balanced sub-problems using graph modeling and partitioning methods. Using our heuristic makes it possible to reduce the solving time by more than 450 times with less than 5% of optimality loss as compared to the exact solution (without subdivision). For a given number of partitions, our solution proved to be at least as close to the optimal as state-of-the-art solutions and 30% closer to the optimal for many workloads. In addition, our solution allowed for a better optimization in solving time as it is more flexible and scalable in terms of number of partitions.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {767–774},
numpages = {8},
keywords = {IoT, data placement, fog, generalized assignment problem, graph partitioning, optimization, storage},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3240508.3240664,
author = {Bhat, Divyashri and Deshmukh, Rajvardhan and Zink, Michael},
title = {Improving QoE of ABR Streaming Sessions through QUIC Retransmissions},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240664},
doi = {10.1145/3240508.3240664},
abstract = {While adaptive bitrate (ABR) streaming has contributed significantly to the reduction of video playout stalling, ABR clients continue to suffer from the variation of bit rate qualities over the duration of a streaming session. Similar to stalling, these variations in bit rate quality have a negative impact on the users' Quality of Experience (QoE). In this paper, we use a trace from a large-scale CDN to show that such quality changes occur in a significant amount of streaming sessions and investigate an ABR video segment retransmission approach to reduce the number of such quality changes. As the new HTTP/2 standard is becoming increasingly popular, we also see an increase in the usage of QUIC as an alternative protocol for the transmission of web traffic including video streaming. Using various network conditions, we conduct a systematic comparison of existing transport layer approaches for HTTP/2 that is best suited for ABR segment retransmissions. Since it is well known that both protocols provide a series of improvements over HTTP/1.1, we perform experiments both in controlled environments and over transcontinental links in the Internet and find that these benefits also "trickle up'' into the application layer when it comes to ABR video streaming where QUIC retransmissions can significantly improve the average quality bitrate while simultaneously minimizing bit rate variations over the duration of a streaming session.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1616–1624},
numpages = {9},
keywords = {abr streaming, dash, http/2, qoe, quic},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@proceedings{10.1145/2898445,
title = {SCC '16: Proceedings of the 4th ACM International Workshop on Security in Cloud Computing},
year = {2016},
isbn = {9781450342858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are very delighted to welcome everybody to the 2016 International Workshop on Security in Cloud Computing (SCC).There is no doubt that cloud computing has changed the paradigm of computing greatly. Hence, the security and privacy problems in cloud computing are of high importance. A lot of research and development efforts have been invested on related topics. The objective of this workshop is thus to bring together researchers and developers in this field, to enable them to share information about their recent progresses.The year of 2016 is the fourth for the SCC workshop. In total, 31 submissions were made from fifteen countries and regions, including Australia, Austria, Canada, China, Colombia, France, Germany, Hong Kong, India, Italy, Japan, Luxembourg, Singapore, United Kingdom, and United States. After a careful review by the program committee, 12 papers were selected for presentation at the workshop. We thank the Program Committee members as well as the external reviewers for their hard work.This year's SCC workshop also features a keynote speech "Giano - toward Large Scale Access Security Management in Private Cloud", by Dr. Ye Wu from Baidu, Inc. Given the leadership of Baidu in the Internet industry and Dr. Wu's splendid record of success in building cloud security systems, we believe this keynote speech will be very beneficial to all participants.},
location = {Xi'an, China}
}

@article{10.1145/3095755,
author = {Zhang, Cong and Liu, Jiangchuan and Wang, Haiyang},
title = {Cloud-Assisted Crowdsourced Livecast},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3095755},
doi = {10.1145/3095755},
abstract = {The past two years have witnessed an explosion of a new generation of livecast services, represented by Twitch.tv, GamingLive, and Dailymotion, to name but a few. With such a livecast service, geo-distributed Internet users can broadcast any event in real-time, for example, game, cooking, drawing, and so on, to viewers of interest. Its crowdsourced nature enables rich interactions among broadcasters and viewers but also introduces great challenges to accommodate their great scales and dynamics. To fulfill the demands from a large number of heterogeneous broadcasters and geo-distributed viewers, expensive server clusters have been deployed to ingest and transcode live streams. Yet our Twitch-based measurement shows that a significant portion of the unpopular and dynamic broadcasters are consuming considerable system resources; in particular, 25% of bandwidth resources and 30% of computational capacity are used by the broadcasters who do not have any viewers at all. In this article, through the real-world measurement and data analysis, we show that the public cloud has great potentials to address these scalability challenges. We accordingly present the design of Cloud-assisted Crowdsourced Livecast (CACL) and propose a comprehensive set of solutions for broadcaster partitioning. Our trace-driven evaluations show that our CACL design can smartly assign ingesting and transcoding tasks to the elastic cloud virtual machines, providing flexible and cost-effective system deployment.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jul},
articleno = {46},
numpages = {22},
keywords = {Crowdsourced livecast, public clouds, resource allocation, workload migration}
}

@article{10.1145/2676869,
author = {Li, Shing-Han and Kao, Yu-Cheng and Zhang, Zong-Cyuan and Chuang, Ying-Ping and Yen, David C.},
title = {A Network Behavior-Based Botnet Detection Mechanism Using PSO and K-means},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/2676869},
doi = {10.1145/2676869},
abstract = {In today's world, Botnet has become one of the greatest threats to network security. Network attackers, or Botmasters, use Botnet to launch the Distributed Denial of Service (DDoS) to paralyze large-scale websites or steal confidential data from infected computers. They also employ “phishing” attacks to steal sensitive information (such as users’ accounts and passwords), send bulk email advertising, and/or conduct click fraud. Even though detection technology has been much improved and some solutions to Internet security have been proposed and improved, the threat of Botnet still exists. Most of the past studies dealing with this issue used either packet contents or traffic flow characteristics to identify the invasion of Botnet. However, there still exist many problems in the areas of packet encryption and data privacy, simply because Botnet can easily change the packet contents and flow characteristics to circumvent the Intrusion Detection System (IDS). This study combines Particle Swarm Optimization (PSO) and K-means algorithms to provide a solution to remedy those problems and develop, step by step, a mechanism for Botnet detection. First, three important network behaviors are identified: long active communication behavior (ActBehavior), connection failure behavior (FailBehavior), and network scanning behavior (ScanBehavior). These behaviors are defined according to the relevant prior studies and used to analyze the communication activities among the infected computers. Second, the features of network behaviors are extracted from the flow traces in the network layer and transport layer of the network equipment. Third, PSO and K-means techniques are used to uncover the host members of Botnet in the organizational network. This study mainly utilizes the flow traces of a campus network as an experiment. The experimental findings show that this proposed approach can be employed to detect the suspicious Botnet members earlier than the detection application systems. In addition, this proposed approach is easy to implement and can be further used and extended in the campus dormitory network, home networks, and the mobile 3G network.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {apr},
articleno = {3},
numpages = {30},
keywords = {Botnet, K-means clustering, network traffic analysis, particle swarm optimization}
}

@inproceedings{10.1145/2810103.2813713,
author = {Pass, Rafael and shelat, abhi},
title = {Micropayments for Decentralized Currencies},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813713},
doi = {10.1145/2810103.2813713},
abstract = {Electronic financial transactions in the US, even those enabled by Bitcoin, have relatively high transaction costs. As a result, it becomes infeasible to make micropayments, i.e. payments that are pennies or fractions of a penny. In order to circumvent the cost of recording all transactions, Wheeler (1996) and Rivest (1997) suggested the notion of a probabilistic payment, that is, one implements payments that have expected value on the order of micro pennies by running an appropriately biased lottery for a larger payment. While there have been quite a few proposed solutions to such lottery-based micropayment schemes, all these solutions rely on a trusted third party to coordinate the transactions; furthermore, to implement these systems in today's economy would require a a global change to how either banks or electronic payment companies (e.g., Visa and Mastercard) handle transactions.We put forth a new lottery-based micropayment scheme for any ledger-based transaction system, that can be used today without any change to the current infrastructure. We implement our scheme in a sample web application and show how a single server can handle thousands of micropayment requests per second. We provide an analysis for how the scheme can work at Internet scale.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {207–218},
numpages = {12},
keywords = {crypto-currency, micropayments},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@inproceedings{10.1145/2733373.2806346,
author = {Xu, Xing and Yang, Yang and Shimada, Atsushi and Taniguchi, Rin-ichiro and He, Li},
title = {Semi-supervised Coupled Dictionary Learning for Cross-modal Retrieval in Internet Images and Texts},
year = {2015},
isbn = {9781450334594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2733373.2806346},
doi = {10.1145/2733373.2806346},
abstract = {Nowadays massive amount of images and texts has been emerging on the Internet, arousing the demand of effective cross-modal retrieval such as text-to-image search and image-to-text search. To eliminate the heterogeneity between the modalities of images and texts, the existing subspace learning methods try to learn a common latent subspace under which cross-modal matching can be performed. However, these methods usually require fully paired samples (images with corresponding texts) and also ignore the class label information along with the paired samples. This may inhibit these methods from learning an effective subspace since the correlations between two modalities are implicitly incorporated. Indeed, the class label information can reduce the semantic gap between different modalities and explicitly guide the subspace learning procedure. In addition, the large quantities of unpaired samples (images or texts) may provide useful side information to enrich the representations from learned subspace. Thus, in this paper we propose a novel model for cross-modal retrieval problem. It consists of 1) a semi-supervised coupled dictionary learning step to generate homogeneously sparse representations for different modalities based on both paired and unpaired samples; 2) a coupled feature mapping step to project the sparse representations of different modalities into a common subspace defined by class label information to perform cross-modal matching. Experiments on a large scale web image dataset MIRFlickr-1M with both fully paired and unpaired settings show the effectiveness of the proposed model on the cross-modal retrieval task.},
booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
pages = {847–850},
numpages = {4},
keywords = {coupled dictionary learning, cross-modal retrieval, semi-supervised learning},
location = {Brisbane, Australia},
series = {MM '15}
}

@inbook{10.1145/3129743.3129744,
title = {Preface},
year = {2018},
isbn = {9781970001839},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3129743.3129744},
abstract = {Our societies are becoming increasingly dependent on emerging technologies and connected computer systems that are increasingly trusted to store, process, and transmit sensitive data. While generally beneficial, this shift also raises many security and privacy challenges. The growing complexity and connectivity offers adversaries a large attack surface. In particular, the connection to the Internet facilitates remote attacks without the need for physical access to the targeted computing platforms. Attackers exploit security vulnerabilities in modern software with the ultimate goal of taking control over the underlying computing platforms. There are various causes of these vulnerabilities, the foremost being that the majority of software (including operating systems) is written in unsafe programming languages (mainly C and C++) and by developers who are by-and-large not security experts.Memory errors are a prominent vulnerability class in modern software: they persist for decades and still are used as the entry point for today's state-of-theart attacks. The canonical example of a memory error is the stack-based buffer overflow vulnerability, where the adversary overflows a local buffer on the stack, and overwrites a function's return address. While modern defenses protect against this attack strategy, many other avenues for exploitation exist, including those that leverage heap, format string, or integer overflow vulnerabilities.Given a memory vulnerability in the program, the adversary typically provides a malicious input that exploits this vulnerability to trigger malicious program actions not intended by the benign program. This class of exploits aims to hijack the control flow of the program and differs from conventional malware, which encapsulates the malicious code inside a dedicated executable that needs to be executed on the target system and typically requires no exploitation of a program bug.As mentioned above, the continued success of these attacks is mainly attributed to the fact that large portions of software programs are implemented in type-unsafe languages (C, C++, or Objective-C) that do not guard against malicious program inputs using bounds checking, automatic memory management, etc. However, even type-safe languages like Java rely on virtual machines and complex runtimes that are in turn implemented in type-unsafe languages out of performance concerns. Unfortunately, as modern applications grow more complex, memory errors and vulnerabilities will likely continue to exist, with no end in sight.Regardless of the attacker's method of choice, exploiting a vulnerability and gaining control over an application's control flow is only the first step of an attack. The second step is to change the behavior of the compromised application to perform malicious actions. Traditionally, this has been realized by injecting malicious code into the application's address space, and later executing the injected code. However, with the widespread enforcement of data execution prevention (DEP), such attacks are more difficult to launch today. Unfortunately, the long-held assumption that only code injection posed a risk was shattered with the introduction of code-reuse attacks, such as return-into-libc and return-oriented programming (ROP). As the name implies, code-reuse attacks do not require any code injection and instead repurpose benign code already resident in memory.Code-reuse techniques are applicable to a wide range of computing platforms: x86-based platforms, embedded systems running on an Atmel AVR processor, mobile devices based on ARM, PowerPC-based Cisco routers, and voting machines deploying a z80 processor. Moreover, the powerfulROP technique is Turing-complete, i.e., it allows an attacker to execute arbitrary malicious code.In fact, the majority of state-of-the-art run-time exploits leverage code-reuse attack techniques, e.g., against Internet Explorer, Apple QuickTime, Adobe Reader, Microsoft Word, or the GnuTLS library. Even large-scale cyberattacks such as the popular Stuxnet worm, which damaged Iranian centrifuge rotors, incorporated code-reuse attack techniques.Indeed, even after more than three decades, memory corruption exploits remain a clear and present danger to the security of modern software and hardware platforms. This is why the research community both in academia and industry have invested major efforts in the recent past to mitigate the threat. Various defenses have been proposed and even deployed by Google, Microsoft, Intel, etc. The most prominent defenses are based on enforcement (e.g., Control-Flow Integrity [CFI]) or randomization (also known as software diversity) of certain program aspects. Both types of defenses have distinct advantages and disadvantages. Randomization makes it hard for attackers to chain together their attack gadgets, is efficient, and can be applied to complex software such as web browsers. It can have different levels of granularity, from a simple Address Space Layout Randomization (ASLR) to fine-grained randomization at function or even instruction level. However, randomization requires high entropy and all randomization schemes are inherently vulnerable to information leakage. CFI, on the other hand, provides guarantees that the application does not deviate from the intended program flow, yet it requires a security and efficiency tradeoff: i.e., coarse-grained CFI have been shown to be vulnerable, and fine-grained CFI can be inefficient without hardware support. Researchers have been working on improving these schemes with novel ideas in both software and hardware design. Many proposed defenses have been bypassed by other attacks, generating a large body of literature on this topic.It seems that the arms race between attackers and defenders continues. Although researchers have raised the bar for adversaries, there are still a number of challenges to tackle against sophisticated attacks. Despite all the recent proposals on various defenses, we cannot claim that the problem is entirely solved. However, our community has gained much insight through recent results on how and to what extent we need to employ certain design principles to significantly reduce the effect of code-reuse attacks.The main purpose of this book is to provide readers with some of the most influential works on run-time exploits and defenses. We hope that this material will inspire readers and generate new ideas and paradigms.},
booktitle = {The Continuing Arms Race: Code-Reuse Attacks and Defenses},
pages = {xi–xiii}
}

@inproceedings{10.1145/2695664.2695678,
author = {Lommatzsch, Andreas and Albayrak, Sahin},
title = {Real-time recommendations for user-item streams},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695678},
doi = {10.1145/2695664.2695678},
abstract = {Recommender systems support users in finding items or users matching their individual preferences or interests. With the growing importance of social networks and the ubiquitous availability of internet connectivity, data streams become one of the most important information sources. Popular streamed data sources are micro blogging services (e.g. "twitter"), update messages in social networks, or articles on online news portals. Traditional recommender algorithms focus on large user-item matrixes applying complex algorithms (e.g. "factorization machines") for extracting the dominant knowledge and reducing the noise. In stream-based scenarios these algorithms cannot be applied due to tight time-constraints and limited resources. In this paper we present a framework optimized for providing recommendations based on streams. We analyze the user-item interaction stream for several online news portals and present the computed characteristics of these streams. Subsequently, we develop several different algorithms optimized for providing recommendations based on streams fulfilling the requirements according to quality, robustness, scalability, and tight time-constraints. We evaluate the algorithms and combine different algorithms in ensembles in order to handle the context-dependent user expectations. The evaluation results show that the developed algorithms outperform traditional recommender approaches and allow us to provide context-aware relevant recommendations.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1039–1046},
numpages = {8},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3343031.3350900,
author = {Lin, Hangyu and Fu, Yanwei and Lu, Peng and Gong, Shaogang and Xue, Xiangyang and Jiang, Yu-Gang},
title = {TC-Net for iSBIR: Triplet Classification Network for Instance-level Sketch Based Image Retrieval},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350900},
doi = {10.1145/3343031.3350900},
abstract = {Sketch has been employed as an effective communication tool to express the abstract and intuitive meaning of object. While content-based sketch recognition has been studied for several decades, the instance-level Sketch Based Image Retrieval (iSBIR) task has attracted significant research attention recently. In many previous iSBIR works -- TripletSN, and DSSA, edge maps were employed as intermediate representations in bridging the cross-domain discrepancy between photos and sketches. However, it is nontrivial to efficiently train and effectively use the edge maps in an iSBIR system. Particularly, we find that such an edge map based iSBIR system has several major limitations. First, the system has to be pre-trained on a significant amount of edge maps, either from large-scale sketch datasets, e.g., TU-Berlin~citeeitz2012hdhso, or converted from other large-scale image datasets, e.g., ImageNet-1Kcitedeng2009imagenet dataset. Second, the performance of such an iSBIR system is very sensitive to the quality of edge maps. Third and empirically, the multi-cropping strategy is essentially very important in improving the performance of previous iSBIR systems. To address these limitations, this paper advocates an end-to-end iSBIR system without using the edge maps. Specifically, we present a Triplet Classification Network (TC-Net) for iSBIR which is composed of two major components: triplet Siamese network, and auxiliary classification loss. Our TC-Net can break the limitations existed in previous works. Extensive experiments on several datasets validate the efficacy of the proposed network and system.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1676–1684},
numpages = {9},
keywords = {sbir, sketch, triplet classification network},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/2810156.2810166,
author = {Song, Tian and Yuan, Haowei and Crowley, Patrick and Zhang, Beichuan},
title = {Scalable Name-Based Packet Forwarding: From Millions to Billions},
year = {2015},
isbn = {9781450338554},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810156.2810166},
doi = {10.1145/2810156.2810166},
abstract = {Named-based packet forwarding represents a core characteristic of many information-centric networking architectures. IP-inspired forwarding methods are not suitable because a) name-based forwarding must support variable-length keys of unbounded length, and b) namespaces for data are substantially larger than the global address prefix rulesets used in today's Internet. In this paper, we introduce and evaluate an approach that can realistically scale variable-length name forwarding to billions of prefixes. Our methods are driven by two key insights. First, we show that, represented by binary strings, a name-based forwarding table of several millions of entries can be notably compressed by a Patricia trie to fit in contemporary fast memory of a line card. Second, we show that it is possible to design and optimize the data structure to make its size dependent only upon the number of rules in a ruleset, rather than the length of rules. We reduce our designs to practice and experimentally evaluate memory requirements and performance. We demonstrate that a ruleset with one million rules based on the Alexa dataset only needs 5.58 MiB memory, which can easily fit in fast memory like SRAM, and with one billion synthetic rules it takes 7.32 GiB memory, which is within the range of DRAM in a line card. These are about an order of magnitude improvement over the state-of-the-art solutions. The above efficient memory size produces high performance. Estimated throughput of the SRAM- and DRAM- based solutions are 284 Gbps and 62 Gbps respectively.},
booktitle = {Proceedings of the 2nd ACM Conference on Information-Centric Networking},
pages = {19–28},
numpages = {10},
keywords = {information-centric networking, longest prefix matching, name-based packet forwarding, named data networking, speculative forwarding},
location = {San Francisco, California, USA},
series = {ACM-ICN '15}
}

@inproceedings{10.1145/3232755.3232763,
author = {Wachs, Matthias and Scheitle, Quirin and Carle, Georg},
title = {Push Away Your Privacy: Precise User Tracking Based on TLS Client Certificate Authentication},
year = {2018},
isbn = {9781450355858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232755.3232763},
doi = {10.1145/3232755.3232763},
abstract = {While the Transport Layer Security (TLS) protocol is typically used to authenticate servers, it also offers the possibility to use Client Certificates for to authenticate clients (CCA). We investigate the use of CCA based on two specific concerns:First, CCA is prone to being used in a context that encodes personal data into client certificates, such as identifying persons, e.g. in voting systems or VPN applications.Second, in versions prior to TLS1.3, the client certificate (as well as the server certificate) is being sent in clear text, permitting systematic and large-scale eavesdropping.Based on these two concerns, we investigate the use of CCA at an ISP uplink. Besides confirming our two concerns by finding, e.g., person names in VPN certificates, we also identify the Apple Push Notification Service (APNs) to leverage TLS CCA to identify client devices. We consider this use highly critical as APNs is an integral part of all Apple operating systems, and APNs establishes a connection immediately upon connecting the device to a network. We show that these properties can be used by various attacker types to track devices (and hence, likely users) with great precision across the global Internet.This work was published in 2017, with the TLS1.3 standardization still ongoing, and we aimed to emphasize the necessity of encrypting client certificates in the TLS handshake, which was adopted in the TLS1.3 standard. Based on work published at TMA'17 [1].[1] Matthias Wachs, Quirin Scheitle, Georg Carle. 2017. Push Away Your Privacy: Precise User Tracking Based on TLS Client Certificate Authentication. In Proceedings of the 2017 Network Traffic Measurement and Analysis Conference (TMA '17)},
booktitle = {Proceedings of the Applied Networking Research Workshop},
pages = {3},
numpages = {1},
keywords = {Privacy, TLS},
location = {Montreal, QC, Canada},
series = {ANRW '18}
}

@inproceedings{10.1145/2785956.2787509,
author = {Levin, Dave and Lee, Youndo and Valenta, Luke and Li, Zhihao and Lai, Victoria and Lumezanu, Cristian and Spring, Neil and Bhattacharjee, Bobby},
title = {Alibi Routing},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787509},
doi = {10.1145/2785956.2787509},
abstract = {There are several mechanisms by which users can gain insight into where their packets have gone, but no mechanisms allow users undeniable proof that their packets did not traverse certain parts of the world while on their way to or from another host. This paper introduces the problem of finding "proofs of avoidance": evidence that the paths taken by a packet and its response avoided a user-specified set of "forbidden" geographic regions. Proving that something did not happen is often intractable, but we demonstrate a low-overhead proof structure built around the idea of what we call "alibis": relays with particular timing constraints that, when upheld, would make it impossible to traverse both the relay and the forbidden regions.We present Alibi Routing, a peer-to-peer overlay routing system for finding alibis securely and efficiently. One of the primary distinguishing characteristics of Alibi Routing is that it does not require knowledge of--or modifications to--the Internet's routing hardware or policies. Rather, Alibi Routing is able to derive its proofs of avoidance from user-provided GPS coordinates and speed of light propagation delays. Using a PlanetLab deployment and larger-scale simulations, we evaluate Alibi Routing to demonstrate that many source-destination pairs can avoid countries of their choosing with little latency inflation. We also identify when Alibi Routing does not work: it has difficulty avoiding regions that users are very close to (or, of course, inside of).},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {611–624},
numpages = {14},
keywords = {alibi routing, censorship avoidance, overlay routing, peer-to-peer, provable route avoidance},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{10.1145/2944789.2944870,
author = {Das, Sima and Das, Sajal K. and Ghosh, Susmita},
title = {Leveraging contact pattern to predict future contact pattern in mobile networks},
year = {2016},
isbn = {9781450343442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2944789.2944870},
doi = {10.1145/2944789.2944870},
abstract = {With advances in the Internet and mobile technology, and decreasing cost of mobile devices, large scale pervasive networks are now ubiquitous in solving many earlier service limitations. Here, the challenge lies in its underlying temporal graph. It introduces technical limitations in efficient routing, maximal coverage with minimal latency, data offloading, to effective dissemination over mobile networks or mobility induced dynamic networks. Efficient solution to these interrelated problems lies in the novel prediction strategies for most accurate future contacts (links or interactions), their future contact time etc. In contrast to the existing strategies that consider either network structure or regular pattern and periodic nature of contacts, we propose a novel stochastic Poisson process model (variants of cascaded non-homogeneous Poisson process) that employ multi-recurrent, dependent contact pattern as its basis. We predict number of contacts relative to a node and over all nodes in any future interval, future contact time over a user and a pair of users. Finally, we validate our model with a widely used empirical data set from mobile network, and compare our model with doubly recurrent and homogeneous Poisson process model to conclude the superiority of our prediction model.},
booktitle = {Proceedings of the 8th ACM International Workshop on Hot Topics in Planet-Scale MObile Computing and Online Social NeTworking},
pages = {13–18},
numpages = {6},
keywords = {contact prediction, mobility induced dynamic networks},
location = {Paderborn, Germany},
series = {HotPOST '16}
}

@inproceedings{10.1145/2695664.2695834,
author = {Duan, Qiang and Zeng, Mengxi and Huang, Jun and Xing, Cong-cong},
title = {Performance analysis for a service delivery platform in software defined network},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695834},
doi = {10.1145/2695664.2695834},
abstract = {Software Defined Networking (SDN) is an emerging networking paradigm that is expected to play a significant role in future networks. Although exciting progress has been made in research and development of SDN technologies, application of this new networking paradigm in large scale networks with heterogeneous autonomous domains, such as the Internet, is still a challenging issue that calls for a new service delivery platform offering a high-level abstraction for network programmability and inter-domain collaboration. In this paper, we first present a service delivery platform that employs the Network-as-a-Service (NaaS) notion for end-to-end service provisioning in SDN. Then we propose a novel modeling and analysis method for evaluating performance of the service delivery platform. The proposed analysis method addresses the challenges brought in by network abstraction and domain diversity in the service delivery platform by applying network calculus techniques. Both analytical and numerical results obtained in this paper indicate that our modeling and analysis well reflect the advantage of centralized network control enabled by SDN for end-to-end service provisioning; thus achieving tighter performance bound compared to traditional methods for network performance analysis.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {2257–2262},
numpages = {6},
keywords = {SDN, naas, performance, service delivery},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/3291842.3291876,
author = {Bo, Xie and Han, Li and Yong, Wang},
title = {An IP Geolocation Database Evaluation and Fusion Model Based on Data Correlation and Delay Similarity},
year = {2018},
isbn = {9781450365857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291842.3291876},
doi = {10.1145/3291842.3291876},
abstract = {IP geolocation database is widely used in many Internet services. At present, there are many inaccurate or missing geolocations in IP geolocation databases. However, the industry lacks an effective method to evaluate them. Based on the assumption that the majority of entries in well-known databases are correct and delay measurement, this paper proposed an IP geolocation database evaluation and fusion model based on data correlation and delay similarity. Firstly, we improved the previous evaluation model based on data-consistency-rate by introducing geolocation coverage rate at different granularities. Secondly, by measuring the delays of IP addresses at large scale, the standard delay of a geographical city is determined, then, we calculated the delay similarity rate of different databases between IP's own delay and its geolocation city's standard delay. Thirdly, we used the weighted voting method to fuse inconsistent geolocations among databases, where the vote share is determined by the improved data-consistency-rate and delay similarity rate, and presented a sole fusion database. Finally, we took 340 million IP addresses allocated to mainland China as an example, compared with the existing model, the accuracy of the model we proposed is increased by 8.79%.},
booktitle = {Proceedings of the 2nd International Conference on Telecommunications and Communication Engineering},
pages = {231–236},
numpages = {6},
keywords = {IP geolocation database, data consistency rate, data correlation, data fusion, delay similarity rate},
location = {Beijing, China},
series = {ICTCE '18}
}

@inproceedings{10.5555/3158161.3158190,
author = {Benzerga, Sh\'{e}h\'{e}razade},
title = {Digital transformation patterns},
year = {2016},
publisher = {The Hillside Group},
address = {USA},
abstract = {Nowadays "digital" is everywhere and is impacting the society and the economy. To stay competitive in the digital era, companies have to be innovative to adapt to the change that comes along with the proliferation of new technologies. One of the most important things to take into account is the business model. It has to be scalable to ensure the development and success of the company in the long term. Depending on the nature of the company and on the types of product a company brings on the market, the transformation will be a different one.In this article the focus lies on understanding, through the identification of patterns, the transformation classical industrial companies mainly developing material products are going through. For example car manufacturers who a few years ago focused on building products for transporting people from A to B today invest in mobility services. Further, we describe patterns for companies like the GAFA (Google, Amazon, Facebook, and Apple) and other tech companies undergoing an opposite transformation. These companies, to enhance and extend their business models, are interested in the business of classical industrial companies, such as the music, media, health, mobility, finance and energy industry. This interest from both types of companies (the classical industries and the digital challengers) in the business of the other fosters innovation. That is probably one of the reasons why in the last years we have seen the rise of the buzz word "digital transformation". Are there best practices that can help companies to transform and adapt to the digital era of today's society? The set of patterns described in this article tries to identify good solutions that can be used by companies which have or are in the process of undergoing a digital transformation.},
booktitle = {Proceedings of the 23rd Conference on Pattern Languages of Programs},
articleno = {24},
numpages = {29},
keywords = {business model, digital transformation, digitalization, innovation, integral industry},
location = {Monticello, Illinois},
series = {PLoP '16}
}

@inproceedings{10.1145/3319535.3363190,
author = {Pes\'{e}, Mert D. and Stacer, Troy and Campos, C. Andr\'{e}s and Newberry, Eric and Chen, Dongyao and Shin, Kang G.},
title = {LibreCAN: Automated CAN Message Translator},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363190},
doi = {10.1145/3319535.3363190},
abstract = {Modern Connected and Autonomous Vehicles (CAVs) are equipped with an increasing number of Electronic Control Units (ECUs), many of which produce large amounts of data. Data is exchanged between ECUs via an in-vehicle network, with the Controller Area Network (CAN) bus being the de facto standard in contemporary vehicles. Furthermore, CAVs have not only physical interfaces but also increased data connectivity to the Internet via their Telematic Control Units (TCUs), enabling remote access via mobile devices. It is also possible to tap into, and read/write data from/to the CAN bus, as data transmitted on the CAN bus is not encrypted. This naturally generates concerns about automotive cybersecurity. One commonality among most vehicular security attacks reported to date is that they ultimately require write access to the CAN bus. In order to cause targeted and intentional changes in vehicle behavior, malicious CAN injection attacks require knowledge of the CAN message format. However, since this format is proprietary to OEMs and can differ even among different models of a single make of vehicle, one must manually reverse-engineer the CAN message format of each vehicle they target --- a time-consuming and tedious process that does not scale. To mitigate this difficulty, we develop LibreCAN, which can translate most CAN messages with minimal effort. Our extensive evaluation on multiple vehicles demonstrates LibreCAN's efficiency in terms of accuracy, coverage, required manual effort and scalability to any vehicle.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2283–2300},
numpages = {18},
keywords = {CAN bus, automotive security, reverse engineering},
location = {London, United Kingdom},
series = {CCS '19}
}

@article{10.1145/3306346.3323013,
author = {Philip, Julien and Gharbi, Micha\"{e}l and Zhou, Tinghui and Efros, Alexei A. and Drettakis, George},
title = {Multi-view relighting using a geometry-aware network},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323013},
doi = {10.1145/3306346.3323013},
abstract = {We propose the first learning-based algorithm that can relight images in a plausible and controllable manner given multiple views of an outdoor scene. In particular, we introduce a geometry-aware neural network that utilizes multiple geometry cues (normal maps, specular direction, etc.) and source and target shadow masks computed from a noisy proxy geometry obtained by multi-view stereo. Our model is a three-stage pipeline: two subnetworks refine the source and target shadow masks, and a third performs the final relighting. Furthermore, we introduce a novel representation for the shadow masks, which we call RGB shadow images. They reproject the colors from all views into the shadowed pixels and enable our network to cope with inacuraccies in the proxy and the non-locality of the shadow casting interactions. Acquiring large-scale multi-view relighting datasets for real scenes is challenging, so we train our network on photorealistic synthetic data. At train time, we also compute a noisy stereo-based geometric proxy, this time from the synthetic renderings. This allows us to bridge the gap between the real and synthetic domains. Our model generalizes well to real scenes. It can alter the illumination of drone footage, image-based renderings, textured mesh reconstructions, and even internet photo collections.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {78},
numpages = {14},
keywords = {deep learning, image relighting, multi-view}
}

@inproceedings{10.1109/ISCA.2016.47,
author = {Zhang, Yunqi and Meisner, David and Mars, Jason and Tang, Lingjia},
title = {Treadmill: attributing the source of tail latency through precise load testing and statistical inference},
year = {2016},
isbn = {9781467389471},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA.2016.47},
doi = {10.1109/ISCA.2016.47},
abstract = {Managing tail latency of requests has become one of the primary challenges for large-scale Internet services. Data centers are quickly evolving and service operators frequently desire to make changes to the deployed software and production hardware configurations. Such changes demand a confident understanding of the impact on one's service, in particular its effect on tail latency (e.g., 95th- or 99th-percentile response latency of the service). Evaluating the impact on the tail is challenging because of its inherent variability. Existing tools and methodologies for measuring these effects suffer from a number of deficiencies including poor load tester design, statistically inaccurate aggregation, and improper attribution of effects. As shown in the paper, these pitfalls can often result in misleading conclusions.In this paper, we develop a methodology for statistically rigorous performance evaluation and performance factor attribution for server workloads. First, we find that careful design of the server load tester can ensure high quality performance evaluation, and empirically demonstrate the inaccuracy of load testers in previous work. Learning from the design flaws in prior work, we design and develop a modular load tester platform, Treadmill, that overcomes pitfalls of existing tools. Next, utilizing Treadmill, we construct measurement and analysis procedures that can properly attribute performance factors. We rely on statistically-sound performance evaluation and quantile regression, extending it to accommodate the idiosyncrasies of server systems. Finally, we use our augmented methodology to evaluate the impact of common server hardware features with Facebook production workloads on production hardware. We decompose the effects of these features on request tail latency and demonstrate that our evaluation methodology provides superior results, particularly in capturing complicated and counter-intuitive performance behaviors. By tuning the hardware features as suggested by the attribution, we reduce the 99th-percentile latency by 43% and its variance by 93%.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
pages = {456–468},
numpages = {13},
keywords = {data center, load testing, tail latency},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}

@inproceedings{10.1145/3133956.3136065,
author = {Tang, Qiang and Yung, Moti},
title = {Cliptography: Post-Snowden Cryptography},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3136065},
doi = {10.1145/3133956.3136065},
abstract = {This tutorial will present a systematic overview of {em kleptography}: stealing information subliminally from black-box cryptographic implementations; and {em cliptography}: defending mechanisms that clip the power of kleptographic attacks via specification re-designs (without altering the underlying algorithms). Despite the laudatory history of development of modern cryptography, applying cryptographic tools to reliably provide security and privacy in practice is notoriously difficult. One fundamental practical challenge, guaranteeing security and privacy without explicit trust in the algorithms and implementations that underlie basic security infrastructure, remains. While the dangers of entertaining adversarial implementation of cryptographic primitives seem obvious, the ramifications of such attacks are surprisingly dire: it turns out that -- in wide generality -- adversarial implementations of cryptographic (both deterministic and randomized) algorithms may leak private information while producing output that is statistically indistinguishable from that of a faithful implementation. Such attacks were formally studied in Kleptography. Snowden revelations has shown us how security and privacy can be lost at a very large scale even when traditional cryptography seems to be used to protect Internet communication, when Kleptography was not taken into consideration. We will first explain how the above-mentioned Kleptographic attacks can be carried out in various settings. We will then introduce several simple but rigorous immunizing strategies that were inspired by folklore practical wisdoms to protect different algorithms from implementation subversion. Those strategies can be applied to ensure security of most of the fundamental cryptographic primitives such as PRG, digital signatures, public key encryptions against kleptographic attacks when they are implemented accordingly. Our new design principles may suggest new standardization methods that help reducing the threats of subverted implementation. We also hope our tutorial to stimulate a community-wise efforts to further tackle the fundamental challenge mentioned at the beginning.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2615–2616},
numpages = {2},
keywords = {backdoor resistance, cliptography, cryptography, implementation subversion, kleptography, steganography},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@inproceedings{10.1145/3308558.3313561,
author = {Liu, Luyang and Huang, Heyan and Gao, Yang and Zhang, Yongfeng and Wei, Xiaochi},
title = {Neural Variational Correlated Topic Modeling},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313561},
doi = {10.1145/3308558.3313561},
abstract = {With the rapid development of the Internet, millions of documents, such as news and web pages, are generated everyday. Mining the topics and knowledge on them has attracted a lot of interest on both academic and industrial areas. As one of the prevalent unsupervised data mining tools, topic models are usually explored as probabilistic generative models for large collections of texts. Traditional probabilistic topic models tend to find a closed form solution of model parameters and approach the intractable posteriors via approximation methods, which usually lead to the inaccurate inference of parameters and low efficiency when it comes to a quite large volume of data. Recently, an emerging trend of neural variational inference can overcome the above issues, which offers a scalable and powerful deep generative framework for modeling latent topics via neural networks. Interestingly, a common assumption for the most neural variational topic models is that topics are independent and irrelevant to each other. However, this assumption is unreasonable in many practical scenarios. In this paper, we propose a novel Centralized Transformation Flow to capture the correlations among topics by reshaping topic distributions. Furthermore, we present the Transformation Flow Lower Bound to improve the performance of the proposed model. Extensive experiments on two standard benchmark datasets have well-validated the effectiveness of the proposed approach.},
booktitle = {The World Wide Web Conference},
pages = {1142–1152},
numpages = {11},
keywords = {Natural language processing;topic model;neural variational inference},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inbook{10.1145/3308560.3317056,
author = {Maddalena, Eddy and Ib\'{a}\~{n}ez, Luis-Daniel and Simperl, Elena and Gomer, Richard and Zeni, Mattia and Song, Donglei and Giunchiglia, Fausto},
title = {Hybrid Human Machine workflows for mobility management},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317056},
abstract = {Sustainable mobility is one of the main goals of both European and United Nations plans for 2030. The concept of Smart Cities has arisen as a way to achieve this goal by leveraging IoT interconnected devices to collect and analyse large quantities of data. However, several works have pointed out the importance of including the human factor, and in particular, citizens, to make sense of the collected data and ensure their engagement along the data value chain. This paper presents the design and implementation of two end-to-end hybrid human-machine workflows for solving two mobility problems: modal split estimation, and mapping mobility infrastructure. For modal split, we combine the use of i-Log, an app to collect data and interact with citizens, with reinforcement learning classifiers to continuously improve the accuracy of the classification, aiming at reducing the required interactions from citizens. For mobility infrastructure, we developed a system that uses remote crowdworkers to explore the city looking for Points of Interest, that is more scalable than sending agents on the field. Crowdsourced maps are then fused with existing maps (if available) to create a final map that then is validated on the field by citizens engaged through the i-Log app.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {102–109},
numpages = {8}
}

@article{10.1145/2822907,
author = {Li, Zechao and Tang, Jinhui and Wang, Xueming and Liu, Jing and Lu, Hanqing},
title = {Multimedia News Summarization in Search},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2822907},
doi = {10.1145/2822907},
abstract = {It is a necessary but challenging task to relieve users from the proliferative news information and allow them to quickly and comprehensively master the information of the whats and hows that are happening in the world every day. In this article, we develop a novel approach of multimedia news summarization for searching results on the Internet, which uncovers the underlying topics among query-related news information and threads the news events within each topic to generate a query-related brief overview. First, the hierarchical latent Dirichlet allocation (hLDA) model is introduced to discover the hierarchical topic structure from query-related news documents, and a new approach based on the weighted aggregation and max pooling is proposed to identify one representative news article for each topic. One representative image is also selected to visualize each topic as a complement to the text information. Given the representative documents selected for each topic, a time-bias maximum spanning tree (MST) algorithm is proposed to thread them into a coherent and compact summary of their parent topic. Finally, we design a friendly interface to present users with the hierarchical summarization of their required news information. Extensive experiments conducted on a large-scale news dataset collected from multiple news Web sites demonstrate the encouraging performance of the proposed solution for news summarization in news retrieval.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
articleno = {33},
numpages = {20},
keywords = {News summarization, hierarchical latent Dirichlet allocation, maximum spanning tree, multimodal, topic structure}
}

@inproceedings{10.1145/2810103.2813699,
author = {Bogdanov, Andrey and Isobe, Takanori},
title = {White-Box Cryptography Revisited: Space-Hard Ciphers},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813699},
doi = {10.1145/2810103.2813699},
abstract = {The need for software security in untrusted environments is ever increasing. White-box cryptography aims to ensure the security of cryptographic algorithms when the attacker has full access to their implementations. However, there is no secure white-box implementation of standard block ciphers such as DES and AES known to date: All published techniques have been practically broken. In this paper, we revisit white-box cryptography and propose a family of white-box secure block ciphers SPACE with several novel features. The design of SPACE is such that the key-extraction security in the white box reduces to the well-studied problem of key recovery for block ciphers (AES in our example) in the standard black-box setting. Moreover, to mitigate code lifting, we introduce the notion of space hardness. It measures the difficulty of compressing the white-box implementation of a cipher, and quantifies security against code lifting by the amount of code that needs to be extracted from the implementation by a white-box attacker to maintain its functionality. SPACE includes several variants with different white-box code sizes. Therefore, it is applicable to a wide range of environments and use cases. One of the variants called N-SPACE can be implemented with different code sizes while keeping the cipher itself unchanged.SPACE offers a high level of space hardness: It is difficult to find a compact but still functional representation of SPACE given its white-box implementation. This property has several useful consequences for applications. First, it gets more challenging for a DRM attacker (e.g. in a pay TV setting) to scale a code-lifting attack and to distribute the break. Moreover, this paves the way for mass-surveillance resistant cryptography: If a large proportion of users dedicates a significant part of their computers' storage (e.g. HDD) to white-box SPACE implementations, it will be much more complex or even infeasible for governmental agencies to deal with the keys of all users simultaneously due to the limited storage available, forcing them to focus on targeted attacks instead. This consequence is especially important given Snowden's revelations on the extent of the mass surveillance practice by NSA and GCHQ. Finally, the usage of SPACE ciphers can mitigate the damage of having malware in security-critical systems such as networks processing top-secret data: As those are typically insulated from the Internet, the capacity of the communication channel from inside to outside the system is often limited, making it infeasible for Trojans to transmit the necessary key material.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1058–1069},
numpages = {12},
keywords = {DRM, code lifting, decomposition, key extraction, malware, mass surveillance, pay TV, space-hard cipher, trojans, white-box cryptography},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@inproceedings{10.1145/2890955.2890960,
author = {Kotronis, Vasileios and Kl\"{o}ti, Rowan and Rost, Matthias and Georgopoulos, Panagiotis and Ager, Bernhard and Schmid, Stefan and Dimitropoulos, Xenofontas},
title = {Stitching Inter-Domain Paths over IXPs},
year = {2016},
isbn = {9781450342117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2890955.2890960},
doi = {10.1145/2890955.2890960},
abstract = {Modern Internet applications, from HD video-conferencing to health monitoring and remote control of power-plants, pose stringent demands on network latency, bandwidth and availability. An approach to support such applications and provide inter-domain guarantees, enabling new avenues for innovation, is using centralized inter-domain routing brokers. These entities centralize routing control for mission-critical traffic across domains, working in parallel to BGP. In this work, we propose using IXPs as natural points for stitching inter-domain paths under the control of inter-domain routing brokers. To evaluate the potential of this approach, we first map the global substrate of inter-IXP pathlets that IXP members could offer, based on measurements for 229 IXPs worldwide. We show that using IXPs as stitching points has two useful properties. Up to 91% of the total IPv4 address space can be served by such inter-domain routing brokers when working in concert with just a handful of large IXPs and their associated ISP members. Second, path diversity on the inter-IXP graph increases by up to 29 times, as compared to current BGP valley-free routing. To exploit the rich path diversity, we introduce algorithms that inter-domain routing brokers can use to embed paths, subject to bandwidth and latency constraints. We show that our algorithms scale to the sizes of the measured graphs and can serve diverse simulated path request mixes. Our work highlights a novel direction for SDN innovation across domains, based on logically centralized control and programmable IXP fabrics.},
booktitle = {Proceedings of the Symposium on SDN Research},
articleno = {17},
numpages = {12},
keywords = {Embedding, EuroIX, Internet Exchange Point, PeeringDB, Routing},
location = {Santa Clara, CA, USA},
series = {SOSR '16}
}

@inproceedings{10.1145/2818000.2818010,
author = {Edwards, Benjamin and Hofmeyr, Steven and Forrest, Stephanie and van Eeten, Michel},
title = {Analyzing and Modeling Longitudinal Security Data: Promise and Pitfalls},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2818010},
doi = {10.1145/2818000.2818010},
abstract = {Many cybersecurity problems occur on a worldwide scale, but we lack rigorous methods for determining how best to intervene and mitigate damage globally, both short- and long-term. Analysis of longitudinal security data can provide insight into the effectiveness and differential impacts of security interventions on a global level. In this paper we consider the example of spam, studying a large high-resolution data set of messages sent from 260 ISPs in 60 countries over the course of a decade. The statistical analysis is designed to avoid common pitfalls that could lead to erroneous conclusions. We show how factors such as geography, national economics, Internet connectivity and traffic flow impact can affect local spam concentrations. Additionally, we present a statistical model to study temporal transitions in the dataset, and we use a simple extension of the model to investigate the effect of historical botnet takedowns on spam levels. We find that in aggregate most historical takedowns are beneficial in the short-term, but few have long-term impact. Further, even when takedowns are effective globally, they can be detrimental in specific geographic regions or countries. The analysis and modeling described here are based on a single data set. However, the techniques are general and could be adapted to other data sets to help improve decision making about when and how to deploy security interventions.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {391–400},
numpages = {10},
keywords = {Spam, statistical model, takedowns},
location = {Los Angeles, CA, USA},
series = {ACSAC '15}
}

@inproceedings{10.1145/3077136.3096470,
author = {Arya, Dhruv and Venkataraman, Ganesh},
title = {Search Without a Query: Powering Job Recommendations via Search Index at LinkedIn},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3096470},
doi = {10.1145/3077136.3096470},
abstract = {The mission of LinkedIn is to connect the world's professionals to make them more productive and successful. LinkedIn operates the world's largest professional network on the Internet with more than 500 Million members in over 200 countries. Core to realizing the mission is to help people find jobs. In this paper, we describe how the jobs recommendations is powered by a search index and some practical challenges involved in scaling such a system.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1347},
numpages = {1},
keywords = {information retrieval, recommender systems},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{10.1109/TCBB.2017.2749225,
author = {Pauleve, Loic},
title = {Reduction of Qualitative Models of Biological Networks for Transient Dynamics Analysis},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {15},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2017.2749225},
doi = {10.1109/TCBB.2017.2749225},
abstract = {Qualitative models of dynamics of signalling pathways and gene regulatory networks allow for the capturing of temporal properties of biological networks while requiring few parameters. However, these discrete models typically suffer from the so-called state space explosion problem which makes the formal assessment of their potential behaviors very challenging. In this paper, we describe a method to reduce a qualitative model for enhancing the tractability of analysis of transient reachability properties. The reduction does not change the dimension of the model, but instead limits its degree of freedom, therefore reducing the set of states and transitions to consider. We rely on a transition-centered specification of qualitative models by the mean of automata networks. Our framework encompasses the usual asynchronous Boolean and multi-valued network, as well as 1-bounded Petri nets. Applied to different large-scale biological networks from the litterature, we show that the reduction can lead to a drastic improvement for the scalability of verification methods.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {1167–1179},
numpages = {13}
}

@article{10.14778/2994509.2994534,
author = {Zhu, Erkang and Nargesian, Fatemeh and Pu, Ken Q. and Miller, Ren\'{e}e J.},
title = {LSH ensemble: internet-scale domain search},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994534},
doi = {10.14778/2994509.2994534},
abstract = {We study the problem of domain search where a domain is a set of distinct values from an unspecified universe. We use Jaccard set containment score, defined as |Q ∩ X|/|Q|, as the measure of relevance of a domain X to a query domain Q. Our choice of Jaccard set containment over Jaccard similarity as a measure of relevance makes our work particularly suitable for searching Open Data and data on the web, as Jaccard similarity is known to have poor performance over sets with large differences in their domain sizes. We demonstrate that the domains found in several real-life Open Data and web data repositories show a power-law distribution over their domain sizes.We present a new index structure, Locality Sensitive Hashing (LSH) Ensemble, that solves the domain search problem using set containment at Internet scale. Our index structure and search algorithm cope with the data volume and skew by means of data sketches using Minwise Hashing and domain partitioning. Our index structure does not assume a prescribed set of data values. We construct a cost model that describes the accuracy of LSH Ensemble with any given partitioning. This allows us to formulate the data partitioning for LSH Ensemble as an optimization problem. We prove that there exists an optimal partitioning for any data distribution. Furthermore, for datasets following a power-law distribution, as observed in Open Data and Web data corpora, we show that the optimal partitioning can be approximated using equi-depth, making it particularly efficient to use in practice.We evaluate our algorithm using real data (Canadian Open Data and WDC Web Tables) containing up over 262 million domains. The experiments demonstrate that our index consistently outperforms other leading alternatives in accuracy and performance. The improvements are most dramatic for data with large skew in the domain sizes. Even at 262 million domains, our index sustains query performance with under 3 seconds response time.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1185–1196},
numpages = {12}
}

@inproceedings{10.1145/2785956.2787503,
author = {Zhou, Dong and Fan, Bin and Lim, Hyeontaek and Andersen, David G. and Kaminsky, Michael and Mitzenmacher, Michael and Wang, Ren and Singh, Ajaypal},
title = {Scaling Up Clustered Network Appliances with ScaleBricks},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787503},
doi = {10.1145/2785956.2787503},
abstract = {This paper presents ScaleBricks, a new design for building scalable, clustered network appliances that must "pin" flow state to a specific handling node without being able to choose which node that should be. ScaleBricks applies a new, compact lookup structure to route packets directly to the appropriate handling node, without incurring the cost of multiple hops across the internal interconnect. Its lookup structure is many times smaller than the alternative approach of fully replicating a forwarding table onto all nodes. As a result, ScaleBricks is able to improve throughput and latency while simultaneously increasing the total number of flows that can be handled by such a cluster. This architecture is effective in practice: Used to optimize packet forwarding in an existing commercial LTE-to-Internet gateway, it increases the throughput of a four-node cluster by 23%, reduces latency by up to 10%, saves memory, and stores up to 5.7x more entries in the forwarding table.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {241–254},
numpages = {14},
keywords = {hashing algorithms, network function virtualization, scalability},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{10.1145/2872427.2882996,
author = {Shuai, Hong-Han and Shen, Chih-Ya and Yang, De-Nian and Lan, Yi-Feng and Lee, Wang-Chien and Yu, Philip S. and Chen, Ming-Syan},
title = {Mining Online Social Data for Detecting Social Network Mental Disorders},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2882996},
doi = {10.1145/2872427.2882996},
abstract = {An increasing number of social network mental disorders (SNMDs), such as Cyber-Relationship Addiction, Information Overload, and Net Compulsion, have been recently noted. Symptoms of these mental disorders are usually observed passively today, resulting in delayed clinical intervention. In this paper, we argue that mining online social behavior provides an opportunity to actively identify SNMDs at an early stage. It is challenging to detect SNMDs because the mental factors considered in standard diagnostic criteria (questionnaire) cannot be observed from online social activity logs. Our approach, new and innovative to the practice of SNMD detection, does not rely on self-revealing of those mental factors via questionnaires. Instead, we propose a machine learning framework, namely, Social Network Mental Disorder Detection (SNMDD), that exploits features extracted from social network data to accurately identify potential cases of SNMDs. We also exploit multi-source learning in SNMDD and propose a new SNMDbased Tensor Model (STM) to improve the performance. Our framework is evaluated via a user study with 3126 online social network users. We conduct a feature analysis, and also apply SNMDD on large-scale datasets and analyze the characteristics of the three SNMD types. The results show that SNMDD is promising for identifying online social network users with potential SNMDs.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {275–285},
numpages = {11},
keywords = {feature extraction, mental disorder detection, online social network, tensor factorization},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1145/3368926.3369679,
author = {Kikuma, Kazuhiro and Yamada, Takeshi and Sato, Koki and Ueda, Kiyoshi},
title = {Preparation Method in Automated Test Case Generation using Machine Learning},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369679},
doi = {10.1145/3368926.3369679},
abstract = {The role of telecom carrier networks has shifted from the provision of telephone services to that of Internet infrastructures, making it inevitable to invest in equipment capable of handling the rapidly increasing traffic due to the distribution of video and other bandwidth-heavy contents. This has given rise to management needs for reducing the cost of communication systems as a whole. Likewise, software used in communication systems is increasingly becoming more complex and larger in scale to accommodate various service requirements, thereby making it more difficult to reduce the cost and period for software development. Further, since telecom carrier networks serve as basic social infrastructures, it is important to maintain their reliability and safety as a critical lifeline. Hence, ill-considered reduction of development costs and shortening of development period that could lead to decline in service quality must be avoided. In telecom carrier networks, communications softwares incorporate numerous quality improvement measures to prevent service interruptions during operation of public networks. The implementation of numerous quality improvement measures, however, has resulted in prolonged development periods and higher costs. To address these issues, we have been working on the automation of software development processes. Among the different processes in software development, testing yields the most influence in software quality. Test cases are therefore written by skilled engineers and are decided after multiple reviews, requiring a large amount of manpower in preparing them. Thoughtless reduction of these steps, however, could diminish software quality. We therefore used the knowhow of skilled engineers in writing test cases as training data to automate the generation of homogeneous test cases through machine learning. Our method automatically extracts homogeneous test cases that are not dependent on skills and knowhow of the engineer writing the test cases from requirements specification documents, which are the products of the basic design process in the past development. However, the required accuracy cannot be obtained by applying simple machine learning. The expansion of training data increases the cost and the quantity of requirement specification documents is limited, so it is necessary to improve the learning efficiency per unit training data. In this paper, we propose a method to increase accuracy by preparation of training data inputted into the machine learning process, and report the results of evaluation of effectiveness of the method.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {393–398},
numpages = {6},
keywords = {CRF, Test case automatic generation, Training data},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3010079.3012015,
author = {Schulz-Zander, Julius and Lisicki, Raphael and Schmid, Stefan and Feldmann, Anja},
title = {SecuSpot: Toward Cloud-Assisted Secure Multi-Tenant WiFi HotSpot Infrastructures},
year = {2016},
isbn = {9781450346733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010079.3012015},
doi = {10.1145/3010079.3012015},
abstract = {Despite the increasing popularity of WiFi networks and the trend toward automated offloading of cellular traffic to WiFi (e.g., HotSpot 2.0), today's WiFi networks still provide a very poor actual coverage: a WiFi equipped device can typically connect to the Internet only through a very small fraction of the "available" access points. Accordingly, there is an enormous potential for multi-tenant WiFi hotspot architectures, which however also introduce more stringent requirements in terms of scalability and security. The latter is particularly critical, as HotSpots are often deployed in untrusted environments, e.g., physically accessible Access Points deployed in the user's premises (e.g., FON) or cafes. This paper proposes a Cloud-assisted multi-tenant and secure WiFi HotSpot infrastructure, called SecuSpot. SecuSpot is based on a modular access point and features interesting deployment flexibilities. These flexibilities can be exploited, e.g., to move security critical functions to the Cloud, and hence prevent eavesdropping even when deployed across untrusted Access Points. At the heart of SecuSpot lies a novel programmable wireless switch, the wSwitch. The wSwitch allows to (de-)multiplex the different tenants already on the HotSpot and to decouple essential security functions (association, authentication, and cryptography).},
booktitle = {Proceedings of the 2016 ACM Workshop on Cloud-Assisted Networking},
pages = {61–66},
numpages = {6},
keywords = {cloud, ieee 802.11, network function virtualization, security, software-defined networking, wifi, wireless},
location = {Irvine, California, USA},
series = {CAN '16}
}

@article{10.1145/3154793,
author = {Gong, Neil Zhenqiang and Liu, Bin},
title = {Attribute Inference Attacks in Online Social Networks},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3154793},
doi = {10.1145/3154793},
abstract = {We propose new privacy attacks to infer attributes (e.g., locations, occupations, and interests) of online social network users. Our attacks leverage seemingly innocent user information that is publicly available in online social networks to infer missing attributes of targeted users. Given the increasing availability of (seemingly innocent) user information online, our results have serious implications for Internet privacy—private attributes can be inferred from users’ publicly available data unless we take steps to protect users from such inference attacks. To infer attributes of a targeted user, existing inference attacks leverage either the user’s publicly available social friends or the user’s behavioral records (e.g., the web pages that the user has liked on Facebook, the apps that the user has reviewed on Google Play), but not both. As we will show, such inference attacks achieve limited success rates. However, the problem becomes qualitatively different if we consider both social friends and behavioral records. To address this challenge, we develop a novel model to integrate social friends and behavioral records, and design new attacks based on our model. We theoretically and experimentally demonstrate the effectiveness of our attacks. For instance, we observe that, in a real-world large-scale dataset with 1.1 million users, our attack can correctly infer the cities a user lived in for 57% of the users; via confidence estimation, we are able to increase the attack success rate to over 90% if the attacker selectively attacks half of the users. Moreover, we show that our attack can correctly infer attributes for significantly more users than previous attacks.},
journal = {ACM Trans. Priv. Secur.},
month = {jan},
articleno = {3},
numpages = {30},
keywords = {Attribute inference, privacy attack, social-behavior-attribute network}
}

@article{10.1145/3364626.3364629,
author = {Green, Frederic},
title = {Review of Handbook of Graph Theory, Combinatorial Optimization, and Algorithms},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0163-5700},
url = {https://doi.org/10.1145/3364626.3364629},
doi = {10.1145/3364626.3364629},
abstract = {Some (admittedly controversial) models of physical reality propose that it is, at the fundamental level, based on a certain type of graph, called a spin-network (and this among other models of physical reality based on graphs). Recently, astronomers have verified that the otherwise empty reaches of intergalactic space are laced with wispy tendrils of hot gas that connect galaxies. On a more quotidian level, the machine on which I'm typing this document is constructed out of components that are essentially graphs, those components being connected by electrical edges forming an even larger graph, and this one device in turn being connected via various links to millions of other nodes in a huge global graph called the internet. The notion of a graph encompasses all levels of reality, from the subatomic through everyday life and reaching out to the cosmological scale.},
journal = {SIGACT News},
month = {sep},
pages = {6–11},
numpages = {6}
}

@inproceedings{10.1145/3175536.3175561,
author = {Sianipar, Johannes and Willems, Christian and Meinel, Christoph},
title = {Signed URL for an Isolated Web Server in a Virtual Laboratory},
year = {2017},
isbn = {9781450354356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175536.3175561},
doi = {10.1145/3175536.3175561},
abstract = {A Virtual laboratory system with high requirements learning units, is struggling in serving a large number of users, because the available hardware resources are limited. One way to increase the scalability is by involving the user on-premise machine as a part of a virtual laboratory. The user on-premise machine replaces a machine in a virtual laboratory. This cannot be done in every learning unit scenario, because the virtual laboratory is isolated and must be kept isolated and secured. In a virtual laboratory for Internet Security e-Learning, a machine that can be replaced by the user on-premise machine is an attacking machine. In this research, we focus on a web based application attack that uses an Internet browser as a tool to do the attack. We propose an approach that can provide a secured access to the web server inside an isolated virtual laboratory, using a signed uniform resource locator (URL) and a reverse proxy. We develop a reverse proxy that able to verify a signed URL, run a shell script and forward the web request to the designated isolated web server.},
booktitle = {Proceedings of the 9th International Conference on Education Technology and Computers},
pages = {218–222},
numpages = {5},
keywords = {Cloud Computing, Reverse Proxy, Signed URL, Virtual Laboratory},
location = {Barcelona, Spain},
series = {ICETC '17}
}

@inproceedings{10.1145/2810156.2810167,
author = {Carofiglio, Giovanna and Gallo, Massimo and Muscariello, Luca and Perino, Diego},
title = {Pending Interest Table Sizing in Named Data Networking},
year = {2015},
isbn = {9781450338554},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810156.2810167},
doi = {10.1145/2810156.2810167},
abstract = {Named Data Networking (NDN) has emerged as a promising candidate for shifting Internet communication model from host-centric to content-centric. A core component of NDN is its stateful forwarding plane: Content Routers keep track of pending requests (Interests) storing them in dedicated tables at routers (Pending Interest Tables). A thorough analysis of PIT scalability is fundamental for deploying NDN as a whole and questions naturally arise about memory requirements and feasibility at wire-speed. While previous works focus on data structures design under the threat of PIT state explosion, we develop for the first time an analytical model of PIT dynamics as a function of relevant system parameters. We provide a closed form characterization of average and maximum PIT size value at steady state. We build an experimental platform with high speed content router implementation to investigate PIT dynamics and to confirm the accuracy of our analytical findings. Finally, we provide guidelines on optimal PIT dimensioning and analyze the case of an ISP aggregation network with a trace-driven packet delay distribution. We conclude that, even in absence of caching and under optimal network bandwidth usage, PIT size results to be small in typical network settings.},
booktitle = {Proceedings of the 2nd ACM Conference on Information-Centric Networking},
pages = {49–58},
numpages = {10},
keywords = {information-centric networking, performance, scalability},
location = {San Francisco, California, USA},
series = {ACM-ICN '15}
}

@inproceedings{10.1145/3335783.3335784,
author = {Schelter, Sebastian and Celebi, Ufuk and Dunning, Ted},
title = {Efficient Incremental Cooccurrence Analysis for Item-Based Collaborative Filtering},
year = {2019},
isbn = {9781450362160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335783.3335784},
doi = {10.1145/3335783.3335784},
abstract = {Recommender systems are ubiquitous in the modern internet, where they help users find items they might like. A widely deployed recommendation approach is item-based collaborative filtering. This approach relies on analyzing large item cooccurrence matrices that denote how many users interacted with a pair of items. The potentially quadratic number of items to compare poses a scalability bottleneck in analyzing such item cooccurrences. Additionally, this problem intensifies in real world use cases with incrementally growing datasets, especially when the recommendation model is regularly recomputed from scratch. We highlight the connection between the growing cost of item-based recommendation and densification processes in common interaction datasets. Based on our findings, we propose an efficient incremental algorithm for item-based collaborative filtering based on cooccurrence analysis. This approach restricts the number of interactions to consider from 'power users' and 'ubiquitous items' to guarantee a provably constant amount of work per user-item interaction to process. We discuss efficient implementations of our algorithm on a single machine as well as on a distributed stream processing engine, and present an extensive experimental evaluation. Our results confirm the asymptotic benefits of the incremental approach. Furthermore, we find that our implementation is an order of magnitude faster than existing open source recommender libraries on many datasets, and at the same time scales to high dimensional datasets which these existing recommenders fail to process.},
booktitle = {Proceedings of the 31st International Conference on Scientific and Statistical Database Management},
pages = {61–72},
numpages = {12},
location = {Santa Cruz, CA, USA},
series = {SSDBM '19}
}

@inproceedings{10.1145/3277103.3277124,
author = {Mayr, Cristina and Gramp\'{\i}n, Eduardo and Risso, Claudio},
title = {Optimal Route Reflection Topology Design},
year = {2018},
isbn = {9781450359221},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277103.3277124},
doi = {10.1145/3277103.3277124},
abstract = {Autonomous Systems (ASes) exchange routing information about networks they can reach in the Internet, and the most widely extended way to connect them is by means of Border Gateway Protocol (BGP) sessions. ASes set up external BGP (eBGP) sessions between the AS border routers (ASBR) of neighboring ASes, and the routing information learned by ASBRs is then redistributed inside the AS through internal BGP (iBGP) sessions. In order to avoid loops, iBGP can not re-advertise prefixes learned from an iBGP neighbor to another iBGP neighbor. To have complete visibility, routers within the same AS are required to be connected in full-mesh. This causes scaling problems, since the number of required sessions grows quadratically with the number of routers involved. For large networks this can lead to administration issues, and therefore, in order to manage scalability, Route Reflection is generally accepted as an alternative to full-mesh. Even though Route Reflectors (RRs) simplify administration, they also introduce new problems such as: routing sub-optimality, increased probability of loops, poor route diversity, among others.The objective of the present work is to optimize (minimize) the number of Route Reflectors (RRs) within the AS, such that no suboptimal route is chosen. In other words, the routes selected with the designated RRs are the same that would have been selected if the iBGP speakers were fully meshed.},
booktitle = {Proceedings of the 10th Latin America Networking Conference},
pages = {65–72},
numpages = {8},
keywords = {BGP, Combinatorial Optimization, Internet Routing, Network Design, Route Reflection},
location = {S\~{a}o Paulo, Brazil},
series = {LANC '18}
}

@inproceedings{10.1145/2910017.2910592,
author = {Ryoo, Jihoon and Yun, Kiwon and Samaras, Dimitris and Das, Samir R. and Zelinsky, Gregory},
title = {Design and evaluation of a foveated video streaming service for commodity client devices},
year = {2016},
isbn = {9781450342971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910017.2910592},
doi = {10.1145/2910017.2910592},
abstract = {Humans see only a tiny region at the center of their visual field with the highest visual acuity, a behavior known as foveation. Visual acuity reduces drastically towards the visual periphery. 'Foveated' video coding/compression techniques exploit this non-uniformity to gain significant efficiency by compressing more in the periphery and less in the center. We propose a practical and scalable method to use such a technique for video streaming service over the Internet. The essential idea is to use a commodity webcam on the user side to provide real-time gaze feedback to the server with the server sending appropriately coded video to the client player. We develop a multi-resolution video coding approach that is scalable in that it is possible to pre-code the video in a small number of copies for a given set of resolutions. The coding approach is designed to match the error performance of an eye tracker built using commodity webcams. We demonstrate that the technique is energy efficient and thus usable in mobile devices. We develop a methodology for performance evaluation of such a system when network budgets may vary and video quality may fluctuate. Finally, we present a comprehensive user study that demonstrates a bandwidth reduction of a factor of 2 for the same user satisfaction.},
booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
articleno = {6},
numpages = {11},
keywords = {foveation, video streaming},
location = {Klagenfurt, Austria},
series = {MMSys '16}
}

@inproceedings{10.1145/3343031.3350856,
author = {Guo, Yuyu and Gao, Lianli and Song, Jingkuan and Wang, Peng and Xie, Wuyuan and Shen, Heng Tao},
title = {Adaptive Multi-Path Aggregation for Human DensePose Estimation in the Wild},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350856},
doi = {10.1145/3343031.3350856},
abstract = {Dense human pose "in the wild'' task aims to map all 2D pixels of the detected human body to a 3D surface by establishing surface correspondences, i.e., surface patch index and part-specific UV coordinates. It remains challenging especially under the condition of "in the wild'', where RGB images capture complex, real-world scenes with background, occlusions, scale variations, and postural diversity. In this paper, we propose an end-to-end deep Adaptive Multi-path Aggregation network (AMA-net) for Dense Human Pose Estimation. In the proposed framework, we address two main problems: 1) how to design a simple yet effective pipeline for supporting distinct sub-tasks (e.g., instance segmentation, body part segmentation, and UV estimation); and 2) how to equip this pipeline with the ability of handling "in the wild''. To solve these problems, we first extend FPN by adding a branch for mapping 2D pixels to a 3D surface in parallel with the existing branch for bounding box detection. Then, in AMA-net, we extract variable-sized object-level feature maps (e.g., 7\texttimes{}7, 14\texttimes{}14, and 28\texttimes{}28), named multi-path, from multi-layer feature maps, which capture rich information of objects and are then adaptively utilized in different tasks. AMA-net is simple to train and adds only a small overhead to FPN. We discover that aside from the deep feature map, Adaptive Multi-path Aggregation is of particular importance for improving the accuracy of dense human pose estimation "in the wild''. The experimental results on the challenging Dense-COCO dataset demonstrate that our approach sets a new record for Dense Human Pose Estimation task, and it significantly outperforms the state-of-the-art methods. Our code: urlhttps://github.com/nobody-g/AMA-net.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {356–364},
numpages = {9},
keywords = {2d-to-3d surface estimation, deep multi-level aggregation, dense human pose estimation, human instance-level analysis},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/2847263.2847311,
author = {Hinkfoth, Matthias and Salomon, Ralf},
title = {Increasing the Utility of Self-Calibration Methods in High-Precision Time Measurement Systems (Abstract Only)},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847311},
doi = {10.1145/2847263.2847311},
abstract = {Asynchronously operating systems, such as tapped delay lines, are the designer?s favorite, if high resolution and precision in time are required. Their drawback, however, is that they require extensive calibration, which prohibits, among other things, sporadic recalibration during the mode of operation. Recent research has shown that the tight coupling of two selective high-precision systems inside a single FPGA substantially reduces the required calibration time: it was reduced from several hours to about 30 minutes. But even this method has not solved the problem that human intervention is required for selecting suitable calibration points. The research presented in this poster suggests that a hybrid approach is able to solve this problem: rather than tightly coupling two systems, the present approach employs hybrid elements, called X-BOUNCE, that seamlessly incorporate an X-ORCA element into a BOUNCE element. In the practical experiments, X-BOUNCE has reduced the required calibration time from 30 minutes to one second and has abandoned any human intervention. Furthermore, the proposed X-BOUNCE element can be realized by just one FPGA-LUT, which allows for easy scalability. The results were produced on a Cyclone II FPGA that has implemented 200 X-BOUNCE elements. Unfortunately, some elements exhibit a calibration inaccuracy that can be as large as 300 ps.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {275},
numpages = {1},
keywords = {calibration, time measurement},
location = {Monterey, California, USA},
series = {FPGA '16}
}

@inproceedings{10.5555/3018076.3018078,
author = {Patki, Tapasya and Lowenthal, David K. and Rountree, Barry L. and Schulz, Martin and de Supinski, Bronis R.},
title = {Economic viability of hardware overprovisioning in power-constrained high performance computing},
year = {2016},
isbn = {9781509038565},
publisher = {IEEE Press},
abstract = {Recent research has established that hardware overprovisioning can improve system power utilization as well as job throughput in power-constrained, high-performance computing environments significantly. These benefits, however, may be associated with an additional infrastructure cost, making hardware overprovisioned systems less viable economically. It is thus important to conduct a detailed cost-benefit analysis before investing in such systems at a large-scale. In this paper, we develop a model to conduct this analysis and show that for a given, fixed infrastructure cost budget and a system power budget, it is possible for hardware overprovisioned systems to lead to a net performance benefit when compared to traditional, worst-case provisioned HPC systems.},
booktitle = {Proceedings of the 4th International Workshop on Energy Efficient Supercomputing},
pages = {8–15},
numpages = {8},
location = {Salt Lake City, Utah},
series = {E2SC '16}
}

@inproceedings{10.1145/2684746.2689115,
author = {Viswanathan, Venkatasubramanian and Ben Atitallah, Rabie and Dekeyser, Jean-Luc and Nakache, Benjamin and Nakache, Maurice},
title = {A Parallel And Scalable Multi-FPGA based Architecture for High Performance Applications (Abstract Only)},
year = {2015},
isbn = {9781450333153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684746.2689115},
doi = {10.1145/2684746.2689115},
abstract = {Several industrial applications are becoming highly sophisticated and distributed as they capture and process real-time data from several sources at the same time. Furthermore, availability of acquisition channels such as I/O interfaces per FPGA, also dictates how applications are partitioned over several devices. Thus computationally intensive, resource consuming functions are implemented on multiple hardware accelerators, making low-latency communication to be a crucial factor. In such applications, communication between multiple devices means using high-speed point-to-point protocols with little flexibility in terms of communication scalability. The problem with the current systems is that, they are usually built to meet the needs of a specific application, i.e., lacks flexibility to change the communication topology or upgrade hardware resources. This leads to obsolescence, hardware redesign cost, and also wastes computing power. Taking this into consideration, we propose a scalable, modular and customizable computing platform, with a parallel full-duplex communication network, that redefines the computation and communication paradigm in such applications. We have implemented a scalable distributed secure H.264 encoding application with 3 channels over 3 customizable FPGA modules. In a distributed architecture, the inter-FPGA communication time is almost completely overshadowed by the overall execution time for bigger data-sets, and is comparable to the overall execution time of a non-distributed architecture, for the same implementation scaled down to 1 channel for 1 FPGA. This makes our architecture highly scalable and suitable for high-performance streaming applications. With 3 detachable FPGA modules, each sending and receive data simultaneously at 3 GB/s each, we measured the total net unidirectional traffic at any given time in the system is 9 GB/s, making the total net bidirectional bandwidth for 6 modules to be 36 GB/s.},
booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {266},
numpages = {1},
keywords = {distributed intensive signal processing, parallel reconfigurable architecture, scalable system},
location = {Monterey, California, USA},
series = {FPGA '15}
}

@inproceedings{10.1145/3338840.3355646,
author = {Shih, Chi-Sheng and Yang, Kai-Wei},
title = {Design and implementation of distributed traceability system for smart factories based on blockchain technology},
year = {2019},
isbn = {9781450368438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338840.3355646},
doi = {10.1145/3338840.3355646},
abstract = {Manufacturing enterprises are facing the challenges of productivity improvement, product and process quality management due to the lack of data interaction and manufacturing process traceability. In modern factory, number of machineries are pipelined into assembly/production lines which may merge and branch to meet the procedure requirements. Each machine on the assembly line generates huge amount of sensing data and manufacturing data. However, many manufacturing system are not ready to manage big data. This paper aims on developing the distributed traceability system with Blockchain technology on IoT devices in order to improve the stability of the factory production line, to reduce defect rate and to bring the operational performance to a new level. Without depending on centralized storage, it is a robust, truly distributed peer-to-peer system and capable of node failure tolerance. With better scalability and data interaction, this distributed ledger network enables the use of data collection with security and potential of public traceability system protocol which allows multiple factories to participate, build an ecosystem of traceability together.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {181–188},
numpages = {8},
keywords = {computer communication networks, computer systems organization, internetworking},
location = {Chongqing, China},
series = {RACS '19}
}

@inproceedings{10.1145/2834965.2834974,
author = {Delbruel, St\'{e}phane and Frey, Davide and Ta\"{\i}ani, Fran\c{c}ois},
title = {Decentralized view prediction for global content placement},
year = {2015},
isbn = {9781450337335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2834965.2834974},
doi = {10.1145/2834965.2834974},
abstract = {A large portion of today's Internet traffic originates from streaming and video services. Storing, indexing, and serving these videos is a daily engineering challenge that requires increasing amounts of efforts and infrastructures. One promising direction to improve video services consists in predicting at upload time where and when a new video might be viewed, thereby optimizing placement and caching decisions. Implementing such a prediction service in a scalable manner poses significant technical challenges. In this paper, we address these challenges in the context of a decentralized storage system consisting of set-top boxes or end nodes. Specifically, we propose a novel data placement algorithm that exploits information about the tags associated with existing content, such as videos, and uses it to infer the number of views that newly uploaded content will have in each country.},
booktitle = {Proceedings of the 14th International Workshop on Adaptive and Reflective Middleware},
articleno = {10},
numpages = {3},
keywords = {User-generated content, YouTube, prediction, tag},
location = {Vancouver, BC, Canada},
series = {ARM 2015}
}

@inproceedings{10.1145/3178372.3179505,
author = {Ertel, Sebastian and Goens, Andr\'{e}s and Adam, Justus and Castrillon, Jeronimo},
title = {Compiling for concise code and efficient I/O},
year = {2018},
isbn = {9781450356442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178372.3179505},
doi = {10.1145/3178372.3179505},
abstract = {Large infrastructures of Internet companies, such as Facebook and Twitter, are composed of several layers of micro-services. While this modularity provides scalability to the system, the I/O associated with each service request strongly impacts its performance. In this context, writing concise programs which execute I/O efficiently is especially challenging. In this paper, we introduce \"{Y}auhau, a novel compile-time solution. \"{Y}auhau reduces the number of I/O calls through rewrites on a simple expression language. To execute I/O concurrently, it lowers the expression language to a dataflow representation. Our approach can be used alongside an existing programming language, permitting the use of legacy code. We describe an implementation in the JVM and use it to evaluate our approach. Experiments show that \"{Y}auhau can significantly improve I/O, both in terms of the number of I/O calls and concurrent execution. \"{Y}auhau outperforms state-of-the-art approaches with similar goals.},
booktitle = {Proceedings of the 27th International Conference on Compiler Construction},
pages = {104–115},
numpages = {12},
keywords = {I/O, concurrency, dataflow},
location = {Vienna, Austria},
series = {CC 2018}
}

@inproceedings{10.1145/3357384.3358002,
author = {Kawabata, Koki and Matsubara, Yasuko and Sakurai, Yasushi},
title = {Automatic Sequential Pattern Mining in Data Streams},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358002},
doi = {10.1145/3357384.3358002},
abstract = {Given a large volume of multi-dimensional data streams, such as that produced by IoT applications, finance and online web-click logs, how can we discover typical patterns and compress them into compact models? In addition, how can we incrementally distinguish multiple patterns while considering the information obtained from a pattern found in a streaming setting? In this paper, we propose a streaming algorithm, namely StreamScope, that is designed to find intuitive patterns efficiently from event streams evolving over time. Our proposed method has the following properties: (a) it is effective: it operates on semi-infinite collections of co-evolving streams and summarizes all the streams into a set of multiple discrete segments grouped by their similarities. (b) it is automatic: it automatically and incrementally recognizes such patterns and generates models for each of them if necessary; (c) it is scalable: the complexity of our method does not depend on the length of the data streams. Our extensive experiments on real data streams demonstrate that StreamScope can find meaningful patterns and achieve great improvements in terms of computational time and memory space over its full batch method competitors.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1733–1742},
numpages = {10},
keywords = {data stream, pattern discovery, time series},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3211954.3211959,
author = {Kawabata, Kouki and Matsubara, Yasuko and Sakurai, Yasushi},
title = {StreamScope: Automatic Pattern Discovery over Data Streams},
year = {2018},
isbn = {9781450358514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211954.3211959},
doi = {10.1145/3211954.3211959},
abstract = {Given a large volume of multi-dimensional data streams, such as that produced by IoT applications, finance and online web-click logs, how can we discover typical patterns and compress them into compact models? In addition, how can we incrementally distinguish multiple patterns while considering the information obtained from a pattern found in a streaming setting? In this paper, we propose a streaming algorithm, namely StreamScope, that is designed to find intuitive patterns efficiently from event streams evolving over time. Our proposed method has the following properties: (a) it is effective: it operates on semi-infinite collections of co-evolving streams and summarizes all the streams into a set of multiple discrete segments grouped by their similarities. (b) it is automatic: it automatically and incrementally recognizes such patterns and generates models for each of them if necessary; (c) it is scalable: the complexity of our method does not depend on the length of the data streams. Our extensive experiments on real datasets demonstrate that StreamScope can find meaningful patterns and achieve great improvements in terms of computational time and memory space over its full batch method competitors.},
booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {5},
numpages = {8},
keywords = {Data stream, Pattern discovery, Time series},
location = {Houston, TX, USA},
series = {aiDM'18}
}

@article{10.14778/2824032.2824099,
author = {Zoumpatianos, Kostas and Idreos, Stratos and Palpanas, Themis},
title = {RINSE: interactive data series exploration with ADS+},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824099},
doi = {10.14778/2824032.2824099},
abstract = {Numerous applications continuously produce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available. An adaptive index data structure, ADS+, which is specifically tailored to solve the problem of indexing and querying very large data series collections has been recently proposed as a solution to this problem. The main idea is that instead of building the complete index over the complete data set up-front and querying only later, we interactively and adaptively build parts of the index, only for the parts of the data on which the users pose queries. The net effect is that instead of waiting for extended periods of time for the index creation, users can immediately start exploring the data series. In this work, we present a demonstration of ADS+; we introduce RINSE, a system that allows users to experience the benefits of the ADS+ adaptive index through an intuitive web interface. Users can explore large datasets and find patterns of interest, using nearest neighbor search. They can draw queries (data series) using a mouse, or touch screen, or they can select from a predefined list of data series. RINSE can scale to large data sizes, while drastically reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), adaptive data series indexing can already answer 3 * 105 queries.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1912–1915},
numpages = {4}
}

@inproceedings{10.1145/2815546.2815583,
author = {Danilov, Dmitri and Lind, Artjom and Vainikko, Eero},
title = {Open Source Platform for Teaching Administration of Unix-like Systems},
year = {2015},
isbn = {9781450336109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815546.2815583},
doi = {10.1145/2815546.2815583},
abstract = {Unix-like operating systems are time-proven leaders of the server-side software and have cumulative share of ~60%. At least 10% of Unix-like systems are used on personal computers. Popularity of cloud based on-demand infrastructure (SaaS) has simplified the process of obtaining the hosting platforms for the developers. As a result, the demand for qualified administrators of Unix-like systems is growing rapidly. Education of Unix-like system administrators is a complicated and uncertain task due to diversity and large amount of different Unix-like platforms. The vendors of commercial distributions offer training courses and certifications. However, a significant number of open source distributions are remaining uncovered by structured and defined training routines. Considering these facts, we organized an open e-course relying on Debian Linux. In this paper, we will share our experience of the course development and teaching. Our main contributions are a scalable, secured and automatic task verification system; and a supplementary infrastructure solution that allows performing the course tasks at any location with internet access. Powering the course with these two developed components resulted in improvement of time resource and course quality. In addition, we will describe our developed solution in detail and analyze qualitative feedback of the students gathered during three years of teaching.},
booktitle = {Proceedings of the 2015 ACM SIGUCCS Annual Conference},
pages = {129–132},
numpages = {4},
keywords = {automatic evaluation, system administration, teaching},
location = {St. Petersburg, Florida, USA},
series = {SIGUCCS '15}
}

@inproceedings{10.1145/3057039.3057056,
author = {Rosselan, M. Z. and Sulaiman, S. I. and Othman, N.},
title = {Evolutionary Programming and Fast-Evolutionary Programming for Sizing and Optimization of Large-Scale Grid-Connected Photovoltaic (GCPV) System},
year = {2017},
isbn = {9781450348096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3057039.3057056},
doi = {10.1145/3057039.3057056},
abstract = {The paper presents the Evolutionary Programming (EP) and Fast-Evolutionary Programming (FEP) for sizing and optimization of Large-Scale Grid Connected Photovoltaic (GCPV) system. The objective of this study is to get the optimal value of Net Present Value for 5MW Solar farm located in Setiu Terengganu. In this study, there are two types of different algorithms that have been used which are Computational Intelligence-Based sizing by using EP and FEP. The performances of these two methods have been compared based on their NPV and Elapsed time with the Iterative Sizing Algorithm (ISA). The results showed that FEP produced the best NPV and much faster in term of elapsed time if compared with EP.},
booktitle = {Proceedings of the 9th International Conference on Computer and Automation Engineering},
pages = {296–301},
numpages = {6},
keywords = {Evolutionary Programming (EP), Fast-Evolutionary Programming (FEP), Net Present Value (NPV), Optimization},
location = {Sydney, Australia},
series = {ICCAE '17}
}

@inproceedings{10.1145/3028842.3028869,
author = {Netshikweta, R. and Hamam, Y. and Du, S.},
title = {Comparative study of load balancing routing algorithms in hybrid networks},
year = {2016},
isbn = {9781450347990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3028842.3028869},
doi = {10.1145/3028842.3028869},
abstract = {In this paper, a comparative study of routing protocols is performed in a hybrid network to recommend the best routing protocol to perform load balancing for Internet traffic. Open Shortest Path First (OSPF), Interior Gateway Routing Protocol (IGRP) and Intermediate System to Intermediate System (IS-IS) routing protocols are compared in OPNET modeller 14 to investigate their capability of ensuring fair distribution of traffic in a hybrid network. The network simulated is scaled to a campus. The network loads are varied in size and performance study is made by running simulations with all the protocols. The only considered performance factors for observation are packet drop, network delay, throughput and network load. IGRP presented better performance as compared to other protocols. The benefit of using IGRP is reduced packet drop, reduced network delay, increased throughput while offering relative better distribution of traffic in a hybrid network.},
booktitle = {Proceedings of the 1st International Conference on Intelligent Information Processing},
articleno = {27},
numpages = {9},
keywords = {hybrid networks, load balancing, routing algorithms},
location = {Wuhan, China},
series = {ICIIP '16}
}

@inproceedings{10.1145/2959424.2959427,
author = {Chiesa, Marco and Demmler, Daniel and Canini, Marco and Schapira, Michael and Schneider, Thomas},
title = {Towards Securing Internet eXchange Points Against Curious onlooKers},
year = {2016},
isbn = {9781450344432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959424.2959427},
doi = {10.1145/2959424.2959427},
abstract = {The growing relevance of Internet eXchange Points (IXPs), where an increasing number of networks exchange routing information, poses fundamental questions regarding the privacy guarantees of confidential business information. To facilitate the exchange of routes among their members, IXPs provide Route Server (RS) services to dispatch the routes according to each member's export policies. Nowadays, to make use of RSes, these policies must be disclosed to the IXP. This state of affairs raises privacy concerns among network administrators and even deters some networks from subscribing to RS services. We design SIXPACK (which stands for "Securing Internet eXchange Points Against Curious onlooKers"), a RS service that leverages Secure Multi-Party Computation (SMPC) techniques to keep export policies confidential, while maintaining the same functionalities as today's RSes. We assess the effectiveness and scalability of our system by evaluating our prototype implementation and using traces of data from one of the largest IXPs in the world.},
booktitle = {Proceedings of the 2016 Applied Networking Research Workshop},
pages = {32–34},
numpages = {3},
keywords = {internet exchange points, privacy, routing, secure multiparty},
location = {Berlin, Germany},
series = {ANRW '16}
}

@inproceedings{10.1145/3309697.3331481,
author = {Vial, Daniel and Subramanian, Vijay},
title = {A Structural Result for Personalized PageRank and its Algorithmic Consequences},
year = {2019},
isbn = {9781450366786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309697.3331481},
doi = {10.1145/3309697.3331481},
abstract = {Many systems, such as the Internet, social networks, and the power grid, can be represented as graphs. When analyzing graphs, it is often useful to compute scores describing the relative importance or distance between nodes. One example is Personalized PageRank (PPR), which assigns to each node v a vector whose i-th entry describes the importance of the i-th node from the perspective of v. PPR has proven useful in many applications, such as recommending who users should follow on social networks (if this i-th entry is large, v may be interested in following the i-th user). Unfortunately, computing n PPR vectors exactly for a graph of n nodes has complexity O(n3), which is infeasible for many graphs of interest. In this work, we devise a scheme to estimate all n PPR vectors with bounded l_1 error and complexity O(nc), where c&lt;2 depends on the degrees of the graph at hand, the desired error tolerance, and a parameter that defines PPR. This improves upon existing methods, the best of which have complexity O(n2 log n) in our setting. Our complexity guarantee holds with high probability, for certain choices of the PPR parameter, and for a certain class of random graphs (roughly speaking, the sparse directed configuration model with heavy-tailed in-degrees); our accuracy guarantee holds with probability 1 and for arbitrary graphs and PPR parameters. The complexity result arises as a consequence of our main (structural) result, which shows that the dimensionality of the set of PPR vectors scales sublinearly in n with high probability, for the same class of random graphs and for a notion of dimensionality similar to matrix rank. It is this coupling of the PPR vectors for the nodes on a common underlying graph that allows for estimating them faster. Hence, at a high level, our scheme is analogous to (but distinct from) low-rank matrix approximation. We also note that our scheme is similar to one that was proposed in [Jeh and Widom 2003] but lacked accuracy and complexity guarantees, so another contribution of our paper is to address this gap in the literature.},
booktitle = {Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {39–40},
numpages = {2},
keywords = {branching processes, directed configuration model, low-rank approximation, mixing times, personalized pagerank},
location = {Phoenix, AZ, USA},
series = {SIGMETRICS '19}
}

@inproceedings{10.1145/2931021.2931022,
author = {Arzt, Steven and Kussmaul, Tobias and Bodden, Eric},
title = {Towards cross-platform cross-language analysis with soot},
year = {2016},
isbn = {9781450343855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931021.2931022},
doi = {10.1145/2931021.2931022},
abstract = {To assess the security and quality of the growing number of programs on desktop computers, mobile devices, and servers, companies often rely on static analysis techniques. While static analysis has been applied successfully to various problems, the academic literature has largely focused on a subset of programming languages and frameworks, and often only on a single language at a time. Many tools have been created for Java and Android. In this paper, we present a first step toward re-using the existing Soot framework and its analyses for other platforms. We implement a frontend for converting the CIL assembly code of the .net Framework into Soot's Jimple code and show that this is possible without modifying Jimple nor overly losing semantic information. The frontend integrates Java/Android with CIL analysis and scales to large programs. A case study demonstrates the detection of real-world malware that uses CIL code inside an Android app to hide its behavior.},
booktitle = {Proceedings of the 5th ACM SIGPLAN International Workshop on State Of the Art in Program Analysis},
pages = {1–6},
numpages = {6},
keywords = {.net., CIL, Compiler, Frontend, Multi-Platform, Soot, Static Analysis},
location = {Santa Barbara, CA, USA},
series = {SOAP 2016}
}

@article{10.1145/3341617.3326140,
author = {Vial, Daniel and Subramanian, Vijay},
title = {A Structural Result for Personalized PageRank and its Algorithmic Consequences},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3341617.3326140},
doi = {10.1145/3341617.3326140},
abstract = {Many systems, such as the Internet, social networks, and the power grid, can be represented as graphs. When analyzing graphs, it is often useful to compute scores describing the relative importance or distance between nodes. One example is Personalized PageRank (PPR), which assigns to each node v a vector whose i-th entry describes the importance of the i-th node from the perspective of v. PPR has proven useful in many applications, such as recommending who users should follow on social networks (if this i-th entry is large, v may be interested in following the i-th user). Unfortunately, computing n PPR vectors exactly for a graph of n nodes has complexity O(n^3), which is infeasible for many graphs of interest. In this work, we devise a scheme to estimate all n PPR vectors with bounded l_1 error and complexity O(nc), where c &lt; 2 depends on the degrees of the graph at hand, the desired error tolerance, and a parameter that defines PPR. This improves upon existing methods, the best of which have complexity O(n2 \l{}og n) in our setting. Our complexity guarantee holds with high probability, for certain choices of the PPR parameter, and for a certain class of random graphs (roughly speaking, the sparse directed configuration model with heavy-tailed in-degrees); our accuracy guarantee holds with probability 1 and for arbitrary graphs and PPR parameters. The complexity result arises as a consequence of our main (structural) result, which shows that the dimensionality of the set of PPR vectors scales sublinearly in n with high probability, for the same class of random graphs and for a notion of dimensionality similar to matrix rank. It is this coupling of the PPR vectors for the nodes on a common underlying graph that allows for estimating them faster. Hence, at a high level, our scheme is analogous to (but distinct from) low-rank matrix approximation. We also note that our scheme is similar to one that was proposed in [Jeh and Widom 2003] but lacked accuracy and complexity guarantees, so another contribution of our paper is to address this gap in the literature.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {jun},
articleno = {25},
numpages = {88},
keywords = {branching processes, directed configuration model, low-rank approximation, mixing times, personalized pagerank}
}

@inproceedings{10.1145/3290420.3290477,
author = {Wang, Fan and Liu, Biying and Yang, Yan and Tang, Shuangshuo and Hu, Xiaopeng},
title = {Multi-scale representation based on convolutional neural networks for tracking},
year = {2018},
isbn = {9781450365345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290420.3290477},
doi = {10.1145/3290420.3290477},
abstract = {Visual Tracking technology is one of the major branches in computer vision. Although it has been studied for many years, there are still a number of challenges need to be overcome. In this paper, we propose a tracking algorithm based on multi-scale convolutional neural networks trained on large amounts of tracking sequence data with ground-truth bounding targets. Instead using the raw pixels to feed to the models, we use the image gradient to learn the object representation. We implement this by generating multiple scale version images from Laplacian pyramid, and we maintain a pool of networks corresponding to each kind of video for each scale and utilize the VGG-net to pre-train our models. From the models, we can extract multi-scale feature representations to encode the appearance. In addition, we improved the multiple instance learning tracking algorithm by introduce a penalty factor in the sigmoid function to solve the saturation problem. Using the multi-scale feature representations, we train a classifier combined with the improved MIL algorithm. The results comparing with several state-of-the-art methods on challenging sequences have proved the effectiveness of our proposed algorithm.},
booktitle = {Proceedings of the 4th International Conference on Communication and Information Processing},
pages = {96–101},
numpages = {6},
keywords = {convolutional neural network, feature extraction, object tracking},
location = {Qingdao, China},
series = {ICCIP '18}
}

@inproceedings{10.1145/3274895.3274929,
author = {Rashidian, Sina and Dong, Xinyu and Jain, Shubham Kumar and Wang, Fusheng},
title = {EaserGeocoder: integrative geocoding with machine learning (demo paper)},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274929},
doi = {10.1145/3274895.3274929},
abstract = {Increased availability of large amounts of address data provides opportunities for data driven studies to improve decision making in business applications and support precision public health with high resolution geolocations. Geocoding large number of addresses is challenging due to high cost and often disclosure of sensitive data to vendors over the Web. Most geocoders take advantage of Web APIs which require sending private addresses over the Internet, which may not be an option for many applications with sensitive data including public health and geo-medicine. Meanwhile, the cost for geocoding massive number of addresses could be high and becomes a major hurdle for many users. To overcome these challenges, we developed an open source on-premise geocoding software EaserGeocoder, which uses a novel integrative geocoding model to achieve high accuracy through integrating multiple open data sources. EaserGeocoder takes advantage of machine learning based approaches to determine best answers from multiple data sources. EaserGeocoder can also be easily parallelized to achieve high scalability through parallelized search and distributed computing. EaserGeocoder is on a par with commercial geocoding systems, outperforms open source systems, and is available for free.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {572–575},
numpages = {4},
keywords = {geocoding, geographic information system, text searching},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1145/2741948.2741961,
author = {Stoenescu, Radu and Olteanu, Vladimir and Popovici, Matei and Ahmed, Mohamed and Martins, Joao and Bifulco, Roberto and Manco, Filipe and Huici, Felipe and Smaragdakis, Georgios and Handley, Mark and Raiciu, Costin},
title = {In-Net: in-network processing for the masses},
year = {2015},
isbn = {9781450332385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2741948.2741961},
doi = {10.1145/2741948.2741961},
abstract = {Network Function Virtualization is pushing network operators to deploy commodity hardware that will be used to run middlebox functionality and processing on behalf of third parties: in effect, network operators are slowly but surely becoming in-network cloud providers. The market for innetwork clouds is large, ranging from content providers, mobile applications and even end-users.We show in this paper that blindly adopting cloud technologies in the context of in-network clouds is not feasible from both the security and scalability points of view. Instead we propose In-Net, an architecture that allows untrusted endpoints as well as content-providers to deploy custom in-network processing to be run on platforms owned by network operators. In-Net relies on static analysis to allow platforms to check whether the requested processing is safe, and whether it contradicts the operator's policies.We have implemented In-Net and tested it in the wide-area, supporting a range of use-cases that are difficult to deploy today. Our experience shows that In-Net is secure, scales to many users (thousands of clients on a single inexpensive server), allows for a wide-range of functionality, and offers benefits to end-users, network operators and content providers alike.},
booktitle = {Proceedings of the Tenth European Conference on Computer Systems},
articleno = {23},
numpages = {15},
location = {Bordeaux, France},
series = {EuroSys '15}
}

@inproceedings{10.1145/3212711.3212712,
author = {Ibrahim, Ahmad},
title = {Securing Embedded Networks through Secure Collective Attestation},
year = {2018},
isbn = {9781450358415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3212711.3212712},
doi = {10.1145/3212711.3212712},
abstract = {Embedded devices are increasingly permeating our environment to collect data and act on the insight derived. Examples of such devices include smart environments and autonomous systems. The increasing ability to connect, communicate with, and remotely control such devices via the legacy internet has raised considerable security and privacy concerns. One key mechanism to protect the software integrity of these devices is attestation.In this dissertation, we devise attestation schemes that are scalable and applicable for large networks of embedded devices. In particular, we present attestation schemes that are capable of detecting remote malware infestation, physical, and run-time attacks in different settings including smart environments and autonomous systems.},
booktitle = {Proceedings of the 2018 Workshop on MobiSys 2018 Ph.D. Forum},
pages = {1–2},
numpages = {2},
location = {Munich, Germany},
series = {MobiSys PhD Forum '18}
}

@inproceedings{10.1145/3240508.3240510,
author = {Wang, Zheng and Bai, Xiang and Ye, Mang and Satoh, Shin'ichi},
title = {Incremental Deep Hidden Attribute Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240510},
doi = {10.1145/3240508.3240510},
abstract = {Person re-identifcation is a key technique to match person images captured in non-overlapping camera views. Due to the sensitivity of visual features to environmental changes, semantic attributes, such as "short-hair" or "long-hair", begin to be investigated to represent person's appearance to improve the re-identifcation performance. Generally, training semantic attribute representations requires massive annotated samples, which limits the applicability on the large-scale practical applications. To alleviate the reliance on annotation efforts, we propose a new person representation with hidden attributes by mining latent information from visual feature in an unsupervised manner. In particular, an auto-encoder model is plugged-in to the deep learning network to compose a Deep Hidden Attribute Network (DHA-Net). The learnt hidden attribute representation preserves the robustness of semantic attributes and simultaneously inherits the discrimination ability of visual features. Experiments conducted on public datasets have validated the effectiveness of DHA-Net. On two large-scale datasets, i.e., Market-1501 and DukeMTMC-reID, the proposed method outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {72–80},
numpages = {9},
keywords = {hidden attribute, person re-identification, unsupervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3123266.3123290,
author = {Li, Xin and Yang, Fan and Cheng, Hong and Chen, Junyu and Guo, Yuxiao and Chen, Leiting},
title = {Multi-Scale Cascade Network for Salient Object Detection},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123290},
doi = {10.1145/3123266.3123290},
abstract = {In this paper we present a novel network architecture, called Multi-Scale Cascade Network (MSC-Net), to identify the most visually conspicuous objects in an image. Our network consists of several stages (sub-networks) for handling saliency detection across different scales. All these sub-networks form a cascade structure (in a coarse-to-fine manner) where the same underlying convolutional feature representations are fully shared. Compared with existing CNN-based saliency models, the MSC-Net can naturally enable the learning process in the finer cascade stages to encode more global contextual information while progressively incorporating the saliency prior knowledge obtained from coarser stages and thus lead to better detection accuracy. We also design a novel refinement module to further filter out errors by considering the intermediate feedback information. Our MSC-Net is highly integrated, end-to-end trainable, and very powerful. The proposed method achieves state-of-the-art performance on five widely-used salient object detection benchmarks, outperforming existing methods and also maintaining high efficiency. Code and pre-trained models are available at https://github.com/lixin666/MSC-NET.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {439–447},
numpages = {9},
keywords = {computer vision, deep learning, salient object detection},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/3308561.3353812,
author = {Nakamura, Karen},
title = {My Algorithms Have Determined You're Not Human: AI-ML, Reverse Turing-Tests, and the Disability Experience},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353812},
doi = {10.1145/3308561.3353812},
abstract = {The past decade has seen an exponential growth in the capabilities and deployment of artificial intelligence systems based on deep neural networks. These are visible through the speech recognition and natural language processing of Alexa/Siri/Google that structure many of our everyday interactions, and the promise of SAE Level 5 autonomous driving provided by Tesla and Waze. Aside from these shiny and visible applications of AI-ML are many other uses that are more subtle: AI-ML is now being used to screen job applicants as well as determine which web ads we are shown. And while many vendors of AI-ML technologies have promised that these tools provide for greater access and freedom from human prejudice, disabled users have found that these tools can embed and deploy newer, subtler forms of discrimination against disabled people. At their worst, AI-ML systems can deny disabled people their humanity.The explosion of AI-ML technologies in the last decade has been driven by at least three factors. First, the deep neural networks algorithms that currently drive much machine learning have been improved dramatically through the use of backpropagation [1], generative adversarial nets [2], and convolution [3], allowing for their deployment across a broad variety of datasets. Second, the cost of computing hardware (especially GPUs) has dropped dramatically while large scale cloud computing facilities and widespread fiber/ broadband/4G has provided for universal availability. Finally, large datasets have come online to aid in the training of the neural nets - for example, the image datasets provided through Google and Facebook, the large natural language datasets driving Amazon Alexa, and so forth.Deep neural networks themselves have two key features or flaws, depending on the perspective. First, they are highly dependent on the diversity of the training dataset used. Second, their internal operations when deployed are entirely opaque not only to the end-user but also to the designers of the system itself.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {1–2},
numpages = {2},
keywords = {artificial intelligence, bias, deep neural networks, disabilities, race},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{10.1145/3137133.3137145,
author = {Kuppannagari, Sanmukh R. and Kannan, Rajgopal and Prasanna, Viktor K.},
title = {Optimal net-load balancing in smart grids with high PV penetration},
year = {2017},
isbn = {9781450355445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3137133.3137145},
doi = {10.1145/3137133.3137145},
abstract = {Mitigating mismatches in supply demand curve is critical for smooth power grid operation. Traditionally, load curtailment techniques such as Demand Response (DR) have been used for this purpose. However, smart grids with high PV penetration sometimes exhibit supply surplus causing over-voltages. Thus, load curtailment techniques cannot be the only component of a net-load balancing framework. Supply curtailment techniques such as Volt-Var Optimizations are complex and computationally expensive. This increases the complexity of net-load balancing systems used by the grid operator and limits their scalability. Recently, new technologies have been developed that enable the rapid and selective connection of PV modules of an installation to the grid. Taking advantage of these advancements, we develop a unified optimal net-load balancing framework which performs both load and solar curtailment. We show that when the available curtailment values are discrete, this problem is NP-hard and develop bounded approximation algorithms for minimizing the curtailment cost. Our algorithms produce fast solutions, given the tight timing constraints required for grid operation. We also incorporate the notion of fairness to ensure that curtailment is evenly distributed among all the nodes. Finally, we develop an online algorithm which performs net-load balancing using only data available for the current interval. Using both theoretical analysis and practical evaluations, we show that our net-load balancing algorithms provide solutions which are close to optimal in a small amount of time.},
booktitle = {Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments},
articleno = {27},
numpages = {10},
keywords = {approximation algorithms, net-load balancing, smart grid},
location = {Delft, Netherlands},
series = {BuildSys '17}
}

@inproceedings{10.1145/3299815.3314433,
author = {Khieu, Brian and Moh, Melody},
title = {CBPKI: Cloud Blockchain-based Public Key Infrastructure},
year = {2019},
isbn = {9781450362511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299815.3314433},
doi = {10.1145/3299815.3314433},
abstract = {This paper proposes a cloud based public key infrastructure utilizing blockchain technology model for replacing the currently outdated traditional variant. Environments such as Big Data and IoT ecosystems have scalable and resilient needs that current public key infrastructure cannot satisfy. Enhancements over past models include the use of blockchains to establish persistent access to certificate data and certificate revocation lists. Further improvements made were the decoupling of data from the certificate authority as well as hosting it on a cloud provider in order to tap into traffic security measures of said provider. This results in a smaller viable attack surface for the proposed model. Instead of holding data within the transaction data fields of blocks, certificate data and status were embedded into smart contracts. Our tests revealed a significant performance increase of our proposed model over that of both traditional and the version that stored data within blocks. Storing the certificate data within smart contracts reduced the size of data to be mined which in turn lowered the time to mine said data to 6.6% of the time used for the block data storage method. Also, the mining gas cost per certificate was consequently cut by a significant 87%. In summary, completely decoupling the certificate authority portion of a public key infrastructure and storing certificate data inside smart contracts yields a sizable performance boost while decreasing the attack surface.},
booktitle = {Proceedings of the 2019 ACM Southeast Conference},
pages = {58–63},
numpages = {6},
keywords = {Blockchain, Certificate Authority, Cloud, Public Key Infrastructure, Smart Contract},
location = {Kennesaw, GA, USA},
series = {ACM SE '19}
}

@article{10.1145/3351474,
author = {Hester, Josiah and Sorber, Jacob},
title = {Batteries not included},
year = {2019},
issue_date = {Fall 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3351474},
doi = {10.1145/3351474},
abstract = {Getting things done amid frequent power failures, batteryless intermittent research is rethinking how we build computing systems and paving the way to a sustainable and scalable digital future. The next trillion devices might be a little weird.},
journal = {XRDS},
month = {sep},
pages = {23–27},
numpages = {5}
}

@inproceedings{10.1145/2847263.2847265,
author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
title = {Going Deeper with Embedded FPGA Platform for Convolutional Neural Network},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847265},
doi = {10.1145/2847263.2847265},
abstract = {In recent years, convolutional neural network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are com-putational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited bandwidth and on-chip memory size limit the performance of FPGA accelerator for CNN.In this paper, we go deeper with the embedded FPGA platform on accelerating CNNs and propose a CNN accelerator design on embedded FPGA for Image-Net large-scale image classification. We first present an in-depth analysis of state-of-the-art CNN models and show that Convolutional layers are computational-centric and Fully-Connected layers are memory-centric.Then the dynamic-precision data quantization method and a convolver design that is efficient for all layer types in CNN are proposed to improve the bandwidth and resource utilization. Results show that only 0.4% accuracy loss is introduced by our data quantization flow for the very deep VGG16 model when 8/4-bit quantization is used. A data arrangement method is proposed to further ensure a high utilization of the external memory bandwidth. Finally, a state-of-the-art CNN, VGG16-SVD, is implemented on an embedded FPGA platform as a case study. VGG16-SVD is the largest and most accurate network that has been implemented on FPGA end-to-end so far. The system on Xilinx Zynq ZC706 board achieves a frame rate at 4.45 fps with the top-5 accuracy of 86.66% using 16-bit quantization. The average performance of convolutional layers and the full CNN is 187.8 GOP/s and 137.0 GOP/s under 150MHz working frequency, which outperform previous approaches significantly.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {26–35},
numpages = {10},
keywords = {bandwidth utilization, convolutional neural network (cnn), dynamic-precision data quantization, embedded fpga},
location = {Monterey, California, USA},
series = {FPGA '16}
}

@inproceedings{10.1145/3358695.3360892,
author = {He, Jing},
title = {A Subgraph Isomorphism Algorithm for Privacy Preserving in Dynamic Social Network},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3360892},
doi = {10.1145/3358695.3360892},
abstract = {Cyberspace topologies can be demonstrated mathematically by a graph theoretic approach which basic structure is in accord with the interconnected world of social networks. Subgraph pattern matching analysis is an approach to performing computer network data analysis that executes efficient continuous queries on dynamic social graphs and minimizes the overhead on both search space and query. Privacy preserving has become a forward-looking interest both in industry and in academia in recent years, especially for social media streams and cyber data sources are prominent examples of high throughput and dynamic graphs, which made us inundate with the stream of updates. Existing research work about anonymizing a social network is still an open question for research. It is because altering attributes of vertices and edges might affect the neighbourhood relationships between vertices; removing or adding vertices and edges might also influence other vertices and edges as well as the properties of the social network. As a matter of fact, measuring information loss in the re-construction of graph is also a problem of research. Therefore, the technical challenges of privacy security have aroused a lot of attention, including rational dataset anonymity strategy, subgraph matching method, user queries processing without compromising sensitive information and and adversary background capability modelling. As today's defenses on the network are fast becoming obsolete and dealing with problems causing by flooding activities and untrustworthy nodes in the network is urgent. To this end, this talk will complement the corresponding defects of existing methods as well as enhance overall security posture. More specifically, this research will investigate: (i) how to perform privacy preserving graph analytics on encrypted graphs, and (ii) how to ensure that the procedure of matching can be protected from privacy breach. In addition, we also focus on incremental processing algorithm that can support high data velocity social data graph with distributed environment. It is hoped that by doing so, it can alleviate the burden of searching (sub-)graph patterns. Finally, quantification analysis on information loss under different data and anonymous structural methods will be studied to find out the most suitable ones for privacy protection. The objective is to build a complete and strong framework aiming at dealing with not only scalability, confidentiality privacy leakage, and authenticity, but also fairness towards aborting behaviors, which can improve the work of privacy-preserving technique to achieve efficiency, safety, and convenience in practical application.To preserve nodes’ privacy, all identities are removed as shown in Figure 1(b). Unfortunately, if an adversary has acquired some knowledge about the neighbors of an individual, the privacy leakage may still happen. If an attacker knows that Dave has two close friends who they are mutual friends and has another three friends who do not know each other, the 1-neighborhood graph of Dave as shown in Figure l (d), so the node representing Dave can be correctly identified since no other vertices don't share the same 1-neighborhood graph in the released network. To protect the privacy network, the traditional method is to guarantee that any individual cannot be located uniquely in the anonymized social network with a probability higher than 1/k, where k is a user- specified parameter carrying the same structural pattern in the k-anonymity model. By adding a noise edge connecting Fred with Greg, the 1-neighborhood graph of every vertex in Figure (e) is not unique, which can ensure an adversary with the 1- neighborhood knowledge cannot target any individual from this anonymous graph with a confidence higher than 1.However, the existing work about anonymizing a social network is limited since altering labels of vertices and edges may affect the neighborhood relationships between vertices, and removing or adding vertices and edges may influence other vertices and edges as well as the properties of the Social network.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {140–141},
numpages = {2},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1145/2847263.2847276,
author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
title = {Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847276},
doi = {10.1145/2847263.2847276},
abstract = {Convolutional Neural Networks (CNNs) have gained popularity in many computer vision applications such as image classification, face detection, and video analysis, because of their ability to train and classify with high accuracy. Due to multiple convolution and fully-connected layers that are compute-/memory-intensive, it is difficult to perform real-time classification with low power consumption on today?s computing systems. FPGAs have been widely explored as hardware accelerators for CNNs because of their reconfigurability and energy efficiency, as well as fast turn-around-time, especially with high-level synthesis methodologies. Previous FPGA-based CNN accelerators, however, typically implemented generic accelerators agnostic to the CNN configuration, where the reconfigurable capabilities of FPGAs are not fully leveraged to maximize the overall system throughput. In this work, we present a systematic design space exploration methodology to maximize the throughput of an OpenCL-based FPGA accelerator for a given CNN model, considering the FPGA resource constraints such as on-chip memory, registers, computational resources and external memory bandwidth. The proposed methodology is demonstrated by optimizing two representative large-scale CNNs, AlexNet and VGG, on two Altera Stratix-V FPGA platforms, DE5-Net and P395-D8 boards, which have different hardware resources. We achieve a peak performance of 136.5 GOPS for convolution operation, and 117.8 GOPS for the entire VGG network that performs ImageNet classification on P395-D8 board.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {16–25},
numpages = {10},
keywords = {convolutional neural networks, fpga, opencl, optimization},
location = {Monterey, California, USA},
series = {FPGA '16}
}

@inproceedings{10.1145/3097983.3098187,
author = {Labutov, Igor and Huang, Yun and Brusilovsky, Peter and He, Daqing},
title = {Semi-Supervised Techniques for Mining Learning Outcomes and Prerequisites},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098187},
doi = {10.1145/3097983.3098187},
abstract = {Educational content of today no longer only resides in textbooks and classrooms; more and more learning material is found in a free, accessible form on the Internet. Our long-standing vision is to transform this web of educational content into an adaptive, web-scale "textbook", that can guide its readers to most relevant "pages" according to their learning goal and current knowledge. In this paper, we address one core, long-standing problem towards this goal: identifying outcome and prerequisite concepts within a piece of educational content (e.g., a tutorial). Specifically, we propose a novel approach that leverages textbooks as a source of distant supervision, but learns a model that can generalize to arbitrary documents (such as those on the web). As such, our model can take advantage of any existing textbook, without requiring expert annotation. At the task of predicting outcome and prerequisite concepts, we demonstrate improvements over a number of baselines on six textbooks, especially in the regime of little to no ground-truth labels available. Finally, we demonstrate the utility of a model learned using our approach at the task of identifying prerequisite documents for adaptive content recommendation --- an important step towards our vision of the "web as a textbook".},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {907–915},
numpages = {9},
keywords = {adaptive hypermedia, educational data mining, graphical models, open corpus adaptive hypermedia, semi-supervised machine learning},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3132211.3134456,
author = {Drolia, Utsav and Guo, Katherine and Narasimhan, Priya},
title = {Precog: &lt;u&gt;p&lt;/u&gt;refetching for image &lt;u&gt;recog&lt;/u&gt;nition applications at the edge},
year = {2017},
isbn = {9781450350877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132211.3134456},
doi = {10.1145/3132211.3134456},
abstract = {Image recognition applications are on the rise. Increasingly, applications on edge devices such as mobile smartphones, drones and cars, are relying on recognition techniques to provide interactive and intelligent functionality. Given the complexity of these techniques, and resource constrained nature of edge devices, applications rely on offloading compute intensive recognition tasks to the cloud. This has also lead to the rise of cloud-based recognition services. This involves sending captured images to remote servers across the Internet, which leads to slower responses. With the rising numbers of edge devices, both, the network and such centralized cloud-based solutions, are likely to be under stress, and lead to further slower responses. To reduce the recognition latency, and provide better scalability to the cloud-based solutions, we propose Precog. Precog employs selective computation on the devices to reduce the need to offload images to the cloud. In coordination with edge servers, it uses prediction to prefetch parts of the trained classifiers used for recognition onto the devices, and uses these smaller models to accelerate recognition on devices. Our evaluation shows that Precog can reduce latency by up to 5\texttimes{}, better utilize edge and cloud resources and also increase accuracy. We believe that Precog is the first system to use devices and edge servers collaboratively to enable prefetching and caching on the devices, and drive down recognition latency for mobile applications.},
booktitle = {Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
articleno = {17},
numpages = {13},
keywords = {caching, edge computing, image recognition, machine learning, mobile, prefetching},
location = {San Jose, California},
series = {SEC '17}
}

@article{10.1109/TNET.2015.2512541,
author = {Kandhway, Kundan and Kuri, Joy},
title = {Optimal Resource Allocation Over Time and Degree Classes for Maximizing Information Dissemination in Social Networks},
year = {2016},
issue_date = {October 2016},
publisher = {IEEE Press},
volume = {24},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2015.2512541},
doi = {10.1109/TNET.2015.2512541},
abstract = {We study the optimal control problem of allocating campaigning resources over the campaign duration and degree classes in a social network. Information diffusion is modeled as a Susceptible-Infected epidemic and direct recruitment of susceptible nodes to the infected informed class is used as a strategy to accelerate the spread of information. We formulate an optimal control problem for optimizing a net reward function, a linear combination of the reward due to information spread and cost due to application of controls. The time varying resource allocation and seeds for the epidemic are jointly optimized. A problem variation includes a fixed budget constraint. We prove the existence of a solution for the optimal control problem, provide conditions for uniqueness of the solution, and prove some structural results for the controls e.g., controls are non-increasing functions of time. The solution technique uses Pontryagin's Maximum Principle and the forward-backward sweep algorithm and its modifications for numerical computations. Our formulations lead to large optimality systems with up to about 200 differential equations and allow us to study the effect of network topology Erdos-R\'{e}nyi/scale-free on the controls. Results reveal that the allocation of campaigning resources to various degree classes depends not only on the network topology but also on system parameters such as cost/abundance of resources. The optimal strategies lead to significant gains over heuristic strategies for various model parameters. Our modeling approach assumes uncorrelated network, however, we find the approach useful for real networks as well. This work is useful in product advertising, political and crowdfunding campaigns in social networks.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {3204–3217},
numpages = {14}
}

@article{10.14778/2856318.2856327,
author = {Epasto, Alessandro and Lattanzi, Silvio and Mirrokni, Vahab and Sebe, Ismail Oner and Taei, Ahmed and Verma, Sunita},
title = {Ego-net community mining applied to friend suggestion},
year = {2015},
issue_date = {December 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/2856318.2856327},
doi = {10.14778/2856318.2856327},
abstract = {In this paper, we present a study of the community structure of ego-networks---the graphs representing the connections among the neighbors of a node---for several online social networks. Toward this goal, we design a new technique to efficiently build and cluster all the ego-nets of a graph in parallel (note that even just building the ego-nets efficiently is challenging on large networks). Our experimental findings are quite compelling: at a microscopic level it is easy to detect high quality communities.Leveraging on this fact we, then, develop new features for friend suggestion based on co-occurrences of two nodes in different ego-nets' communities. Our new features can be computed efficiently on very large scale graphs by just analyzing the neighborhood of each node. Furthermore, we prove formally on a stylized model, and by experimental analysis that this new similarity measure outperforms the classic local features employed for friend suggestions.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {324–335},
numpages = {12}
}

@inproceedings{10.1145/3123266.3123432,
author = {Li, Junnan and Wong, Yongkang and Zhao, Qi and Kankanhalli, Mohan S.},
title = {Attention Transfer from Web Images for Video Recognition},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3123432},
doi = {10.1145/3123266.3123432},
abstract = {Training deep learning based video classifiers for action recognition requires a large amount of labeled videos. The labeling process is labor-intensive and time-consuming. On the other hand, large amount of weakly-labeled images are uploaded to the Internet by users everyday. To harness the rich and highly diverse set of Web images, a scalable approach is to crawl these images to train deep learning based classifier, such as Convolutional Neural Networks (CNN). However, due to the domain shift problem, the performance of Web images trained deep classifiers tend to degrade when directly deployed to videos. One way to address this problem is to fine-tune the trained models on videos, but sufficient amount of annotated videos are still required. In this work, we propose a novel approach to transfer knowledge from image domain to video domain. The proposed method can adapt to the target domain (i.e. video data) with limited amount of training data. Our method maps the video frames into a low-dimensional feature space using the class-discriminative spatial attention map for CNNs. We design a novel Siamese EnergyNet structure to learn energy functions on the attention maps by jointly optimizing two loss functions, such that the attention map corresponding to a ground truth concept would have higher energy. We conduct extensive experiments on two challenging video recognition datasets (i.e. TVHI and UCF101), and demonstrate the efficacy of our proposed method.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1–9},
numpages = {9},
keywords = {action recognition, attention map, domain adaptation},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1145/2847263.2847316,
author = {Davis, James J. and Hung, Eddie and Levine, Joshua M. and Stott, Edward A. and Cheung, Peter Y.K. and Constantinides, George A.},
title = {Knowledge is Power: Module-level Sensing for Runtime Optimisation (Abstact Only)},
year = {2016},
isbn = {9781450338561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2847263.2847316},
doi = {10.1145/2847263.2847316},
abstract = {We propose the compile-time instrumentation of coexisting modules?IP blocks, accelerators, etc.?implemented in FPGAs. The efficient mapping of tasks to execution units can then be achieved, for power and/or timing performance, by tracking dynamic power consumption and/or timing slack online at module-level granularity. Our proposed instrumentation is transparent, thereby not affecting circuit functionality. Power and timing overheads have proven to be small and tend to be outweighed by the exposed runtime benefits.Dynamic power consumption can be inferred through the measurement of switching activity on indicative, frequently toggling nets. Online analysis is able to derive a live power breakdown by building and updating a model fed with per-module activity counts and system-wide power consumption. Such a model can be continuously refined and its use allows the tracking of unpredictable phenomena, including degradation.Online measurement of slack in critical (and near-critical) paths facilitates the safe erosion of static timing analysis-derived guardbands. This then enables the co-optimisation of power and timing performance under given external operating constraints, including those which change over time. Assuming functional compatibility, high-priority tasks would suit execution within modules with excess slack. This could be reduced via dynamic frequency scaling, thereby increasing throughput.},
booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {276},
numpages = {1},
keywords = {instrumentation, online algorithms, optimisation, power measurement, runtime management, task mapping, timing slack measurement},
location = {Monterey, California, USA},
series = {FPGA '16}
}

@inproceedings{10.1145/3240508.3240657,
author = {Shi, Hengcan and Li, Hongliang and Wu, Qingbo and Meng, Fanman and Ngan, King N.},
title = {Boosting Scene Parsing Performance via Reliable Scale Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240657},
doi = {10.1145/3240508.3240657},
abstract = {Segmenting objects on suitable scales is a key factor to improve the scene parsing performance. Existing methods either simply average multi-scale results or predict scales by weakly-supervised models, due to the lack of scale labels. In this paper, we propose a novel fully-supervised Scale Prediction Model. On one hand, the proposed Scale Prediction Model learns parsing scales by the strong scale supervision, which is automatically generated from the scene parsing ground truth without any extra manually annotation. On the other hand, we explore the relationship between scale and object class, and propose to use the object class information to further improve the reliability of the scale prediction. The proposed Scale Prediction Model improves 23.1%, 20.1% and 29.3% scale prediction accuracies on the NYU Depth v2, PASCAL-Context and SIFT Flow datasets, respectively. Based on the Scale Prediction Model, we design a Scale Parsing Net (SPNet) for scene parsing, which segments each object on the scale predicted by the Scale Prediction Model. Moreover, SPNet leverages the intermediate result (i.e., the object class) to refine the parsing results. The experiment results show that SPNet outperforms many state-of-the-art methods on multiple scene parsing datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {492–500},
numpages = {9},
keywords = {deep learning, scale prediction, scene parsing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3292500.3330873,
author = {Wang, Qinyong and Yin, Hongzhi and Wang, Hao and Nguyen, Quoc Viet Hung and Huang, Zi and Cui, Lizhen},
title = {Enhancing Collaborative Filtering with Generative Augmentation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330873},
doi = {10.1145/3292500.3330873},
abstract = {Collaborative filtering (CF) has become one of the most popular and widely used methods in recommender systems, but its performance degrades sharply for users with rare interaction data. Most existing hybrid CF methods try to incorporate side information such as review texts to alleviate the data sparsity problem. However, the process of exploiting and integrating side information is computationally expensive. Existing hybrid recommendation methods treat each user equally and ignore that the pure CF methods have already achieved both effective and efficient recommendation performance for active users with sufficient interaction records and the little improvement brought by side information to these active users is ignorable. Therefore, they are not cost-effective solutions. One cost-effective idea to bypass this dilemma is to generate sufficient "real" interaction data for the inactive users with the help of side information, and then a pure CF method could be performed on this augmented dataset effectively. However, there are three major challenges to implement this idea. Firstly, how to ensure the correctness of the generated interaction data. Secondly, how to combine the data augmentation process and recommendation process into a unified model and train the model end-to-end. Thirdly, how to make the solution generalizable for various side information and recommendation tasks. In light of these challenges, we propose a generic and effective CF model called AugCF that supports a wide variety of recommendation tasks. AugCF is based on Conditional Generative Adversarial Nets that additionally consider the class (like or dislike) as a feature to generate new interaction data, which can be a sufficiently real augmentation to the original dataset. Also, AugCF adopts a novel discriminator loss and Gumbel-Softmax approximation to enable end-to-end training. Finally, extensive experiments are conducted on two large-scale recommendation datasets, and the experimental results show the superiority of our proposed model.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {548–556},
numpages = {9},
keywords = {adversarial training, collaborative filtering, data sparsit},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/2823355,
author = {Huang, Zhichuan and Zhu, Ting and Irwin, David and Mishra, Aditya and Menasche, Daniel and Shenoy, Prashant},
title = {Minimizing Transmission Loss in Smart Microgrids by Sharing Renewable Energy},
year = {2016},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/2823355},
doi = {10.1145/2823355},
abstract = {Renewable energy (e.g., solar energy) is an attractive option to provide green energy to homes. Unfortunately, the intermittent nature of renewable energy results in a mismatch between when these sources generate energy and when homes demand it. This mismatch reduces the efficiency of using harvested energy by either (i) requiring batteries to store surplus energy, which typically incurs ∼ 20% energy conversion losses, or (ii) using net metering to transmit surplus energy via the electric grid’s AC lines, which severely limits the maximum percentage of renewable penetration possible. In this article, we propose an alternative structure where nearby homes explicitly share energy with each other to balance local energy harvesting and demand in microgrids. We develop a novel energy sharing approach to determine which homes should share energy, and when to minimize system-wide energy transmission losses in the microgrid. We evaluate our approach in simulation using real traces of solar energy harvesting and home consumption data from a deployment in Amherst, MA. We show that our system (i) reduces the energy loss on the AC line by 64% without requiring large batteries, (ii) performance scales up with larger battery capacities, and (iii) is robust to different energy consumption patterns and energy prediction accuracy in the microgrid.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {dec},
articleno = {5},
numpages = {22},
keywords = {Microgrid, battery, energy sharing, renewable energy}
}

@inproceedings{10.1145/2940116.2940117,
author = {Casas, Pedro and D'Alconzo, Alessandro and Zseby, Tanja and Mellia, Marco},
title = {Big-DAMA: Big Data Analytics for Network Traffic Monitoring and Analysis},
year = {2016},
isbn = {9781450344265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940116.2940117},
doi = {10.1145/2940116.2940117},
abstract = {The complexity of the Internet has dramatically increased in the last few years, making it more important and challenging to design scalable Network Traffic Monitoring and Analysis (NTMA) applications and tools. Critical NTMA applications such as the detection of anomalies, network attacks and intrusions, require fast mechanisms for online analysis of thousands of events per second, as well as efficient techniques for offline analysis of massive historical data. We are witnessing a major development in Big Data Analysis Frameworks (BDAFs), but the application of BDAFs and scalable analysis techniques to the NTMA domain remains poorly understood and only in-house and difficult to benchmark solutions are conceived. In this position paper we describe the basis of the Big-DAMA research project, which aims at tackling this growing need by benchmarking and developing novel scalable techniques and frameworks capable to analyze both online network traffic data streams and offline massive traffic datasets.},
booktitle = {Proceedings of the 2016 Workshop on Fostering Latin-American Research in Data Communication Networks},
pages = {1–3},
numpages = {3},
keywords = {Big Data, Data Mining, Data Stream Processing, Machine Learning, Network Traffic Monitoring and Analysis},
location = {Florianopolis, Brazil},
series = {LANCOMM '16}
}

@inproceedings{10.1145/3240876.3240885,
author = {Xiao, Shaoning and Li, Yimeng and Ye, Yunan and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhu, Jiang and Zhuang, Yueting},
title = {Video question answering via multi-granularity temporal attention network learning},
year = {2018},
isbn = {9781450365208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240876.3240885},
doi = {10.1145/3240876.3240885},
abstract = {This work aims to address the problem of video question answering (VideoQA) with a novel model and a new open-ended VideoQA dataset. VideoQA is a challenging field in visual information retrieval, which generates the answer according to video content and question. The existing works mostly focus on overall frame-level visual understanding to tackle the problem, which neglects finer-grained and temporal information inside the video, or just combines the multi-grained representations simply by concatenation or addition. Thus, we propose the multi-granularity temporal attention network (MGTA-Net) that enables to search for the specific frames in a video that are holistically and locally related to the answer. We first learn the mutual attention representations of multi-grained visual content and question. Then the mutually attended features are combined hierarchically using a double layer LSTM to generate the answer. The effectiveness of our model is demonstrated on the large scale dataset.},
booktitle = {Proceedings of the 10th International Conference on Internet Multimedia Computing and Service},
articleno = {46},
numpages = {5},
keywords = {temporal co-attention, video question answering, visual information retrieval},
location = {Nanjing, China},
series = {ICIMCS '18}
}

@inproceedings{10.1145/2896377.2901480,
author = {Yang, Sen and Lin, Bill and Xu, Jun},
title = {Safe Randomized Load-Balanced Switching by Diffusing Extra Loads},
year = {2016},
isbn = {9781450342667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896377.2901480},
doi = {10.1145/2896377.2901480},
abstract = {Load-balanced switch architectures are known to be scalable in both size and speed, which is of interest due to the continued exponential growth in Internet traffic. However, the main drawback of load-balanced switches is that packets can depart out of order from the switch. Randomized load-balancing of application flows by means of hashing on the packet header is a well-known simple solution to this packet reordering problem in which all packets belonging to the same application flow are routed through the same intermediate port and hence the same path through the switch. Unfortunately, this method of load-balancing can lead to instability, depending on the mix of flow sizes and durations in the group of flows that gets randomly assigned to route through the same intermediate port. In this paper, we show that the randomized load-balancing of application flows can be enhanced to provably guarantee both stability and packet ordering by extending the approach with safety mechanisms that can uniformly diffuse packets across the switch whenever there is a build-up of packets waiting to route through the some intermediate port. Although simple and intuitive, our experimental results show that our extended randomized load-balancing approach significantly outperforms existing load-balanced switch architectures.},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
pages = {397–398},
numpages = {2},
keywords = {algorithm, design, load-balanced switches, performance, throughput guarantees},
location = {Antibes Juan-les-Pins, France},
series = {SIGMETRICS '16}
}

@inproceedings{10.1145/2964284.2967262,
author = {Cao, Jiewei and Huang, Zi and Wang, Peng and Li, Chao and Sun, Xiaoshuai and Shen, Heng Tao},
title = {Quartet-net Learning for Visual Instance Retrieval},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2967262},
doi = {10.1145/2964284.2967262},
abstract = {Recently, neuron activations extracted from a pre-trained convolutional neural network (CNN) show promising performance in various visual tasks. However, due to the domain and task bias, using the features generated from the model pre-trained for image classification as image representations for instance retrieval is problematic. In this paper, we propose quartet-net learning to improve the discriminative power of CNN features for instance retrieval. The general idea is to map the features into a space where the image similarity can be better evaluated. Our network differs from the traditional Siamese-net in two ways. First, we adopt a double-margin contrastive loss with a dynamic margin tuning strategy to train the network which leads to more robust performance. Second, we introduce in the mimic learning regularization to improve the generalization ability of the network by preventing it from overfitting to the training data. Catering for the network learning, we collect a large-scale dataset, namely GeoPair, which consists of 68k matching image pairs and 63k non-matching pairs. Experiments on several standard instance retrieval datasets demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {456–460},
numpages = {5},
keywords = {convolutional neural networks, feature learning},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.1145/3154487,
author = {Yang, Sen and Lin, Bill and Xu, Jun},
title = {Safe Randomized Load-Balanced Switching By Diffusing Extra Loads},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3154487},
doi = {10.1145/3154487},
abstract = {Load-balanced switch architectures are known to be scalable in both size and speed, which is of interest due to the continued exponential growth in Internet traffic. However, the main drawback of load-balanced switches is that packets can depart out of order from the switch. Randomized load-balancing of application flows by means of hashing on the packet header is a well-known simple solution to this packet reordering problem in which all packets belonging to the same application flow are routed through the same intermediate port and hence the same path through the switch. Unfortunately, this method of load-balancing can lead to instability, depending on the mix of flow sizes and durations in the group of flows that gets randomly assigned to route through the same intermediate port. In this paper, we show that the randomized load-balancing of application flows can be enhanced to provably guarantee both stability and packet ordering by extending the approach with safety mechanisms that can uniformly diffuse packets across the switch whenever there is a build-up of packets waiting to route through some intermediate port. Although simple and intuitive, our experimental results show that our extended randomized load-balancing approach outperforms existing load-balanced switch architectures.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {dec},
articleno = {29},
numpages = {37},
keywords = {load-balanced switches, low latency, packet reordering, throughput guarantees}
}

@inproceedings{10.1145/3219617.3219673,
author = {Yang, Sen and Lin, Bill and Xu, Jun},
title = {Safe Randomized Load-Balanced Switching by Diffusing Extra Loads},
year = {2018},
isbn = {9781450358460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219617.3219673},
doi = {10.1145/3219617.3219673},
abstract = {Load-balanced switch architectures are known to be scalable in both size and speed, which is of interest due to the continued exponential growth in Internet traffic. However, the main drawback of load-balanced switches is that packets can depart out of order from the switch. Randomized load-balancing of application flows by means of hashing on the packet header is a well-known simple solution to this packet reordering problem in which all packets belonging to the same application flow are routed through the same intermediate port and hence the same path through the switch. Unfortunately, this method of load-balancing can lead to instability, depending on the mix of flow sizes and durations in the group of flows that gets randomly assigned to route through the same intermediate port. In this paper, we show that the randomized load-balancing of application flows can be enhanced to provably guarantee both stability and packet ordering by extending the approach with safety mechanisms that can uniformly diffuse packets across the switch whenever there is a build-up of packets waiting to route through some intermediate port. Although simple and intuitive, our experimental results show that our extended randomized load-balancing approach outperforms existing load-balanced switch architectures.},
booktitle = {Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems},
pages = {135–137},
numpages = {3},
keywords = {load-balanced switches, low latency, packet reordering, throughput guarantees},
location = {Irvine, CA, USA},
series = {SIGMETRICS '18}
}

@inproceedings{10.1145/2783258.2788613,
author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
title = {Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2788613},
doi = {10.1145/2783258.2788613},
abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1721–1730},
numpages = {10},
keywords = {additive models, classification, healthcare, intelligibility, interaction detection, logistic regression, risk prediction},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/2964284.2971473,
author = {Zhao, Sicheng},
title = {Image Emotion Computing},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2971473},
doi = {10.1145/2964284.2971473},
abstract = {Images can convey rich semantics and induce strong emotions in viewers. My research aims to predict image emotions from different aspects with respect to two main challenges: affective gap and subjective evaluation. To bridge the affective gap, we extract emotion features based on principles-of-art to recognize image-centric dominant emotions. As the emotions that are induced in viewers by an image are highly subjective and different, we propose to predict user-centric personalized emotion perceptions for each viewer and image-centric emotion probability distribution for each image. To tackle the subjective evaluation issue, we set up a large scale image emotion dataset from Flickr, named Image-Emotion-Social-Net, on both dimensional and categorical emotion representations with over 1 million images and about 8,000 users. Different types of factors may influence personalized image emotion perceptions, including visual content, social context, temporal evolution and location influence. We make an initial attempt to jointly combine them by the proposed rolling multi-task hypergraph learning. Both discrete and continuous emotion distributions are modelled via shared sparse learning. Further, several potential applications based on image emotions are designed and implemented.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1435–1439},
numpages = {5},
keywords = {emotion distribution, hypergraph learning, image emotion, personalized perceptions, principles-of-art, sparse learning},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.1109/TCBB.2019.2961667,
author = {Huang, Hai-Hui and Liang, Yong},
title = {A Novel Cox Proportional Hazards Model for High-Dimensional Genomic Data in Cancer Prognosis},
year = {2019},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2961667},
doi = {10.1109/TCBB.2019.2961667},
abstract = {The Cox proportional hazards model is a popular method to study the connection between feature and survival time. Because of the high-dimensionality of genomic data, existing Cox models trained on any specific dataset often generalize poorly to other independent datasets. In this paper, we suggest a novel strategy for the Cox model. This strategy is included a new learning technique, self-paced learning (SPL), and a new gene selection method, SCAD-Net penalty. The SPL method is adopted to aid to build a more accurate prediction with its built-in mechanism of learning from easy samples first and adaptively learning from hard samples. The SCAD-Net penalty has fixed the problem of the SCAD method without an inherent mechanism to fuse the prior graphical information. We combined the SPL with the SCAD-Net penalty to the Cox model (SSNC). The simulation shows that the SSNC outperforms the benchmark in terms of prediction and gene selection. The analysis of a large-scale experiment across several cancer datasets shows that the SSNC method not only results in higher prediction accuracies but also identifies markers that satisfactory stability across another validation dataset. The demo code for the proposed method is provided in supplemental file.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {dec},
pages = {1821–1830},
numpages = {10}
}

@article{10.1145/3355089.3356574,
author = {Gao, Yue and Guo, Yuan and Lian, Zhouhui and Tang, Yingmin and Xiao, Jianguo},
title = {Artistic glyph image synthesis via one-stage few-shot learning},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3355089.3356574},
doi = {10.1145/3355089.3356574},
abstract = {Automatic generation of artistic glyph images is a challenging task that attracts many research interests. Previous methods either are specifically designed for shape synthesis or focus on texture transfer. In this paper, we propose a novel model, AGIS-Net, to transfer both shape and texture styles in one-stage with only a few stylized samples. To achieve this goal, we first disentangle the representations for content and style by using two encoders, ensuring the multi-content and multi-style generation. Then we utilize two collaboratively working decoders to generate the glyph shape image and its texture image simultaneously. In addition, we introduce a local texture refinement loss to further improve the quality of the synthesized textures. In this manner, our one-stage model is much more efficient and effective than other multi-stage stacked methods. We also propose a large-scale dataset with Chinese glyph images in various shape and texture styles, rendered from 35 professional-designed artistic fonts with 7,326 characters and 2,460 synthetic artistic fonts with 639 characters, to validate the effectiveness and extendability of our method. Extensive experiments on both English and Chinese artistic glyph image datasets demonstrate the superiority of our model in generating high-quality stylized glyph images against other state-of-the-art methods.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {185},
numpages = {12},
keywords = {deep learning, font genration, generative adversarial networks, image-to-image translation, style transfer}
}

@inproceedings{10.1145/3240508.3240509,
author = {Zhao, Jian and Li, Jianshu and Cheng, Yu and Sim, Terence and Yan, Shuicheng and Feng, Jiashi},
title = {Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240509},
doi = {10.1145/3240508.3240509},
abstract = {Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database "Multi-Human Parsing (MHP)" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {792–800},
numpages = {9},
keywords = {generative adversarial networks, human parsing, multi-human parsing, nested adversarial learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1145/3065949,
author = {Shen, Jiaxing and Cao, Jiannong and Liu, Xuefeng and Zhang, Chisheng},
title = {DMAD: Data-Driven Measuring of Wi-Fi Access Point Deployment in Urban Spaces},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3065949},
doi = {10.1145/3065949},
abstract = {Wireless networks offer many advantages over wired local area networks such as scalability and mobility. Strategically deployed wireless networks can achieve multiple objectives like traffic offloading, network coverage, and indoor localization. To this end, various mathematical models and optimization algorithms have been proposed to find optimal deployments of access points (APs).However, wireless signals can be blocked by the human body, especially in crowded urban spaces. As a result, the real coverage of an on-site AP deployment may shrink to some degree and lead to unexpected dead spots (areas without wireless coverage). Dead spots are undesirable, since they degrade the user experience in network service continuity, on one hand, and, on the other hand paralyze some applications and services like tracking and monitoring when users are in these areas. Nevertheless, it is nontrivial for existing methods to analyze the impact of human beings on wireless coverage. Site surveys are too time consuming and labor intensive to conduct. It is also infeasible for simulation methods to predict the number of on-site people.In this article, we propose DMAD, a Data-driven Measuring of Wi-Fi Access point Deployment, which not only estimates potential dead spots of an on-site AP deployment but also quantifies their severity, using simple Wi-Fi data collected from the on-site deployment and shop profiles from the Internet. DMAD first classifies static devices and mobile devices with a decision-tree classifier. Then it locates mobile devices to grid-level locations based on shop popularities, wireless signal, and visit duration. Last, DMAD estimates the probability of dead spots for each grid during different time slots and derives their severity considering the probability and the number of potential users.The analysis of Wi-Fi data from static devices indicates that the Pearson Correlation Coefficient of wireless coverage status and the number of on-site people is over 0.7, which confirms that human beings may have a significant impact on wireless coverage. We also conduct extensive experiments in a large shopping mall in Shenzhen. The evaluation results demonstrate that DMAD can find around 70% of dead spots with a precision of over 70%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {11},
numpages = {29},
keywords = {AP deployment measuring, Wi-Fi AP, data-driven approach, room-level localization}
}

@inproceedings{10.5555/3242181.3242420,
author = {Celik, Bilge and Van Gorp, Pieter M. E. and Snoeck, Andre C. J. and van Riet, Remi C. and de Winter, Peter J. and Wilbik, Anna},
title = {A model based simulation toolkit for evaluating renal replacement policies},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Renal failure concerns progressive loss of kidney function. Renal Replacement Therapy (RRT) is a costly, long-running process that includes several decision points in different stages. Small changes in the protocol can impact significantly the expenditures and healthcare outcomes. Unfortunately, policy makers have very little support for benchmarking improvement alternatives. The existing models are designed to fit certain applications with preset parameters and design choices which do not match with the requirements of a policy analysis. A generic approach is required to analyze the effects of different design options adjustable to finer scales. To remedy this, this paper describes a novel toolkit for evaluating renal replacement policies, containing a parametrized colored Petri-Net which can be configured for the specifics of local settings. The model is made available for open access to overcome the non-replicability issue of existing models.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {225},
numpages = {12},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3341302.3342070,
author = {Dahlberg, Axel and Skrzypczyk, Matthew and Coopmans, Tim and Wubben, Leon and Rozpundefineddek, Filip and Pompili, Matteo and Stolk, Arian and Pawe\l{}czak, Przemys\l{}aw and Knegjens, Robert and de Oliveira Filho, Julio and Hanson, Ronald and Wehner, Stephanie},
title = {A link layer protocol for quantum networks},
year = {2019},
isbn = {9781450359566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341302.3342070},
doi = {10.1145/3341302.3342070},
abstract = {Quantum communication brings radically new capabilities that are provably impossible to attain in any classical network. Here, we take the first step from a physics experiment to a quantum internet system. We propose a functional allocation of a quantum network stack, and construct the first physical and link layer protocols that turn ad-hoc physics experiments producing heralded entanglement between quantum processors into a well-defined and robust service. This lays the groundwork for designing and implementing scalable control and application protocols in platform-independent software. To design our protocol, we identify use cases, as well as fundamental and technological design considerations of quantum network hardware, illustrated by considering the state-of-the-art quantum processor platform available to us (Nitrogen-Vacancy (NV) centers in diamond). Using a purpose built discrete-event simulator for quantum networks, we examine the robustness and performance of our protocol using extensive simulations on a supercomputing cluster. We perform a full implementation of our protocol in our simulator, where we successfully validate the physical simulation model against data gathered from the NV hardware. We first observe that our protocol is robust even in a regime of exaggerated losses of classical control messages with only little impact on the performance of the system. We proceed to study the performance of our protocols for 169 distinct simulation scenarios, including trade-offs between traditional performance metrics such as throughput, and the quality of entanglement. Finally, we initiate the study of quantum network scheduling strategies to optimize protocol performance for different use cases.},
booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
pages = {159–173},
numpages = {15},
keywords = {link layer, quantum internet, quantum networks},
location = {Beijing, China},
series = {SIGCOMM '19}
}

@inproceedings{10.1145/3277570.3277574,
author = {Krmelj, Gregor R. and Pan\v{c}ur, Matja\v{z} and Grohar, Miha and Ciglari\v{c}, Mojca},
title = {OpenSPA - An Open and Extensible Protocol for Single Packet Authorization},
year = {2018},
isbn = {9781450365154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277570.3277574},
doi = {10.1145/3277570.3277574},
abstract = {Applications are vulnerable. Opening such applications to the Internet creates a big attack surface for potential exploit. The use of common network defenses such as firewalls helps mitigate the risks, however possibility of a secure scalable system that assigns network access to a service purely by identifying a device by a static IP address is a delusion. Firewalls need to improve to support dynamic allocation of device access.Such a technique would allow services to be hidden to the general public, unauthorized to access them, but would at the same time allow authorized users global connectivity.Single Packet Authorization (SPA) is an approach, building on firewall functionality which hides services from unauthorized users and helps mitigate common network attacks such as Distributed Denial of Service (DDoS) attacks by stopping them earlier in the network stack.In this paper we introduce OpenSPA, a SPA protocol suitable for deployment in various complex networking environments and enabling flexibility to support different network policies. With support for IPv6 as well as extensible support for custom user programmable authentication, authorization and firewall logic.},
booktitle = {Proceedings of the Central European Cybersecurity Conference 2018},
articleno = {4},
numpages = {6},
keywords = {Firewall, Hidden Services, Network Protocol Design, Network Security, SPA},
location = {Ljubljana, Slovenia},
series = {CECC 2018}
}

@inproceedings{10.5555/2820158.2820164,
author = {Niemann, Raik and Ivanov, Todor},
title = {Evaluating the energy efficiency of data management systems},
year = {2015},
publisher = {IEEE Press},
abstract = {Nowadays developers and end users of data management systems are challenged with the reduction of the "energy consumption footprint" of existing implementations and configurations. In other words, the energy efficiency has to be optimized, either by increasing the performance or by consuming less resources.In fact, there is a big number of factors that influence the performance and energy efficiency of a particular data management system. For example, the replacement of hardware components or the surrounding operating system can have a significant impact. Both developers and end users put much effort into finding performance "bottlenecks", better hardware resource utilization and configurations. Besides, when it comes to a scale-out scenario, end users often face the situation to find a hardware configuration that offers both a reasonable performance and energy consumption, i.e. a resource planning.This paper proposes a new approach to evaluate the performance of a data management system and the impact on the energy efficiency with the goal to optimize it. The approach introduces a Queued Petri Nets model, whose simulation runs are intended to drastically reduce the investments, both in time and hardware, compared to traditional ways, for example regression and compatibility tests. The model's prediction in terms of performance and energy efficiency were evaluated and compared to the actual experimental results. On average the predicted and experimental results (response time and energy efficiency) differ by 24 percent.},
booktitle = {Proceedings of the Fourth International Workshop on Green and Sustainable Software},
pages = {22–28},
numpages = {7},
keywords = {data management system, energy efficiency, performance, queued petri nets},
location = {Florence, Italy},
series = {GREENS '15}
}

@inproceedings{10.1145/3308558.3313418,
author = {Li, Xiucheng and Cong, Gao and Sun, Aixin and Cheng, Yun},
title = {Learning Travel Time Distributions with Deep Generative Model},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313418},
doi = {10.1145/3308558.3313418},
abstract = {Travel time estimation of a given route with respect to real-time traffic condition is extremely useful for many applications like route planning. We argue that it is even more useful to estimate the travel time distribution, from which we can derive the expected travel time as well as the uncertainty. In this paper, we develop a deep generative model - DeepGTT - to learn the travel time distribution for any route by conditioning on the real-time traffic. DeepGTT interprets the generation of travel time using a three-layer hierarchical probabilistic model. In the first layer, we present two techniques, amortization and spatial smoothness embeddings, to share statistical strength among different road segments; a convolutional neural net based representation learning component is also proposed to capture the dynamically changing real-time traffic condition. In the middle layer, a nonlinear factorization model is developed to generate auxiliary random variable i.e., speed. The introduction of this middle layer separates the statical spatial features from the dynamically changing real-time traffic conditions, allowing us to incorporate the heterogeneous influencing factors into a single model. In the last layer, an attention mechanism based function is proposed to collectively generate the observed travel time. DeepGTT describes the generation process in a reasonable manner, and thus it not only produces more accurate results but also is more efficient. On a real-world large-scale data set, we show that DeepGTT produces substantially better results than state-of-the-art alternatives in two tasks: travel time estimation and route recovery from sparse trajectory data.},
booktitle = {The World Wide Web Conference},
pages = {1017–1027},
numpages = {11},
keywords = {Deep generative models, Travel time distribution learning, VAEs},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3240508.3240611,
author = {Han, Chaojun and Shen, Fumin and Liu, Li and Yang, Yang and Shen, Heng Tao},
title = {Visual Spatial Attention Network for Relationship Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240611},
doi = {10.1145/3240508.3240611},
abstract = {Visual relationship detection, which aims to predict a &lt;subject, predicate, object&gt; triplet with the detected objects, has attracted increasing attention in the scene understanding study. During tackling this problem, dealing with varying scales of the subjects and objects is of great importance, which has been less studied. To overcome this challenge, we propose a novel Vision Spatial Attention Network (VSA-Net), which employs a two-dimensional normal distribution attention scheme to effectively model small objects. In addition, we design a Subject-Object-layer (SO-layer) to distinguish between the subject and object to attain more precise results. To the best of our knowledge, VSA-Net is the first end-to-end attention mechanism based visual relationship detection model. Extensive experiments on the benchmark datasets (VRD and VG) show that, by using pure vision information, our VSA-Net achieves state-of-the-art performance for predicate detection, phrase detection, and relationship detection.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {510–518},
numpages = {9},
keywords = {attention, object detection, relationship detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/2783258.2783269,
author = {Zhang, Weinan and Wang, Jun},
title = {Statistical Arbitrage Mining for Display Advertising},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783269},
doi = {10.1145/2783258.2783269},
abstract = {We study and formulate arbitrage in display advertising. Real-Time Bidding (RTB) mimics stock spot exchanges and utilises computers to algorithmically buy display ads per impression via a real-time auction. Despite the new automation, the ad markets are still informationally inefficient due to the heavily fragmented marketplaces. Two display impressions with similar or identical effectiveness (e.g., measured by conversion or click-through rates for a targeted audience) may sell for quite different prices at different market segments or pricing schemes. In this paper, we propose a novel data mining paradigm called Statistical Arbitrage Mining (SAM) focusing on mining and exploiting price discrepancies between two pricing schemes. In essence, our SAMer is a meta-bidder that hedges advertisers' risk between CPA (cost per action)-based campaigns and CPM (cost per mille impressions)-based ad inventories; it statistically assesses the potential profit and cost for an incoming CPM bid request against a portfolio of CPA campaigns based on the estimated conversion rate, bid landscape and other statistics learned from historical data. In SAM, (i) functional optimisation is utilised to seek for optimal bidding to maximise the expected arbitrage net profit, and (ii) a portfolio-based risk management solution is leveraged to reallocate bid volume and budget across the set of campaigns to make a risk and return trade-off. We propose to jointly optimise both components in an EM fashion with high efficiency to help the meta-bidder successfully catch the transient statistical arbitrage opportunities in RTB. Both the offline experiments on a real-world large-scale dataset and online A/B tests on a commercial platform demonstrate the effectiveness of our proposed solution in exploiting arbitrage in various model settings and market environments.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1465–1474},
numpages = {10},
keywords = {display ads, real-time bidding, statistical arbitrage},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@inproceedings{10.1145/2940157.2940163,
author = {Baig, Roger and Dalmau, Llu\'{\i}s and Roca, Ramon and Navarro, Leandro and Freitag, Felix and Sathiaseelan, Arjuna},
title = {Making Community Networks economically sustainable, the guifi.net experience},
year = {2016},
isbn = {9781450344234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940157.2940163},
doi = {10.1145/2940157.2940163},
abstract = {Community networks have flourished around the world as a complementary model for enabling access to the Internet and its services. Nevertheless there is still an on-going debate on how to make them sustainable and scaleable beyond the voluntary effort and non-refundable contributions. The approach taken by Guifi.net has been to enable professional activity and to develop a set of tools to ensure the reinvestment of a fraction of the benefits of this professional activity. This has contributed to build the largest community network, with an annual turnover of millions of Euros and creation of dozens of direct jobs. The implementation of these tools is producing extensive data sets that allow to characterize key parameters in the deployment and the operation of these infrastructures, to examine behaviours and trends and to identify good and bad practices, fraud, etc. A more detailed knowledge of the economic aspects has a positive impact on reducing the uncertainty of investments, expansion plans and operations.},
booktitle = {Proceedings of the 2016 Workshop on Global Access to the Internet for All},
pages = {31–36},
numpages = {6},
keywords = {Capital expenditure, Community networks, Cost-sharing, Economic sustainability, Operational expenditure},
location = {Florianopolis, Brazil},
series = {GAIA '16}
}

@inproceedings{10.1145/3231644.3231654,
author = {Wang, Zichao and Lan, Andrew S. and Nie, Weili and Waters, Andrew E. and Grimaldi, Phillip J. and Baraniuk, Richard G.},
title = {QG-net: a data-driven question generation model for educational content},
year = {2018},
isbn = {9781450358866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231644.3231654},
doi = {10.1145/3231644.3231654},
abstract = {The ever growing amount of educational content renders it increasingly difficult to manually generate sufficient practice or quiz questions to accompany it. This paper introduces QG-Net, a recurrent neural network-based model specifically designed for automatically generating quiz questions from educational content such as textbooks. QG-Net, when trained on a publicly available, general-purpose question/answer dataset and without further fine-tuning, is capable of generating high quality questions from textbooks, where the content is significantly different from the training data. Indeed, QG-Net outperforms state-of-the-art neural network-based and rules-based systems for question generation, both when evaluated using standard benchmark datasets and when using human evaluators. QG-Net also scales favorably to applications with large amounts of educational content, since its performance improves with the amount of training data.},
booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
articleno = {7},
numpages = {10},
location = {London, United Kingdom},
series = {L@S '18}
}

@inproceedings{10.5555/3329995.3330003,
author = {Blondin, Michael and Haase, Christoph},
title = {Logics for continuous reachability in petri nets and vector addition systems with states},
year = {2017},
isbn = {9781509030187},
publisher = {IEEE Press},
abstract = {This paper studies sets of rational numbers definable by continuous Petri nets and extensions thereof. First, we identify a polynomial-time decidable fragment of existential FO(Q, +,&lt;) and show that the sets of rationals definable in this fragment coincide with reachability sets of continuous Petri nets. Next, we introduce and study continuous vector addition systems with states (CVASS), which are vector addition systems with states in which counters may hold non-negative rational values, and in which the effect of a transition can be scaled by a positive rational number smaller or equal to one. This class strictly generalizes continuous Petri nets by additionally allowing for discrete control-state information. We prove that reachability sets of CVASS are equivalent to the sets of rational numbers definable in existential FO(Q, +, &lt;) from which we can conclude that reachability in CVASS is NP-complete. Finally, our results explain and yield as corollaries a number of polynomial-time algorithms for decision problems that have recently been studied in the literature.},
booktitle = {Proceedings of the 32nd Annual ACM/IEEE Symposium on Logic in Computer Science},
articleno = {8},
numpages = {12},
location = {Reykjav\'{\i}k, Iceland},
series = {LICS '17}
}

@inproceedings{10.1145/3195528.3195533,
author = {Masabo, Emmanuel and Kaawaase, Kyanda Swaib and Sansa-Otim, Julianne},
title = {Big data: deep learning for detecting malware},
year = {2018},
isbn = {9781450357197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195528.3195533},
doi = {10.1145/3195528.3195533},
abstract = {Malicious software, commonly known as malware are constantly getting smarter with the capabilities of undergoing self-modifications. They are produced in big numbers and widely deployed very fast through the Internet-capable devices. This is therefore a big data problem and remains challenging in the research community. Existing detection methods should be enhanced in order to effectively deal with today's malware. In this paper, we propose a novel real-time monitoring, analysis and detection approach that is achieved by applying big data analytics and machine learning in the development of a general detection model. The learnings achieved through big data render machine learning more efficient. Using the deep learning approach, we designed and developed a scalable detection model that brings improvement to the existing solutions. Our experiments achieved an accuracy of 97% and ROC of 0.99.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering in Africa},
pages = {20–26},
numpages = {7},
keywords = {big data analytics, deep learning, machine learning, malware detection},
location = {Gothenburg, Sweden},
series = {SEiA '18}
}

@inproceedings{10.1145/3152824.3152826,
author = {Engelmann, Felix and Kopp, Henning and Kargl, Frank and Glaser, Florian and Weinhardt, Christof},
title = {Towards an economic analysis of routing in payment channel networks},
year = {2017},
isbn = {9781450351737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152824.3152826},
doi = {10.1145/3152824.3152826},
abstract = {Payment channel networks are supposed to overcome technical scalability limitations of blockchain infrastructure by employing a special overlay network with fast payment confirmation and only sporadic settlement of netted transactions on the blockchain. However, they introduce economic routing constraints that limit decentralized scalability and are currently not well understood. In this paper, we model the economic incentives for participants in payment channel networks. We provide the first formal model of payment channel economics and analyze how the cheapest path can be found. Additionally, our simulation assesses the long-term evolution of a payment channel network. We find that even for small routing fees, sometimes it is cheaper to settle the transaction directly on the blockchain.},
booktitle = {Proceedings of the 1st Workshop on Scalable and Resilient Infrastructures for Distributed Ledgers},
articleno = {2},
numpages = {6},
keywords = {bitcoin, blockchain, ethereum, payment channel, simulation},
location = {Las Vegas, Nevada},
series = {SERIAL '17}
}

@inproceedings{10.1145/3177540.3177562,
author = {Mantik, Stefanus and Posser, Gracieli and Chow, Wing-Kai and Ding, Yixiao and Liu, Wen-Hao},
title = {ISPD 2018 Initial Detailed Routing Contest and Benchmarks},
year = {2018},
isbn = {9781450356268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177540.3177562},
doi = {10.1145/3177540.3177562},
abstract = {In advanced technology nodes, detailed routing becomes the most complicated and runtime consuming stage. To spur detailed routing research, ISPD 2018 initial detailed routing contest is hosted and it is the first ISPD contest on detailed routing problem. In this contest, the benchmarks synthesized by industrial tool and library are released, which consider the design rules like spacing table, cut spacing, end-of-line spacing, and min-area rules. In addition, the global routing guide is provided associated to each benchmark, and detailed routers are required to honor the routing guides as much as possible meanwhile minimize design-rule-checking (DRC) violations. The biggest benchmark released in this contest has near-millions of nets, so the runtime and memory scalability for detailed routers need to be well addressed. To reduce routers' runtime, the deterministic multithreading framework is encouraged but optional in this contest.},
booktitle = {Proceedings of the 2018 International Symposium on Physical Design},
pages = {140–143},
numpages = {4},
keywords = {contest, design rules, detailed routing, routability, routing},
location = {Monterey, California, USA},
series = {ISPD '18}
}

@article{10.1145/3197517.3201340,
author = {V\'{e}voda, Petr and Kondapaneni, Ivo and K\v{r}iv\'{a}nek, Jaroslav},
title = {Bayesian online regression for adaptive direct illumination sampling},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201340},
doi = {10.1145/3197517.3201340},
abstract = {Direct illumination calculation is an important component of any physically-based Tenderer with a substantial impact on the overall performance. We present a novel adaptive solution for unbiased Monte Carlo direct illumination sampling, based on online learning of the light selection probability distributions. Our main contribution is a formulation of the learning process as Bayesian regression, based on a new, specifically designed statistical model of direct illumination. The net result is a set of regularization strategies to prevent over-fitting and ensure robustness even in early stages of calculation, when the observed information is sparse. The regression model captures spatial variation of illumination, which enables aggregating statistics over relatively large scene regions and, in turn, ensures a fast learning rate. We make the method scalable by adopting a light clustering strategy from the Lightcuts method, and further reduce variance through the use of control variates. As a main design feature, the resulting algorithm is virtually free of any preprocessing, which enables its use for interactive progressive rendering, while the online learning still enables super-linear convergence.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {125},
numpages = {12},
keywords = {adaptive sampling, direct illumination, learning, visibility}
}

@inproceedings{10.1145/3243176.3243179,
author = {LeBeane, Michael and Hamidouche, Khaled and Benton, Brad and Breternitz, Mauricio and Reinhardt, Steven K. and John, Lizy K.},
title = {ComP-net: command processor networking for efficient intra-kernel communications on GPUs},
year = {2018},
isbn = {9781450359863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243176.3243179},
doi = {10.1145/3243176.3243179},
abstract = {Current state-of-the-art in GPU networking advocates a host-centric model that reduces performance and increases code complexity. Recently, researchers have explored several techniques for networking within a GPU kernel itself. These approaches, however, suffer from high latency, waste energy on the host, and are not scalable with larger/more GPUs on a node. In this work, we introduce Command Processor Networking (ComP-Net), which leverages the availability of scalar cores integrated on the GPU itself to provide high-performance intra-kernel networking. ComP-Net enables efficient synchronization between the Command Processors and Compute Units on the GPU through a line locking scheme implemented in the GPU's shared last-level cache. We illustrate that ComP-Net can improve application performance by up to 20% and provide up to 50% reduction in energy consumption vs. competing networking techniques across a Jacobi stencil, allreduce collective, and machine learning applications.},
booktitle = {Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques},
articleno = {29},
numpages = {13},
keywords = {GPUs, RDMA networks, programming models},
location = {Limassol, Cyprus},
series = {PACT '18}
}

