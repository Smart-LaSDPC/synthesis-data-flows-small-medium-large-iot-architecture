@inproceedings{10.1145/3434581.3434715,
author = {Li, Geng},
title = {Security Architecture of Computer Communication System Based on Internet of Things},
year = {2020},
isbn = {9781450375764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434581.3434715},
doi = {10.1145/3434581.3434715},
abstract = {In large-scale service collaboration environment, security and privacy protection are the main factors affecting the development of IoT service applications. The security and privacy requirements of IoT services mainly focus on three aspects: secure data sharing, secure service collaboration, and minimum privacy leakage of service users. Based on the characteristics of the Internet of Things service and its security and privacy requirements, this paper proposes an access control policy management mechanism for secure data sharing, an access control architecture that protects data confidentiality and service policy privacy, and ensures a combination of security services and minimum privacy leakage of users. The method has practical value and theoretical guiding significance in constructing secure IoT services. Designed the overall communication architecture of a computer communication system suitable for the Internet of Things, emphasized the importance of the security management of the computer communication system of the Internet of Things, and explained the Internet of Things from three different logical levels: perception layer, network transmission layer and application layer The overall security architecture of the computer communication system of the Internet of Things is designed.},
booktitle = {Proceedings of the 2020 International Conference on Aviation Safety and Information Technology},
pages = {693–697},
numpages = {5},
keywords = {Internet of Things services, Security protocol, security architecture, security data},
location = {Weihai City, China},
series = {ICASIT 2020}
}

@inproceedings{10.1145/3582197.3582229,
author = {Butt, Talal Ashraf},
title = {Edge Intelligence based Social Internet of Things for Smart Cities},
year = {2023},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582197.3582229},
doi = {10.1145/3582197.3582229},
abstract = {Social Internet of Things (SIoT), inspired by human social networks, is the next phase in the evolution of the Internet of Things (IoT) that enables a plethora of devices with diverse sensors to build and maintain social relationships among them. These social relationships can be created autonomously by devices based on their mutual interests, context and the requirements of different applications. The smart city paradigm is built on large-scale IoT. It can benefit from SIoT to improve the provision of value-added services for citizens by exploiting the social relationships of devices. This paper reviews the SIoT concepts and challenges and proposes an architecture to reap its potential for smart city applications. Furthermore, it emphasizes the societal implication of SIoT in a smart city scenario.},
booktitle = {Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
pages = {184–189},
numpages = {6},
keywords = {Cognitive Systems, Internet of Things, Smart cities, Social Internet of Things},
location = {<conf-loc>, <city>Shanghai</city>, <country>China</country>, </conf-loc>},
series = {ICIT '22}
}

@inproceedings{10.1145/3517077.3517111,
author = {Tang, Wanpeng and Cui, Shigang},
title = {Research on the design of custom alarm system for breeding industry},
year = {2022},
isbn = {9781450387408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517077.3517111},
doi = {10.1145/3517077.3517111},
abstract = {This paper is about the research of signal acquisition and image processing. It is about an alarm system and its alarm method customized for the breeding industry. The device includes: sensor group, cloud platform of the internet of things for breeding industry and mobile alarm terminal; The sensor group is used to collect the signal group and send the collected signal group to the IOT cloud platform of the farm; The breeding industry internet of things cloud platform is used to receive the signal group sent by the sensor group, and according to the preset rules, when the signal group does not meet the threshold, the alarm command is sent to the mobile alarm terminal; The mobile alarm terminal is used to receive the alarm commands sent by the Internet of things cloud platform of the breeding industry farm and give voice alarm according to the text-to-language conversion technology. This research system can provide the alarm device customized for the breeding industry with convenient use, simple operation and clear function, which can meet the requirements of front-line staff in the breeding industry to receive alarm notification conveniently, timely and accurately, and is suitable for large-scale promotion in the breeding industry.},
booktitle = {Proceedings of the 2022 7th International Conference on Multimedia and Image Processing},
pages = {205–210},
numpages = {6},
location = {Tianjin, China},
series = {ICMIP '22}
}

@inproceedings{10.1145/3465481.3470063,
author = {Schermann, Raphael and Toegl, Ronald and Steger, Christian},
title = {Managing Anonymous Keys in a Fog-Computing Platform},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470063},
doi = {10.1145/3465481.3470063},
abstract = {Fog Computing is a decentralized infrastructure layer between Cloud and Edge Devices moving the computation closer to the edge, allowing good latency and bandwidth even for large-scale Internet of Things deployments. Still, devices using fog services are exposed to the immediate application environment and potentially malicious users, thus security, privacy, and trust are critical issues. To provide trust and privacy within fog infrastructures, enabling the secured execution of future Internet of Things services, lightweight collective and distributed attestation mechanism for the bulk attestation of the edge devices and the fog infrastructure can be used, especially leveraging Direct Anonymous Attestation, an anonymous attestation signature that allows attesting to the state of the host system, without violating the specified privacy of the host. As in all cryptographic schemes the management and protection of keys is of the highest significance. We present key management for a fog architecture in the context of the RAINBOW fog platform and show how the computations of a recently published proof-of-concept implementation of Direct Anonymous Attestation can be distributed in our specific fog environment. We provide details on an embedded system-level implementation and performance benchmarks for Internet of Things applications keys stored with proper hardware-based protection within a Trusted Platform Module.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {110},
numpages = {5},
keywords = {Fog Computing, Trusted Computing},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/3578339.3578347,
author = {Sun, Xinjie and Lyu, Zhan and Shen, Zhangliang and Hu, Can and An, Xianyi},
title = {Research on the Application of Energy Wisdom Based on Power Iot and Big Data},
year = {2023},
isbn = {9781450397070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578339.3578347},
doi = {10.1145/3578339.3578347},
abstract = {Energy is an important symbol of the progress of human civilization, social development and economic level of the improvement of power resources and environmental requirements are increasingly high, currently China is weak in this area. Therefore, big data technology and Internet of things technology is needed to support the construction of energy wisdom, application and security protection measures. Secondly, the power industry has some limitations due to its own characteristics, such as small scale and slow operation, which leads to large-scale distributed power supply access to meet the needs of users, while the traditional distribution network can not meet the requirements of scheduling and controlling load changes. This paper elaborates the concept of Internet of Things and big data, introduces the security architecture of power IoT based on energy interconnection, analyses the power IoT technology based on energy interconnection, and discusses the application of energy wisdom based on IoT and big data, with a view to solving the problems of data, application and key security of power information system through the wide application of power IoT technology.},
booktitle = {Proceedings of the 2022 4th International Conference on Big-Data Service and Intelligent Computation},
pages = {51–55},
numpages = {5},
keywords = {Application, Big Data, Energy Interconnection, Energy Wisdom, Power Iot},
location = {Xiamen, China},
series = {BDSIC '22}
}

@inproceedings{10.1145/3590837.3590939,
author = {Salunke, Mahendra Balkrishna and Mahalle, Parikshit N. and Shinde, Gitanjali Rahul},
title = {Importance of Lightweight Algorithm for Embedded Security in Machine-to-Machine Communication towards Internet of Things},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590939},
doi = {10.1145/3590837.3590939},
abstract = {The Internet of Things (IoT) has been regarded as an important technological advancement that has changed how people think about and performs their activities. It is a framework that aims to create an efficient and effective method of managing various assets. The IoT also offers a paradigm shift that will enable the treatment of both physical and virtual assets uniformly. The increasing number of devices connected to the Internet will allow users to collect and exchange large amounts of data, making better decisions and enhancing their lives. One of the most important factors that the Internet is constantly working on is the development of new connectivity methods. One of these involves the Internet of Things, a framework for exchanging information among various sensors and devices. The rapid emergence and evolution of machine-to-machine communications will bring new opportunities for various applications. Unfortunately, many challenges still need to be resolved to make this technology work properly. One of the most important factors that need to be considered is the availability of secure infrastructure. One of the most important factors that need to be considered when implementing the security features of the Internet of Things is the availability of lightweight security algorithms. These are designed to help prevent unauthorized access and manipulation of the data sent and received by various devices. To function properly, the algorithms should not require a lot of resources, such as processing power. One of the advantages of implementing lightweight security algorithms is that they can help reduce the overall overhead of the security operations of the IoT. This is especially important since large-scale deployments are expected to involve thousands or millions of devices. This paper aims to comprehensively overview the various advantages of implementing these security algorithms.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {102},
numpages = {6},
keywords = {IOT, Lightweight algorithms, M2M, MiWi, SPINS, Security},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1145/3410530.3414613,
author = {Xu, Susu and Pan, Shijia and Yu, Tong},
title = {CML-IOT 2020: the second workshop on continual and multimodal learning for internet of things},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414613},
doi = {10.1145/3410530.3414613},
abstract = {With the deployment of Internet of Things (IoT), large amount of sensors are connected into the Internet, providing large-amount, streaming, and multimodal data. These data have distinct statistical characteristics over time and sensing modalities, which are hardly captured by traditional learning methods. Continual and multimodal learning allows integration, adaptation, and generalization of the knowledge learned from experiential data collected with heterogeneity to new situations. Therefore, continual and multimodal learning is an important step to enable efficient ubiquitous computing on IoT devices. The major challenges to combine continual learning and multimodal learning with real-world data include 1) how to fuse and transfer knowledge between the multimodal data under constrained computational resources, 2) how to learn continually despite the missing, imbalanced or noisy data under constrained computational resources, 3) how to effectively reserve privacy and retain security when learning knowledge from streaming and multimodal data collected by multiple stakeholders, and 4) how to develop large-scale distributed learning systems to efficiently learn from continual and multimodal data.We organize this workshop to bring people working on different disciplines together to tackle these challenges in this topic. This workshop aims to explore the intersection and combination of continual machine learning and multimodal modeling with applications in the Internet of Things. The workshop welcomes works addressing these issues in different applications/domains as well as algorithmic and systematic approaches to leverage continual learning on multimodal data. We further seek to develop a community that systematically handles the streaming multimodal data widely available in real-world ubiquitous computing systems. In 2019, we held the First Workshop on Continual and Multimodal Learning for Internet of Things (https://cmliot2019.github.io/) with Ubicomp 2019, London, UK. The First workshop accepted 12 papers from 17 submissions. The one-day agenda included 3 sessions and attracted around 20 attendees from academia and industries to discuss and share visions.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {616–618},
numpages = {3},
keywords = {continual learning, internet of things, multimodal learning, ubiquitous computing},
location = {Virtual Event, Mexico},
series = {UbiComp/ISWC '20 Adjunct}
}

@article{10.1145/3641290,
author = {Yang, Haotian and Zhang, Xiaoyu and Wu, Zihan and Wang, Liangmin and Chen, Xiao and Liu, Lu},
title = {Co-Sharding: A Sharding Scheme for Large-Scale Internet of Things Application},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641290},
doi = {10.1145/3641290},
abstract = {Blockchain technology finds widespread application in the management of Internet of Things (IoT) devices. In response to the challenges posed by performance scalability and the convergence of multiple ledgers stemming from an expanding network, this study introduces the concept of Co-Sharding. Within this framework, the ledger maintained by sub-chains overseeing IoT operations in distinct geographic regions is conceptualized as a shard within the Large-scale Internet of Things (LIOT) ledger. Meanwhile, elected nodes within each region assume responsibility for maintaining a coordinating shard, facilitating cross-regional communication and data interaction. Furthermore, our work presents a multi-objective optimization algorithm grounded in the multi-shard paradigm to enact a scheduling strategy that spans various regions. We undertake a series of pertinent experiments and conduct a comparative analysis of scheduling algorithms within the context of a real-world cross-regional agricultural IoT system, utilizing actual operational data. The comparative results demonstrate that, in comparison to intra-sub-region scheduling, the Co-Sharding approach enhances machine utilization rates by approximately 30% and reduces scheduling time by around 18% when confronted with a task count of twelve. In terms of performance, Co-Sharding also exhibits the capability to reduce the storage requirements of lightweight nodes within each region by approximately 39%, while concurrently improving throughput by approximately 1.5 times when contrasted with a single-chain architecture.},
note = {Just Accepted},
journal = {Distrib. Ledger Technol.},
month = {jan},
keywords = {Large-scale IoT, cross-region, coordinating shard, scheduling model}
}

@inproceedings{10.1145/3373477.3373498,
author = {WU, Xia and Kong, Fanfei and Shi, Jupeng and Bao, Lina and Gao, Feng and Li, Jing},
title = {A blockchain internet of things data integrity detection model},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373498},
doi = {10.1145/3373477.3373498},
abstract = {The Industrial Internet of Things (IIoT) plays an indispensable role in Industry 4.0. People are committed to a universal, scalable, and secure IIoT system for use in a variety of industries. However, existing IIoT systems are not prone to provide stable services due to single points of failure and malicious attacks. In the era of big data, countless machines and equipment are generating data all the time, which can be used for scientific research, commodity production, and improving the efficiency of industrial development. Since the Internet of Things (IoT) connects all the smart machines together, ensuring data integrity is of great significance. How to ensure and detect the data integrity of the IoT is a topic of concern. This paper designs and implements a distributed blockchain data simulation system for data integrity assurance and detection in the context of IIoT applications. A method for assuring and detecting the integrity of IoT data is presented. A complete simulation system of IoT data based on blockchain is designed and implemented. The time consumed in the block mining process under different parameters is measured, and the correlation between the block mining time and different parameters is found, and the law of the influence of different parameters on the block mining time is obtained.},
booktitle = {Proceedings of the 1st International Conference on Advanced Information Science and System},
articleno = {21},
numpages = {7},
keywords = {IIoT, IoT, SHA-256, blockchain, data integrity},
location = {Singapore, Singapore},
series = {AISS '19}
}

@article{10.1145/3446346,
author = {Baheti, Shrey and Badiger, Shreyas and Simmhan, Yogesh},
title = {VIoLET: An Emulation Environment for Validating IoT Deployments at Large Scales},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3446346},
doi = {10.1145/3446346},
abstract = {Internet of Things (IoT) deployments have been growing manifold, encompassing sensors, networks, edge, fog, and cloud resources. Despite the intense interest from researchers and practitioners, most do not have access to large-scale IoT testbeds for validation. Simulation environments that allow analytical modeling are a poor substitute for evaluating software platforms or application workloads in realistic computing environments. Here, we propose a virtual environment for validating Internet of Things at large scales (VIoLET), an emulator for defining and launching large-scale IoT deployments within cloud VMs. It allows users to declaratively specify container-based compute resources that match the performance of native IoT compute devices using Docker. These can be inter-connected by complex topologies on which bandwidth and latency rules are enforced. Users can configure synthetic sensors for data generation as well. We also incorporate models for CPU resource dynamism, and for failure and recovery of the underlying devices. We offer a detailed comparison of VIoLET’s compute and network performance between the virtual and physical deployments, evaluate its scaling with deployments with up to 1, 000 devices and 4, 000 device-cores, and validate its ability to model resource dynamism. Our extensive experiments show that the performance of the virtual IoT environment accurately matches the expected behavior, with deviations levels within what is seen in actual physical devices. It also scales to 1, 000s of devices and at a modest cloud computing costs of under 0.15% of the actual hardware cost, per hour of use, with minimal management effort. This IoT emulation environment fills an essential gap between IoT simulators and real deployments.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {jul},
articleno = {25},
numpages = {39},
keywords = {Internet of Things, cloud computing, distributed systems, edge computing, emulation, fog computing, scalability, virtual environment}
}

@inproceedings{10.1145/3454127.3456577,
author = {Chaib Ainou, Taki Eddine and Ayad, Soheyb and Sadek Terrissa, Labib},
title = {A Survey on SDN based energy-efficiency approaches in IoT: ‎Systematic‎ survey on energy conservation methods in IoT},
year = {2021},
isbn = {9781450388719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3454127.3456577},
doi = {10.1145/3454127.3456577},
abstract = {The Internet of Things, enabled by the rapid spread of telecommunication technologies, has become an integral part of our lives. However, it still faces some challenges that hinder its wide deployment. the number of connected things is counting in billions which theoretically generates massive data that leads to important energy consumption and expands the CO2 emission in Cloud infrastructures and backbone networks, also reduce the life-time of things at the Edge level. Software-Defined Networking is a new paradigm based on the centralization of large-scale network infrastructure control, which offers the flexibility of software and hardware device management. In this survey paper, we highlight the main energy-efficiency existing approaches and their limits in SDN architectures for IoT infrastructures.},
booktitle = {Proceedings of the 4th International Conference on Networking, Information Systems &amp; Security},
articleno = {8},
numpages = {7},
keywords = {Backbone, Datacenters, Edge, Energy-efficiency, Internet of Things, Software-Defined Networking, Wireless Sensor Networks},
location = {KENITRA, AA, Morocco},
series = {NISS '21}
}

@inproceedings{10.1145/3549737.3549764,
author = {Vonitsanos, Gerasimos and Panagiotakopoulos, Theodor and Kanavos, Andreas and Kameas, Achilles},
title = {An Apache Spark Framework for IoT-enabled Waste Management in Smart Cities},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549764},
doi = {10.1145/3549737.3549764},
abstract = {The diffusion of small low cost sensors has opened new opportunities in terms of designing real-time monitoring systems in several application fields. Internet of Things (IoT) is a new branch of Information and Communication Technologies that connects vast amounts of heterogeneous sensing devices and other smart objects through different network protocols in order to provide large scale interoperability and underpin the development of novel applications. A fundamental application domain regarding IoT refers to smart cities, which offer a fertile ground for implementing technological advances to enhance urban management, ensure environmental sustainability and improve the quality of living. In this paper, we propose an innovative framework aiming at collecting, monitoring and processing streams of data received in real-time by IoT sensor devices while measuring the waste level of waste bins in a distributed environment.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {23},
numpages = {7},
keywords = {Apache Spark, Classification, Internet of Things (IoT), Machine Learning, Smart Cities, Urban Waste},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3408127.3408180,
author = {Yang, Pinglin and Guo, Gaizhi},
title = {Research on Big Data Parallel Processing Platform Based on Postal Industry},
year = {2020},
isbn = {9781450376877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408127.3408180},
doi = {10.1145/3408127.3408180},
abstract = {With the development of cloud computing, big data, and the Internet of Things, for the data collection and daily drama of the postal express industry, in the face of such a large-scale data set, the traditional storage and calculation related theories and methods can no longer meet the massive and multi-source access and processing of heterogeneous data. The article analyzes the characteristics of postal data, focuses on the Hadoop and Spark platform architectures, and compares the performance differences between the two platforms through experiments. At the same time, according to the characteristics of Hadoop and Spark big data platforms, they learn from each other's strengths and apply them to different stages of the postal big data system.},
booktitle = {Proceedings of the 2020 4th International Conference on Digital Signal Processing},
pages = {305–309},
numpages = {5},
keywords = {Big data, Big data platform, Hadoop, Postal, Spark},
location = {Chengdu, China},
series = {ICDSP '20}
}

@inproceedings{10.1145/3487075.3487109,
author = {Zhang, Qiang and Yang, Kun and Qin, Shuqi and Mei, Haibo and Shen, Jun},
title = {Competition-free Period Interleaved Time-Slot Allocation Strategy for Data Transmission of IoT System},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487109},
doi = {10.1145/3487075.3487109},
abstract = {The rapid development of wireless communication has accelerated the arrival of the Internet of Things (IoT) era. This article conducts in-depth research on heterogeneous data transmission in large-scale ZigBee presented IoT networks. We start from three aspects of data type classification of the IoT system, then design a time slot request framework and a time slot allocation algorithm involving the terminal and coordinator. We intend to improve the peak-shift transmission during the non-competition period to adapt to more practical IoT application scenarios. The proposed solution can ensure the reliable data transmission of the entire network, then the data transmission delay can be reduced to satisfy the heterogeneous transmission requirements. In the end, the proposed solution will be validated via simulation, and the numerical data demonstrates that our proposed solution outperforms.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
articleno = {34},
numpages = {5},
keywords = {Congestion, Heterogeneous data transmission, IoT, Reliability, Time slot allocation},
location = {Sanya, China},
series = {CSAE '21}
}

@article{10.1145/3433542,
author = {Tan, Liang and Shi, Na and Yu, Keping and Aloqaily, Moayad and Jararweh, Yaser},
title = {A Blockchain-empowered Access Control Framework for Smart Devices in Green Internet of Things},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3433542},
doi = {10.1145/3433542},
abstract = {Green Internet of things (GIoT) generally refers to a new generation of Internet of things design concept. It can save energy and reduce emissions, reduce environmental pollution, waste of resources, and harm to human body and environment, in which green smart device (GSD) is a basic unit of GIoT for saving energy. With the access of a large number of heterogeneous bottom-layer GSDs in GIoT, user access and control of GSDs have become more and more complicated. Since there is no unified GSD management system, users need to operate different GIoT applications and access different GIoT cloud platforms when accessing and controlling these heterogeneous GSDs. This fragmented GSD management model not only increases the complexity of user access and control for heterogeneous GSDs, but also reduces the scalability of GSDs applications. To address this issue, this article presents a blockchain-empowered general GSD access control framework, which provides users with a unified GSD management platform. First, based on the World Wide Web Consortium (W3C) decentralized identifiers (DIDs) standard, users and GSD are issued visual identity (VID). Then, we extended the GSD-DIDs protocol to authenticate devices and users. Finally, based on the characteristics of decentralization and non-tampering of blockchain, a unified access control system for GSD was designed, including the registration, granting, and revoking of access rights. We implement and test on the Raspberry Pi device and the FISCO-BCOS alliance chain. The experimental results prove that the framework provides a unified and feasible way for users to achieve decentralized, lightweight, and fine-grained access control of GSDs. The solution reduces the complexity of accessing and controlling GSDs, enhances the scalability of GSD applications, as well as guarantees the credibility and immutability of permission data and identity data during access.},
journal = {ACM Trans. Internet Technol.},
month = {jun},
articleno = {80},
numpages = {20},
keywords = {Green Internet of Things, access control, decentralized identifier, blockchain, application fragmentation}
}

@inproceedings{10.1145/3503047.3503076,
author = {Jung, Youna and Goldsmith, Noah and Barker, John},
title = {Survey of Trust Management on Mission-oriented Internet of Things},
year = {2022},
isbn = {9781450385862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503047.3503076},
doi = {10.1145/3503047.3503076},
abstract = {The Internet of Things (IoT) enables us to use diverse sensing data and control IoT devices, sometimes, networks remotely. This technology makes our lives easier and more comfortable. However, the services currently provided by IoT systems are limited within pre-defined sets of devices and it hinders the development of large-scale and complicated IoT services that could be provided through dynamic collaboration between IoT devices across networks. To realize the mission-oriented IoT (MIoT) systems, we must address the security issues of the MIoT systems. Among various security issues, in this paper, we focus on trust management on MIoT. We analyze existing work on trust management to see if they are suitable for the MIoT systems. Then, we identify potential issues and discuss challenges for the trust management on MIoT.},
booktitle = {Proceedings of the 3rd International Conference on Advanced Information Science and System},
articleno = {26},
numpages = {5},
location = {Sanya, China},
series = {AISS '21}
}

@inproceedings{10.1145/3447555.3466571,
author = {Meisenbacher, Stefan and Schwenk, Karl and Galenzowski, Johannes and Waczowicz, Simon and Mikut, Ralf and Hagenmeyer, Veit},
title = {Smart Charging of Electric Vehicles with Cloud-based Optimization and a Lightweight User Interface: A Real-World Application in the Energy Lab 2.0: Poster},
year = {2021},
isbn = {9781450383332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447555.3466571},
doi = {10.1145/3447555.3466571},
abstract = {Smart Charging (SC) of Electric Vehicles (EVs) integrates them into the power system to support grid stability by power management. Large-scale adoption of SC requires a high level of EV user acceptance. Therefore, it is imperative to make the underlying charging scheme tangible for the user. We propose a web app for the user to start, adjust and monitor the charging process via a User Interface (UI). We outline the integration of this web app into an Internet of Things (IoT) architecture to establish communication with the charging station. Two scenarios demonstrate the operation of the system. Future field studies on SC should involve the EV user due to individual preferences and responses to incentive schemes. Therefore, we propose the Smart Charging Wizard with a customizable UI and optimization module for future research and collaborative development.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
pages = {284–285},
numpages = {2},
keywords = {Battery Aging, Electric Vehicles, Smart Charging, Web Applications},
location = {Virtual Event, Italy},
series = {e-Energy '21}
}

@inproceedings{10.1145/3567445.3567451,
author = {Elhabbash, Abdessalam and Elkhatib, Yehia and Bouloukakis, Georgios and Salama, Maria},
title = {A Middleware for Automatic Composition and Mediation in IoT Systems},
year = {2023},
isbn = {9781450396653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567445.3567451},
doi = {10.1145/3567445.3567451},
abstract = {This paper presents Hetero-Genius, a middleware architecture that enables construction and mediation in Internet of Things (IoT) systems. IoT systems are deployed across physical spaces such as urban parks, residential areas, and highways. The services provided by such IoT deployments are constrained to specific devices and deployment contexts. While existing interoperability solutions enable the “design time” development and deployment of IoT systems, it is often essential to dynamically compose systems that consist of other “small scale” IoT systems. To achieve this, post-deployment composition is needed, i.e., runtime composition of diverse IoT devices and capabilities. Hetero-Genius supports system and service discoverability, as well as automatic composability. We demonstrate this using a real-world Internet of Vehicles (IoV) scenario. Our experimental evaluation shows that developers can save up to 47% of their time when using Hetero-Genius, as well as improve code correctness by 55% on average.},
booktitle = {Proceedings of the 12th International Conference on the Internet of Things},
pages = {127–134},
numpages = {8},
keywords = {Internet of Things, Interoperability, Mediation, Ontology, Semantics, System composition},
location = {Delft, Netherlands},
series = {IoT '22}
}

@inproceedings{10.1145/3487075.3487108,
author = {Guo, Sushu and Chen, Wenlong and Wang, Jiacheng},
title = {An Overlapping Routing Tree Transmission Model Based on Segment Identification: OTSI Model},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487108},
doi = {10.1145/3487075.3487108},
abstract = {In the Internet of Things (IOT) based on IPv6. For large-scale multi-gateway WSN, the sensor device has its own certain limitations, and the processing capability of the node is very limited. Due to the limitations of the traditional WSN routing transmission protocol, this paper proposes an overlapping routing tree transmission model (OTSI) based on segment identifiers and a method to generate the model. We design a segment identifier based on the model, specifying transmission gateways and service demands for nodes in different manifestations of segment identifiers. We also designed a data transmission model of the OTSI in different scenarios. Through the OMNeT simulation experiment, it's found that the model can effectively specify the transmission gateway for the node according to the service demand, and achieve the balance of traffic transmission.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
articleno = {33},
numpages = {7},
keywords = {IOT, IPv6, Load balancing, Routing transmission, Routing tree},
location = {Sanya, China},
series = {CSAE '21}
}

@inproceedings{10.5555/3400397.3400614,
author = {Mittal, Saurabh and Tolk, Andreas and Pyles, Andrew and Van Balen, Nicolas and Bergollo, Kevin},
title = {Digital twin modeling, co-simulation and cyber use-case inclusion methodology for IoT systems},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Cyber Physical Systems (CPS) and Internet of Things (IoT) communities are often asked to test devices regarding their effects on underlying infrastructure. Usually, only one or two devices are given to the testers, but hundreds or thousands are needed to really test IoT effects. This proposition makes IoT Test &amp; Evaluation (T&amp;E) cost and management prohibitive. One possible approach is to develop a digital twin of the IoT device and employ many replicas of the twin in a simulation environment comprised of various simulators that mimic the IoT device's operational environment. Cyber attack experimentation is a critical aspect of IoT T&amp;E and without such a virtual T&amp;E environment, it is almost impossible to study large scale effects. This paper will present a digital twin engineering methodology as applicable to IoT device T&amp;E and cyber experimentation.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2653–2664},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3575879.3575976,
author = {Eleftherakis, George and Baxhaku, Fesal and Vasilescu, Anca},
title = {Bio-inspired Adaptive Architecture for Wireless Sensor Networks},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575976},
doi = {10.1145/3575879.3575976},
abstract = {Wireless Sensor Networks (WSN) are expected to revolutionize daily life by connecting everyday objects with sensing capabilities, offering numerous opportunities for a wide range of applications. To facilitate the integration and enable all these opportunities to be realized, it becomes a necessity for middleware architectures that: (a) perform well in non-well-defined infrastructures, (b) are able to deal with the large number of users and heterogeneous devices integrated into it (ultra scalable), and (c) enable autonomy of the system overall. This work introduces a bio-inspired middleware optimized for wireless sensor networks proposing a refinement, the regional network, in a work published earlier as a bio-inspired self-adaptive architecture for the Internet of Things, while providing a comparison of other similar middleware approaches and a discussion on the motivating health monitoring scenario.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {116–122},
numpages = {7},
keywords = {Bio-inspired Adaptive Systems, IoT, Middleware, Wireless Sensor Networks, eHealth},
location = {<conf-loc>, <city>Athens</city>, <country>Greece</country>, </conf-loc>},
series = {PCI '22}
}

@inproceedings{10.1145/3550355.3552405,
author = {Li, Jia and Nejati, Shiva and Sabetzadeh, Mehrdad and McCallen, Michael},
title = {A domain-specific language for simulation-based testing of IoT edge-to-cloud solutions},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552405},
doi = {10.1145/3550355.3552405},
abstract = {The Internet of things (IoT) is increasingly prevalent in domains such as emergency response, smart cities and autonomous vehicles. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. In this paper, we propose a domain-specific language (DSL) for generating edge-to-cloud simulators. An edge-to-cloud simulator executes the functionality of a large array of edge devices that communicate with cloud applications. Our DSL, named IoTECS, is the result of a collaborative project with an IoT analytics company, Cheetah Networks. The industrial use case that motivates IoTECS is ensuring the scalability of cloud applications by putting them under extreme loads from IoT devices connected to the edge. We implement IoTECS using Xtext and empirically evaluate its usefulness. We further reflect on the lessons learned.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {367–378},
numpages = {12},
keywords = {IoT, domain-specific languages, simulation, stress testing, xtext},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3581783.3613910,
author = {Chen, Chang-Wen Chen},
title = {Internet of Video Things: Technical Challenges and Emerging Applications},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613910},
doi = {10.1145/3581783.3613910},
abstract = {The worldwide flourishing of the Internet of Things (IoT) in the past decade has enabled numerous new applications through the internetworking of a wide variety of devices and sensors. In recent years, visual sensors have seen a considerable boom in IoT systems because they are capable of providing richer and more versatile information. Internetworking of large-scale visual sensors has been named the Internet of Video Things (IoVT). IoVT has a new array of unique characteristics in terms of sensing, transmission, storage, and analysis, all are fundamentally different from the conventional IoT. These new characteristics of IoVT are expected to impose significant challenges on existing technical infrastructures. In this keynote talk, an overview of recent advances in various fronts of IoVT will be introduced and a broad range of technological and systematic challenges will be addressed. Several emerging IoVT applications will be discussed to illustrate the great potential of IoVT in a broad range of practical scenarios.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {1–2},
numpages = {2},
keywords = {internet of things, internet of video things, massive camera sensors, video coding, video data networking, wireless networks},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1145/3566097.3567938,
author = {Ma, Tianliang and Deng, Zhihui and Shao, Leilai},
title = {AutoFlex: Unified Evaluation and Design Framework for Flexible Hybrid Electronics},
year = {2023},
isbn = {9781450397834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3566097.3567938},
doi = {10.1145/3566097.3567938},
abstract = {Flexible hybrid electronics (FHE), integrating high performance silicon chips with multi-functional sensors and actuators on flexible substrates, can be intimately attached onto irregular surfaces without compromising their functionalities, thus enabling more innovations in healthcare, internet of things (IoTs) and various human-machine interfaces (HMIs). Recent developments on compact models and process design kits (PDKs) of flexible electronics have made designs of small to medium flexible circuits feasible. However, the absence of a unified model and comprehensive evaluation benchmarks for flexible electronics makes it infeasible for a designer to fairly compare different flexible technologies and to explore potential design options for a heterogeneous FHE design. In this paper, we present AutoFlex, a unified evaluation and design framework for flexible hybrid electronics, where device parameters can be extracted automatically and performance can be evaluated comprehensively from device levels, digital blocks to large-scale digital circuits. Moreover, a ubiquitous FHE sensor acquisition system, including a flexible multi-functional sensor array, scan drivers, amplifiers and a silicon based analog-to-digital converter (ADC), is developed to reveal the design challenges of a representative FHE system.},
booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
pages = {757–762},
numpages = {6},
keywords = {design automation, flexible electronics, flexible hybrid electronics, hardware/software co-design, heterogeneous system design},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {ASPDAC '23}
}

@inproceedings{10.1145/3402597.3402609,
author = {Wambura, Stephen and Li, He and Nigussie, Alemu},
title = {Fast Memory-efficient Extreme Events Prediction in Complex Time series},
year = {2020},
isbn = {9781450387644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402597.3402609},
doi = {10.1145/3402597.3402609},
abstract = {This paper proposes a generic memory-efficient framework for realtime stochastic extreme events prediction in complex time series systems such as intrusion detection, Internet of Things (IoT), social networks, stock markets etc. Ideally we exploit the expressiveness of deep neural networks and temporal nature of sequence-to-sequence structures (parallel Convolutional and recurrent neural networks) glued on Convolutional Quantile Loss and memory network to model explicitly extreme events. Convolutional Quantile Loss is used to predict future extreme events, while memory network is used to memorize extreme events in future observations. We show that the approach can capture long and short-term temporal effects as well as other non-linear dynamic patterns across multiple probabilistic time series with reliable principled uncertainty estimates. We demonstrate and validate empirically the effectiveness of the proposed framework via extensive experiments and rigorous evaluation on large-scale real world datasets. The experimental results showcase that the proposed method is fast, robust, accurate and has superior performance compared to the well-known prediction methods.},
booktitle = {Proceedings of the 2020 3rd International Conference on Robot Systems and Applications},
pages = {60–69},
numpages = {10},
keywords = {events, neural networks, prediction, time series},
location = {Chengdu, China},
series = {ICRSA '20}
}

@inproceedings{10.1145/3508397.3564841,
author = {Babbar, Himanshi and Rani, Shalli and Singh, Aman and Gianini, Gabriele},
title = {A Multiple-Path Routing Model for Quality of Service in Software Defined Networking},
year = {2022},
isbn = {9781450392198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508397.3564841},
doi = {10.1145/3508397.3564841},
abstract = {The Internet of Things (IoT) has recently emerged as a new family of technologies that allow a great number of things to be connected over heterogeneous networks. Conventional networks, however, face a technological barrier in efficiently handling such a large number of devices. The approach based on software-defined networks (SDNs), with its speed and flexibility, has recently been introduced into IoT area to potentially achieve scalability and adaptability, resulting in the SDN-IoT, a unique IoT design. In this work, we describe a new multiple-path routing model for the SDN-IoT. This model consists of two components: 1) a congested source path discovery technique; 2) a multi-path selection method taking into account route similarity and priority. In this paper we first describe the SDN based Quality of Service and highlight the unique features of this routing technique, then report on the performance evaluation of such system, which achieves an approximate 17% decrease in packet loss ratio over the existing approaches.},
booktitle = {Proceedings of the 14th International Conference on Management of Digital EcoSystems},
pages = {74–79},
numpages = {6},
keywords = {multipath routing, quality of service, software defined networks},
location = {Venice, Italy},
series = {MEDES '22}
}

@inproceedings{10.1109/MODELS-C.2019.00020,
author = {Rossi, Maria Teresa and De Sanctis, Martina and Iovino, Ludovico and Rutle, Adrian},
title = {A multilevel modelling approach for tourism flows detection},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00020},
doi = {10.1109/MODELS-C.2019.00020},
abstract = {Application development in the Internet of Things (IoT) faces various issues such as lack of separation of concerns and lack of high-level abstraction to address its large scale and heterogeneity. MDE supports the management of this heterogeneity raising the level of abstraction and thanks to its core operations. Multilevel modelling makes it possible to extend MDE techniques to more than two meta-levels permitting model elements to have a dual type-instance dimension, making it particularly suitable for this application domain. People flow monitoring and detection is one of the hot topics in smart cities projects. In this paper, we exploit MDE techniques, through multilevel modelling approaches, to design the infrastructure supporting a solution part of a comprehensive project related to urban informatics. Moreover, even if we target the people flow monitoring and detection scenario, the provided multilevel approach is open and extensible to further IoT scenarios, to specifically manage the evolutionary nature of the IoT.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {103–112},
numpages = {10},
location = {Munich, Germany},
series = {MODELS '19}
}

@inbook{10.1145/3570361.3615731,
author = {Tong, Shuai and Wang, Jiliang},
title = {Designing, Building, and Characterizing Large-Scale LoRa Networks for Smart City Applications},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3615731},
abstract = {LoRa, as a representative Low-Power Wide-Area Network (LPWAN) technology, holds tremendous potential for various Internet of Things (IoT) applications. However, as there are few real large-scale deployments, it is unclear whether and how well LoRa can eventually meet its prospects. In this paper, we demystify the real performance of LoRa by deploying LoRa systems in both campus-scale testbeds and citywide applications. Our LoRa network consisting of 100 gateways and 19,821 LoRa end nodes, covering an area of 130 km2 for 12 applications. Our measurement focuses on following perspectives: (i) Coverage performance of the LoRa network; (ii) Gateway efficiency and deployment optimization; (iii) Validation of two LoRa optimization mechanisms. The results reveal that LoRa performance in urban settings is bottlenecked by the prevalent blind spots, and there is a gap between the gateway efficiency and network coverage for gateway deployment. Our measurement provides insights for large-scale LoRa network deployment and also for future academic research to fully unleash the potential of LoRa.},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {124},
numpages = {3}
}

@inproceedings{10.1145/3466933.3466983,
author = {Otavio Duarte, Luiz and de Lima Prestes, Jos\'{e} Augusto},
title = {IoT solution information security certification conceptual framework: IoT solution information securityOn improving the transparency and accountability of IoT Solutions through an Open World perspective},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466983},
doi = {10.1145/3466933.3466983},
abstract = {The rapid growth of Internet of Things (IoT) solutions development and the rise of agile development utilization, combined with the so-called “low touch economy” and the recent discussions on privacy and data protection brought several demands related to Information Security. Despite the existence of several efforts – either academic or not – focused on the definition and implementation strategies for certification of Information Security models designed for Information Technology and Communications (ICT) solutions, these aren't widely adopted. In addition, there are significant differences between typical IoT solutions and ICT solutions as traditionally presented, which ends up demanding different certification strategies. Continuous and more dynamic certification models (using cutting edge technologies such as blockchain, self-regulation, analytics, and artificial intelligence) are demanded in this context. This work discusses more effective forms of certification, using innovative edge concepts and technologies, at first aiming to identify a set of inhibiting factors, offenders, challenges or issues that need to be addressed correctly when developing an effective large-scale security certification model.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Information Systems},
articleno = {47},
numpages = {9},
keywords = {Edge Devices, Information Security, Internet of Things, Security Certification, Security Compliance},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI '21}
}

@inproceedings{10.1145/3538806.3538813,
author = {Liu, Yanhui},
title = {Design and Implementation of Network Structure Visualization System Based on B/S Architecture},
year = {2022},
isbn = {9781450387439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538806.3538813},
doi = {10.1145/3538806.3538813},
abstract = {With the rapid development of big data, cloud computing technology, Internet of Things technology and so on, the network scale in WAN and LAN is constantly expanding. The data mining of different networks and the division of network-related topology have become the main issues of multi-layer complex network node setting and network topology analysis. Based on B/S web browser service architecture,As well as the front-end and back-end technologies of Vue, Threejs, Flask, igraph, NetworkX, etc., set up multiple network visualization interactive systems, and set up network model building modules, network layout and attribute mapping modules, network interaction analysis modules, etc., which are used for setting network visualization nodes, and visual coding and visual mapping of attribute information of different network topologies and data in the network.To deepen users' systematic understanding of multi-layer network relationships and network node topology.},
booktitle = {Proceedings of the 10th International Conference on Communications and Broadband Networking},
pages = {14–19},
numpages = {6},
keywords = {Design, Key words: B/S architecture, Network visualization system, realize},
location = {Shanghai, China},
series = {ICCBN '22}
}

@inproceedings{10.1145/3416507.3423188,
author = {Stevens, Clay and Alhanahnah, Mohannad and Yan, Qiben and Bagheri, Hamid},
title = {Comparing formal models of IoT app coordination analysis},
year = {2020},
isbn = {9781450381260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416507.3423188},
doi = {10.1145/3416507.3423188},
abstract = {The rising popularity of the Internet-of-Things (IoT) devices has driven their increasing adoption in various settings, such as modern homes. IoT systems integrate such physical devices with third-party apps, which can coordinate in arbitrary ways. However, malicious or undesired coordination can lead to serious vulnerabilities. This paper explores two different ways, i.e., a commonly-used state-based approach and a holistic, rule-based approach, to formally model app coordination and the safety and security thereof in the context of IoT platforms. The less common rule-base approach allows for a smaller, more scalable model. We realize both modeling approaches using bounded model checking with Alloy to automatically identify potential cases where apps exhibit coordination relationships. We evaluate the effectiveness of the modeling approaches by checking a corpus of real-world IoT apps of Samsung SmartThings and IFTTT. The experimental results demonstrate that our rule-based modeling leads to a more scalable analysis.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Security from Design to Deployment},
pages = {3–10},
numpages = {8},
keywords = {Coordination Threats, Formal Verification, IoT Safety},
location = {Virtual, USA},
series = {SEAD 2020}
}

@inproceedings{10.1145/3487664.3487785,
author = {Khanh Duy, Truong and K\"{u}ng, Josef and Huu Hanh, Hoang},
title = {Survey on IoT Data Analytics with Semantic Approaches},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487785},
doi = {10.1145/3487664.3487785},
abstract = {Data generated from the Internet of Things (IoT) devices that are mostly cheap enough for any specific use case. It shows the ability to gather data about the physical environment and to understand real-time context, combining with other heterogeneous data sources such as sensor networks, social media, crowdsource data collections, etc. Data analytics can enable a massive set of new services for IoT applications. The management of data in an ultra-scale network which is continuously expanding leads to concerns in data analytics and management. The researchers have examined the challenge of interoperability of applications and services among IoT applications to address them. The common problems of interoperability come from different levels, from syntactic to semantic. In this paper, we take a broad view of current IoT analytics work where Semantic Web approaches aim to solve the semantic interoperability by exploring recent studies in IoT systems. The paper taxonomized literature based on the interoperability requirement of the IoT system. This study identifies the opportunity resulting from the convergence of the Semantic Web and IoT data analytics.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {199–204},
numpages = {6},
keywords = {Internet of things, Semantic Data Analytics, Semantic Middleware, Semantic Web, Sensor network, Web of Things},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3401025.3403777,
author = {Giouroukis, Dimitrios and Dadiani, Alexander and Traub, Jonas and Zeuch, Steffen and Markl, Volker},
title = {A survey of adaptive sampling and filtering algorithms for the internet of things},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3403777},
doi = {10.1145/3401025.3403777},
abstract = {The Internet of Things (IoT) represents one of the fastest emerging trends in the area of information and communication technology. The main challenge in the IoT is the timely gathering of data streams from potentially millions of sensors. In particular, those sensors are widely distributed, constantly in transit, highly heterogeneous, and unreliable. To gather data in such a dynamic environment efficiently, two techniques have emerged over the last decade: adaptive sampling and adaptive filtering. These techniques dynamically reconfigure rates and filter thresholds to trade-off data quality against resource utilization.In this paper, we survey representative, state-of-the-art algorithms to address scalability challenges in real-time and distributed sensor systems. To this end, we cover publications from top peer-reviewed venues for a period larger than 12 years. For each algorithm, we point out advantages, disadvantages, assumptions, and limitations. Furthermore, we outline current research challenges, future research directions, and aim to support readers in their decision process when designing extremely distributed sensor systems.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {27–38},
numpages = {12},
keywords = {adaptive filtering, adaptive sampling, sensor networks},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3627345.3627363,
author = {Rey, William P. and Javier, Lance Jhudiel P. and Manguiat, Justin Miguel N. and Tolentino, Wesley Mikhail C.},
title = {Cattle Care: An IoT Cattle Health Monitoring System (CHMS) for Backyard Dairy Cattle Farmers},
year = {2023},
isbn = {9798400708046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627345.3627363},
doi = {10.1145/3627345.3627363},
abstract = {Cattle health monitoring is not unusual in today's time, there are techniques or practices used to monitor their health conditions consistently to maintain wellness to be able to produce as much milk as they can. Unfortunately, it is laborious and time-consuming for small-scale dairy farmers, more so, by utilizing traditional methods. This research highlights an automated, Internet of Things-based monitoring system for dairy cow health. The system includes hardware devices to collect health parameters from the cattle and an end-user web-based application for farmers to be able to observe and make decisions remotely. The technology was tested in a real-world environment and showed its ability to properly monitor and inform farmers about cattle's wellbeing. Using the system, farmers have improved the caregiving towards the cattle. Evidence of this is shown by the cattle's milk yield which increased more than it has ever been since it is on the farm.},
booktitle = {Proceedings of the 2023 8th International Conference on Cloud Computing and Internet of Things},
pages = {124–130},
numpages = {7},
keywords = {Internet of Things, cattle health monitoring, microcontrollers, sensors},
location = {<conf-loc>, <city>Okinawa</city>, <country>Japan</country>, </conf-loc>},
series = {CCIOT '23}
}

@inproceedings{10.5555/3466184.3466431,
author = {Al-Zoubi, Khaldoon and Wainer, Gabriel},
title = {Modelling fog &amp; cloud collaboration methods on large scale},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Fog Computing is expected to decentralize clouds to improve users quality of service, in technologies like the Internet of Things, by pushing some computing onto mini-clouds (called Fogs) located close to end users. To enhance M&amp;S with this concept, we have developed a complete Fog/Cloud collaboration methods to conduct simulation experiments: Users manipulate their experiments though nearby Fogs servers while M&amp;S resources are dynamically discovered and allocated throughout the Fogs/Cloud. We have already built those methods using privately owned clouds. However, it was difficult in practice to study those methods scalability using real system setups. As a result, we present here the simulation model that we developed to mimic this real system in order to study the proposed collaboration methods on large scale. This model was validated with reference to the real system. The results have clearly shown the scalability of those proposed methods at the structure and coordination levels.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2161–2172},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3543712.3543750,
author = {Mayer, Barbara and Lackner, Katharina Maria},
title = {Selection of an IoT Platform: A Framework for a Two-Stage Multi-Criteria Decision Making Process},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543750},
doi = {10.1145/3543712.3543750},
abstract = {Internet of Things (IoT) is one of the main enablers of industries’ digital transformation. IoT platforms have become essential parts in IoT architectures to enable the generation of added value from gathering data on the field level. A big variety of products and solutions are available on the market. However, industries, IoT applications, and IoT architectures differ considerably regarding their requirements in functionality and scale. The selection process thus poses major challenges for industrial companies. Several sets of criteria have been presented in literature, but there is still a lack of methods and tools for a practical individual selection process. This paper provides a framework for an individual two-stage selection process based on a multi-criteria assessment. This allows companies firstly to weight the given set of criteria according to their individual needs and secondly, to follow a two-stage assessment including an interview-related part and a practical evaluation. Additionally, the paper shows first validation results based on the selection process for an IoT platform for the Institute’s research and learning factory, the Smart Production Lab.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {281–286},
numpages = {6},
keywords = {IoT, IoT platform, multi-criteria assessment, software assessment},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@inproceedings{10.1145/3538969.3543811,
author = {Niavis, Harris and Loupos, Konstantinos},
title = {ConSenseIoT: A Consensus Algorithm for Secure and Scalable Blockchain in the IoT context},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3543811},
doi = {10.1145/3538969.3543811},
abstract = {Data protection and privacy is a major concern in the Internet of Things (IoT) ecosystem, and the excessive use of IoT devices may risk the security of the network. Blockchain solutions are used to enhance the trustworthiness and eliminate the need for trusted third parties by providing mechanisms to reach consensus in a network of trustless participants. The consensus algorithms employed by blockchain architectures ensure the integrity of the data stored in the blockchain, the resiliency of the network and manage the security of devices. However, current solutions are compute intensive affecting the performance of the network and consuming much energy. In this work, we introduce a consensus algorithm for offering secure distributed consensus among IoT devices without affecting the performance of the network. The algorithm is inspired by existing solutions, employs decentralised identities, verifiable credentials and a decentralised trust management mechanism to guarantee security, privacy and trustworthiness of transactions. Finally, our algorithm combines technologies for operating in a distributed manner which favors the scalability and allow the effective integration in large scale networks.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {103},
numpages = {6},
keywords = {Blockchain, IoT, consensus algorithm, trust},
location = {Vienna, Austria},
series = {ARES '22}
}

@inproceedings{10.1145/3485447.3511976,
author = {Hui, Shuodi and Wang, Huandong and Wang, Zhenhua and Yang, Xinghao and Liu, Zhongjin and Jin, Depeng and Li, Yong},
title = {Knowledge Enhanced GAN for IoT Traffic Generation},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511976},
doi = {10.1145/3485447.3511976},
abstract = {Network traffic data facilitates understanding the Internet of Things (IoT) behaviors and improving IoT service quality in the real world. However, large-scale IoT traffic data is rarely accessible, and privacy issues also impede realistic data sharing even with anonymous personal identifiable information. Researchers propose to generate synthetic IoT traffic but fail to cover the multiple services provided by widespread real-world IoT devices. In this work, we take the first step to generate large-scale IoT traffic via a knowledge-enhanced generative adversarial network (GAN) framework, which introduces both the semantic knowledge (e.g., location and environment information) and the network structure knowledge for various IoT devices via a knowledge graph. We use a condition mechanism to incorporate the knowledge and device category for IoT traffic generation. Then, we adopt LSTM and a self-attention mechanism to capture the temporal correlation in the traffic series. Extensive experiment results show that the synthetic IoT traffic datasets generated by our proposed model outperform state-of-art baselines in terms of data fidelity and applications. Moreover, our proposed model is able to generate realistic data by only training on small real datasets with knowledge enhanced.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3336–3346},
numpages = {11},
keywords = {GAN, IoT, knowledge graph, traffic generation},
location = {<conf-loc>, <city>Virtual Event, Lyon</city>, <country>France</country>, </conf-loc>},
series = {WWW '22}
}

@inproceedings{10.1145/3583120.3589816,
author = {Sun, Zehua and Ni, Tao and Yang, Huanqi and Liu, Kai and Zhang, Yu and Gu, Tao and Xu, Weitao},
title = {Demo Abstract: A Novel Firmware Update Over-The-Air System for LoRa Networks},
year = {2023},
isbn = {9798400701184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583120.3589816},
doi = {10.1145/3583120.3589816},
abstract = {LoRa has emerged as a novel Internet of Things (IoT) communication paradigm, featuring with long-range and low-power transmission capabilities. With the widespread deployment of LoRa networks, the demand to perform Firmware Update Over-The-Air (FUOTA) tasks has become increasingly critical for unattended LoRa devices. However, in practice, three fundamental problems that hinder the performance of FUOTA tasks are revealed, including low energy efficiency, poor transmission reliability, and biased multicast grouping. In this demo, we present a novel FUOTA system, the first work that offers an effective and sustainable solution to achieve energy-efficient and reliable over-the-air firmware updates in LoRa networks. In particular, this system incorporates threefold key modules: delta scripting, channel coding, and beamforming. The delta scripting algorithm unlocks the capability of incremental update, the channel coding scheme ensures the reliability and robustness of large-scale firmware image distribution, and the beamforming strategy as an optional module can further serve the unicast user. Thus, this demo presents a working example of functionality customization to show the efficacy and feasibility of our FUOTA system in LoRa networks.},
booktitle = {Proceedings of the 22nd International Conference on Information Processing in Sensor Networks},
pages = {362–363},
numpages = {2},
keywords = {Firmware Update Over-The-Air, LoRa},
location = {San Antonio, TX, USA},
series = {IPSN '23}
}

@inproceedings{10.1145/3375246.3375255,
author = {Bannour, Boutheina and Lapitre, Arnault},
title = {Heuristic-aided symbolic simulation for trickle-based wireless sensors networks configuration},
year = {2020},
isbn = {9781450377775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375246.3375255},
doi = {10.1145/3375246.3375255},
abstract = {Wireless Sensor Networks (WSN) as parts of the so-called Internet of Things (IoT) are facing the challenge of upgrading their firmware very often, in particular when security flaws are found. This large-scale operation can be remotely conducted starting from frontier devices (or nodes) connected to the internet, and gradually spread among the rest of the network. The Trickle algorithm is an efficient well-known algorithm for versioned information dissemination in WSN, and is applicable for this kind of operations. The algorithm when well configured, allows: i) the reduction of the number of packets (messages) exchanged between devices to save their batteries, and ii) a quick propagation of new firmware versions to minimize periods during which the devices are outdated. In this paper, we develop model-based symbolic simulation for configuring Trickle-based WSN combined with an exploration heuristic to cope with combinatorial behavior of the network many-nodes, while still being able to highlight critical scenarios of outdated nodes' situations. Our simulation techniques are implemented in the tool Diversity, which shows promising usability and first results.},
booktitle = {Proceedings of the Conference on Rapid Simulation and Performance Evaluation: Methods and Tools},
articleno = {1},
numpages = {7},
location = {Bologna, Italy},
series = {RAPIDO '20}
}

@inproceedings{10.1145/3377049.3377089,
author = {Mahbub, Tasmima Noushiba and Reza, S. M. Salim and Hossain, Dilshad Ara and Raju, Mehedi Hasan and Arifeen, Md Murshedul and Ayob, Afida},
title = {ANFIS based authentication performance evaluation for enhancing security in Internet of Things},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377089},
doi = {10.1145/3377049.3377089},
abstract = {In this paper, we introduce a robust authentication mechanism for identification in order to secure the wireless channel in IoT networks. Internet of Things is viewed as the most scaled down segment of nano sensor systems that have a significant potential to be embraced continuously for its large array of applications. However, when it comes to nanotechnology, smart identification technique with a secured communication channel is one of the main requirements. Operation with light computational complexity like XOR and Rotation have been used to enhance the security of the network channel. The proposed authentication mechanism has three levels of authentication. Moreover, ANFIS based authentication performance evaluation model has been developed and analyzed based on successful authentication rate and simulated with Fuzzy logic. Simulation has been conducted using MATLAB. It is being concluded after simulation that successful authentication should be 70% for any two levels and no less than 30% in any level. Finally, the proposed mechanism enhances the security in IoT networks which will indicate our mechanism as an effective one.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {23},
numpages = {4},
keywords = {Artificial Intelligence (AI), Authentication, Cyber Security, Internet of Things (IoT), Performance analysis, RFID TAG},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@article{10.1145/3616014,
author = {Savaglio, Claudio and Barbuto, Vincenzo and Awan, Faraz Malik and Minerva, Roberto and Crespi, Noel and Fortino, Giancarlo},
title = {Opportunistic Digital Twin: an Edge Intelligence enabler for Smart City},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3616014},
doi = {10.1145/3616014},
abstract = {Although Digital Twins (DTs) became very popular in industry, nowadays they represent a pre-requisite of many systems across different domains, by taking advantage of the disrupting digital technologies such as Artificial Intelligence (AI), Edge Computing and Internet of Things (IoT). In this paper we present our “opportunistic” interpretation, which advances the traditional DT concept and provides a valid support for enabling next-generation solutions in dynamic, distributed and large scale scenarios as smart cities. Indeed, by collecting simple data from the environment and by opportunistically elaborating them through AI techniques directly at the network edge (also referred to as Edge Intelligence), a digital version of a physical object can be built from the bottom up as well as dynamically manipulated and operated in a data-driven manner, thus enabling prompt responses to external stimuli and effective command actuation. To demonstrate the viability of our Opportunistic Digital Twin (ODT) a real use case focused on a traffic prediction task has been incrementally developed and presented, showing improved inference performance and reduced network latency, bandwidth and power consumption.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
keywords = {Digital Twins, Edge Intelligence, Internet of Things, Synthetic Sensing}
}

@inproceedings{10.1145/3394885.3431617,
author = {Amrouch, Hussam and Hu, Xiaobo Sharon and Imani, Mohsen and Laguna, Ann Franchesca and Niemier, Michael and Thomann, Simon and Yin, Xunzhao and Zhuo, Cheng},
title = {Cross-layer Design for Computing-in-Memory: From Devices, Circuits, to Architectures and Applications},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431617},
doi = {10.1145/3394885.3431617},
abstract = {The era of Big Data, Artificial Intelligence (AI) and Internet of Things (IoT) is approaching, but our underlying computing infrastructures are not sufficiently ready. The end of Moore's law and process scaling as well as the memory wall associated with von Neumann architectures have throttled the rapid development of conventional architectures based on CMOS technology, and cross-layer efforts that involve the interactions from low-end devices to high-end applications have been prominently studied to overcome the aforementioned challenges. On one hand, various emerging devices, e.g., Ferroelectric FET, have been proposed to either sustain the scaling trends or enable novel circuit and architecture innovations. On the other hand, novel computing architectures/algorithms, e.g., computing-in-memory (CiM), have been proposed to address the challenges faced by conventional von Neumann architectures. Naturally, integrated approaches across the emerging devices and computing architectures/algorithms for data-intensive applications are of great interests. This paper uses the FeFET as a representative device, and discuss about the challenges, opportunities and contributions for the emerging trends of cross-layer co-design for CiM.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {132–139},
numpages = {8},
keywords = {Ferroelectric, content addressable memory, cross-layer design},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@inproceedings{10.1145/3462203.3475914,
author = {Vasconcelos, Dinarte and Yin, Myat Su and Wetjen, Fabian and Herbst, Alexander and Ziemer, Tim and F\"{o}rster, Anna and Barkowsky, Thomas and Nunes, Nuno and Haddawy, Peter},
title = {Counting Mosquitoes in the Wild: An Internet of Things Approach},
year = {2021},
isbn = {9781450384780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462203.3475914},
doi = {10.1145/3462203.3475914},
abstract = {Counting mosquitoes in the wild is a crucial capability for monitoring, prediction, and control of vector-borne diseases. Current approaches are mainly manual, where specially designed mosquito traps or ovitraps are placed in areas of interest and recovered the next day. The counting itself is performed in an entomological laboratory, where individual mosquitoes are classified into species and counted. This process is costly, slow and inefficient. At the same time, mosquito counting is most relevant in tropical and sub-tropical countries, where mosquitoes spread deadly diseases like malaria, yellow fever and dengue fever. Many countries in these regions have relatively weak public health systems and so cannot support large-scale vector counting efforts. In this paper, we present a system architecture and a prototype to count mosquitoes in the wild with an Internet of Things approach. A sensor board is developed to gather audio data, and models are developed to detect, classify, and count mosquito species. Here, we present our prototype and an extensive background study of classifying mosquitoes based on sound recordings and some preliminary results and discussion.},
booktitle = {Proceedings of the Conference on Information Technology for Social Good},
pages = {43–48},
numpages = {6},
keywords = {IoT, Vector-borne diseases, bio-acoustics, dengue fever, malaria, mosquitoes, sensor networks},
location = {Roma, Italy},
series = {GoodIT '21}
}

@inproceedings{10.1145/3410530.3414320,
author = {Hu, Zhizhang and Yu, Tong and Zhang, Yue and Pan, Shijia},
title = {Fine-grained activities recognition with coarse-grained labeled multi-modal data},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414320},
doi = {10.1145/3410530.3414320},
abstract = {Fine-grained human activities recognition focuses on recognizing event- or action-level activities, which enables a new set of Internet-of-Things (IoT) applications such as behavior analysis. Prior work on fine-grained human activities recognition relies on supervised sensing, which makes the fine-grained labeling labor-intensive and difficult to scale up. On the other hand, it is much more practical to collect coarse-grained label at the level of activity of daily living (e.g., cooking, working), especially for real-world IoT systems. In this paper, we present a framework that learns fine-grained human activities recognition with coarse-grained labeled and a small amount of fine-grained labeled multi-modal data. Our system leverages the implicit physical knowledge on the hierarchy of the coarse- and fine-grained labels and conducts data-driven hierarchical learning that take into account the coarse-grained supervised prediction for fine-grained semi-supervised learning. We evaluated our framework and CFR-TSVM algorithm on the data gathered from real-world experiments. Results show that our CFR-TSVM achieved an 81% recognition accuracy over 10 fine-grained activities, which reduces the prediction error of the semi-supervised learning baseline TSVM by half.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {644–649},
numpages = {6},
location = {Virtual Event, Mexico},
series = {UbiComp/ISWC '20 Adjunct}
}

@inproceedings{10.1145/3539597.3575783,
author = {Wang, Zhaonan and Jiang, Renhe and Fan, Zipei and Song, Xuan and Shibasaki, Ryosuke},
title = {Towards an Event-Aware Urban Mobility Prediction System},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3575783},
doi = {10.1145/3539597.3575783},
abstract = {Today, thanks to the rapid developing mobile and sensor networks in IoT (Internet of Things) systems, spatio-temporal big data are being constantly generated. They have brought us a data-driven possibility to sense and understand crowd mobility on a city scale. A fundamental task towards the next-generation mobility services, such as Intelligent Transportation Systems (ITS), Mobility-as-a-Service (MaaS), is spatio-temporal predictive modeling of the geo-sensory signals. There is a recent line of research leveraging deep learning techniques to boost the forecasting performance on such tasks. While simulating the regularity of mobility behaviors (e.g., routines, periodicity) in a more sophisticated way, the existing studies ignore an important part of urban activities, i.e., events. Including holidays, extreme weathers, pandemic, accidents, various urban events happen from time to time and cause non-stationary phenomena, which by nature make the spatio-temporal forecasting task challenging. We thereby envision an event-aware urban mobility prediction model that is capable of fast adapting and making reliable predictions in different scenarios, which is crucial to decision making towards emergency response and urban resilience.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1303–1304},
numpages = {2},
keywords = {emergency management, events, non-stationary phenomena, smart mobility, spatio-temporal forecasting, urban resilience},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {WSDM '23}
}

@inproceedings{10.1145/3465480.3466931,
author = {Moussa, Rim},
title = {Scalable analytics of air quality batches with Apache Spark and Apache Sedona},
year = {2021},
isbn = {9781450385558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465480.3466931},
doi = {10.1145/3465480.3466931},
abstract = {According to the American National Institute of Environmental Health Sciences (NIEHS), air pollutants are harmful to the health of humans and other living beings, and cause damage to the climate and to the ecosystem by polluting lakes, streams, and soils. Recent developments in sensor technology, and Internet of Things (IoT) technologies provide an opportunity to use sensor networks to measure air quality, in real time, at a large number of locations. The adoption and deployment of IoT technologies for sensing air quality raises a challenging research agenda related to big data processing, such as, data analysis, scalable architectures, and algorithms for best managing and processing IoT data at different edges in the IoT ecosystem.In response to the DEBS'2021 contest, we design and implement a scalable solution for comparing previous year and current year air quality indexes for German Cities, as well as the calculus of cities' longest streaks of good air quality. Our solution is designed to be scalable. It's based on primo Apache Spark - an open-source unified analytics engine for large-scale data processing, and secundo Apache Sedona for creating spatial indexes, and performing spatial operations over large-scale spatial data.},
booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
pages = {154–159},
numpages = {6},
keywords = {air quality index, big data, internet of things, time-series analytic},
location = {Virtual Event, Italy},
series = {DEBS '21}
}

@article{10.1145/3402452,
author = {Liu, Liang and Chen, Bo and Ma, Huadong},
title = {SDCN: Sensory Data-Centric Networking for Building the Sensing Layer of IoT},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3402452},
doi = {10.1145/3402452},
abstract = {Building an open global sensing layer is critical for the Internet of Things (IoT). In this article, we present a Sensory Data-Centric Networking (SDCN) architecture for inter-networking two main networked sensing systems in IoT—wireless sensor networks and mobile sensing networks. Specifically, the proposed SDCN is a systematic solution including NDNs for sensor nodes in the Zigbee network, NDNm for mobilephones in the Wi-Fi network, and NDNg for gateways. Considering the sensing requirement of IoT, we first design a novel Spatio-Temporal 16 Tree (ST16T) naming scheme associated with the scope-matching method. Based on the naming scheme, we further propose the related discovery methods, network switching mechanism, forwarding, and routing strategies according to the features of large-scale sensing and resource-constrained environment. A proof-of-concept prototype is implemented and further is deployed on our campus (BUPT) and the Great Wall (Shaanxi, China) for Environment Monitoring Project. Several experiments are conducted on the deployed platform. The experimental results show that SDCN outperforms the state-of-the-arts and gains a great performance improvement in terms of energy consumption, data collection efficiency, memory footprint, and time delay.},
journal = {ACM Trans. Sen. Netw.},
month = {nov},
articleno = {6},
numpages = {25},
keywords = {Information-centric networking, internet of things, wireless sensor network}
}

@article{10.1145/3446937,
author = {Chen, Ning and Qiu, Tie and Daneshmand, Mahmoud and Wu, Dapeng Oliver},
title = {Robust Networking: Dynamic Topology Evolution Learning for Internet of Things},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3446937},
doi = {10.1145/3446937},
abstract = {The Internet of Things (IoT) has been extensively deployed in smart cities. However, with the expanding scale of networking, the failure of some nodes in the network severely affects the communication capacity of IoT applications. Therefore, researchers pay attention to improving communication capacity caused by network failures for applications that require high quality of services (QoS). Furthermore, the robustness of network topology is an important metric to measure the network communication capacity and the ability to resist the cyber-attacks induced by some failed nodes. While some algorithms have been proposed to enhance the robustness of IoT topologies, they are characterized by large computation overhead, and lacking a lightweight topology optimization model. To address this problem, we first propose a novel robustness optimization using evolution learning (ROEL) with a neural network. ROEL dynamically optimizes the IoT topology and intelligently prospects the robust degree in the process of evolutionary optimization. The experimental results demonstrate that ROEL can represent the evolutionary process of IoT topologies, and the prediction accuracy of network robustness is satisfactory with a small error ratio. Our algorithm has a better tolerance capacity in terms of resistance to random attacks and malicious attacks compared with other algorithms.},
journal = {ACM Trans. Sen. Netw.},
month = {jun},
articleno = {28},
numpages = {23},
keywords = {Robustness optimization, Internet of Things, dynamic topology, evolution learning}
}

@inproceedings{10.1145/3585967.3585974,
author = {Xing, Zichen and Yi, Yunhui and He, Xiandeng and Chai, Junwei and Luo, Yuanxinyu and Zhang, Xingcai},
title = {Sum-rate Maximization for RIS-assisted IoT},
year = {2023},
isbn = {9781450398466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585967.3585974},
doi = {10.1145/3585967.3585974},
abstract = {Abstract—With the development of smart devices, Internet of Things(IoT) has requirements for high coverage, high reliability, and low power consumption for wireless communication systems. The emergence of reconfigurable intelligent surfaces(RIS) provides an achievable solution for further development of IoT. RIS consists of passive low-cost components, which can reshaping the wireless channel. Thus it can improve multi-stream transmission gain, enhance edge coverage and realize large-scale Device-to-Device communication. In this paper, we consider RIS-assisted multiple-input single-output(MISO) communication systems, and our goal is to maximize the sum-rate of all IoT receiving devices by jointly designing the beamforming of access points(AP) and the phase shift of RIS elements. For the non-convex problem form, we propose the Improved Elite Genetic Algorithm(IEGA) to obtain a smooth solution of the problem. Numerical results demonstrate the effectiveness of RIS and the proposed joint algorithm for the performance improvement of IoT wireless communication systems.We analyzed the impact of the deployment of RIS and the number of RIS elements on the sum-rate at the receiving devices, which facilitates the balance between the cost and benefit of increasing RIS elements in practical deployments.},
booktitle = {Proceedings of the 2023 10th International Conference on Wireless Communication and Sensor Networks},
pages = {38–43},
numpages = {6},
keywords = {IEGA, IoT, MISO, RIS},
location = {<conf-loc>, <city>Chengdu</city>, <country>China</country>, </conf-loc>},
series = {icWCSN '23}
}

@article{10.1145/3417295,
author = {Huang, Feiran and Li, Chaozhuo and Gao, Boyu and Liu, Yun and Alotaibi, Sattam and Chen, Hao},
title = {Deep Attentive&nbsp;Multimodal Network Representation Learning for Social Media Images},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3417295},
doi = {10.1145/3417295},
abstract = {The analysis for social networks, such as the socially connected Internet of Things, has shown a deep influence of intelligent information processing technology on industrial systems for Smart Cities. The goal of social media representation learning is to learn dense, low-dimensional, and continuous representations for multimodal data within social networks, facilitating many real-world applications. Since social media images are usually accompanied by rich metadata (e.g., textual descriptions, tags, groups, and submitted users), simply modeling the image is not effective to learn the comprehensive information from social media images. In this work, we treat the image and its textual description as multimodal content, and transform other metainformation into the links between contents (such as two images marked by the same tag or submitted by the same user). Based on the multimodal content and social links, we propose a Deep Attentive Multimodal Graph Embedding model named DAMGE for more effective social image representation learning. We introduce both small- and large-scale datasets to conduct extensive experiments, of which the results confirm the superiority of the proposal on the tasks of social image classification and link prediction.},
journal = {ACM Trans. Internet Technol.},
month = {jun},
articleno = {69},
numpages = {17},
keywords = {Social image, graph convolutional network, multimodal, attention network, representation learning}
}

@article{10.1145/3537899,
author = {Salim, Sara and Moustafa, Nour and Turnbull, Benjamin and Razzak, Imran},
title = {Perturbation-enabled Deep Federated Learning for Preserving Internet of Things-based Social Networks},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3537899},
doi = {10.1145/3537899},
abstract = {Federated Learning (FL), as an emerging form of distributed machine learning (ML), can protect participants’ private data from being substantially disclosed to cyber adversaries. It has potential uses in many large-scale, data-rich environments, such as the Internet of Things (IoT), Industrial IoT, Social Media (SM), and the emerging SM 3.0. However, federated learning is susceptible to some forms of data leakage through model inversion attacks. Such attacks occur through the analysis of participants’ uploaded model updates. Model inversion attacks can reveal private data and potentially undermine some critical reasons for employing federated learning paradigms. This article proposes novel differential privacy (DP)-based deep federated learning framework. We theoretically prove that our framework can fulfill DP’s requirements under distinct privacy levels by appropriately adjusting scaled variances of Gaussian noise. We then develop a Differentially Private Data-Level Perturbation (DP-DLP) mechanism to conceal any single data point’s impact on the training phase. Experiments on real-world datasets, specifically the social media 3.0, Iris, and Human Activity Recognition (HAR) datasets, demonstrate that the proposed mechanism can offer high privacy, enhanced utility, and elevated efficiency. Consequently, it simplifies the development of various DP-based FL models with different tradeoff preferences on data utility and privacy levels.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {120},
numpages = {19},
keywords = {Differential privacy, data perturbation, deep federated learning, model inversion attacks, privacy preservation}
}

@article{10.1145/3568113.3568117,
author = {Josephson, Colleen and Shuai, Weitao and Marcano, Gabriel and Pannuto, Pat and Hester, Josiah and Wells, George},
title = {The Future of Clean Computing May Be Dirty},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {2375-0529},
url = {https://doi.org/10.1145/3568113.3568117},
doi = {10.1145/3568113.3568117},
abstract = {The emergence of the Internet of Things and pervasive sensor networks have generated a surge of research in energy scavenging techniques. We know well that harvesting RF, solar, or kinetic energy enables the creation of battery-free devices that can be used where frequent battery changes or dedicated power lines are impractical. One unusual yet ubiquitous source of power is soil (earth itself) - or more accurately, bacterial communities in soil. Microbial fuel cells (MFCs) are electrochemical cells that harness the activities of microbes that naturally occur in soil, wetlands, and wastewater. MFCs have been a topic of research in environmental engineering and microbiology for decades, but are a relatively new topic in electronics design and research. Most low-power electronics have traditionally opted for batteries, RF energy, or solar cells. This is changing, however, as the limitations and costs of these energy sources hamper our ability to deploy useful systems that last for decades in challenging environments. If large-scale, long-term applications like underground infrastructure monitoring, smart farming, and sensing for conservation are to be possible, we must rethink the energy source.},
journal = {GetMobile: Mobile Comp. and Comm.},
month = {oct},
pages = {9–15},
numpages = {7}
}

@article{10.1145/3418501,
author = {Hoseiny, Farooq and Azizi, Sadoon and Shojafar, Mohammad and Tafazolli, Rahim},
title = {Joint QoS-aware and Cost-efficient Task Scheduling for Fog-cloud Resources in a Volunteer Computing System},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3418501},
doi = {10.1145/3418501},
abstract = {Volunteer computing is an Internet-based distributed computing in which volunteers share their extra available resources to manage large-scale tasks. However, computing devices in a Volunteer Computing System (VCS) are highly dynamic and heterogeneous in terms of their processing power, monetary cost, and data transferring latency. To ensure both of the high Quality of Service (QoS) and low cost for different requests, all of the available computing resources must be used efficiently. Task scheduling is an NP-hard problem that is considered as one of the main critical challenges in a heterogeneous VCS. Due to this, in this article, we design two task scheduling algorithms for VCSs, named Min-CCV and Min-V. The main goal of the proposed algorithms is jointly minimizing the computation, communication, and delay violation cost for the Internet of Things (IoT) requests. Our extensive simulation results show that proposed algorithms are able to allocate tasks to volunteer fog/cloud resources more efficiently than the state-of-the-art. Specifically, our algorithms improve the deadline satisfaction task rates around 99.5% and decrease the total cost between 15 to 53% in comparison with the genetic-based algorithm.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {86},
numpages = {21},
keywords = {Volunteer computing, fog computing, cloud computing, task scheduling, quality of service (QoS), cost-efficient}
}

@article{10.1109/TNET.2020.2979372,
author = {Fernandes, Ramon and Marcon, C\'{e}sar and Cataldo, Rodrigo and Sep\'{u}lveda, Johanna},
title = {Using Smart Routing for Secure and Dependable NoC-Based MPSoCs},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2979372},
doi = {10.1109/TNET.2020.2979372},
abstract = {The Internet-of-Things (IoT) boosted the building of computational systems that share computation, communication and storage resources for uncountable types of applications. MultiProcessor System-on-Chip (MPSoC) is a fundamental component of such systems offering large parallelism degree in an ocean of processors and memories connected through one or more Network-on-Chips (NoCs). Therefore, a massive quantity of sensitive information of several applications can share computation and communication resources of the MPSoCs demanding security mechanisms and policies. Besides, the advances of CMOS technologies increases the quantity of static and dynamic faults, requiring a dependable and resilient target architecture, which can be partially fulfilled by an effective and efficient NoC design. This work addresses fault tolerance and security at NoC level with SDR, a routing algorithm that includes the concept of security zones in the MPSoC while providing support for dependable routing avoiding faulty links. The proposed routing algorithm prioritizes communication paths deemed secure in 2D mesh NoCs with deadlock freedom. Experimental results employing realistic workload scenarios based on the NASA Numeric Aerodynamic Simulation (NAS) Parallel Benchmark (NPB) and a fault model for 65nm and 22nm CMOS fabrication technologies demonstrates the scalability, security, and dependability of SDR.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1158–1171},
numpages = {14}
}

@inproceedings{10.1145/3485832.3488007,
author = {Sivakumaran, Pallavi and Blasco, Jorge},
title = {argXtract: Deriving IoT Security Configurations via Automated Static Analysis of Stripped ARM Cortex-M Binaries},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3488007},
doi = {10.1145/3485832.3488007},
abstract = {Recent high-profile attacks on the Internet of Things (IoT) have brought to the forefront the vulnerabilities in “smart” devices, and have revealed poor device configuration to be the root cause in many cases. This has resulted in IoT technologies and devices being subjected to numerous security analyses. For the most part, automated analyses have been confined to IoT hub or gateway devices, which tend to feature traditional operating systems such as Linux or VxWorks. However, most IoT peripherals, by their very nature of being resource-constrained, lacking traditional operating systems, implementing a wide variety of communication technologies, and (increasingly) featuring the ARM Cortex-M architecture, have only been the subject of smaller-scale analyses, typically confined to a certain class or brand of device. We bridge this gap with argXtract, a framework for performing automated static analysis of stripped Cortex-M binaries, to enable bulk extraction of security-relevant configuration data. Through a case study of 200+ Bluetooth Low Energy binaries targeting Nordic Semiconductor chipsets, as well as smaller studies against STMicroelectronics BlueNRG binaries and Nordic ANT binaries, argXtract has discovered widespread security and privacy issues in IoT, including minimal or no protection for data, weakened pairing mechanisms, and potential for device and user tracking.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {861–876},
numpages = {16},
keywords = {ANT, ARM, Bluetooth Low Energy, Cortex-M, Firmware Analysis, IoT, Nordic, STMicroelectronics, Stripped Binaries},
location = {<conf-loc>, <city>Virtual Event</city>, <country>USA</country>, </conf-loc>},
series = {ACSAC '21}
}

@inproceedings{10.1145/3592307.3592333,
author = {Ong, Yi Jun and Durga Kumar, Burra Venkata},
title = {Distributed Internet of Things Load Balancing using Deep Reinforcement Learning},
year = {2023},
isbn = {9798400700002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592307.3592333},
doi = {10.1145/3592307.3592333},
abstract = {The current emerging fifth-generation (5G) system has a significant impact on the usage of Internet of Things (IoT) devices. Load balancing plays an important role in the distributed system, as it is directly associated with the performance of the whole system. Therefore, in this paper, the author recommends a load-balancing architecture for distributed IoT systems based on deep reinforcement learning (DRL), which is capable of dealing with dynamic and large-scale network situations. Specifically, the author recommends implementing a two-layer load balancing architecture. The top layer uses the long short term memory (LSTM) based Dueling Double Deep Q-Learning Network (D3QN) model for clustering the IoT devices. The bottom layer uses the plain DRL with more than one behavior policy on joint exploration. The key objective of this paper is to improve load balancing by using the technique mentioned above, as the data source and the infrastructure of the distributed IoT system can be dynamic. Experiments are conducted by using real-world datasets for evaluating the implementation. The outcome shows that the implementation of DRL on load balancing has indeed achieved a significant improvement in the performance of the distributed IoT system compared to other simplified DRL models and static clustering methods.},
booktitle = {Proceedings of the 2023 6th International Conference on Electronics, Communications and Control Engineering},
pages = {169–173},
numpages = {5},
keywords = {Distributed System, Dueling Double Deep Q-Learning Network, Internet of Things, Load Balancing, Long Short Term Memory, Two Layer Architecture},
location = {<conf-loc>, <city>Fukuoka</city>, <country>Japan</country>, </conf-loc>},
series = {ICECC '23}
}

@inproceedings{10.1145/3629188.3629201,
author = {HOSSAN, SAKHAOUTH and MAHMUD, FARHAN and Roy, Palash and Razzaque, Md. Abdur and Rahman, Md. Mustafizur},
title = {Energy and Latency-aware Computation Load Distribution of Hybrid Split and Federated Learning on IoT Devices},
year = {2023},
isbn = {9798400708787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629188.3629201},
doi = {10.1145/3629188.3629201},
abstract = {Split learning (SL) and Federated Learning (FL) are popular distributed learning frameworks used to increase data privacy and reduce computation loads of Internet of Things (IoT) devices. However, one of the major challenges of distributed learning on IoT devices is determining the portion of computation load to be assigned for the devices compared to the server side. The contributions of the existing works in the literature are either limited by consideration of homogeneous resources available at all IoT devices or by not distributing computation loads among the devices in an efficient way. In this paper, we propose an adaptive clustering-based computation load distribution method for IoT devices, with heterogeneous resource capacities, participating in the model training. The clustering makes the optimal determination of the split point of the learning model, which is scalable even for a large number of devices. The numerical evaluation of the proposed learning model implemented using Python 3.0 and the comparative performance results show that the proposed load distribution policy for the learning models reduces the time by 160 times on average compared to the usual brute force method.},
booktitle = {Proceedings of the 10th International Conference on Networking, Systems and Security},
pages = {61–68},
numpages = {8},
keywords = {Clustering, Federated Learning, Internet of Things, Load distribution, Resource Heterogeneity, Split Learning},
location = {<conf-loc>, <city>Khulna</city>, <country>Bangladesh</country>, </conf-loc>},
series = {NSysS '23}
}

@article{10.1109/TNET.2021.3075468,
author = {Wu, Di and Xu, He and Jiang, Zhongkai and Yu, Weiren and Wei, Xuetao and Lu, Jiwu},
title = {EdgeLSTM: Towards Deep and Sequential Edge Computing for IoT Applications},
year = {2021},
issue_date = {Aug. 2021},
publisher = {IEEE Press},
volume = {29},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3075468},
doi = {10.1109/TNET.2021.3075468},
abstract = {The time series data generated by massive sensors in Internet of Things (IoT) is extremely dynamic, heterogeneous, large scale and time-dependent. It poses great challenges (&lt;italic&gt;e.g.&lt;/italic&gt; accuracy, reliability, stability) on the real-time analysis and decision making for different IoT applications. In this paper, we design, implement and evaluate EdgeLSTM, a unified data-driven system to enhance IoT computing at the network edge. The EdgeLSTM leverages the grid long short-term memory (Grid LSTM) to provide an agile solution for both deep and sequential computation, therefore can address important features such as large-scale, variety, time dependency and real time in IoT data. Our system exploits the advantages of Grid LSTM network and extends it with a multiclass support vector machine by rigorous regularization and optimization approaches, which not only has strong prediction capability of time series data, but also achieves fine-grained multiple classification through the predictive error. We deploy the EdgeLSTM into four IoT applications, including data prediction, anomaly detection, network maintenance and mobility management by extensive experiments. Our evaluation results of real-world time series data with different short-term and long-term time dependency from these typical IoT applications show that our EdgeLSTM system can guarantee robust performance in IoT computing.},
journal = {IEEE/ACM Trans. Netw.},
month = {may},
pages = {1895–1908},
numpages = {14}
}

@inproceedings{10.1145/3412382.3459211,
author = {Chowdhury, Tahiya},
title = {Towards Reducing Labeling Efforts in IoT-based Machine Learning Systems: PhD Forum Abstract},
year = {2021},
isbn = {9781450380980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412382.3459211},
doi = {10.1145/3412382.3459211},
abstract = {The number of Internet-of-Things (IoT) and edge devices has exploded in recent years. Coupled with recent advances in learning methodologies, these can make the vision of smart building a reality and transform how people interact with their environment. Deep learning-driven systems have been already successful in many IoT application areas of pervasive sensing and ubiquitous computing including human activity monitoring and occupancy detection. However, the robustness of such systems at scale is dependent on the availability of a large amount of data labeled by a human expert. Acquiring human annotation involves high effort, high cost, and is often error-prone. Therefore, learning to execute tasks in a machine learning system without manual inspection is of prime interest as it can open up opportunities for continual learning in the long-term deployment of IoT systems. In this work, we propose deep neural network-based approaches for segmentation and classification tasks in sensor streams with no or limited labels. We focus on reducing human labeling efforts in machine learning systems by 1) leveraging temporal features learned through deep neural networks for downstream tasks and 2) learning robust features in dynamic environments by utilizing multiple sensing sources and semi-supervised learning (e.g. small amount of labeled data).},
booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
pages = {416–417},
numpages = {2},
keywords = {Algorithms for sensing, Deep learning, Human-centered computing},
location = {Nashville, TN, USA},
series = {IPSN '21}
}

@inproceedings{10.1145/3528227.3528569,
author = {Lakshman, Shashank Bangalore and Eisty, Nasir U.},
title = {Software engineering approaches for TinyML based IoT embedded vision: a systematic literature review},
year = {2023},
isbn = {9781450393324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528227.3528569},
doi = {10.1145/3528227.3528569},
abstract = {Internet of Things (IoT) has catapulted human ability to control our environments through ubiquitous sensing, communication, computation, and actuation. Over the past few years, IoT has joined forces with Machine Learning (ML) to embed deep intelligence at the far edge. TinyML (Tiny Machine Learning) has enabled the deployment of ML models for embedded vision on extremely lean edge hardware, bringing the power of IoT and ML together. However, TinyML powered embedded vision applications are still in a nascent stage, and they are just starting to scale to widespread real-world IoT deployment. To harness the true potential of IoT and ML, it is necessary to provide product developers with robust, easy-to-use software engineering (SE) frameworks and best practices that are customized for the unique challenges faced in TinyML engineering. Through this systematic literature review, we aggregated the key challenges reported by TinyML developers and identified state-of-art SE approaches in large-scale Computer Vision, Machine Learning, and Embedded Systems that can help address key challenges in TinyML based IoT embedded vision. In summary, our study draws synergies between SE expertise that embedded systems developers and ML developers have independently developed to help address the unique challenges in the engineering of TinyML based IoT embedded vision.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering Research and Practice for the IoT},
pages = {33–40},
numpages = {8},
keywords = {IoT, TinyML, embedded vision, software engineering, systematic literature review},
location = {Pittsburgh, Pennsylvania},
series = {SERP4IoT '22}
}

@inproceedings{10.1145/3445814.3446723,
author = {Ranganathan, Parthasarathy and Stodolsky, Daniel and Calow, Jeff and Dorfman, Jeremy and Guevara, Marisabel and Smullen IV, Clinton Wills and Kuusela, Aki and Balasubramanian, Raghu and Bhatia, Sandeep and Chauhan, Prakash and Cheung, Anna and Chong, In Suk and Dasharathi, Niranjani and Feng, Jia and Fosco, Brian and Foss, Samuel and Gelb, Ben and Gwin, Sara J. and Hase, Yoshiaki and He, Da-ke and Ho, C. Richard and Huffman Jr., Roy W. and Indupalli, Elisha and Jayaram, Indira and Kongetira, Poonacha and Kyaw, Cho Mon and Laursen, Aaron and Li, Yuan and Lou, Fong and Lucke, Kyle A. and Maaninen, JP and Macias, Ramon and Mahony, Maire and Munday, David Alexander and Muroor, Srikanth and Penukonda, Narayana and Perkins-Argueta, Eric and Persaud, Devin and Ramirez, Alex and Rautio, Ville-Mikko and Ripley, Yolanda and Salek, Amir and Sekar, Sathish and Sokolov, Sergey N. and Springer, Rob and Stark, Don and Tan, Mercedes and Wachsler, Mark S. and Walton, Andrew C. and Wickeraad, David A. and Wijaya, Alvin and Wu, Hon Kwan},
title = {Warehouse-scale video acceleration: co-design and deployment in the wild},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446723},
doi = {10.1145/3445814.3446723},
abstract = {Video sharing (e.g., YouTube, Vimeo, Facebook, TikTok) accounts for the majority of internet traffic, and video processing is also foundational to several other key workloads (video conferencing, virtual/augmented reality, cloud gaming, video in Internet-of-Things devices, etc.). The importance of these workloads motivates larger video processing infrastructures and – with the slowing of Moore’s law – specialized hardware accelerators to deliver more computing at higher efficiencies. This paper describes the design and deployment, at scale, of a new accelerator targeted at warehouse-scale video transcoding. We present our hardware design including a new accelerator building block – the video coding unit (VCU) – and discuss key design trade-offs for balanced systems at data center scale and co-designing accelerators with large-scale distributed software systems. We evaluate these accelerators “in the wild" serving live data center jobs, demonstrating 20-33x improved efficiency over our prior well-tuned non-accelerated baseline. Our design also enables effective adaptation to changing bottlenecks and improved failure management, and new workload capabilities not otherwise possible with prior systems. To the best of our knowledge, this is the first work to discuss video acceleration at scale in large warehouse-scale environments.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {600–615},
numpages = {16},
keywords = {domain-specific accelerators, hardware-software codesign, video transcoding, warehouse-scale computing},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{10.1145/3437959.3459254,
author = {Clemen, Thomas and Ahmady-Moghaddam, Nima and Lenfers, Ulfia A. and Ocker, Florian and Osterholz, Daniel and Str\"{o}bele, Jonathan and Glake, Daniel},
title = {Multi-Agent Systems and Digital Twins for Smarter Cities},
year = {2021},
isbn = {9781450382960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437959.3459254},
doi = {10.1145/3437959.3459254},
abstract = {An intelligent combination of the Internet of Things (IoT) and approaches to modeling and simulation is one of the most challenging endeavors for future cities, manufacturing industries, and predictive maintenance. Digital Twins take on a unique role here. However, the question of what a Digital Twin is and what differentiates it from a regular model is still open. We present an experimental setup for integrating an existing simulation model of Hamburg's traffic system with the city's real-time sensor network. The Digital Twin is implemented using the large-scale multi-agent framework MARS. The entire process from the model description to retrieving real-time data from the IoT sensors and incorporating it in the simulation is presented. As a first prototypical example, a multi-modal mobility model was connected to real-world bike-sharing locations in Hamburg. We find that the combination of multi-agent systems and IoT sensors as a Digital Twin shows enormous potential for city planners, policy stakeholders, and other decision-makers. By correcting the course of a simulation via real-time data, the corridor-of-uncertainty that is intrinsic to some simulation models' use can be reduced significantly. Furthermore, any divergence of simulated and sampled data can lead to a deeper understanding of complex adaptive systems like big cities.},
booktitle = {Proceedings of the 2021 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {45–55},
numpages = {11},
keywords = {digital twins, internet of things, multi-agent systems},
location = {Virtual Event, USA},
series = {SIGSIM-PADS '21}
}

@inproceedings{10.1145/3616388.3617540,
author = {Gopalakrishnan, Sathish and Sherif, Yousef},
title = {Djenne: Dependable and Decentralized Computation for Networked Embedded Systems},
year = {2023},
isbn = {9798400703669},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616388.3617540},
doi = {10.1145/3616388.3617540},
abstract = {How should we build applications for large-scale networked embedded systems -- now in the incarnation of the Internet of Things -- when we do not want to rely on the existence of a persistent connection to a remote data center? We present the design and implementation of a system, which we call Djenne, that can aggregate the computational power of the distributed devices because the increased capacity of these devices does allow for substantial work closer to the devices. The challenge that we overcome with Djenne is dependability: how can we cope with failures and the dynamics of wireless network links in such systems? Our design uses the actor model of computation and relies on replicated services to improve reliability and to create opportunities for parallelism that increase task throughput. The key innovations in our work are the use of adaptive mechanisms for rerouting data when system conditions change significantly as well as a holistic recovery approach when computations need to be repeated in the distributed system. Via experimental evaluation, we find that Djenne can improve throughput by 30% to 190% for different use cases while ensuring resilience in the face of intermittent failures.},
booktitle = {Proceedings of the Int'l ACM Conference on Modeling Analysis and Simulation of Wireless and Mobile Systems},
pages = {243–252},
numpages = {10},
keywords = {dependability, distributed computing, embedded systems},
location = {<conf-loc>, <city>Montreal</city>, <state>Quebec</state>, <country>Canada</country>, </conf-loc>},
series = {MSWiM '23}
}

@inproceedings{10.1145/3617184.3630150,
author = {Zhou, Hao and Sheng, Zhiwei},
title = {A Federated Learning based Botnet Detection Method for Industrial Internet of Things},
year = {2023},
isbn = {9798400708800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617184.3630150},
doi = {10.1145/3617184.3630150},
abstract = {Botnets in the Industrial Internet of Things (IIoT) significantly threaten system security. Currently, mainstream machine learning-based botnet detection techniques rely on centralized large-scale data training; however, this approach neglects privacy protection and data security issues. In addition, the complexity and heterogeneity of IIoT make the detection model unable to adapt to different industrial enterprises. In this paper, we propose a federated learning-based botnet detection approach for IIoT, where multiple heterogeneous industrial enterprises can train models using local data under the remote coordination of a centralized server, and then upload the model parameters to the central server to complete the aggregation. This approach allows the botnet detection model to better adapt to the local environment while avoiding sharing raw data. Meanwhile, the method is robust to federated learning poisoning attacks. In this paper, we have conducted tests using the N-BaIoT dataset. Our method achieves 99.63% of the F1 value and 99.26% of the MCC value on a new device that is not involved in the training. The detection performance is almost comparable to that of the centrally trained method, and outperforms other similar methods.},
booktitle = {Proceedings of the 8th International Conference on Cyber Security and Information Engineering},
pages = {282–288},
numpages = {7},
keywords = {Botnets, Federated Learning, Industrial Internet of Things, Malicious Detection, Privacy Protection},
location = {<conf-loc>, <city>Putrajaya</city>, <country>Malaysia</country>, </conf-loc>},
series = {ICCSIE '23}
}

@article{10.1145/3432312,
author = {Wang, Beilun and Zhang, Jiaqi and Zhang, Yan and Wang, Meng and Wang, Sen},
title = {Scalable Estimator for Multi-task Gaussian Graphical Models Based in an IoT Network},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3432312},
doi = {10.1145/3432312},
abstract = {Recently, the Internet of Things (IoT) receives significant interest due to its rapid development. But IoT applications still face two challenges: heterogeneity and large scale of IoT data. Therefore, how to efficiently integrate and process these complicated data becomes an essential problem. In this article, we focus on the problem that analyzing variable dependencies of data collected from different edge devices in the IoT network. Because data from different devices are heterogeneous and the variable dependencies can be characterized into a graphical model, we can focus on the problem that jointly estimating multiple, high-dimensional, and sparse Gaussian Graphical Models for many related tasks (edge devices). This is an important goal in many fields. Many IoT networks have collected massive multi-task data and require the analysis of heterogeneous data in many scenarios. Past works on the joint estimation are non-distributed and involve computationally expensive and complex non-smooth optimizations. To address these problems, we propose a novel approach: Multi-FST. Multi-FST can be efficiently implemented on a cloud-server-based IoT network. The cloud server has a low computational load and IoT devices use asynchronous communication with the server, leading to efficiency. Multi-FST shows significant improvement, over baselines, when tested on various datasets.},
journal = {ACM Trans. Sen. Netw.},
month = {jun},
articleno = {23},
numpages = {33},
keywords = {Big data, internet of things, multi-task learning}
}

@article{10.1145/3546951,
author = {Xie, Ying and Liu, Xiaohui and Obaidat, Mohammad S. and Li, Xiong and Vijayakumar, Pandi},
title = {Nondeterministic Evaluation Mechanism for User Recruitment in Mobile Crowd-Sensing},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3546951},
doi = {10.1145/3546951},
abstract = {Based on the Internet of Behavior (IoB), mobile crowd-sensing (MCS) utilizes the Internet of Things (IoT) to recruit users by analyzing behavioral patterns. MCS is widely used in numerous large-scale and complex monitoring services, but it cannot provide stable and high-quality services due to nondeterministic user mobility and behaviors, which has a vital impact on recruiting high-quality users. In this article, a stochastic semi-algebraic hybrid system (SSAHS) model is constructed to characterize the user mobility and behaviors of the MCS systems. Based on the definition of probabilistic path and task execution rate, a nondeterministic evaluation mechanism is proposed to measure nondeterministic user mobility and behaviors and to give the probability of the user completing the MCS task under the specified time bound and space conditions. The greater the probability is, the higher the quality of the user. Furthermore, a user recruitment scheme based on a nondeterministic evaluation mechanism (NUR) is developed. The NUR employs historical user data to predict user mobility and behaviors; high-quality users are recruited to quickly upload reliable sensing data. We conduct simulation experiments based on a real-world user trace dataset, Geolife.The results show that compared with competing recruitment strategies, NUR achieves a higher quality of service for the same MCS sensing tasks.},
journal = {ACM Trans. Sen. Netw.},
month = {apr},
articleno = {34},
numpages = {18},
keywords = {Internet of Behavior, mobile crowd-sensing, stochastic semi-algebraic hybrid system, user recruitment, nondeterministic evaluation}
}

@inproceedings{10.5555/3578948.3578964,
author = {Rottleuthner, Michel and Schmidt, Thomas C and Wahlisch, Matthias},
title = {Dynamic Clock Reconfiguration for the Constrained IoT and its Application to Energy-efficient Networking},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Clock configuration takes a key role in tuning constrained general-purpose microcontrollers for performance, timing accuracy, and energy efficiency. Configuring the underlying clock tree, however, involves a large parameter space with complex dependencies and dynamic constraints. We argue for clock configuration as a generic operating system module that bridges the gap between highly configurable but complex embedded hardware and easy application development. In this paper, we propose a method and a runtime subsystem for dynamic clock reconfiguration on constrained Internet of Things (IoT) devices named ScaleClock. ScaleClock derives measures to dynamically optimize clock configurations by abstracting the hardware-specific clock trees. The ScaleClock system service grants portable access to the optimization potential of dynamic clock scaling for applications. We implement the approach on the popular IoT operating system RIOT for two target platforms of different manufacturers and evaluate its performance in static and dynamic scenarios on real devices. We demonstrate the potential of ScaleClock by designing a platform-independent dynamic voltage and frequency scaling (DVFS) mechanism that enables RIOT to autonomously adapt the hardware performance to requirements of the software currently executed. In a use case study, we manage to boost energy efficiency of constrained network communication by reducing the MCU consumption by 40 % at negligible performance impact.},
booktitle = {Proceedings of the 2022 International Conference on Embedded Wireless Systems and Networks},
pages = {168–179},
numpages = {12},
keywords = {Management Keywords Embedded Systems, Energy, DVFS},
location = {<conf-loc>, <city>Linz</city>, <country>Austria</country>, </conf-loc>},
series = {EWSN '22}
}

@article{10.1145/3630614.3630617,
author = {Wang, Wenpeng and Sobral, Victor A. Leal and Billah, Md Fazlay Rabbi Masum and Saoda, Nurani and Nasir, Nabeel and Campbell, Bradford},
title = {Low Power but High Energy: The Looming Costs of Billions of Smart Devices},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3630614.3630617},
doi = {10.1145/3630614.3630617},
abstract = {Individually, wall-powered Internet of Things devices are small: in form factor, in complexity, in function, and in power draw. However, at scale, and certainly at the scale optimistic forecasters project, these small devices add up to be a big energy problem. Just adding a single two watt sensor to each US building would add to more annual energy consumption than some small countries. Wall-powered IoT devices are also easier to create than their energy-constrained (i.e. battery-powered) counterparts, and marketed as more convenient (no hub required!), leading to their continued growth. Yet, unlike other energy consuming devices, there are no Energy Star (or equivalent) standards for smart devices. Despite having very infrequent active times, they draw power for functions like AC-DC conversion, wireless communication, and wakeup word detection continuously. Further, the discrete nature of devices and siloed nature of IoT ecosystems leads to significant redundancy in IoT devices.We posit that new techniques are needed to reverse this trend. This includes new techniques for auditing devices, systems that leverage existing devices rather than requiring new ones, and architectures that have less reliance on the cloud (and the energy overhead of network usage and cloud compute). The IoT is pitched to improve energy efficiency and reduce users' carbon footprints, but we need a new research agenda to ensure the devices themselves are not the next problem.},
journal = {SIGENERGY Energy Inform. Rev.},
month = {oct},
pages = {10–14},
numpages = {5},
keywords = {carbon footprint, energy consumption, internet of things, smart devices}
}

@article{10.1145/3463575,
author = {Yan, Qiben and Lou, Jianzhi and Vuran, Mehmet C. and Irmak, Suat},
title = {Scalable Privacy-preserving Geo-distance Evaluation for Precision Agriculture IoT Systems},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3463575},
doi = {10.1145/3463575},
abstract = {Precision agriculture has become a promising paradigm to transform modern agriculture. The recent revolution in big data and Internet-of-Things (IoT) provides unprecedented benefits including optimizing yield, minimizing environmental impact, and reducing cost. However, the mass collection of farm data in IoT applications raises serious concerns about potential privacy leakage that may harm the farmers’ welfare. In this work, we propose a novel scalable and private geo-distance evaluation system, called SPRIDE, to allow application servers to provide geographic-based services by computing the distances among sensors and farms privately. The servers determine the distances without learning any additional information about their locations. The key idea of SPRIDE is to perform efficient distance measurement and distance comparison on encrypted locations over a sphere by leveraging a homomorphic cryptosystem. To serve a large user base, we further propose SPRIDE+ with novel and practical performance enhancements based on pre-computation of cryptographic elements. Through extensive experiments using real-world datasets, we show SPRIDE+ achieves private distance evaluation on a large network of farms, attaining 3+ times runtime performance improvement over existing techniques. We further show SPRIDE+ can run on resource-constrained mobile devices, which offers a practical solution for privacy-preserving precision agriculture IoT applications.},
journal = {ACM Trans. Sen. Netw.},
month = {jul},
articleno = {38},
numpages = {30},
keywords = {Privacy-preserving data analysis, distance evaluation, precision agriculture, IoT}
}

@article{10.5555/3586589.3586917,
author = {Swenson, Brian and Murray, Ryan and Poor, H. Vincent and Kar, Soummya},
title = {Distributed stochastic gradient descent: nonconvexity, nonsmoothness, and convergence to local minima},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Gradient-descent (GD) based algorithms are an indispensable tool for optimizing modern machine learning models. The paper considers distributed stochastic GD (D-SGD)--a network-based variant of GD. Distributed algorithms play an important role in large-scale machine learning problems as well as the Internet of Things (IoT) and related applications. The paper considers two main issues. First, we study convergence of D-SGD to critical points when the loss function is nonconvex and nonsmooth. We consider a broad range of nonsmooth loss functions including those of practical interest in modern deep learning. It is shown that, for each fixed initialization, D-SGD converges to critical points of the loss with probability one. Next, we consider the problem of avoiding saddle points. It is well known that classical GD avoids saddle points; however, analogous results have been absent for distributed variants of GD. For this problem, we again assume that loss functions may be nonconvex and nonsmooth, but are smooth in a neighborhood of a saddle point. It is shown that, for any fixed initialization, D-SGD avoids such saddle points with probability one. Results are proved by studying the underlying (distributed) gradient flow, using the ordinary differential equation (ODE) method of stochastic approximation.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {328},
numpages = {62},
keywords = {nonconvex optimization, distributed optimization, stochastic optimization, saddle point, gradient descent}
}

@inproceedings{10.1145/3583120.3589564,
author = {Hekmati, Arvin},
title = {PhD Forum Abstract: DDoS attack detection in IoT systems using Neural Networks},
year = {2023},
isbn = {9798400701184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583120.3589564},
doi = {10.1145/3583120.3589564},
abstract = {This short paper summarizes our recent/ongoing works [2, 3, 4] on detecting DDoS attacks in IoT systems. In our studies, we conducted a thorough examination of using machine learning to detect Distributed Denial of Service (DDoS) attacks in large-scale Internet of Things (IoT) systems. Unlike prior works and typical DDoS attacks that focus on individual nodes transmitting high volumes of packets, we explored the more sophisticated and advanced future attacks that use a large number of IoT devices while hiding the attack by having each node transmit at a volume that mimics benign traffic. We introduced innovative correlation-aware architectures that consider the correlation between the traffic of IoT nodes and compare the effectiveness of centralized and distributed detection models. Through extensive analysis, we evaluated the proposed architectures using five different neural network models trained on a real-world IoT dataset of 4060 nodes. Our results showed that the combination of long short-term memory (LSTM) and transformer-based models with the correlation-aware architectures offer superior performance, in terms of F1 score and binary accuracy, compared to the other models and architectures, especially when the attacker conceals its actions by following benign traffic distribution on each transmitting node. Furthermore, we investigated the performance of heuristics for selecting a subset of nodes to share their data in resource-constrained scenarios for correlation-aware architectures.},
booktitle = {Proceedings of the 22nd International Conference on Information Processing in Sensor Networks},
pages = {340–341},
numpages = {2},
keywords = {DDoS attack, IoT, dataset, neural networks},
location = {San Antonio, TX, USA},
series = {IPSN '23}
}

@inproceedings{10.1145/3372224.3419182,
author = {Li, Songfan and Zhang, Chong and Song, Yihang and Zheng, Hui and Liu, Lu and Lu, Li and Li, Mo},
title = {Internet-of-microchips: direct radio-to-bus communication with SPI backscatter},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419182},
doi = {10.1145/3372224.3419182},
abstract = {Energy consumption of Internet-of-Things end devices is a major constraint that limits their long-term and large-scale deployment. Conventionally, the radios and processors used in these end devices are major power consumption that drains at the level of milliwatts (mWs). However, in recent decades, backscatter communication has dramatically reduced the power consumed by the radios in end devices to microwatts (μWs), and thus the processor remains the major bottleneck for energy optimization.In this paper, we propose a processor-free architecture as a novel design that allows the radio to interface directly with peripheral sensor chips for control and data collection, thereby separating the processors from the end device design to significantly reduce the energy consumed by end devices. The main problem is that the peripheral chips are designed to be accessed by the processor via a standard digital bus and they cannot communicate directly with the radio. In order to support such processor-free design, we propose radio-to-bus (R2B) as a novel communication paradigm that allows direct data exchange between a backscatter radio and the serial peripheral interface (SPI) bus. We implement the processor-free architecture in proof-of-concept prototypes and demonstrate that the power consumption decreases by 4.5 times compared with the conventional end device design.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {25},
numpages = {14},
keywords = {SPI, backscatter, internet of things},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@article{10.1145/3511902,
author = {Wang, Jin and Chen, Jiahao and Xiong, Neal and Alfarraj, Osama and Tolba, Amr and Ren, Yongjun},
title = {S-BDS: An Effective Blockchain-based Data Storage Scheme in Zero-Trust IoT},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3511902},
doi = {10.1145/3511902},
abstract = {With the development of the Internet of Things (IoT), a large-scale, heterogeneous, and dynamic distributed network has been formed among IoT devices. There is an extreme need to establish a trust mechanism between devices, and blockchain can provide a zero-trust security framework for IoT. However, the efficiency of the blockchain is far from meeting the application requirements of the IoT, which has become the biggest resistance to the application of the blockchain in the IoT. Therefore, this paper combines sharding to build an effective Blockchain-based IoT data storage scheme (S-BDS). Sharding can solve the problem of blockchain capacity and scalability. While the blockchain provides data immutability and traceability for the IoT, it also brings huge demands for data credibility verification. The communication delay in the IoT system seriously affects the security of the system, while the Merkle proof of traditional blockchain occupies a lot of communication resources. This paper constructs Insertable Vector Commitment (IVC) in the bilinear group and replaces the Merkle tree with IVC to store IoT data in the blockchain. The construct has small-sized proof. It also has the ability to record the number of updates, which can prevent replay-attacks. Experiments show that each block processes 1,000 transactions, the proof size of a single data piece is 30% of the original scheme, and proofs from different shards can be aggregated. IVC can effectively reduce communication congestion and improve the stability and security of the IoT system.},
journal = {ACM Trans. Internet Technol.},
month = {aug},
articleno = {42},
numpages = {23},
keywords = {Blockchain, zero-trust, Internet of Things, date storage, cryptographic commitment}
}

@inproceedings{10.1145/3432291.3432293,
author = {Ma, Wenting and Zhang, Zhipeng and Xu, Qingqing and Chen, Wai},
title = {An Automatic Scheme for Optimizing the Size of Deep Networks},
year = {2020},
isbn = {9781450375733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3432291.3432293},
doi = {10.1145/3432291.3432293},
abstract = {Large-scale datasets and complex architectures promote the development of CNN models. Although with stronger representation power, larger CNNs are more resource-hungry, which makes it difficult to deploy on resource-constrained Internet of Things (IoT) devices. Another serious challenge occurring with larger CNNs is their susceptibility to overfit with a small training dataset. In this paper, we ask the question: can we find an optimized compact model for a particular data set? We propose a novel scheme to optimize the model size so as to obtain a compact model instead of a larger model. The optimized model achieves higher accuracy than the widely used deeper model. In addition, it decreases the run-time memory and reduces the number of computing operations. This is achieved by applying Minimum Description Length (MDL) to find the optimal size of the model for a particular data set mathematically. MDL--the information-theoretic model selection principle assumes that the simplest, most compact representation model is the best model and most probable explanation of the data. We call our approach OptSize, model size is automatically identified, yielding compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet on various image classification datasets. The result shows that compact nets obtained by our proposed method perform better to complex, well-engineered, deeper convolutional architectures.},
booktitle = {Proceedings of the 2020 3rd International Conference on Signal Processing and Machine Learning},
pages = {21–27},
numpages = {7},
keywords = {Deep Learning, Minimum Description Length (MDL), Model Selection},
location = {Beijing, China},
series = {SPML '20}
}

@inproceedings{10.1145/3486611.3486646,
author = {Wang, Wenpeng and Liu, Zetian and Gao, Jiechao and Saoda, Nurani and Campbell, Bradford},
title = {UbiTrack: enabling scalable &amp; low-cost device localization with onboard wifi},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3486646},
doi = {10.1145/3486611.3486646},
abstract = {Wireless sensing and the Internet of Things support real-time monitoring and data-driven control of the built environment, enabling more sustainable and responsive infrastructure. As buildings and physical structures tend to be large and complex, instrumenting them to support a wide range of applications often requires numerous sensors distributed over a large area. One impediment to this type of large-scale sensing is simply tracking where exactly devices are over time, as the physical infrastructure is updated and interacted with over time. Having low-cost but accurate localization for devices (instead of users) would enable scalable IoT network management, but current localization approaches do not provide a suitable tradeoff in terms of cost, energy, and accuracy for low power devices in unknown environments.We propose UbiTrack, a low-cost indoor positioning system that enables accurate tracking for single antenna commodity WiFi devices, without the need for a complex antenna array. UbiTrack leverages two-way channel state information (CSI) across all WiFi channels to measure the distance between nodes, and uses a new probabilistic localization algorithm based on Bayesian estimation to locate each device. We demonstrate the system on commodity $4.00 ESP32 WiFi chips and realize 1-meter level position accuracy in an indoor environment. This approach provides localization for everyday IoT devices, enabling more scalable deployments and new IoT applications.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {11–20},
numpages = {10},
keywords = {bayesian estimation, indoor localization, smart IoT devices, wifi channel state information (CSI)},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3419604.3419761,
author = {Serhani, Abdellatif and Naja, Najib and Jamali, Abdellah},
title = {ARG-RPL: Arrangement Graph-, Region-Based Routing Protocol for Internet of Things},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419761},
doi = {10.1145/3419604.3419761},
abstract = {Internet of Things, is an innovative technology which allows the connection of physical things with the digital world through the use of heterogeneous networks and communication technologies. The Routing Protocol for low power and Lossy networks (RPL) is standardized as a routing protocol for LLNs. However, more and more of experimental results demonstrate that RPL performs poorly in throughput and adaptability to network dynamics. In this study, we applied properties of arrangement graphs to design a newly structured routing protocol, extension of RPL, named as Arrangement Graph based Adaptive routing protocol ARG-RPL that enhances the supports of high throughput, adaptivity and mobility for RPL without any modification or assumption on the OFs. In such protocol, the IDs between the two adjacent nodes differ only one digit and thus, self configuration and self optimization in LLNs networks are easy while keeping the low maintenance cost. Distributed algorithms have been developed, consisting of two stages: initialization stage and reactive routing discovery stage. We implement ARG-RPL on the Contiki operating system, and construct extensive evaluation using a large-scale simulations on Cooja. Analysis of experimental results show that the establishment of the system and the routing processing could achieve better performance than those obtained in the RPL and ER-RPL.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {44},
numpages = {8},
keywords = {Arrangement Graph, IPv6, Internet of Things, RPL},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1145/3372278.3391936,
author = {Aslam, Asra},
title = {Object Detection for Unseen Domains while Reducing Response Time using Knowledge Transfer in Multimedia Event Processing},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3391936},
doi = {10.1145/3372278.3391936},
abstract = {Event recognition is among one of the popular areas of smart cities that has attracted great attention for researchers. Since Internet of Things (IoT) is mainly focused on scalar data events, research is shifting towards the Internet of Multimedia Things (IoMT) and is still in infancy. Presently multimedia event-based solutions provide low response-time, but they are domain-specific and can handle only familiar classes (bounded vocabulary). However multiple applications within smart cities may require processing of numerous familiar as well as unseen concepts (unbounded vocabulary) in the form of subscriptions. Deep neural network-based techniques are popular for image recognition, but have the limitation of training of classifiers for unseen concepts as well as the requirement of annotated bounding boxes with images. In this work, we explore the problem of training of classifiers for unseen/unknown classes while reducing response-time of multimedia event processing (specifically object detection). We proposed two domain adaptation based models while leveraging Transfer Learning (TL) and Large Scale Detection through Adaptation (LSDA). The preliminary results show that proposed framework can achieve 0.5 mAP (mean Average Precision) within 30 min of response-time for unseen concepts. We expect to improve it further using modified LSDA while applying fastest classification (MobileNet) and detection (YOLOv3) network, along with elimination of requirement of annotated bounding boxes.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {373–377},
numpages = {5},
keywords = {domain adaptation, event-based systems, internet of multimedia things, object detection, smart cities, transfer learning},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@article{10.1109/TNET.2021.3085031,
author = {Yu, Tianqi and Wang, Xianbin and Hu, Jianling},
title = {A Fast Hierarchical Physical Topology Update Scheme for Edge-Cloud Collaborative IoT Systems},
year = {2021},
issue_date = {Oct. 2021},
publisher = {IEEE Press},
volume = {29},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3085031},
doi = {10.1109/TNET.2021.3085031},
abstract = {The awareness of physical network topology in a large-scale Internet of Things (IoT) system is critical to enable location-based service provisioning and performance optimization. However, due to the dynamics and complexity of IoT networks, it is usually very difficult to discover and update the physical topology of the large-scale IoT systems in real-time. Considering the stringent latency requirements in IoT systems, while the initial processing time for topology discovery can be tolerated, latency due to real-time topology update constitutes an even higher level of challenge. In this paper, a novel fast hierarchical topology update scheme is proposed for the large-scale IoT systems enabled by using the edge-cloud collaborative architecture. Specifically, an event-driven neighbor update algorithm, termed as TriggerOn, is firstly developed to update the local neighbor table of the end devices when device association or disassociation occurs. Based on the updated neighbor tables, the physical topology update of the subnet is conducted at the coordinated edge device, where a hybrid multidimensional scaling (MDS) based 3D localization algorithm is developed to locate the newly associated devices. Simulation results have indicated that as compared to the benchmark methods, the neighbor discovery latency has been reduced dramatically, and the 3D localization accuracy has been improved. Furthermore, the overall latency incurred by the proposed hierarchical physical topology update scheme is significantly lower than the distributed consensus-based update scheme, especially for the large-scale IoT subnets.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {2254–2266},
numpages = {13}
}

@article{10.1145/3383685,
author = {Goldstein, Orpaz and Kachuee, Mohammad and Karkkainen, Kimmo and Sarrafzadeh, Majid},
title = {Target-Focused Feature Selection Using Uncertainty Measurements in Healthcare Data},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3383685},
doi = {10.1145/3383685},
abstract = {Healthcare big data remains under-utilized due to various incompatibility issues between the domains of data analytics and healthcare. The lack of generalizable iterative feature acquisition methods under budget and machine learning models that allow reasoning with a model’s uncertainty are two examples. Meanwhile, a boost to the available data is currently under way with the rapid growth in the Internet of Things applications and personalized healthcare. For the healthcare domain to be able to adopt models that take advantage of this big data, machine learning models should be coupled with more informative, germane feature acquisition methods, consequently adding robustness to the model’s results. We introduce an approach to feature selection that is based on Bayesian learning, allowing us to report the level of uncertainty in the model, combined with false-positive and false-negative rates. In addition, measuring target-specific uncertainty lifts the restriction on feature selection being target agnostic, allowing for feature acquisition based on a target of focus. We show that acquiring features for a specific target is at least as good as deep learning feature selection methods and common linear feature selection approaches for small non-sparse datasets, and surpasses these when faced with real-world data that is larger in scale and sparseness.},
journal = {ACM Trans. Comput. Healthcare},
month = {may},
articleno = {15},
numpages = {17},
keywords = {Bayesian learning, Healthcare feature selection, health informatics, healthcare big data, machine learning, machine learning for health}
}

@article{10.1145/3390605,
author = {Ismail, Leila and Materwala, Huned},
title = {Computing Server Power Modeling in a Data Center: Survey, Taxonomy, and Performance Evaluation},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3390605},
doi = {10.1145/3390605},
abstract = {Data centers are large-scale, energy-hungry infrastructure serving the increasing computational demands as the world is becoming more connected in smart cities. The emergence of advanced technologies such as cloud-based services, internet of things (IoT), and big data analytics has augmented the growth of global data centers, leading to high energy consumption. This upsurge in energy consumption of the data centers not only incurs the issue of surging high cost (operational and maintenance) but also has an adverse effect on the environment. Dynamic power management in a data center environment requires the cognizance of the correlation between the system and hardware-level performance counters and the power consumption. Power consumption modeling exhibits this correlation and is crucial in designing energy-efficient optimization strategies based on resource utilization. Several works in power modeling are proposed and used in the literature. However, these power models have been evaluated using different benchmarking applications, power-measurement techniques, and error-calculation formulas on different machines. In this work, we present a taxonomy and evaluation of 24 software-based power models using a unified environment, benchmarking applications, power-measurement techniques, and error formulas, with the aim of achieving an objective comparison. We use different server architectures to assess the impact of heterogeneity on the models’ comparison. The performance analysis of these models is elaborated in the article.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {58},
numpages = {34},
keywords = {Data center, energy-efficiency, green computing, machine learning, resource utilization, server power consumption modeling}
}

@article{10.1145/3448414,
author = {Liu, Meng and Hu, Hongsheng and Xiang, Haolong and Yang, Chi and Lyu, Lingjuan and Zhang, Xuyun},
title = {Clustering-based Efficient Privacy-preserving Face Recognition Scheme without Compromising Accuracy},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3448414},
doi = {10.1145/3448414},
abstract = {Recently, biometric identification has been extensively used for border control. Some face recognition systems have been designed based on Internet of Things. But the rich personal information contained in face images can cause severe privacy breach and abuse issues during the process of identification if a biometric system has compromised by insiders or external security attacks. Encrypting the query face image is the state-of-the-art solution to protect an individual’s privacy but incurs huge computational cost and poses a big challenge on time-critical identification applications. However, due to their high computational complexity, existing methods fail to handle large-scale biometric repositories where a target face is searched. In this article, we propose an efficient privacy-preserving face recognition scheme based on clustering. Concretely, our approach innovatively matches an encrypted face query against clustered faces in the repository to save computational cost while guaranteeing identification accuracy via a novel multi-matching scheme. To the best of our knowledge, our scheme is the first to reduce the computational complexity from O(M) in existing methods to approximate O(√M), where M is the size of a face repository. Extensive experiments on real-world datasets have shown the effectiveness and efficiency of our scheme.},
journal = {ACM Trans. Sen. Netw.},
month = {jun},
articleno = {31},
numpages = {27},
keywords = {Secure two-party computation, face recognition, privacy-preserving, clustering, computational complexity}
}

@inproceedings{10.1145/3626641.3626936,
author = {Farrel, Gabrielle Evan and Yahya, Widhi and Basuki, Achmad and Amron, Kasyful and Siregar, Reza Andria},
title = {Scalable Edge Computing Cluster Using a Set of Raspberry Pi: A Framework},
year = {2023},
isbn = {9798400708503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626641.3626936},
doi = {10.1145/3626641.3626936},
abstract = {In the context of edge computing, a cluster of small single-board computers like Raspberry Pi could serve as robust servers. These clusters not only offer robust server capabilities but also exhibit a remarkable versatility in handling incoming data streams from a multitude of sensors within the Internet of Things (IoT) ecosystem, particularly in underserved rural areas. Simultaneously, they can seamlessly double up as servers for web-based applications tailored to the specific requirements of small businesses. However, the operational context of such an edge computing cluster can present challenges. For instance, dynamic load fluctuations, ranging from high to low demands, may lead to performance service degradation or underutilized services. This is a typical problem in distributed computing environments, where the heterogeneity of devices, dynamic conditions, and reliability of connections can create scalability issues. This paper addresses these challenges through a selective set of integrated software suites aiming to autoscale an edge computing cluster. The software suites consist of a lightweight Kubernetes distribution called K3s, with an automation framework executed through Ansible. Rigorous testing, primarily focused on web-based applications, has showcased the efficacy of this approach. A compelling comparison has been drawn between this optimized edge computing setup and conventional desktop-based servers, emphasizing superior power efficiency and commendable performance levels. The service scaling can reduce power consumption by up to 45%.},
booktitle = {Proceedings of the 8th International Conference on Sustainable Information Engineering and Technology},
pages = {287–296},
numpages = {10},
location = {<conf-loc>, <city>Badung, Bali</city>, <country>Indonesia</country>, </conf-loc>},
series = {SIET '23}
}

@article{10.1145/3590149,
author = {Meng, Xiangwei and Yang, Ce and Qi, Yiren and Liang, Wei and Xu, Zisang and Li, Kuanching and Deng, Hai},
title = {A Novel Multi-Party Authentication Scheme for FCN-based MIoT Systems in Natural Language Processing Environment},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3590149},
doi = {10.1145/3590149},
abstract = {Natural language processing (NLP) assists to increase the efficiency of human and Multimedia Internet of Things (MIoT) interaction. Notably, large-scale NLP tasks can be offloaded from a cloud server to fog nodes closer to a mobile terminal device for lower response latency. But communication security is ongoing issues that need to be addressed. Effective mutual authentication among multiple entities is essential to ensure the security of MIoT systems based on a dynamic Fog Computing Network (FCN). However, the existing schemes are unsuitable for the dynamic FCN due to the security vulnerabilities such as the linkable sessions. To solve this problem, an Anonymous Multi-Party Authentication (AMPA) scheme is proposed to address the challenges of secure FCN-based MIoT communications in this paper. The proposed scheme uses a bilinear pairing operation to realize the authentication between the fog nodes and cloud server and to establish the group key. Besides, the scheme allows cloud-authenticated terminal devices to be added to the FCN and reduces the need for the resource-limited terminal device to perform many authentication protocols. The security analysis is carried out to demonstrate that AMPA scheme can meet various safety requirements. Performance evaluations shown that the proposed AMPA scheme achieves satisfactory performance.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
keywords = {Multimedia Internet of Things, natural language processing, fog computing network, authentication protocol}
}

@inproceedings{10.1145/3488932.3523256,
author = {Hao, Feng and van Oorschot, Paul C.},
title = {SoK: Password-Authenticated Key Exchange -- Theory, Practice, Standardization and Real-World Lessons},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3523256},
doi = {10.1145/3488932.3523256},
abstract = {Password-authenticated key exchange (PAKE) is a major area of cryptographic protocol research and practice. Many PAKE proposals have emerged in the 30 years following the original 1992 Encrypted Key Exchange (EKE), some accompanied by new theoretical models to support rigorous analysis. To reduce confusion and encourage practical development, major standards bodies including IEEE, ISO/IEC and the IETF have worked towards standardizing PAKE schemes, with mixed results. Challenges have included contrasts between heuristic protocols and schemes with security proofs, and subtleties in the assumptions of such proofs rendering some schemes unsuitable for practice. Despite initial difficulty identifying suitable use cases, the past decade has seen PAKE adoption in numerous large-scale applications such as Wi-Fi, Apple's iCloud, browser synchronization, e-passports, and the Thread network protocol for Internet of Things devices. Given this backdrop, we consolidate three decades of knowledge on PAKE protocols, integrating theory, practice, standardization and real-world experience. We provide a thorough and systematic review of the field, a summary of the state-of-the-art, a taxonomy to categorize existing protocols, and a comparative analysis of protocol performance using representative schemes from each taxonomy category. We also review real-world applications, summarize lessons learned, and highlight open research problems related to PAKE protocols.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {697–711},
numpages = {15},
keywords = {key exchange, pake, password authenticated key exchange},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@inproceedings{10.1145/3378904.3378907,
author = {Venkatraman, Sitalakshmi and Overmars, Anthony and Fahd, Kiran and Parvin, Sazia and Kaspi, Samuel},
title = {Security Challenges for Big Data and IoT},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378904.3378907},
doi = {10.1145/3378904.3378907},
abstract = {Recently, two terms, namely Big Data and Internet of Things (IoT) have gained popularity individually. However, their interconnections are not fully explored and understood. It is expected that the fusion of Big Data and IoT would create many complex systems for smart cities. While the data from IoT lies in Big Data, the scale of operations are completely different in terms of providing the required real-time analytics for such smart systems. Even though NoSQL databases and other next generation solutions could be deployed to achieve real-time responses, the major security challenges need to be understood as mission critical and sensitive data intertwines Big Data and IoT. In this paper, we identify the security challenges shared by the closely-knit Big Data and IoT in three main risk areas: i) NoSQL security vulnerabilities, ii) mobile IoT (M-IoT) security and privacy constraints and iii) encryption key security threats. We perform a comparative study of security vulnerabilities of NoSQL databases and identify the security and privacy constraints of M-IoT networks. Encryption key attacks for resource constrained IoT devices are also illustrated mathematically. Overall, this paper explores new research directions in these prime areas of security and privacy that would result in solution opportunities for a meaningful fusion of Big Data and IoT for a smart environment.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
pages = {1–6},
numpages = {6},
keywords = {Big data, IoT, NoSQL, attacks, challenges, factorization, security},
location = {Singapore, China},
series = {BDET 2020}
}

@inproceedings{10.1145/3372454.3372478,
author = {Puangpontip, Supadchaya and Hewett, Rattikorn},
title = {Assessing Reliability of Big Data Stream for Smart City},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372478},
doi = {10.1145/3372454.3372478},
abstract = {Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.},
booktitle = {Proceedings of the 3rd International Conference on Big Data Research},
pages = {18–23},
numpages = {6},
keywords = {Data Reliability, IoT, Smart City, Theory of evidence},
location = {Cergy-Pontoise, France},
series = {ICBDR '19}
}

@inproceedings{10.1145/3384544.3384579,
author = {Rattanalerdnusorn, Ekkachan and Pattaranantakul, Montida and Thaenkaew, Phithak and Vorakulpipat, Chalee},
title = {IoTDePT: Detecting Security Threats and Pinpointing Anomalies in an IoT environment},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384579},
doi = {10.1145/3384544.3384579},
abstract = {Internet of Things (IoT) has an ever increasing impact on today's society. It has penetrated all aspects of human life that utilize smart solutions and serves as an ecosystem. IoT provides inexpensive way to interconnects various things and objects together towards delivering smarter services, e.g., smart home applications, smart cities, and smart cars. Meanwhile, however, cyber attacks have also increased along with all these developments. Due to resource constraints, IoT-enabled devices are considered as a potential major victim vulnerable to exploitation by attackers that can setup an IoT botnet and launch a large scale cyber attack, e.g., DDoS attacks. When such attacks occur, it is an extremely difficult task to track down or trace back the root cause. It is also a challenging task to identify where the attacks happened, for example, in which location the anomalies are located (i.e., the compromised IoT devices). To address this issue, the paper presents IoTDePT -- a framework of threat detection and identification that aims to detect and identify malware threats in an IoT environment at a fine-grained level. Clearly, detecting the malware threats and pinpointing the exact geographical locations of the compromised IoT devices that propagated the threats. Two potential use cases of darknet have been exemplified to illustrate the usage of our proposal. The preliminary results show that the proposed approach can successfully detect the malware threats, while achieving accuracy in pinpointing a compromised device's location.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {232–236},
numpages = {5},
keywords = {IoTs, darknet, malware detection, threat monitoring},
location = {Langkawi, Malaysia},
series = {ICSCA '20}
}

@inproceedings{10.1145/3587135.3592204,
author = {Ghanathe, Nikhil P. and Wilton, Steve},
title = {T-RecX: Tiny-Resource Efficient Convolutional neural networks with early-eXit},
year = {2023},
isbn = {9798400701405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587135.3592204},
doi = {10.1145/3587135.3592204},
abstract = {Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is gaining popularity due to recent breakthroughs in ML and Internet of Things (IoT). Most tinyML research focuses on model compression techniques that trade accuracy (and model capacity) for compact models to fit into the KB-sized tiny-edge devices. In this paper, we show how such models can be enhanced by the addition of an early exit intermediate classifier. If the intermediate classifier exhibits sufficient confidence in its prediction, the network exits early thereby, resulting in considerable savings in time. Although early exit classifiers have been proposed in previous work, these previous proposals focus on large networks, making their techniques suboptimal/impractical for tinyML applications. Our technique is optimized specifically for tiny-CNN sized models. In addition, we present a method to alleviate the effect of network overthinking by leveraging the representations learned by the early exit. We evaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image classification, keyword spotting and visual wake word detection tasks. Our results show that T-RecX 1) improves the accuracy of baseline network, 2) achieves 31.58% average reduction in FLOPS in exchange for one percent accuracy across all evaluated models. Furthermore, we show that our methods consistently outperform popular prior works on the tiny-CNNs we evaluate.},
booktitle = {Proceedings of the 20th ACM International Conference on Computing Frontiers},
pages = {123–133},
numpages = {11},
keywords = {Adaptive inference, Dynamic neural networks, Early-exit networks, Edge computing, TinyML},
location = {Bologna, Italy},
series = {CF '23}
}

@inproceedings{10.1145/3576914.3587509,
author = {Hall, Jared and Sventek, Joe},
title = {DRAEC: An Adaptive Edge Computing Framework for Enforcing Operational Policy in CPS-IoT Systems},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3587509},
doi = {10.1145/3576914.3587509},
abstract = {As Cyber Physical-Internet of Things Systems (CPS-IoT Systems) continue to advance, we have noticed an expansion of the variety of entity types deployed in these systems. With this expansion, the complexity of handling vastly different communication protocols, processes, and data within the system in a timely manner has become a significant challenge. One potential method of quickly processing this data is to push essential computation concerning the Transitional State Change (TSC) feedback loop from the cloud to the edge of the system. In this paper, we propose DRAEC, a novel edge computing framework that uses agent-based Artificial Intelligence (AI) and Complex-Event Processing (CEP) to significantly decrease the latency of the TSC feedback loop and subsequently increase the system’s scalability. This increase in performance is achieved by using a CEP system to synthesize discreet events from CPS-IoT entity telemetry, which our novel Dynamic Reactive Agent then uses to quickly generate and enforce controls on a user’s physical environment via a user-defined operational policy. We then present supporting results, showing that our novel edge computing framework enables a CPS-IoT control system to outperform a cloud-centric variant by 20-43x (depending on the connection type) when comparing the latency of the TSC feedback loop; it also enables support of significantly larger scale (10-100x) systems while maintaining the same level of service as the cloud-centric version.},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {203–208},
numpages = {6},
keywords = {Artificial Intelligence, Cloud Computing, Complex Event Processing, Cyber-Physical Systems, Edge computing, Internet of Things, IoT policy Enforcement},
location = {San Antonio, TX, USA},
series = {CPS-IoT Week '23}
}

@article{10.1145/3453169,
author = {Liu, Yi and Zhao, Ruihui and Kang, Jiawen and Yassine, Abdulsalam and Niyato, Dusit and Peng, Jialiang},
title = {Towards Communication-Efficient and Attack-Resistant Federated Edge Learning for Industrial Internet of Things},
year = {2021},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3453169},
doi = {10.1145/3453169},
abstract = {Federated Edge Learning (FEL) allows edge nodes to train a global deep learning model collaboratively for edge computing in the Industrial Internet of Things (IIoT), which significantly promotes the development of Industrial 4.0. However, FEL faces two critical challenges: communication overhead and data privacy. FEL suffers from expensive communication overhead when training large-scale multi-node models. Furthermore, due to the vulnerability of FEL to gradient leakage and label-flipping attacks, the training process of the global model is easily compromised by adversaries. To address these challenges, we propose a communication-efficient and privacy-enhanced asynchronous FEL framework for edge computing in IIoT. First, we introduce an asynchronous model update scheme to reduce the computation time that edge nodes wait for global model aggregation. Second, we propose an asynchronous local differential privacy mechanism, which improves communication efficiency and mitigates gradient leakage attacks by adding well-designed noise to the gradients of edge nodes. Third, we design a cloud-side malicious node detection mechanism to detect malicious nodes by testing the local model quality. Such a mechanism can avoid malicious nodes participating in training to mitigate label-flipping attacks. Extensive experimental studies on two real-world datasets demonstrate that the proposed framework can not only improve communication efficiency but also mitigate malicious attacks while its accuracy is comparable to traditional FEL frameworks.},
journal = {ACM Trans. Internet Technol.},
month = {dec},
articleno = {59},
numpages = {22},
keywords = {Federated edge learning, edge intelligence, local differential privacy, gradient leakage attack, poisoning attack}
}

@article{10.1109/TNET.2021.3112480,
author = {Ma, Xiaobo and Qu, Jian and Li, Jianfeng and Lui, John C. S. and Li, Zhenhua and Liu, Wenmao and Guan, Xiaohong},
title = {Inferring Hidden IoT Devices and User Interactions via Spatial-Temporal Traffic Fingerprinting},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3112480},
doi = {10.1109/TNET.2021.3112480},
abstract = {With the popularization of Internet of Things (IoT) devices in smart home and industry fields, a huge number of IoT devices are connected to the Internet. However, what devices are connected to a network may not be known by the Internet Service Provider (ISP), since many IoT devices are placed within small networks (e.g., home networks) and are hidden behind network address translation (NAT). Without pinpointing IoT devices in a network, it is unlikely for the ISP to appropriately configure security policies and effectively manage the network. Additionally, inferring fine-grained user interactions of IoT devices is also an interesting yet unresolved problem. In this paper, we design an efficient and scalable system via spatial-temporal traffic fingerprinting from an ISP’s perspective in consideration of practical issues like learning-testing asymmetry. Our system can accurately identify typical IoT devices in a network, with the additional capability of identifying what devices are hidden behind NAT and the number of each type of device that share the same IP address. Our system can also detect user interactions and meanwhile identify their (concurrent) number through a multi-output regression model. Through extensive evaluation, we demonstrate that the system can generally identify IoT devices with an F1-Score above 0.999, and estimate the number of the same type of IoT device behind NAT with an average error below 5%. By studying 29 user interactions of 7 devices, we show that our system is promising in detecting user interactions.},
journal = {IEEE/ACM Trans. Netw.},
month = {sep},
pages = {394–408},
numpages = {15}
}

@inproceedings{10.1145/3412841.3441949,
author = {Shahzad, Aamir and Zhang, Kaiwen and Gherbi, Abdelouahed},
title = {Privacy-preserving smart grid traceability using blockchain over IoT connectivity},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441949},
doi = {10.1145/3412841.3441949},
abstract = {Smart grid (SG) technology comes with various advantages, among others, of efficient power distribution, reduction in cost consumption, maintain energy loss, and peak quality level in its supply chain. SG aims to provide all required and advanced features to overcome the issues of the existing power grid and to fulfill the rapidly growing demands of power generation, transmission, distribution, and monitoring energy consumption. However, SG has been facing various challenges to ensure the nature of every transaction, transaction verification, and recording in the power supply chain, especially in the context of a large scale IoT(Internet of things) network. Another issue is maintaining the privacy of every transaction, as well as the privacy of participating entities in the power supply chain. In this paper, we propose a blockchain-based proof-of-concept solution to manage every transaction that occurs in an IoT-aided smart grid system. Our solution, IoT-aided smart grid system with blockchain, provides an immutable transaction record, which is always shared and transparent to each participant of the system. Transactions are carried through IoT objects connected in a smart grid, and are recorded, verified, and validated on a blockchain immutable ledger. Furthermore, to verify and maintain the privacy of participants, cryptographic pseudonyms are used by each participant to interact with the SG supply chain, without revealing personal identities and important private information of the participants to malicious entities in the system.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {699–706},
numpages = {8},
keywords = {blockchain, hyper-ledger fabric, internet of things, privacy, smart grid, supply chain, traceability},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3477495.3532087,
author = {Sarikaya, Ruhi},
title = {Intelligent Conversational Agents for Ambient Computing},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532087},
doi = {10.1145/3477495.3532087},
abstract = {We are in the midst of an AI revolution. Three primary disruptive changes set off this revolution: 1) increase in compute power, mobile internet, and advances in deep learning. The next decade is expected to be about the proliferation of Internet-of-Things (IoT) devices and sensors, which will generate exponentially larger amounts of data to reason over and pave the way for ambient computing. This will also give rise to new forms of interaction patterns with these systems. Users will have to interact with these systems under increasingly richer context and in real-time. Conversational AI has a critical role to play in this revolution, but only if it delivers on its promise of enabling natural, frictionless, and personalized interactions in any context the user is in, while hiding the complexity of these systems through ambient intelligence. However, current commercial conversational AI systems are trained primarily with a supervised learning paradigm, which is difficult, if not impossible, to scale by manually annotating data for increasingly complex sets of contextual conditions. Inherent ambiguity in natural language further complicates the problem. We need to devise new forms of learning paradigms and frameworks that will scale to this complexity. In this talk, we present some early steps we are taking with Alexa, Amazon's Conversational AI system, to move from supervised learning to self-learning methods, where the AI relies on customer interactions for supervision in our journey to ambient intelligence.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {5},
numpages = {1},
keywords = {ambient computing and intelligence, any terms, conversational ai, intelligent assistants, ruhi sarikaya},
location = {<conf-loc>, <city>Madrid</city>, <country>Spain</country>, </conf-loc>},
series = {SIGIR '22}
}

@inproceedings{10.1145/3383845.3383871,
author = {Chen, Yen-Hung and Jan, Pi-Tzong and Lai, Ching-Neng and Huang, ChunWei and Chang, Chih-Han and Huang, Yo-Cih},
title = {Detecting Linking Flooding Attacks using Deep Convolution Network},
year = {2020},
isbn = {9781450376778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383845.3383871},
doi = {10.1145/3383845.3383871},
abstract = {With the development of technology, a new kind of Distributed Denial-of-Service (DDoS) attack named link-flooding attack (LFA) has been widely applied to congest critical network links and to paralyze the network service. This is mainly due to LFA is easily implemented, obfuscated, and occulted by launching large-scale legitimate low-speed flows to paralyze target network areas. Many solutions are proposed to detect LFA, they are designed by hand-crafted algorithms and hardly keep up the developing progress of self-organizing network structures and emerging network protocols. This study proposes a Deep-Learning based LFA defense framework, called DCN (Deep Convolution Network), that applies Convolution Neural Networks to statistically monitoring the network status through end-to-end functionality (Input: network status snapshot; Output: LFA attack or not attack) without any manual intervention. The experiment results demonstrate DCN can accurately detect DCN in varying network structure and flow patterns. Furthermore, DCN also provides quantitative security risk analysis by using learning time as the control variable, network structure as the independent variable, and time to identify LFA as the dependent variable. The contributions of DCN are (1) providing an autonomic LFA defense framework without any manual intervention, (2) providing objective and quantitative analytical security risk evaluating indicator, and (3) allowing cloud computing and Internet of Things company focuses on their service and leaves security defending to DCN.},
booktitle = {Proceedings of the 2020 the 3rd International Conference on Computers in Management and Business},
pages = {70–74},
numpages = {5},
keywords = {DDoS, Deep Learning, LFA, SDN},
location = {Tokyo, Japan},
series = {ICCMB '20}
}

@article{10.1145/3582900.3582907,
author = {Sarikaya, Ruhi},
title = {Intelligent Conversational Agents for Ambient Computing: A Keynote at SIGIR 2022},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3582900.3582907},
doi = {10.1145/3582900.3582907},
abstract = {We are in the midst of an AI revolution. Three primary disruptive changes set off this revolution: 1) increase in compute power, mobile internet, and advances in deep learning. The next decade is expected to be about the proliferation of Internet-of-Things (IoT) devices and sensors, which will generate exponentially larger amounts of data to reason over and pave the way for ambient computing. This will also give rise to new forms of interaction patterns with these systems. Users will have to interact with these systems under increasingly richer context and in real-time. Conversational AI has a critical role to play in this revolution, but only if it delivers on its promise of enabling natural, frictionless, and personalized interactions in any context the user is in, while hiding the complexity of these systems through ambient intelligence. However, current commercial conversational AI systems are trained primarily with a supervised learning paradigm, which is difficult, if not impossible, to scale by manually annotating data for increasingly complex sets of contextual conditions. Inherent ambiguity in natural language further complicates the problem. We need to devise new forms of learning paradigms and frameworks that will scale to this complexity. In this talk, we present some early steps we are taking with Alexa, Amazon's Conversational AI system, to move from supervised learning to self-learning methods, where the AI relies on customer interactions for supervision in our journey to ambient intelligence.Date: 14 July 2022.},
journal = {SIGIR Forum},
month = {jan},
articleno = {4},
numpages = {10}
}

@inproceedings{10.1145/3397536.3422220,
author = {Daghistani, Anas and Aref, Walid G. and Ghafoor, Arif},
title = {TrioStat: Online Workload Estimation in Distributed Spatial Data Streaming Systems},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422220},
doi = {10.1145/3397536.3422220},
abstract = {The wide spread of GPS-enabled devices and the Internet of Things (IoT) has increased the amount of spatial data being generated every second. The current scale of spatial data cannot be handled using centralized systems. This has led to the development of distributed spatial data streaming systems that scale to process in real-time large amounts of streamed spatial data. The performance of distributed streaming systems relies on how even the workload is distributed among their machines. However, it is challenging to estimate the workload of each machine because spatial data and query streams are skewed and rapidly change with time and users' interests. Moreover, a distributed spatial streaming system often does not maintain a global system workload state because it requires high network and processing overheads to be collected from the machines in the system.This paper introduces TrioStat; an online workload estimation technique that relies on a probabilistic model for estimating the workload of partitions and machines in a distributed spatial data streaming system. It is infeasible to collect and exchange statistics with a centralized unit because it requires high network overhead. Instead, TrioStat uses a decentralised technique to collect and maintain the required statistics in real-time locally in each machine. TrioStat enables distributed spatial data streaming systems to compare the workloads of machines as well as the workloads of data partitions. TrioStat requires minimal network and storage overhead. Moreover, the required storage is distributed across the system's machines.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {78–86},
numpages = {9},
keywords = {Workload estimation, collecting statistics, distributed streaming systems, load balancing, spatial stream processing},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/3416921.3416932,
author = {Zia, Unsub and Scotney, Bryan and McCartney, Mark and Martinez, Jorge and AbuTair, Mamun and Sajjad, Ali},
title = {A scalable and secure model for surveillance cameras in resource constrained IoT systems},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416921.3416932},
doi = {10.1145/3416921.3416932},
abstract = {In wireless multimedia surveillance networks (WMSNs), the sensors generate continuous data streams which are saved on cloud servers for processing and future use. Large-scale security and monitoring decisions are based on the video data fed back to the cloud by the visual sensors. Internet of Things (IoT) networks are deployed in certain resource constrained scenarios where edge computing is not available for data analytics and security considering the IoT devices have also limited resources such as memory, power and processing capabilities. In this paper, the problems of limited resources and data security are addressed by the proposal of a secure model for video surveillance systems working in resource constrained IoT assisted scenarios. The suggested approach comprises of the following four main stages: i) save only those video frames in which any activity is detected, ii) encrypt the saved frames for secure transmission, iii) synchronize the encrypted frames between cloud and sensor node and iv) remove the transmitted frames from sensor and decrypt the stored data on cloud. The performance of the encryption process for resource constrained devices is analyzed on different types of sensor nodes during experimentation. The results prove that the proposed method automates and speeds up the process of live video data extraction and occupies less space on cloud when compared to the conventional approach for saving surveillance videos. Furthermore, adding encryption to video frames ensures integrity of video data during their journey from sensor to the cloud.},
booktitle = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
pages = {92–96},
numpages = {5},
keywords = {IoT, resource constrained video surveillance, scalability, security},
location = {Virtual, United Kingdom},
series = {ICCBDC '20}
}

@article{10.1145/3461012,
author = {Lin, Yuxiang and Dong, Wei and Gao, Yi and Gu, Tao},
title = {SateLoc: A Virtual Fingerprinting Approach to Outdoor LoRa Localization Using Satellite Images},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3461012},
doi = {10.1145/3461012},
abstract = {With the increasing relevance of the Internet of Things and large-scale location-based services, LoRa localization has been attractive due to its low-cost, low-power, and long-range properties. However, existing localization approaches based on received signal strength indicators are either easily affected by signal fading of different land-cover types or labor intensive. In this work, we propose SateLoc, a LoRa localization system that utilizes satellite images to generate virtual fingerprints. Specifically, SateLoc first uses high-resolution satellite images to identify land-cover types. With the path loss parameters of each land-cover type, SateLoc can automatically generate a virtual fingerprinting map for each gateway. We then propose a novel multi-gateway combination strategy, which is weighted by the environmental interference of each gateway, to produce a joint likelihood distribution for localization and tracking. We implement SateLoc with commercial LoRa devices without any hardware modification, and evaluate its performance in a 227,500-m urban area. Experimental results show that SateLoc achieves a median localization error of 43.5 m, improving more than 50% compared to state-of-the-art model-based approaches. Moreover, SateLoc can achieve a median tracking error of 37.9 m with the distance constraint of adjacent estimated locations. More importantly, compared to fingerprinting-based approaches, SateLoc does not require the labor-intensive fingerprint acquisition process.},
journal = {ACM Trans. Sen. Netw.},
month = {jul},
articleno = {43},
numpages = {28},
keywords = {LoRa localization and tracking, land-cover information}
}

@inproceedings{10.1145/3510456.3514163,
author = {Schmiedmayer, Paul and Chatley, Robert and Bernius, Jan Philip and Krusche, Stephan and Chaika, Konstantin and Krinkin, Kirill and Bruegge, Bernd},
title = {Global software engineering in a global classroom},
year = {2022},
isbn = {9781450392259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510456.3514163},
doi = {10.1145/3510456.3514163},
abstract = {Due to globalization, many software projects have become large-scale and distributed tasks that require software engineers to learn and apply techniques for distributed requirements analysis, modeling, development, and deployment. Globally-distributed projects require special skills in communication across different locations and time zones in all stages of the project. There has been advancement in teaching these concepts at universities, but adapting global software engineering in a curriculum is still in infancy.The main reasons are the effort and coordination required by teachers to set up the project, manage distributed development and enable distributed delivery. It becomes even more difficult when teaching distributed software engineering involving Internet of Things (IoT) applications. The situation has changed with recent advances in continuous deployment and cloud platform services that make globally-distributed projects more feasible, teachable, and learnable, even for short-term projects. However, no experience report in education research describes a truly distributed global setup in continuous software engineering for IoT applications.This paper describes a ten-day project involving three universities in different countries with 21 students located across the world to substantiate this claim. It provides teachers with recommendations for conducting a global software engineering course in a global setting. Recommendations include access for all students to (remote) hardware, stable network infrastructure in all locations, the use of a central development platform for continuous integration and deployment, and the application of distributed pair deployment.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {113–121},
numpages = {9},
keywords = {continuous deployment, continuous software engineering, distributed, internet of things, open source, remote, teaching},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEET '22}
}

@inproceedings{10.1145/3495018.3495462,
author = {Chi, Youshen},
title = {Application and Research of Deep Mining of Health Medical Big Data Based on Internet of Things},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495462},
doi = {10.1145/3495018.3495462},
abstract = {Traditional data mining algorithms are mostly difficult to handle large data sets, but need to manage big data and discover hidden knowledge. Therefore, the combination of data mining algorithms and IoT technology is the development trend of data processing in the future, but because of the difficulty, not all algorithms can be implemented on the cloud platform, so the research results are not enough. The core idea of cloud computing is architecture. All data mining algorithm improvement strategies need to be designed according to the characteristics of the architecture, and each part must implement specific functions. It is difficult or impossible to implement an algorithm that cannot be decomposed into the above form in the architecture. With the widespread application of IoT technology, a large number of unstructured, distributed and even data mining requirements in mobile devices pose serious challenges to existing data mining technologies. How to use cloud computing technology to achieve large-scale distributed data collection, transmission and mining has become an important research direction. This paper investigates some health care big data, combines the Internet of Things cloud data platform, and introduces the traditional association rules algorithm and its interest level in mining technology. On this basis, the improved algorithm is improved and compared with other traditional algorithms. Establish a simulation platform to implement algorithms and mine training data. It proves that the improved algorithm has lower spatial complexity and faster processing rate in data mining, and analyzes the data mining technology in health medical data.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1659–1668},
numpages = {10},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1145/3510360,
author = {Oser, Pascal and van der Heijden, Rens W. and L\"{u}ders, Stefan and Kargl, Frank},
title = {Risk Prediction of IoT Devices Based on Vulnerability Analysis},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3510360},
doi = {10.1145/3510360},
abstract = {Internet of Things (IoT) devices are becoming more widespread not only in areas such as smart homes and smart cities but also in research and office environments. The sheer number, heterogeneity, and limited patch availability provide significant challenges for the security of both office networks and the Internet in general. The systematic estimation of device risks, which is essential for mitigation decisions, is currently a skill-intensive task that requires expertise in network vulnerability scanning, as well as manual effort in firmware binary analysis.This article introduces SAFER,1 the Security Assessment Framework for Embedded-device Risks, which enables a semi-automated risk assessment of IoT devices in any network. SAFER combines information from network device identification and automated firmware analysis to estimate the current risk associated with the device. Based on past vulnerability data and vendor patch intervals for device models, SAFER extrapolates those observations into the future using different automatically parameterized prediction models. Based on that, SAFER also estimates an indicator for future security risks. This enables users to be aware of devices exposing high risks in the future.One major strength of SAFER over other approaches is its scalability, achieved through significant automation. To demonstrate this strength, we apply SAFER in the network of a large multinational organization, to systematically assess the security level of hundreds of IoT devices on large-scale networks.Results indicate that SAFER successfully identified 531 out of 572 devices leading to a device identification rate of 92.83&nbsp;%, analyzed 825 firmware images, and predicted the current and future security risk for 240 devices.},
journal = {ACM Trans. Priv. Secur.},
month = {may},
articleno = {14},
numpages = {36},
keywords = {IoT, security risk assessment, device identification, firmware analysis, vulnerability analysis, risk prediction, future risk, safer network, CERN}
}

@article{10.1145/3555308,
author = {Kong, Linghe and Tan, Jinlin and Huang, Junqin and Chen, Guihai and Wang, Shuaitian and Jin, Xi and Zeng, Peng and Khan, Muhammad and Das, Sajal K.},
title = {Edge-computing-driven Internet of Things: A Survey},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3555308},
doi = {10.1145/3555308},
abstract = {The Internet of Things (IoT) is impacting the world’s connectivity landscape. More and more IoT devices are connected, bringing many benefits to our daily lives. However, the influx of IoT devices poses non-trivial challenges for the existing cloud-based computing paradigm. In the cloud-based architecture, a large amount of IoT data is transferred to the cloud for data management, analysis, and decision making. It could not only cause a heavy workload on the cloud but also result in unacceptable network latency, ultimately undermining the benefits of cloud-based computing. To address these challenges, researchers are looking for new computing models for the IoT. Edge computing, a new decentralized computing model, is valued by more and more researchers in academia and industry. The main idea of edge computing is placing data processing in near-edge devices instead of remote cloud servers. It is promising to build more scalable, low-latency IoT systems. Many studies have been proposed on edge computing and IoT, but a comprehensive survey of this crossover area is still lacking.In this survey, we first introduce the impact of edge computing on the development of IoT and point out why edge computing is more suitable for IoT than other computing paradigms. Then, we analyze the necessity of systematical investigation on the edge-computing-driven IoT (ECDriven-IoT) and summarize new challenges occurring in ECDriven-IoT. We categorize recent advances from bottom to top, covering six aspects of ECDriven-IoT. Finally, we conclude lessons learned and propose some challenging},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {174},
numpages = {41},
keywords = {Edge computing, Internet of Things}
}

@inproceedings{10.1145/3340531.3412051,
author = {Hartmann, Valentin and Modi, Konark and Pujol, Josep M. and West, Robert},
title = {Privacy-Preserving Classification with Secret Vector Machines},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412051},
doi = {10.1145/3340531.3412051},
abstract = {Today, large amounts of valuable data are distributed among millions of user-held devices, such as personal computers, phones, or Internet-of-things devices. Many companies collect such data with the goal of using it for training machine learning models allowing them to improve their services. User-held data is, however, often sensitive, and collecting it is problematic in terms of privacy. We address this issue by proposing a novel way of training a supervised classifier in a distributed setting akin to the recently proposed federated learning paradigm, but under the stricter privacy requirement that the server that trains the model is assumed to be untrusted and potentially malicious. We thus preserve user privacy by design, rather than by trust. In particular, our framework, called secret vector machine (SecVM), provides an algorithm for training linear support vector machines (SVM) in a setting in which data-holding clients communicate with an untrusted server by exchanging messages designed to not reveal any personally identifiable information. We evaluate our model in two ways. First, in an offline evaluation, we train SecVM to predict user gender from tweets, showing that we can preserve user privacy without sacrificing classification performance. Second, we implement SecVM's distributed framework for the Cliqz web browser and deploy it for predicting user gender in a large-scale online evaluation with thousands of clients, outperforming baselines by a large margin and thus showcasing that SecVM is suitable for production environments.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {475–484},
numpages = {10},
keywords = {SVM, federated learning, privacy},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1145/3519422,
author = {Liu, Genggeng and Zhu, Yuhan and Xu, Saijuan and Tang, Hao and Chen, Yeh-Cheng},
title = {Performance-Driven X-Architecture Routing Algorithm&nbsp;for Artificial Intelligence Chip Design in Smart Manufacturing},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3519422},
doi = {10.1145/3519422},
abstract = {The new 7-nm Artificial Intelligence (AI) chip is an important milestone recently announced by the IBM research team, with a very important optimization goal of performance. This chip technology can be extended to various business scenarios in the Internet of Things. As the basic model for very large scale integration routing, the Steiner minimal tree can be used in various practical problems, such as wirelength optimization and timing closure. Further considering the X-architecture and the routing resources within obstacles, an effective performance-driven X-architecture routing algorithm for AI chip design in smart manufacturing is proposed to improve the delay performance of the chip. First, a special particle swarm optimization algorithm is presented to solve the discrete length-restricted X-architecture Steiner minimum tree problem in combination with genetic operations, and a particle encoding scheme is presented to encode each particle into an initial routing tree. Second, two lookup tables based on pins and obstacles are established to provide a fast information query for the whole algorithm flow. Third, a strategy of candidate point selection is designed to make the particles satisfy the constraints. Finally, a refinement strategy is implemented to further improve the quality of the final routing tree. Compared with other state-of-the-art algorithms, the proposed algorithm achieves a better total wirelength, which is an important index of performance, thus better satisfying the demand for delay performance of AI chip design in smart manufacturing.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {aug},
articleno = {38},
numpages = {20},
keywords = {Artificial intelligence chip design, Steiner minimal tree, smart manufacturing, particle swarm optimization, X-architecture routing, resource relaxation, refinement strategy}
}

@inproceedings{10.1145/3576842.3582377,
author = {Yu, Xiaofan and Cherkasova, Lucy and Vardhan, Harsh and Zhao, Quanling and Ekaireb, Emily and Zhang, Xiyuan and Mazumdar, Arya and Rosing, Tajana},
title = {Async-HFL: Efficient and Robust Asynchronous Federated Learning in Hierarchical IoT Networks},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582377},
doi = {10.1145/3576842.3582377},
abstract = {Federated Learning (FL) has gained increasing interest in recent years as a distributed on-device learning paradigm. However, multiple challenges remain to be addressed for deploying FL in real-world Internet-of-Things (IoT) networks with hierarchies. Although existing works have proposed various approaches to account data heterogeneity, system heterogeneity, unexpected stragglers and scalibility, none of them provides a systematic solution to address all of the challenges in a hierarchical and unreliable IoT network. In this paper, we propose an asynchronous and hierarchical framework (Async-HFL) for performing FL in a common three-tier IoT network architecture. In response to the largely varied networking and system processing delays, Async-HFL employs asynchronous aggregations at both the gateway and cloud levels thus avoids long waiting time. To fully unleash the potential of Async-HFL in converging speed under system heterogeneities and stragglers, we design device selection at the gateway level and device-gateway association at the cloud level. Device selection module chooses diverse and fast edge devices to trigger local training in real-time while device-gateway association module determines the efficient network topology periodically after several cloud epochs, with both modules satisfying bandwidth limitations. We evaluate Async-HFL’s convergence speedup using large-scale simulations based on ns-3 and a network topology from NYCMesh. Our results show that Async-HFL converges 1.08-1.31x faster in wall-clock time and saves up to 21.6% total communication cost compared to state-of-the-art asynchronous FL algorithms (with client selection). We further validate Async-HFL on a physical deployment and observe its robust convergence under unexpected stragglers.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {236–248},
numpages = {13},
keywords = {Asynchronous FL., Federated Learning, Hierarchical Sensor and IoT Networks},
location = {<conf-loc>, <city>San Antonio</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {IoTDI '23}
}

@article{10.1145/3572899,
author = {Liu, Xiao and Gao, Bonan and Suleiman, Basem and You, Han and Ma, Zisu and Liu, Yu and Anaissi, Ali},
title = {Privacy-Preserving Personalized Fitness Recommender System P3FitRec: A Multi-level Deep Learning Approach},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3572899},
doi = {10.1145/3572899},
abstract = {Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users’ privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this article, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable Internet of Things (IoT) devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data, minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, and weight. Our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared with similar studies.1 Furthermore, our approach is novel compared with existing studies, as it does not require collecting and using users’ sensitive information. Thus, it preserves the users’ privacy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {76},
numpages = {24},
keywords = {Personalization, fitness, recommender system, deep learning, sensors}
}

@proceedings{10.1145/3477231,
title = {NoCArc '21: Proceedings of the 14th International Workshop on Network on Chip Architectures},
year = {2021},
isbn = {9781450387118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Powerful forces of innovation and demands from computing infrastructure are pushing the boundaries of traditional processing platforms in an unprecedented manner. Artificial Intelligence, Cybersecurity, Big Data, autonomous vehicles, 5G or NextG communication technologies, and Internet of Things (IoT) are some of the application areas where field of multi/many-core processing platforms are being used. This propels research towards novel architectures consisting of heterogeneous, accelerator-rich scenarios with highly specialized chiplets handling specialized task threads. Such massive integration both inside and across a single die imposes ever-increasing demand on the interconnection sub-system. Networks-on-Chips have been the de facto mechanism of connecting a scalable number of cores within the same die. However, the increasing power-performance demands as well the security requirements of the application domains require a constant churn of the interconnection architectures. The use of emerging technologies such as interposers, wireless and optical interconnects help alleviate some of these challenges while exposing other novel opportunities. This evolving landscape makes the interconnection architecture a critical determinant of power-performance bottlenecks as well as reliability and security of the overall system they are deployed in.In this context, NoCArc continues its evolution and seeks to be a focused forum for researchers and practitioners to present and discuss innovative ideas and solutions related to the design, implementation, and application of interconnect fabrics within complex multi/many-core systems with conventional or emerging technologies such as 3D, optical, or wireless communications at the chip or system scale.},
location = {Virtual Event, Greece}
}

@article{10.1145/3364181,
author = {Hatzivasilis, George and Soultatos, Othonas and Ioannidis, Sotiris and Spanoudakis, George and Katos, Vasilios and Demetriou, Giorgos},
title = {MobileTrust: Secure Knowledge Integration in VANETs},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3364181},
doi = {10.1145/3364181},
abstract = {Vehicular Ad hoc NETworks (VANET) are becoming popular due to the emergence of the Internet of Things and ambient intelligence applications. In such networks, secure resource sharing functionality is accomplished by incorporating trust schemes. Current solutions adopt peer-to-peer technologies that can cover the large operational area. However, these systems fail to capture some inherent properties of VANETs, such as fast and ephemeral interaction, making robust trust evaluation of crowdsourcing challenging. In this article, we propose MobileTrust—a hybrid trust-based system for secure resource sharing in VANETs. The proposal is a breakthrough in centralized trust computing that utilizes cloud and upcoming 5G technologies to provide robust trust establishment with global scalability. The ad hoc communication is energy-efficient and protects the system against threats that are not countered by the current settings. To evaluate its performance and effectiveness, MobileTrust is modelled in the SUMO simulator and tested on the traffic features of the small-size German city of Eichstatt. Similar schemes are implemented in the same platform to provide a fair comparison. Moreover, MobileTrust is deployed on a typical embedded system platform and applied on a real smart car installation for monitoring traffic and road-state parameters of an urban application. The proposed system is developed under the EU-founded THREAT-ARREST project, to provide security, privacy, and trust in an intelligent and energy-aware transportation scenario, bringing closer the vision of sustainable circular economy.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {mar},
articleno = {33},
numpages = {25},
keywords = {CPS, GPU, IoT, MEC, VANET, circular economy, cloud, mobility, parallel computing, privacy, reputation, trust}
}

@inproceedings{10.1145/3360774.3360798,
author = {Lunardi, Roben C. and Michelin, Regio A. and Neu, Charles V. and Nunes, Henry C. and Zorzo, Avelino F. and Kanhere, Salil S.},
title = {Impact of consensus on appendable-block blockchain for IoT},
year = {2020},
isbn = {9781450372831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360774.3360798},
doi = {10.1145/3360774.3360798},
abstract = {The Internet of Things (IoT) is transforming our physical world into a complex and dynamic system of connected devices on an unprecedented scale. Connecting everyday physical objects is creating new business models, improving processes and reducing costs and risks. Recently, blockchain technology has received a lot of attention from the community as a possible solution to overcome security issues in IoT. However, traditional blockchains (such as the ones used in Bitcoin and Ethereum) are not well suited to the resource-constrained nature of IoT devices and also with the large volume of information that is expected to be generated from typical IoT deployments. To overcome these issues, several researchers have presented lightweight instances of blockchains tailored for IoT. For example, proposing novel data structures based on blocks with decoupled and appendable data. However, these researchers did not discuss how the consensus algorithm would impact their solutions, i.e., the decision of which consensus algorithm would be better suited was left as an open issue. In this paper, we improved an appendable-block blockchain framework to support different consensus algorithms through a modular design. We evaluated the performance of this improved version in different emulated scenarios and studied the impact of varying the number of devices and transactions and employing different consensus algorithms. Even adopting different consensus algorithms, results indicate that the latency to append a new block is less than 161ms (in the more demanding scenario) and the delay for processing a new transaction is less than 7ms, suggesting that our improved version of the appendable-block blockchain is efficient and scalable, and thus well suited for IoT scenarios.},
booktitle = {Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {228–237},
numpages = {10},
keywords = {IoT, blockchain, consensus, distributed ledgers, security},
location = {Houston, Texas, USA},
series = {MobiQuitous '19}
}

@inproceedings{10.1145/3538969.3539009,
author = {Khoury, Joseph and Safaei Pour, Morteza and Bou-Harb, Elias},
title = {A Near Real-Time Scheme for Collecting and Analyzing IoT Malware Artifacts at Scale},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3539009},
doi = {10.1145/3538969.3539009},
abstract = {The chronic proliferation of Internet of Things (IoT) botnet malware activities coupled with an unprecedented rise in security vulnerabilities convene a new world of opportunities for perpetrators and unveil a new set of hurdles in deriving relevant IoT malware intelligence. Such shortfall within the IoT paradigm exacerbates the capabilities for largely identifying the prevailing IoT malware threats, the origin of the IoT attacks, as well as, the security deficit associated with the IoT paradigm. Previous work has vastly studied IoT malware activities in the wild but has not profiled at a large scale malicious activities to collect in near real-time central IoT artifacts much-needed to understand and eventually elevate the security posture of the IoT ecosystem. To this end, we propose in this work a near real-time collection scheme to collect and analyze at large IoT malware artifacts essential for understanding the prevalent cyber security risks. We leverage in this work a large network telescope comprising of 16.7 million IPs as one extensive honeypot to examine evidence of malicious IoT probes in the wild. Subsequently, we employ a deception technique to respond to these probes and eventually establish bogus connections to collect IoT malware artifacts. In only 120 hours of near real-time measurements, our proposed scheme collected 80,569,070 interactions originating from 30,190 malware-infected IoT devices. Accordingly, we derive pivotal IoT malware intelligence which includes system commands, file-less attacks evidence, payload URLs, Executable and Linkable Format (ELF) binaries, log-in credentials, malicious LDAP servers, and unique insights on the abuse of the recent Log4shell security vulnerability in distributing IoT malware binaries.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {32},
numpages = {11},
keywords = {IoT malware artifacts, IoT malware intelligence, IoT security, Log4Shell security vulnerability, network telescope},
location = {Vienna, Austria},
series = {ARES '22}
}

@article{10.1145/3609336,
author = {Hamed, Naeima and Gaglione, Andrea and Gluhak, Alex and Rana, Omer and Perera, Charith},
title = {Query Interface for Smart City Internet of Things Data Marketplaces: A Case Study},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3609336},
doi = {10.1145/3609336},
abstract = {Cities are increasingly becoming augmented with sensors through public, private, and academic sector initiatives. Most of the time, these sensors are deployed with a primary purpose (objective) in mind (e.g., deploy sensors to understand noise pollution) by a sensor owner (i.e., the organization that invests in sensing hardware, e.g., a city council). Over the past few years, communities undertaking smart city development projects have understood the importance of making the sensor data available to a wider community—beyond their primary usage. Different business models have been proposed to achieve this, including creating data marketplaces. The vision is to encourage new startups and small and medium-scale businesses to create novel products and services using sensor data to generate additional economic value. Currently, data are sold as pre-defined independent datasets (e.g., noise level and parking status data may be sold separately). This approach creates several challenges, such as (i) difficulties in pricing, which leads to higher prices (per dataset); (ii) higher network communication and bandwidth requirements; and (iii) information overload for data consumers (i.e., those who purchase data). We investigate the benefit of semantic representation and its reasoning capabilities toward creating a business model that offers data on demand within smart city Internet of Things data marketplaces. The objective is to help data consumers (i.e., small and medium enterprises) acquire the most relevant data they need. We demonstrate the utility of our approach by integrating it into a real-world IoT data marketplace (developed by the synchronicity-iot.eu project). We discuss design decisions and their consequences (i.e., tradeoffs) on the choice and selection of datasets. Subsequently, we present a series of data modeling principles and recommendations for implementing IoT data marketplaces.},
journal = {ACM Trans. Internet Things},
month = {sep},
articleno = {19},
numpages = {39},
keywords = {Internet of Things, semantic interoperability, data discovery, multi-dimensional querying, linked data, knowledge management}
}

@article{10.1145/3582011,
author = {Abu-Khadrah, Ahmed and Ali, Ali Mohd and Jarrah, Muath},
title = {An Amendable Multi-Function Control Method using Federated Learning for Smart Sensors in Agricultural Production Improvements},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3582011},
doi = {10.1145/3582011},
abstract = {Communications and Computer Engineering Department, Faculty of Engineering, Al-Ahliyya Amman University, Amman 19328, JordanSchool of Information Technology, Skyline University, Sharjah, 1797, UAESmart Sensors are used for monitoring, sensing, and actuating controls in small and large-scale agricultural plots. From soil features to crop health and climatic observations, the smart sensors integrate with sophisticated technologies such as the Internet of Things or cloud for decentralized processing and global actuation. Considering this integration, an Amendable Multi-Function Sensor Control (AMFSC) is introduced in this proposal. This proposed method focuses on sensor operations that aid agricultural production improvements. The agriculture hindering features from the soil, temperature, and crop infections are sensed and response is actuated based on controlled operations. The control operations are performed according to the sensor control validation and modified control acute sensor, which helps to maximize productivity. The sensor control and operations are determined using federated learning from the accumulated data in the previous sensing intervals. This learning validates the current sensor data with the optimal data stored for different crops and environmental factors in the past. Depending on the computed, sensed, and optimal (adaptable) data, the sensor operation for actuation is modified. This modification is recommended for crop and agriculture development to maximize agricultural productivity. In particular, the sensing and actuation operations of the smart sensors for different intervals are modified to maximize production and adaptability. The efficiency of the system was evaluated using different parameters and the system maximizes the analysis rate (12.52%), control rate (7%), adaptability (9.65%) and minimizes the analysis time (7.12%), and actuation lag (8.97%)},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = {feb},
keywords = {Agricultural Production, Federation Learning, Multi-Function sensor, Sensor Control, Smart Sensors, Productivity Analysis, Modified Control}
}

@inproceedings{10.1145/3378679.3394538,
author = {Happ, Daniel and Bayhan, Suzan},
title = {On the impact of clustering for IoT analytics and message broker placement across cloud and edge},
year = {2020},
isbn = {9781450371322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378679.3394538},
doi = {10.1145/3378679.3394538},
abstract = {With edge computing emerging as a promising solution to cope with the challenges of Internet of Things (IoT) systems, there is an increasing need to automate the deployment of large-scale applications along with the publish/subscribe brokers they communicate over. Such a placement must adjust to the resource requirements of both applications and brokers in the heterogeneous environment of edge, fog, and cloud. In contrast to prior work focusing only on the placement of applications, this paper addresses the problem of jointly placing IoT applications and the pub/sub brokers on a set of network nodes, considering an application provider who aims at minimizing total end-to-end delays of all its subscribers. More specifically, we devise two heuristics for joint deployment of brokers and applications and analyze their performance in comparison to the current cloud-based IoT solutions wherein both the IoT applications and the brokers are located solely in the cloud. As an application provider should consider not only the location of the application users but also how they are distributed across different network components, we use von Mises distributions to model the degree of clustering of the users of an IoT application. Our simulations show that superior performance of our heuristics in comparison to cloud-based IoT operation is most pronounced under a high degree of clustering. When users of an IoT application are in close network proximity of the IoT sensors, cloud-based IoT unnecessarily introduces latency to move the data from the edge to the cloud and vice versa while processing could be performed at the edge or the fog layers.},
booktitle = {Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking},
pages = {43–48},
numpages = {6},
keywords = {cloud computing, edge computing, fog computing, internet of things, placement, publish/subscribe},
location = {Heraklion, Greece},
series = {EdgeSys '20}
}

@inproceedings{10.1145/3543507.3583452,
author = {Hu, Yue and Zhang, Yuhang and Wang, Yanbing and Work, Daniel},
title = {Detecting Socially Abnormal Highway Driving Behaviors via Recurrent Graph Attention Networks},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583452},
doi = {10.1145/3543507.3583452},
abstract = {With the rapid development of Internet of Things technologies, the next generation traffic monitoring infrastructures are connected via the web, to aid traffic data collection and intelligent traffic management. One of the most important tasks in traffic is anomaly detection, since abnormal drivers can reduce traffic efficiency and cause safety issues. This work focuses on detecting abnormal driving behaviors from trajectories produced by highway video surveillance systems. Most of the current abnormal driving behavior detection methods focus on a limited category of abnormal behaviors that deal with a single vehicle without considering vehicular interactions. In this work, we consider the problem of detecting a variety of socially abnormal driving behaviors, i.e., behaviors that do not conform to the behavior of other nearby drivers. This task is complicated by the variety of vehicular interactions and the spatial-temporal varying nature of highway traffic. To solve this problem, we propose an autoencoder with a Recurrent Graph Attention Network that can capture the highway driving behaviors contextualized on the surrounding cars, and detect anomalies that deviate from learned patterns. Our model is scalable to large freeways with thousands of cars. Experiments on data generated from traffic simulation software show that our model is the only one that can spot the exact vehicle conducting socially abnormal behaviors, among the state-of-the-art anomaly detection models. We further show the performance on real world HighD traffic dataset, where our model detects vehicles that violate the local driving norms.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3086–3097},
numpages = {12},
keywords = {Anomaly detection, Graph neural networks, Intelligent transportation, Spatial-temporal learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3576842.3582376,
author = {Nouma, Saif E. and Yavuz, Attila A.},
title = {Practical Cryptographic Forensic Tools for Lightweight Internet of Things and Cold Storage Systems},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582376},
doi = {10.1145/3576842.3582376},
abstract = {Internet of Things (IoT) and Storage-as-a-Service (STaaS) continuum permit cost-effective maintenance of security-sensitive information collected by IoT devices over cloud systems. It is necessary to guarantee the security of sensitive data in IoT-STaaS applications. Especially, log entries trace critical events in computer systems and play a vital role in the trustworthiness of IoT-STaaS. An ideal log protection tool must be scalable and lightweight for vast quantities of resource-limited IoT devices while permitting efficient and public verification at STaaS. However, the existing cryptographic logging schemes either incur significant computation/signature overhead to the logger or extreme storage and verification costs to the cloud. There is a critical need for a cryptographic forensic log tool that respects the efficiency requirements of the IoT-STaaS continuum. In this paper, we created novel digital signatures for logs called Optimal Signatures for secure Logging (), which are the first (to the best of our knowledge) to offer both small-constant signature and public key sizes with near-optimal signing and batch verification via various granularities. We introduce new design features such as one-time randomness management, flexible aggregation along with various optimizations to attain these seemingly conflicting properties simultaneously. Our experiments show that &nbsp;offers 50 \texttimes{} faster verification (for 235 entries) than the most compact alternative with equal signature sizes, while also being several magnitudes of more compact than its most logger efficient counterparts. These properties make &nbsp;an ideal choice for the IoT-STaaS, wherein lightweight logging and efficient batch verification of massive-size logs are vital for the IoT edge and cold storage servers, respectively.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {340–353},
numpages = {14},
keywords = {Authentication, cold storage, digital signatures, secure logs},
location = {<conf-loc>, <city>San Antonio</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {IoTDI '23}
}

@article{10.1145/3617593,
author = {Weiss, Michael and Tonella, Paolo},
title = {Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617593},
doi = {10.1145/3617593},
abstract = {Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of AI tasks. Often consisting of hundreds of million, if not hundreds of billion, parameters, these DNNs are too large to be deployed to or efficiently run on resource-constrained devices such as mobile phones or Internet of Things microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this article, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs while only marginally impacting the overall system accuracy. Our architecture furthermore foresees a second supervisor to monitor the remote predictions and identify inputs for which not even these can be trusted, allowing to raise an exception or run a fallback strategy instead. We evaluate the cost savings and the ability to detect incorrectly predicted inputs on four diverse case studies: IMDb movie review sentiment classification, GitHub issue triaging, ImageNet image classification, and SQuADv2 free-text question answering. In all four case studies, we find that BiSupervised allows to reduce cost by at least 30% while maintaining similar system-level prediction performance. In two case studies (IMDb and SQuADv2), we find that BiSupervised even achieves a higher system-level accuracy, at reduced cost, compared to a remote-only model. Furthermore, measurements taken on our setup indicate a large potential of BiSupervised to reduce average prediction latency.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {28},
numpages = {29},
keywords = {Datasets, neural networks, gaze detection, text tagging}
}

@inproceedings{10.1145/3516807.3516820,
author = {Xu, Yuanjia and Wu, Heng and Zhang, Wenbo and Hu, Yi},
title = {EOP: efficient operator partition for deep learning inference over edge servers},
year = {2022},
isbn = {9781450392518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3516807.3516820},
doi = {10.1145/3516807.3516820},
abstract = {Recently, Deep Learning (DL) models have demonstrated great success for its attractive ability of high accuracy used in artificial intelligence Internet of Things applications. A common deployment solution is to run such DL inference tasks on edge servers. In a DL inference, each operator takes tensors as input and run in a tensor virtual machine, which isolates resource usage among operators. Nevertheless, existing edge-based DL inference approaches can not efficiently use heterogeneous resources (e.g., CPU and low-end GPU) on edge servers and result in sub-optimal DL inference performance, since they can only partition operators in a DL inference with equal or fixed ratios. It is still a big challenge to support partition optimizations over edge servers for a wide range of DL models, such as Convolution Neural Network (CNN), Recurrent Neural Network (RNN) and Transformers. In this paper, we present EOP, an Efficient Operator Partition approach to optimize DL inferences over edge servers, to address this challenge. Firstly, we carry out a large-scale performance evaluation on operators running on heterogeneous resources, and reveal that many operators do not follow similar performance variation when input tensors change. Secondly, we employ three categorized patterns to estimate the performance of operators, and then efficiently partition key operators and tune partition ratios. Finally, we implement EOP on TVM, and experiments over a typical edge server show that EOP improves the inference performance by up to 1.25−1.97\texttimes{} for various DL models compared to state-of-the-art approaches.},
booktitle = {Proceedings of the 18th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {45–57},
numpages = {13},
keywords = {deep learning inferences, edge servers, heterogeneous resources, input tensors, operator partition},
location = {Virtual, Switzerland},
series = {VEE 2022}
}

@article{10.1145/3387705,
author = {Xu, Zichuan and Zhang, Zhiheng and Liang, Weifa and Xia, Qiufen and Rana, Omer and Wu, Guowei},
title = {QoS-Aware VNF Placement and Service Chaining for IoT Applications in Multi-Tier Mobile Edge Networks},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3387705},
doi = {10.1145/3387705},
abstract = {Mobile edge computing and network function virtualization (NFV) paradigms enable new flexibility and possibilities of the deployment of extreme low-latency services for Internet-of-Things (IoT) applications within the proximity of their users. However, this poses great challenges to find optimal placements of virtualized network functions (VNFs) for data processing requests of IoT applications in a multi-tier cloud network, which consists of many small- or medium-scale servers, clusters, or cloudlets deployed within the proximity of IoT nodes and a few large-scale remote data centers with abundant computing and storage resources. In particular, it is challenging to jointly consider VNF instance placement and routing traffic path planning for user requests, as they are not only delay sensitive but also resource hungry.In this article, we consider admissions of NFV-enabled requests of IoT applications in a multi-tier cloud network, where users request network services by issuing service requests with service chain requirements, and the service chain enforces the data traffic of the request to pass through the VNFs in the chain one by one until it reaches its destination. To this end, we first formulate the throughput maximization problem with the aim to maximize the system throughput. We then propose an integer linear program solution if the problem size is small; otherwise, we devise an efficient heuristic that jointly takes into account VNF placements to both cloudlets and data centers and routing path finding for each request. For a special case of the problem with a set of service chains, we propose an approximation algorithm with a provable approximation ratio. Next, we also devise efficient learning-based heuristics for VNF provisioning for IoT applications by incorporating the mobility and energy conservation features of IoT devices. We finally evaluate the performance of the proposed algorithms by simulations. The simulation results show that the performance of the proposed algorithms is promising.},
journal = {ACM Trans. Sen. Netw.},
month = {jun},
articleno = {23},
numpages = {27},
keywords = {Internet of Things, approximation algorithms, mobile edge clouds, network function virtualization, quality of services}
}

@article{10.1109/TNET.2020.2963886,
author = {Rahman, Mahbubur and Saifullah, Abusayeed},
title = {Integrating Low-Power Wide-Area Networks for Enhanced Scalability and Extended Coverage},
year = {2020},
issue_date = {Feb. 2020},
publisher = {IEEE Press},
volume = {28},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2963886},
doi = {10.1109/TNET.2020.2963886},
abstract = {Low-Power Wide-Area Networks (LPWANs) are evolving as an enabling technology for Internet-of-Things (IoT) due to their capability of communicating over long distances at very low transmission power. Existing LPWAN technologies, however, face limitations in meeting scalability and covering very wide areas which make their adoption challenging for future IoT applications, especially in infrastructure-limited rural areas. To address this limitation, in this paper, we consider achieving scalability and extended coverage by integrating multiple LPWANs. &lt;italic&gt;&lt;bold&gt;SNOW (Sensor Network Over White Spaces)&lt;/bold&gt;&lt;/italic&gt;, a recently proposed LPWAN architecture over the TV white spaces, has demonstrated its advantages over existing LPWANs in performance and energy-efficiency. In this paper, we propose to scale up LPWANs through a seamless integration of multiple SNOWs which enables concurrent inter-SNOW and intra-SNOW communications. We then formulate the tradeoff between scalability and inter-SNOW interference as a constrained optimization problem whose objective is to maximize scalability by managing white space spectrum sharing across multiple SNOWs. We also prove the NP-hardness of this problem. To this extent, We propose an intuitive polynomial-time heuristic algorithm for solving the scalability optimization problem which is highly efficient in practice. For the sake of theoretical bound, we also propose a simple polynomial-time &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$frac {1}{2}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;-approximation algorithm for the scalability optimization problem. Hardware experiments through deployment in an area of (&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$25times 15$ &lt;/tex-math&gt;&lt;/inline-formula&gt;)km&lt;sup&gt;2&lt;/sup&gt; as well as large scale simulations demonstrate the effectiveness of our algorithms and feasibility of achieving scalability through seamless integration of SNOWs with high reliability, low latency, and energy efficiency.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {413–426},
numpages = {14}
}

@article{10.1145/3625095,
author = {Liu, Jinhui and Zhang, Feng},
title = {Language Model Method for Collocation Rules of Parts of Speech in Machine Translation System},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3625095},
doi = {10.1145/3625095},
abstract = {With the development of the times, modern society has now entered the Internet of Things (IoT) information age and Machine Translation (MT) plays an important role in increasingly frequent cross-language communication. In recent years, China's artificial intelligence industry has been in a stage of rapid construction, and the scale of its core industries has grown explosively, and a large number of artificial intelligence companies, including issuers, have emerged. Part of speech has always been a major problem in MT. One of the reasons is that there are a large number of multi-category words in Chinese and a large number of polysemy words in English, so part of speech collocation problems account for a large proportion of MT errors, which to some extent affects the credibility and accuracy of the translation. To reduce the error problem in MT of part of speech collocation, this paper used Machine Learning (ML) methods to study the Language Model (LM) of part of speech collocation based on recurrent neural network (NN) and compared it with the traditional statistical LM. In terms of the accuracy rate of the two LMS in the automatic evaluation index of machine translation, the experimental results show that the recursive NN LM established by the ML method had an accuracy rate of 80.42% and 83.57% respectively for the part-of-speech matching rules of the IoT machine translation system in the dialogue between Chinese and English and the translation of articles. The accuracy of traditional statistical LM evaluation was 71.29% and 69.52%, respectively. Compared to traditional statistical LM, the accuracy of translation was higher. This showed that the recurrent NN LM reduced the number of errors in the collocation of parts of speech in MT and improved the accuracy and credibility of MT.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {Machine Translation, Part Collocation Rules, Machine Learning, Language Model, Neural Networks}
}

@inproceedings{10.1145/3502300.3502305,
author = {Yao, Le and Zhuang, Honglin and Su, Qianye and Lin, Zhechao and Gu, Jiaxiang},
title = {Automatic Smart Device Identification Based on Web Fingerprint and Neural Network},
year = {2022},
isbn = {9781450390552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502300.3502305},
doi = {10.1145/3502300.3502305},
abstract = {Cyberspace smart devices refer to the general term for devices that can connect to the Internet and have data processing functions. With the development of the Internet of Things technology, new smart devices such as webcams have brought convenience to people's lives, but also exposed many security vulnerabilities. Discovering smart devices in cyberspace is the prerequisite and basis for implementing cyberspace security management. However, the traditional pattern matching identification method requires manual extraction of device keywords, and keeping the keywords intact and updating the fingerprint database hinders accurate and large-scale device discovery. In this regard, this paper proposes a smart device identification method based on Web fingerprints and neural network. We use asynchronous stateless scanning and web crawlers to obtain the target's HTTP response data, extract the text in the response data based on natural language processing technology, and use neural networks to build a classification model. After processing, the response data of each IP is converted into concise text as a feature vector, and these texts are finally used to train the neural network model to realize the identification of smart devices. In order to prove the effectiveness of the algorithm, this paper uses Python to implement a prototype system and conduct experimental verification to evaluate its performance. Experiments show that among the four neural network models used, the RCNN model can converge in the shortest time and reach a training accuracy of 98.66%, and an accuracy of 90.59% on the test set. It is verified that the algorithm proposed in this paper is feasible and has a good ability to recognize smart devices.},
booktitle = {Proceedings of the 2021 3rd International Conference on Big-Data Service and Intelligent Computation},
pages = {33–41},
numpages = {9},
keywords = {crawlers, cyberspace security, fingerprint, identification, smart devices, webpage},
location = {Xiamen, China},
series = {BDSIC '21}
}

@inproceedings{10.1145/3465481.3465747,
author = {Paul, Sebastian and Schick, Felix and Seedorf, Jan},
title = {TPM-Based Post-Quantum Cryptography: A Case Study on Quantum-Resistant and Mutually Authenticated TLS for IoT Environments},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3465747},
doi = {10.1145/3465481.3465747},
abstract = {The prospect of large-scale quantum computers necessitates the design, development, and standardization of post-quantum cryptography (PQC). Industrial control systems (ICS) and critical infrastructures are expected to be among the first industrial environments to adopt PQC. As their components have a long life span (≥ 10 years) and are increasingly interconnected to form an Industrial Internet of Things (IIoT), they require strong and long-lasting security guarantees. Because of these high-security requirements, IIoT products are also increasingly equipped with additional hardware security elements&nbsp;—&nbsp;often Trusted Platform Modules (TPMs). In this work, we study how the current TPM 2.0 specification can supplement the migration towards PQC. Therefore, we integrate the post-quantum (PQ) key exchange CRYSTALS-Kyber, the post-quantum signature scheme SPHINCS, and TPM functionality into the open-source TLS library Mbed&nbsp;TLS. For our performance evaluations we propose three post-quantum TLS cipher suites alongside two different TPM utilization strategies. We report the standalone performance of the aforementioned post-quantum schemes under our proposed TPM utilizations and compare it to current elliptic curve cryptography (ECC). Finally, we report the handshake duration of post-quantum and mutually authenticated TLS (mTLS) connections for our proposed cipher suites with regards to the different TPM utilization scenarios. Our results show that the integration of PQC into mTLS is generally feasible, thus ensuring additional post-quantum client authentication. Regarding our TPM utilizations, we observe a significant decrease in performance when offloading computations of hash functions. However, offloading the generation of random numbers to TPMs in our integrated post-quantum schemes proves to be efficient, ultimately enhancing overall system security.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {3},
numpages = {10},
keywords = {Mbed TLS, Mutual Authentication, Post-Quantum Cryptography, Transport Layer Security, Trusted Platform Module},
location = {Vienna, Austria},
series = {ARES '21}
}

@article{10.14778/3554821.3554839,
author = {Lakshman, Sarath and Gupta, Apaar and Suri, Rohan and Lashley, Scott and Liang, John and Duvuru, Srinath and Mayuram, Ravi},
title = {Magma: a high data density storage engine used in couchbase},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554839},
doi = {10.14778/3554821.3554839},
abstract = {We present Magma, a write-optimized high data density key-value storage engine used in the Couchbase NoSQL distributed document database. Today's write-heavy data-intensive applications like ad-serving, internet-of-things, messaging, and online gaming, generate massive amounts of data. As a result, the requirement for storing and retrieving large volumes of data has grown rapidly. Distributed databases that can scale out horizontally by adding more nodes can be used to serve the requirements of these internet-scale applications. To maintain a reasonable cost of ownership, we need to improve storage efficiency in handling large data volumes per node, such that we don't have to rely on adding more nodes. Our current generation storage engine, Couchstore is based on a log-structured append-only copy-on-write B+Tree architecture. To make substantial improvements to support higher data density and write throughput, we needed a storage engine architecture that lowers write amplification and avoids compaction operations that rewrite the whole database files periodically.We introduce Magma, a hybrid key-value storage engine that combines LSM Trees and a segmented log approach from log-structured file systems. We present a novel approach to performing garbage collection of stale document versions avoiding index lookup during log segment compaction. This is the key to achieving storage efficiency for Magma and eliminates the need for random I/Os during compaction. Magma offers significantly lower write amplification, scalable incremental compaction, and lower space amplification while not regressing the read amplification. Through the efficiency improvements, we improved the single machine data density supported by the Couchbase Server by 3.3x and lowered the memory requirement by 10x, thereby reducing the total cost of ownership up to 10x. Our evaluation results show that Magma outperforms Couchstore and RocksDB in write-heavy workloads.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3496–3508},
numpages = {13}
}

@proceedings{10.1145/3530050,
title = {BiDEDE '22: Proceedings of The International Workshop on Big Data in Emergent Distributed Environments},
year = {2022},
isbn = {9781450393461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Today, new forms of distributed environments beyond Cloud Computing occur that offer new kinds of applications, but pose new challenges for data management. The recent efforts for serverless computing aim at simplifying the process of deploying code in the Cloud into production by hiding scaling, capacity planning and maintenance operations from the developer or operator. Other initiatives work on avoiding the communication to the Cloud by deploying and running environments for data processing near data sources in Internet-of-Things scenarios (e.g., fog and edge computing) for large-scale smart homes, companies and cities, and near the applications (e.g., Cloudlets for mobile applications and Offline First technologies for web applications).Research on distributed data management evolves addressing new challenges specific to these new environments. Properties of emergent distributed environments regarding capabilities of nodes, bandwidth for communication, battery lifetime of nodes, reliability of nodes and communication, and heterogeneity of configurations impact data management mechanisms and approaches, such as those for fault tolerance, replication, resource provisioning, buffer management, query processing and optimization, and transaction management. In addition, federated approaches and polystores spanning over several emergent distributed environments are also remaining research challenges based on the need for combining these different distributed environments into one distributed runtime environment for easy handling of Big Data in different models and globally optimizing data management tasks across these different environments.The goal of this workshop is to bring together academic researchers and industry practitioners to discuss the challenges and solutions, including new approaches, techniques and applications, that significantly would advance the state of the art of Big Data in emergent distributed environments.},
location = {Philadelphia, Pennsylvania}
}

@inproceedings{10.1145/3459960.3461561,
author = {Karatza, Helen},
title = {Large Scale Real-time Distributed Systems – Resource Allocation and Scheduling Issues},
year = {2021},
isbn = {9781450390576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459960.3461561},
doi = {10.1145/3459960.3461561},
abstract = {Due to the advances in networks and computing systems, many aspects of our daily life depend on distributed interconnected computing resources. Large-scale distributed systems offer computational services to scientists, consumers and enterprises. Efficient management of distributed resources is crucial to use effectively the power of these systems and achieve good performance. Large-scale distributed systems are usually real-time as they are used for serving applications which require real-time processing. It is essential that appropriate resource allocation and scheduling techniques are utilized ensuring timeliness. Cloud computing, as a large-scale distributed computing paradigm based on a pay-as-you-go pricing model, has been extensively used for the deployment of complex computationally intensive applications. Particularly important in cloud computing is to run delay-sensitive applications. This can be achieved due to cloud’s high-performance computing capabilities for real-time execution. However, approaches to resolve other issues such as cost and energy conservation are necessary. In the last years, there is an expansion of the Internet of Things (IoT). There is a plethora of IoT applications which generate huge amounts of data and it is important to process these data in real-time and provide fast decisions. As a result, fog computing has emerged as a computing model which extends the cloud to the edge of the network, thus reducing the latency of IoT data transmission. The computational capacity of fog resources is usually limited, therefore it is necessary to employ algorithms that involve the collaboration between the cloud and fog resources. Consequently, appropriate scheduling of time-sensitive applications is required to exploit the capacity of cloud and fog computing so that the workload’s deadlines are met. In this keynote, we will present and discuss various aspects of large-scale real-time distributed systems, from the perspective of resource allocation and scheduling and we will conclude with future directions in this research area.},
booktitle = {7th Conference on the Engineering of Computer Based Systems},
articleno = {1},
numpages = {2},
keywords = {large-scale distributed systems, real-time systems, scheduling},
location = {Novi Sad, Serbia},
series = {ECBS 2021}
}

@inproceedings{10.1145/3466772.3467054,
author = {Al-Shawabka, Amani and Pietraski, Philip and Pattar, Sudhir B. and Restuccia, Francesco and Melodia, Tommaso},
title = {DeepLoRa: Fingerprinting LoRa Devices at Scale Through Deep Learning and Data Augmentation},
year = {2021},
isbn = {9781450385589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466772.3467054},
doi = {10.1145/3466772.3467054},
abstract = {The Long Range (LoRa) protocol for low-power wide-area networks (LPWANs) is a strong candidate to enable the massive roll-out of the Internet of Things (IoT) because of its low cost, impressive sensitivity (-137dBm), and massive scalability potential. As tens of thousands of tiny LoRa devices are deployed over large geographic areas, a key component to the success of LoRa will be the development of reliable and robust authentication mechanisms. To this end, Radio Frequency Fingerprinting (RFFP) through deep learning (DL) has been heralded as an effective zero-power supplement or alternative to energy-hungry cryptography. Existing work on LoRa RFFP has mostly focused on small-scale testbeds and low-dimensional learning techniques; however, many challenges remain. Key among them are authentication techniques robust to a wide variety of channel variations over time and supporting a vast population of devices.In this work, we advance the state of the art by presenting (i) the first massive experimental evaluation of DL RFFP and (ii) new data augmentation techniques for LoRa designed to counter the degradation introduced by the wireless channel. Specifically, we collected and publicly shared more than 1TB of waveform data from 100 bit-similar devices (with identical manufacturing processes) over different deployment scenarios (outdoor vs. indoor) and spanning several days. We train and test diverse DL models (convolutional and recurrent neural networks) using either preamble or payload data slices. We compare three different representations of the received signal: (i) IQ, (ii) amplitude-phase, and (iii) spectrogram. Finally, we propose a novel data augmentation technique called DeepLoRa to enhance the LoRa RFFP performance. Results show that (i) training the CNN models with IQ representation is not always the best combo in fingerprinting LoRa radios; training CNNs and RNN-LSTMs with amplitude-phase and spectrogram representations may increase the fingerprinting performance in small and medium-scale testbeds; (ii) using only payload data in the fingerprinting process outperforms preamble only data, and (iii) DeepLoRa data augmentation technique improves the classification accuracy from 19% to 36% in the RFFP challenging case of training on data collected on a different day than the testing data. Moreover, DeepLoRa raises the accuracy from 82% to 91% when training and testing 100 devices with data collected on the same day.},
booktitle = {Proceedings of the Twenty-Second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {251–260},
numpages = {10},
keywords = {Datasets, Deep Learning, LoRa, Radio Fingerprinting, Testbed},
location = {Shanghai, China},
series = {MobiHoc '21}
}

@inproceedings{10.1145/3384419.3430574,
author = {Imteaj, Ahmed},
title = {Distributed machine learning for collaborative mobile robots: PhD forum abstract},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430574},
doi = {10.1145/3384419.3430574},
abstract = {The Internet-of-things (IoT) devices and technologies led to a revolutionary breakthrough over the data collection procedure and Machine Learning (ML) approaches of a distributed network. It is preferable to store sensitive data on-device without sharing with a centralized computation agent and carry-out computation at the edge devices to ensure security and privacy. A recently invented distributed ML technique, Federated Learning (FL) holds the same theme that allows the edge devices to perform training on their edges and obtains a final model by learning from the model information of all the distributed edge clients. As the clients' raw data remain at local and models are generated on clients' edge, so it enhances security, privacy, and reduces computation cost in large-scale ML problems. The FL technique deals with various distributed clients that may have statistical heterogeneity and systems heterogeneity. This paper aims at dealing with such heterogeneity within an FL environment by monitoring each client's activities and leveraging resources based on the required computation during the model training phase. For each training round, we consider the proficient and trustworthy client by inspecting their resource-availability and previous history. We assign a trust score to each client based on their performance and update that score after each training period. To bring systems heterogeneity within our FL environment, we consider distributed mobile robots as FL clients with heterogeneous system configurations in terms of memory, processor, bandwidth, or battery life to understand their performance and resource-constraint behavior in a real-world setting. We filter-out the weak clients who cannot perform computation based on their available resources and exclude the untrustworthy clients who has previous record of repeatedly infusing incorrect or diverge model information, or, become stragglers during FL training. After eliminating the stragglers and untrustworthy FL clients, we conduct local training on each selected FL clients. To further mitigate the straggler issue, we enable asynchronous FL technique that can handle the clients' variant response time and continue FL training without waiting for a particular client for a long period.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {798–799},
numpages = {2},
keywords = {IoT, client activity, distributed mobile robots, federated learning (FL), resource-limitations, straggler},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@article{10.1145/3403581,
author = {Murillo, Yuri and Chiumento, Alessandro and Reynders, Brecht and Pollin, Sofie},
title = {An All-wireless SDN Framework for BLE Mesh},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3403581},
doi = {10.1145/3403581},
abstract = {The Internet of Things (IoT) paradigm combines the interconnection of massive amounts of battery-constrained and low-computational-power devices with low-latency and high-reliability network requirements. Additionally, diverse end-to-end services and applications with different Quality of Service (QoS) requirements are expected to coexist in the same network infrastructure. Software-defined Networking (SDN) is a paradigm designed to solve these problems, but its implementation in wireless networks and especially in the resource-constrained IoT systems is extremely challenging and has seen very limited adoption, since it requires isolation of data and control plane information flows and a reliable and scalable control plane. In this work, Bluetooth Low Energy (BLE) mesh is introduced as an adequate technology for an all-wireless SDN-BLE implementation, which is a technology that has become the de-facto standard for IoT. The proposed SDN-BLE framework uses a routing network slice for the data plane information flow and a flooding network slice for the control plane information flow, ensuring their isolation while still being transmitted over the wireless medium. The design and implementation of all the classical SDN layers on a hybrid BLE mesh testbed is given, where the data plane is formed by the BLE nodes and the control plane can be centralized on a server or distributed over several WiFi gateways. Several controllers are described and implemented, allowing the framework to obtain end-to-end network knowledge to manage individual nodes over the air and configure their behavior to meet application requirements. An experimental characterization of the SDN-BLE framework is given, where the impact of the different parameters of the system on the network reliability, overhead, and energy consumption is studied. Additionally, the distributed versus centralized control plane operation modes are experimentally characterized, and it is shown that the distributed approach can provide the same performance as the centralized one when careful system design is performed. Finally, a proof of concept for the SDN-BLE framework is presented, where a network congestion is automatically detected and the nodes responsible of such congestion are identified and reconfigured over the air, bypassing the congested links, to resume regular network performance.},
journal = {ACM Trans. Internet Things},
month = {aug},
articleno = {27},
numpages = {30},
keywords = {BLE, BLE mesh, FruityMesh, IoT, SDN, WSN, testbed design, trickle}
}

@inproceedings{10.1145/3372177.3373342,
author = {Karpova, G. A. and Kuchumov, A. V. and Testina, Y. S. and Voloshinova, M. V.},
title = {Digitalization of a Tourist Destination},
year = {2020},
isbn = {9781450372442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372177.3373342},
doi = {10.1145/3372177.3373342},
abstract = {Today digitalization is no longer an innovative process but a necessity, since it enhances interconnection among subjects using digital technologies; it is involved in all the economy branches, including the tourism industry. One of the most important study objects in tourism is a tourist destination. Its digitalization needs to be interconnected with digitalization of tourism on all the levels of economy.Tourist destination digitalization can be viewed from the perspectives of both a process and a systematic approach. However, they do not deny but complement each other.The article reveals the tools using which an increase in the level of digitalization of the territory is achieved. The following 10 types of instruments were highlighted: digital marketing, cloud technologies, information support, Internet of things, virtual tourism, smart territory, navigation satellite systems and geographic information systems, electronic document management, artificial intelligence, big data, as well as some distributed registry utilities. Their application in the complex will increase the income of the territory, optimize tourist flows, and allow to effectively make management decisions.We have developed a rank scale of digitalization and an index of digitalization of the tourist destination as methods for assessing the level of digitalization.The rank scale is applied when considering a digital destination according to the process approach; its values depend on the number of digital tools used by the tourist destination in the digitalization process. We need to remark that in order to achieve the minimum level of digitalization, a destination has to have its own Internet website and use one or two of the digitalization tools described, and to achieve the maximum level---from nine to ten tools.The tourism destination digitalization index is used when considering digitalization as a system. It is based on an assessment of the effectiveness of digitalization and represents the ratio of the gross tourist product of a destination and the cost of implementing digital tools.Different levels of use and a set of digital tools characterize tourist destinations; however, they should all strive to improve their level of digitalization.},
booktitle = {Proceedings of the 2019 International SPBPU Scientific Conference on Innovations in Digital Economy},
articleno = {39},
numpages = {6},
keywords = {Digitalization, Digitalization index, Tourism, Tourist destination},
location = {Saint Petersburg, Russian Federation},
series = {SPBPU IDE '19}
}

@article{10.1145/3507911,
author = {Pavlopoulou, Niki and Curry, Edward},
title = {PoSSUM: An Entity-centric Publish/Subscribe System for Diverse Summarization in Internet of Things},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3507911},
doi = {10.1145/3507911},
abstract = {Users are interested in entity information provided by multiple sensors in the Internet of Things. The challenges regarding this environment span from data-centric ones due to data integration, heterogeneity, and enrichment, to user-centric ones due to the need for high-level data interpretation and usability for non-expert users, to system-centric ones due to resource constraints. Publish/Subscribe systems (PSSs) are suitable schemes for large-scale applications, but they are limited in dealing with the data and user challenges. In this article, we propose PoSSUM, a novel entity-centric PSS that provides entity summaries for user-friendly subscriptions through data integration, a novel Density-Based VARiance Clustering (DBVARC) for diverse entity summarization that is parameter-free and partly incremental, reasoning rules, and a novel Triple2Rank scoring for top-k filtering based on importance, informativeness, and diversity. We introduce a novel evaluation methodology that creates ground truths and metrics that capture the quality of entity summaries. We compare our approach with a previous dynamic approach and a static diverse entity summarization approach that we adapted to dynamic environments. The evaluation results for two use cases, Healthcare and Smart Cities, show that when users are provided with less information, their data diversity desire could reach up to 80%. Summarization approaches achieve from 80% to 99% message reduction, with PoSSUM having the best-ranking quality for more than half of the entities by a significant margin. PoSSUM has the highest conceptual clustering F-score, ranging from 0.69 to 0.83, and a redundancy-aware F-score up to 0.95, with cases, where it is almost two times better than the other approaches. PoSSUM takes 50% or less clustering processing time and it performs scoring significantly faster for larger windows. It also has comparable end-to-end latency and throughput values, and it occupies a third of the memory compared to the second-best approach.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {73},
numpages = {30},
keywords = {Diverse entity summarization, triple ranking, Top-k filtering, Internet of Things}
}

@inproceedings{10.1145/3444465.3444535,
author = {Dashkina, Alexandra and Khalyapina, Ludmila and Kobicheva, Aleksandra and Lazovskaya, Tatiana and Malykhina, Galina and Tarkhov, Dmitriy},
title = {Neural Network Modeling as a Method for Creating Digital Twins: From Industry 4.0 to Industry 4.1},
year = {2021},
isbn = {9781450388313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444465.3444535},
doi = {10.1145/3444465.3444535},
abstract = {Digital twins are one of the key technologies behind the Fourth Industrial Revolution. In the coming years they will be introduced on a large scale in the industry and in other spheres. A wide range of digital twins will be in demand: from separate components to complex technical facilities, such as automobiles, airplanes, manufacturing lines, factories, corporations, etc. To provide their successful interaction, it is important to create digital twins on the uniform principles. Currently, creating a digital twin is a complex scientific issue. It presents difficulties because it is necessary not only to describe physical (or chemical, biological, etc.) processes going on in the object, but also to envisage significant changes of its properties in the course of its operation. In this case the digital twin is supposed to adapt to the changes in the original object in accordance with the data received from the sensors.The aim of the research was to define the strategies of solving the current problems in such areas as digital twins, the internet of things and cyberphysical systems.In order to achieve this aim, the following problems were supposed to be solved:- Consider the definitions of the digital twin suggested in the world scientific literature- Find a unified data-driven real-time approach to creating digital twins- Suggest using the neural network approach in creating digital twins.During the use of the modelled object, specifics of the physical processes going on in it and object properties can change. The model is supposed to adapt in accordance with these changes, which is rather difficult if a model is generated by applying computer-aided engineering software packages (CAE) based on classical numerical methods.We consider the multistage technique as more promising. It involves building an adaptive model at the second stage. Such a model can be specified and redesigned based on real-time data. Since neural networks have proved to be efficient in solving complicated problems related to data processing, we recommend using them as the basic class of mathematical models for creating digital twins.},
booktitle = {Proceedings of the 2nd International Scientific Conference on Innovations in Digital Economy},
articleno = {18},
numpages = {5},
keywords = {Industry 4.0, data set, digital twins, mathematical model, neural network modelling},
location = {Saint, Petersburg, Russian Federation},
series = {SPBPU IDE '20}
}

@article{10.1145/3605949,
author = {Pal, Ranjan and Sequeira, Rohan Xavier and Yin, Xinlong and Zeijlemaker, Sander and Kotala, Vineeth},
title = {How Should Enterprises Quantify and Analyze (Multi-Party) APT Cyber-Risk Exposure in their Industrial IoT Network?},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2158-656X},
url = {https://doi.org/10.1145/3605949},
doi = {10.1145/3605949},
abstract = {Industrial Internet of Things (IIoT) networks (e.g., a smart grid industrial control system) are increasingly on the rise, especially in smart cities around the globe. They contribute to meeting the day-to-day needs (e.g., power, water, manufacturing, transportation) of the civilian society, alongside making societal businesses more efficient, productive, and profitable. However, it is also well known that IoT devices often operate on poorly configured security settings. This increases the chances of occurrence of (nation-sponsored) stealthy spread-based APT malware attacks in IIoT networks that might go undetected over a considerable period of time. Such attacks usually generate a negative first-party QoS impact with financial consequences for companies owning such IIoT network infrastructures. This impact spans (i.e., aggregates) space (i.e., the entire IIoT network or a sub-network) and time (i.e., duration of business disruption), and is a measure of significant interest to managers running their businesses atop such networks. It is of little use to network resilience boosting managers if they have to wait for a cyber-attack to happen to gauge this impact. Consequently, one of the questions that intrigues us is: can managers estimate this first-party impact prior to APT cyber-attack(s) causing financial damage to companies?In this paper, we propose the first computationally efficient and quantitative network theory framework to (a) characterize this first-party impact apriori as a statistical distribution over multiple attack configurations in a family of malware-driven APT cyber-attacks specifically launched on businesses running atop IIoT networks, (b) accurately compute the statistical moments (e.g., mean) of the resulting impact distribution, and (c) tightly bound the accuracy of worst-case risk estimate of such a distribution - captured through the tail of the distribution, using the Conditional Value at Risk (CVaR) metric. In relation to (a) above, our methodology extends the seminal Factor Analysis of Information Risk (FAIR) cyber-risk quantification methodology that does not explicitly account for network interconnections among system-risk contributing variables. We validate the effectiveness of our theory using trace-driven Monte Carlo simulations based upon test-bed experiments conducted in the FIT IoT-Lab. We further illustrate quantitatively that even if spread-based APT cyber-attacks induce a statistically light-tailed first-party cyber-loss distribution on an IIoT networked enterprise in the worst case, the aggregate multi-party cyber-risk distribution incurred by the same enterprise in supply-chain ecosystems could be heavy-tailed. This will pose significant market scale-up challenges to cyber-security improving commercial cyber (re-)insurance businesses. We subsequently propose managerial action items to mitigate the first-party cyber-risk exposure emanating from any given IIoT driven enterprise.},
note = {Just Accepted},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {oct},
keywords = {IIoT, APT, network, cyber-risk, security, CVaR, supply chain, insurance}
}

@inproceedings{10.1145/3488661.3494033,
author = {Kandoi, Rajat and Hartke, Klaus},
title = {Operating large-scale IoT systems through declarative configuration APIs},
year = {2021},
isbn = {9781450391368},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488661.3494033},
doi = {10.1145/3488661.3494033},
abstract = {Configuration activities constitute a large part of the work in operating an IoT system. Such activities include the onboarding of devices and rollout of firmware updates. Configuration activities must be carefully vetted to prevent faulty states, putting a lot of pressure on IoT system operators to get it right. The problem is further exacerbated due to the heterogeneity, scalability, and distributed computation challenges typical of large-scale IoT systems. Therefore ensuring safe and reliable operations requires careful design. In this paper, we argue that a declarative approach to configuration, complemented with a discovery-driven API design is ideally suited to solve these challenges. We present suitable abstractions needed to realize such a declarative configuration API. Our experiences show that the proposed abstractions and API model are well suited for the purpose of large-scale IoT systems, and allow for high degree of safety and reliability.},
booktitle = {Proceedings of the 2021 Workshop on Descriptive Approaches to IoT Security, Network, and Application Configuration},
pages = {22–25},
numpages = {4},
location = {Virtual Event, Germany},
series = {DAI-SNAC '21}
}

@inproceedings{10.1145/3381271.3381288,
author = {Yuan, Yaohui and Wang, Xingjun and Bin, Guangxiang},
title = {Analysis of user behavior in a large-scale internet video-on-demand(VoD) system},
year = {2020},
isbn = {9781450376648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3381271.3381288},
doi = {10.1145/3381271.3381288},
abstract = {With the development and popularity of the Internet, Internet traffic has increased dramatically. Numerous studies have shown that video accounts for a large percentage of Internet traffic, and this percentage is still rising in the future. A good understanding of user behavior in online video systems can help us design, configure and manage video content distribution to alleviate network stress. In this paper, we did a detailed analysis of user behavior data for Internet video. Our research shows that the user's daily access and online pattern of users have a fixed pattern, and the user's access behavior conforms to Zipf's law. Besides, we optimized the fit of Zipf-like distribution of video's popularity. Finally, we built a reliable simulation system that simulates user behavior data. Overall, we believe that the results presented in this paper are very important and valuable to the whole network.},
booktitle = {Proceedings of the 5th International Conference on Multimedia and Image Processing},
pages = {153–158},
numpages = {6},
keywords = {Zipf's law, user behavior, video characteristics},
location = {Nanjing, China},
series = {ICMIP '20}
}

@inproceedings{10.1145/3558819.3565078,
author = {Huang, Yao and Zhang, Jingqiu and Sun, He and Xie, Min and Li, Qingyu},
title = {Research and implementation of IP address management in medium and large-scale local area networks},
year = {2022},
isbn = {9781450397414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558819.3565078},
doi = {10.1145/3558819.3565078},
abstract = {In order to improve the management level of IP address in medium and large-scale local area networks, realize the dynamic and unified distribution of IP addresses of the user, simplify the terminal Internet parameter setting, and improve the security level of the information system, this project researches the deployment of DHCP server in medium and large-scale local area networks, supporting Web visual management, and realizing the functions of assigning fixed IP addresses to users in the network, and carrying out management and statistical analysis. Through the simulation analysis in VMware Workstation, the functions of the above network architecture are verified. This project can be popularized and applied in banking network system and other networks, which can effectively improve the management efficiency and strengthen the unified and centralized management of network addresses and related information.},
booktitle = {Proceedings of the 7th International Conference on Cyber Security and Information Engineering},
pages = {204–207},
numpages = {4},
location = {<conf-loc>, <city>Brisbane</city>, <state>QLD</state>, <country>Australia</country>, </conf-loc>},
series = {ICCSIE '22}
}

@inproceedings{10.1145/3394171.3413899,
author = {Liu, Tie and Xu, Mai and Li, Shengxi and Ding, Rui and Liu, Huaida},
title = {MRS-Net: Multi-Scale Recurrent Scalable Network for Face Quality Enhancement of Compressed Videos},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413899},
doi = {10.1145/3394171.3413899},
abstract = {The past decade has witnessed the explosive growth of faces in video multimedia systems, e.g., videoconferencing and live shows. However, these videos are normally compressed at low bit-rates due to the bandwidth-hungry issue, leading to heavy quality degradation on face regions. This paper addresses the problem of face quality enhancement in compressed videos. Specifically, we establish a compressed face video (CFV) database, which includes 87,607 faces in 113 raw video sequences and their corresponding 904 compressed sequences. We find that the faces of compressed videos exhibit tremendous scale variation and quality fluctuation. Motivated by scalable video coding, we propose a multi-scale recurrent scalable network (MRS-Net) to enhance the quality of multi-scale faces in compressed videos. The MRS-Net is comprised by one base and two refined enhancement levels, corresponding to the quality enhancement of small-, medium- and large-scale faces, respectively. In the multi-level architecture of our MRS-Net, small-/medium-scale face quality enhancement serves as the basis for facilitating the quality enhancement of medium-/large-scale faces. Finally, experimental results show that our MRS-Net method is effective in enhancing the quality of multi-scale faces for compressed videos, significantly outperforming other state-of-the-art methods.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {3292–3301},
numpages = {10},
keywords = {database, face quality enhancement, scalable structure},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3567445.3571113,
author = {Ramanathan, Ganesh and Marella, Srinivas},
title = {Every Thing Under the Sun: How Web of Things and Semantic Data Brings Benefit to Small-Scale Photovoltaic Installations},
year = {2023},
isbn = {9781450396653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567445.3571113},
doi = {10.1145/3567445.3571113},
abstract = {Small-scale photovoltaic (PV) systems of up to a few kilowatts capacities are becoming increasingly available and affordable for off-grid installations. However, in our experience with using PV energy in farming in India, we found that many installations had faults or were lying underutilized. Though integrating such systems into IoT applications is now practical, analyzing the system’s performance and utilization requires knowledge of the components and the system design. Off-band infusion of this knowledge into the software applications leads to tight coupling and vertical silos. To address this challenge, we have developed an ontology to describe small-scale PV installations, which enables us to represent subsystems and their components in the form of Web of Things (WoT) Thing Descriptions. We show that our approach results in technical and semantic integration of the PV system into IoT applications, allowing the development of reusable fault detection and optimization programs. This reduces the cost of developing solutions to monitor and optimize the usage of PV systems, thereby bringing benefits to the farming community by improving their livelihood.},
booktitle = {Proceedings of the 12th International Conference on the Internet of Things},
pages = {231–238},
numpages = {8},
keywords = {Photovoltaics, Renewable Energy, Semantic Data, Web of Things},
location = {Delft, Netherlands},
series = {IoT '22}
}

@inproceedings{10.1145/3485730.3493684,
author = {Nagothu, Deeraj and Xu, Ronghua and Chen, Yu and Blasch, Erik and Aved, Alexander},
title = {Detecting Compromised Edge Smart Cameras using Lightweight Environmental Fingerprint Consensus},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3493684},
doi = {10.1145/3485730.3493684},
abstract = {Rapid advances in the Internet of Video Things (IoVT) deployment in modern smart cities has enabled secure infrastructures with minimal human intervention. However, attacks on audio-video inputs affect the reliability of large-scale multimedia surveillance systems as attackers are able to manipulate the perception of live events. For example, Deepfake audio/video attacks and frame duplication attacks can cause significant security breaches. This paper proposes a Lightweight Environmental Fingerprint Consensus based detection of compromised smart cameras in edge surveillance systems (LEFC). LEFC is a partial decentralized authentication mechanism that leverages Electrical Network Frequency (ENF) as an environmental fingerprint and distributed ledger technology (DLT). An ENF signal carries randomly fluctuating spatio-temporal signatures, which enable digital media authentication. With the proposed DLT consensus mechanism named Proof-of-ENF (PoENF) as a backbone, LEFC can estimate and authenticate the media recording and detect byzantine nodes controlled by the perpetrator. The experimental evaluation shows feasibility and effectiveness of proposed LEFC scheme under a distributed byzantine network environment.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {505–510},
numpages = {6},
keywords = {Deepfake Detection, Electrical Network Frequency (ENF) Signals, Environmental Fingerprint, Proof-of-ENF (PoENF) Consensus},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{10.1145/3587135.3592179,
author = {Kunkel, Julian Martin and Boehme, Christian and Decker, Jonathan and Magugliani, Fabrizio and Pleiter, Dirk and Koller, Bastian and Sivalingam, Karthee and Pllana, Sabri and Nikolov, Alexander and Soyturk, Mujdat and Racca, Christian and Bartolini, Andrea and Tate, Adrian and Yaman, Berkay},
title = {DECICE: Device-Edge-Cloud Intelligent Collaboration Framework},
year = {2023},
isbn = {9798400701405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587135.3592179},
doi = {10.1145/3587135.3592179},
abstract = {DECICE is a Horizon Europe project that is developing an AI-enabled open and portable management framework for automatic and adaptive optimization and deployment of applications in computing continuum encompassing from IoT sensors on the Edge to large-scale Cloud / HPC computing infrastructures. In this paper, we describe the DECICE framework and architecture. Furthermore, we highlight use-cases for framework evaluation: intelligent traffic intersection, magnetic resonance imaging, and emergency response.},
booktitle = {Proceedings of the 20th ACM International Conference on Computing Frontiers},
pages = {266–271},
numpages = {6},
keywords = {AI-enabled Computing Continuum, Cloud-Edge Orchestration, Cognitive Cloud, Digital Twin, KubeEdge},
location = {Bologna, Italy},
series = {CF '23}
}

@inproceedings{10.1145/3549206.3549224,
author = {Thakkar, Riddhi and Bhavsar, Madhuri},
title = {Achieving multilevel elasticity for distributed stream processing systems in the cloud environment: A review and conceptual framework},
year = {2022},
isbn = {9781450396752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549206.3549224},
doi = {10.1145/3549206.3549224},
abstract = {Recent awareness and advances in technology have triggered excessive use of social media, IoT devices, remote sensing devices, mobile applications, web applications, and gaming more than ever before in time. Such platforms are hosting their applications on the cloud as it provides various services on a pay-per-use basis. A Cloud Service Provider (CSP) should deliver all its services very swiftly to process real-time applications on time. Real-time stream computations are characteristically long-lived and receive data in an unpredictable form, requiring a fair amount of resources for their processing in constrained time. Such a dynamic nature of applications demands resource elasticity at runtime. The cloud architecture is stacked with different types of resources, each having a discrete adaption process with distinct elasticity properties. Scaling the absolute amount of resources leads to performance boosting. Recent literature landscapes the elasticity at Virtual Machine (VM) level, describing various techniques for scaling VMs. Each technique targets a distinct aspect with specific assumptions. However, the literature lacks a comprehensive survey at the operator level, where actual processing takes place and has a higher impact on the performance of the system. Compared to other works in the literature, this work presents a detailed analysis of various approaches targeting elasticity at the operator level of cloud architecture for stream processing applications, along with the conceptual framework, scaling at the operator, VM, and server levels. We have also discussed the various elastic approaches for scaling the resources at multilevel: VM and operator-level concurrently, for Distributed Stream Processing (DSP) applications running on the cloud. Conceptually, with the proposed framework, we can attain maximum resource utilization at each layer. In future work, we will evaluate the proposed framework with real-world application.},
booktitle = {Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing},
pages = {81–90},
numpages = {10},
keywords = {Big data, Distributed Stream Processing, Elasticity, Multi-level framework},
location = {Noida, India},
series = {IC3-2022}
}

@inproceedings{10.1145/3510450.3517295,
author = {Gregory, Lucas and Jerbi, Khaled and Raulet, Mickael and Toullec, Eric},
title = {Extend CMAF usage for large scale video delivery},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517295},
doi = {10.1145/3510450.3517295},
abstract = {During the last decade, an immense momentum drove HTTP adaptive streaming to become the key protocol for video streaming. This keen interest can be explained by the increasing bandwidth of internet connections and the tremendous progress of mobile networks with the emergence of LTE technologies. Moreover, using TCP based protocol is a grail for both developers and final users as it is supported by most of the connected devices and browsers, and it significantly reduces the handling of network related issues.Among several protocols, the market has been dominated by two protocols: MPEG-DASH and HTTP live streaming. This situation presented a major drawback as content providers need to package the same content twice to ensure covering most of the users. The obvious solution was to unify the packaging and that is what happened with the advent of the playlist agnostic container format called 'Common Media Application Format' (CMAF) [4]. In addition to reducing the number of stored segments for OTT support, CMAF improved the versatility and the interoperability with modern technologies. It has also provided solutions to metadata carriage and to significantly reduce the OTT latency which used to be the bottleneck of the technology.Things will not stop here for CMAF as recently, in March 2020, an ingest protocol based on CMAF was published by the CMAFIF and the DASH-IF and was revised during 2021 [2]. This protocol was initially aiming at defining a push-based communication between an encoder and a receiving entity such as a just-in-time packager or a Content Delivery Network, but it revealed to be a game changer to put CMAF everywhere. Indeed, today's first mile delivery uses mainly MPEG-2 TS and protocols like SRT or ZIXI for B2B headend and this way of encapsulation involves a loss of information as we need to go through specific sub-protocols to carry on meta data such as Advertising, thumbnails, and timed text over MPEG-2 TS. This loss can be avoided by using a complete content carriage with ISO BMFF [3]. Moreover, the ingest protocol allows going beyond the classic flow and make CMAF a robust communication support at large scale between any entities exchanging video streams. It can be a communication between encoders and transcoders or between encoders and storage entities or between a storage and a transcoder or even between a playout and a transcoder.In this paper we present the CMAF ingest protocol and its key features. We also detail the benefits of using this technology compared to those of other existing formats in terms of redundancy, metadata carriage and support of recent codecs and timed text. Then, we present the evolution of CMAF thanks to the ingest protocol to handle video transmission at large scale in a fully standardized fashion. Finally, we will show concrete implementation architecture of communication between an encoder and a smart packager using timed text and timed metadata tracks for SCTE35 carriage [1, 5].},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {117},
numpages = {1},
keywords = {CMAF, OTT, live media ingest protocol},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3447548.3467188,
author = {Hashemi, Helia and Pappu, Aasish and Tian, Mi and Chandar, Praveen and Lalmas, Mounia and Carterette, Benjamin},
title = {Neural Instant Search for Music and Podcast},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467188},
doi = {10.1145/3447548.3467188},
abstract = {Over recent years, podcasts have emerged as a novel medium for sharing and broadcasting information over the Internet. Audio streaming platforms originally designed for music content, such as Amazon Music, Pandora, and Spotify, have reported a rapid growth, with millions of users consuming podcasts every day. With podcasts emerging as a new medium for consuming information, the need to develop information access systems that enable efficient and effective discovery from a heterogeneous collection of music and podcasts is more important than ever. However, information access in such domains still remains understudied. In this work, we conduct a large-scale log analysis to study and compare podcast and music search behavior on Spotify, a major audio streaming platform. Our findings suggest that there exist fundamental differences in user behavior while searching for podcasts compared to music. Specifically, we identify the need to improve podcast search performance. We propose a simple yet effective transformer-based neural instant search model that retrieves items from a heterogeneous collection of music and podcast content. Our model takes advantage of multi-task learning to optimize for a ranking objective in addition to a query intent type identification objective. Our experiments on large-scale search logs show that the proposed model significantly outperforms strong baselines for both podcast and music queries.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2984–2992},
numpages = {9},
keywords = {instant search, music search, neural information retrieval, podcast search},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3485983.3493354,
author = {Ifath, Md. Monzurul Amin and Neves, Miguel and Haque, Israat},
title = {Raptor: rapid prototyping of distributed stream processing applications at scale},
year = {2021},
isbn = {9781450390989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485983.3493354},
doi = {10.1145/3485983.3493354},
abstract = {Stream processing applications are becoming increasingly important in areas such as IoT, video analytics and social media. As a result, developers and operators must meet stringent time-to-market and scale requirements before bringing them to production. Unfortunately, testing a networked stream processing system is currently a cumbersome process that usually requires an expensive testbed and deep expertise on both networking and distributed systems. In this poster, we present Raptor, a tool for the fast prototyping of large-scale networked stream processing applications. Raptor builds on Mininet and Apache Kafka, two widely adopted platforms, to enable stakeholders to easily test their solutions under various operational conditions. Through a reasonably large setup (20 nodes) running on a single server, we show how unbalanced Kafka's leader selection algorithm can be and its implications on the overall system's throughput. We envision this work can help paving the way for more reproducible research in the stream processing domain, currently a first-class network application.},
booktitle = {Proceedings of the 17th International Conference on Emerging Networking EXperiments and Technologies},
pages = {485–486},
numpages = {2},
keywords = {network applications, reproducibility, stream processing},
location = {Virtual Event, Germany},
series = {CoNEXT '21}
}

@inproceedings{10.1145/3607947.3607969,
author = {Apat, Hemant K and Alkhayyat, Ahmed and Vidyarthi, Ankit and Barik, Rabindra K},
title = {Leveraging Towards Serverless Edge Computing Model for Intelligent IoMT Applications},
year = {2023},
isbn = {9798400700224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607947.3607969},
doi = {10.1145/3607947.3607969},
abstract = {Internet of Medical Things(IoMT) devices connect billions of sensors consisting of a large volume of computation-intensive and time-sensitive tasks in the health sector. These tasks need to be processed or executed by different computational devices used in the network infrastructure for getting meaningful information. Due to the resource-constrained nature of IoMT devices, the devices mostly offload these tasks to various edge devices. The IoMT-Cloud paradigm provides different services storage, computation, and communication for the execution of various IoMT applications with elasticity and scalability on the pay-per-use model. Despite the wonderful benefits of cloud computing, there are various issues that remain unsolved like the response time of an IoMT application must satisfy hard deadlines, and Quality of Service(QoS) parameters. In order to improve the response time, and QoS, a decentralized distributed computing paradigm named Edge computing has emerged as a potential solution to provide computing and networking resources at the edge of the network for different IoT application domains. In this article, a serverless edge computing framework is proposed that provides various IoMT services to emerging healthcare applications. It also provides the mathematical derivations of the model along with its client IoMT applications layer, edge layer, and cloud layer.},
booktitle = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
pages = {127–132},
numpages = {6},
keywords = {FaaS, Internet of Medical Things, QoS, cloud computing, edge computing, fog computing, serverless computing},
location = {<conf-loc>, <city>Noida</city>, <country>India</country>, </conf-loc>},
series = {IC3-2023}
}

@inproceedings{10.1145/3523227.3546765,
author = {Wei, Yingcan and Langer, Matthias and Yu, Fan and Lee, Minseok and Liu, Jie and Shi, Ji and Wang, Zehuan},
title = {A GPU-specialized Inference Parameter Server for Large-Scale Deep Recommendation Models},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3546765},
doi = {10.1145/3523227.3546765},
abstract = {Recommendation systems are of crucial importance for a variety of modern apps and web services, such as news feeds, social networks, e-commerce, search, etc. To achieve peak prediction accuracy, modern recommendation models combine deep learning with terabyte-scale embedding tables to obtain a fine-grained representation of the underlying data. Traditional inference serving architectures require deploying the whole model to standalone servers, which is infeasible at such massive scale. In this paper, we provide insights into the intriguing and challenging inference domain of online recommendation systems. We propose the HugeCTR Hierarchical Parameter Server (HPS), an industry-leading distributed recommendation inference framework, that combines a high-performance GPU embedding cache with an hierarchical storage architecture, to realize low-latency retrieval of embeddings for online model inference tasks. Among other things, HPS features (1) a redundant hierarchical storage system, (2) a novel high-bandwidth cache to accelerate parallel embedding lookup on NVIDIA GPUs, (3) online training support and (4) light-weight APIs for easy integration into existing large-scale recommendation workflows. To demonstrate its capabilities, we conduct extensive studies using both synthetically engineered and public datasets. We show that our HPS can dramatically reduce end-to-end inference latency, achieving 5~62x speedup (depending on the batch size) over CPU baseline implementations for popular recommendation models. Through multi-GPU concurrent deployment, the HPS can also greatly increase the inference QPS.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {408–419},
numpages = {12},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{10.1145/3395245.3396446,
author = {Kamilin, Mohd Hafizuddin Bin and Ahmadon, Mohd Anuaruddin Bin and Yamaguchi, Shingo},
title = {Evaluation of Process Arrangement Methods Based on Resource Constraint for IoT System},
year = {2020},
isbn = {9781450377058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395245.3396446},
doi = {10.1145/3395245.3396446},
abstract = {In this paper, we evaluate the computation time taken to create a timely optimized scheduler that needs to adhere to the resource constraint by comparing our proposed Sort and Fit method with trivial methods using Brute-force and 0--1 Knapsack Problem. When unoptimized, the scheduler not only takes a longer duration but also increases the operating cost. Due to the scheduling problem itself is an NP-hard, the simplest solution requires an upgrade to expand the resource constraint. Although there are multiple solutions, it cannot be applied in a real-time system and limited only to a specific problem. The major contribution of this paper is our proposed method can compute in near real-time while the accuracy is comparable to the trivial methods in near real-time. This show that trivial methods are suitable for small scale IoT system while our proposed method is suitable for large scale IoT system.},
booktitle = {Proceedings of the 2020 8th International Conference on Information and Education Technology},
pages = {290–294},
numpages = {5},
keywords = {0-1 Knapsack Problem, Internet of Things, Scheduling, Sort and Fit},
location = {Okayama, Japan},
series = {ICIET 2020}
}

@inproceedings{10.5555/3400397.3400555,
author = {Mizuta, Hideyuki},
title = {Jam tail estimation using vehicle and road agents},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Today, an increasing number of vehicles use IoT devices to communicate with a control center to obtain such traffic information as road congestion conditions and the current shortest route. We analyze the enormous amount of data obtained from these vehicles and detect jam tails even if the percentage of vehicles with IoT devices is small. For effective performance and improved accuracy when analyzing an enormous amount of data for a wide road area, we use a multi-agent system to collect and analyze the IoT data, which is stored in memory with a hierarchical structure organized by vehicle agents and road agents. This structure enables time series data to be analyzed from the viewpoint of each vehicle and to be aggregated for jam analysis from the viewpoint of each road. Furthermore, we use a large-scale traffic simulator to evaluate the behavior of this IoT agent system.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1965–1976},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3560905.3568106,
author = {Arora, Nivedita and Iyer, Vikram and Oh, Hyunjoo and Abowd, Gregory D. and Hester, Josiah D.},
title = {Circularity in Energy Harvesting Computational "Things"},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568106},
doi = {10.1145/3560905.3568106},
abstract = {We have witnessed explosive growth in computing devices at all scales, in particular with small wireless devices that can permeate most of our physical world. The IoT industry is helping to fuel this insatiable desire for more and more data. We have to balance this growth with an understanding of its environmental impact. Indeed, the ENSsys community must take leadership in putting sustainability up front as a primary design principle for the future of IoT and related areas, expanding the research mandate beyond the intricacies of the computing systems in isolation to encompass and integrate the materials, new applications, and circular lifecycle of electronics in the IoT. Our call to action is seeded with a circularity-focused computing agenda that demands a cross-stack research program for energy-harvesting computational things.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {931–933},
numpages = {3},
keywords = {circular electronics, intermittent computing, recycle, sustainability, sustainable HCI, transient electronics, upcycle},
location = {<conf-loc>, <city>Boston</city>, <state>Massachusetts</state>, </conf-loc>},
series = {SenSys '22}
}

@inproceedings{10.1145/3427228.3427235,
author = {Thomasset, Corentin and Barrera, David},
title = {SERENIoT: Distributed Network Security Policy Management and Enforcement for Smart Homes},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427235},
doi = {10.1145/3427228.3427235},
abstract = {Selectively allowing network traffic has emerged as a dominant approach for securing consumer IoT devices. However, determining what the allowed behavior of an IoT device should be remains an open challenge. Proposals to date have relied on manufacturers and trusted parties to provide allow lists of network traffic, but these proposals require manufacturer involvement or placing trust in an additional stakeholder. Alternatively, locally monitoring devices can allow building allow lists of observed behavior, but devices may not exhaust their functionality set during the observation period, and the behavior may change following a software update which requires re-training. This paper proposes a blockchain-based system for determining whether an IoT device is behaving like other devices of the same type. Our system, SERENIoT, overcomes the challenge of initially determining the correct behavior for a device. Nodes in the SERENIoT public blockchain submit summaries of the network behavior observed for connected IoT devices and build allow lists of behavior observed by the majority of nodes. Changes in behavior through software updates are automatically added to the allow list once the update is broadly deployed. Through a proof-of-concept implementation of SERENIoT on a small IoT network and a large-scale Amazon EC2 simulation, we evaluate the security, scalability, and performance of our system.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {542–555},
numpages = {14},
keywords = {IoT security, blockchain, intrusion detection, traffic filtering},
location = {<conf-loc>, <city>Austin</city>, <country>USA</country>, </conf-loc>},
series = {ACSAC '20}
}

@inproceedings{10.1145/3450268.3453527,
author = {Jafarizadeh, Mehdi and Liu, Xingzhi and Zheng, Rong},
title = {SoftBLE: An SDN Framework for BLE-based IoT Networks},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453527},
doi = {10.1145/3450268.3453527},
abstract = {Today's Industrial IoT (IIoT) applications often employ large-scale and dense sensor deployments for environmental monitoring. A hierarchical Bluetooth Low Energy (BLE) based architecture can facilitate power efficiency and reliability for data collection in dense networks. But if the network is static with fixed parameter settings, it can not be adaptable to dynamic application requirements. Although BLE is a parametric protocol, it does not provide any built-in feature for parameter tuning. To achieve network adaptability, we introduce and design SoftBLE, a Software Defined Networking (SDN) framework that provides controllability for BLE based 2-tier networks. It takes advantages of advanced control knobs recently available in BLE protocol stacks. SoftBLE is complemented by two orchestration algorithms to optimize gateway and sensor parameters. Evaluation results from both an experimental testbed and a large-scale simulation study show that almost all the SoftBLE sensors can save around 70% of transmission power while keeping Packet Reception Rate (PRR) above 99.9%.},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {221–233},
numpages = {13},
keywords = {Bluetooth Low Energy, Internet of Things, Software Defined Networking},
location = {Charlottesvle, VA, USA},
series = {IoTDI '21}
}

@inproceedings{10.1145/3528082.3544832,
author = {A, Harish S and Kothapalli, Hemanth and Lahoti, Shubham and Kataoka, Kotaro and Tammana, Praveen},
title = {IoT MUD enforcement in the edge cloud using programmable switch},
year = {2022},
isbn = {9781450393294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528082.3544832},
doi = {10.1145/3528082.3544832},
abstract = {Targeted data breaches and cybersecurity attacks involving IoT devices are becoming ever more concerning. To combat these threats and risks, the IETF standardized Manufacturer Usage Description (MUD), which allows IoT device vendors to specify the intended communication patterns (MUD profile) of an IoT device. MUD profile enables validation of the actual communication pattern of an IoT device with the intended behavior at run-time. However, the MUD specification was primarily intended for enforcement at the Local Area Network (LAN) of the IoT device, thus fragmenting the solution across multiple heterogeneous networks. MUD enforcement at higher levels in the network hierarchy (e.g., private edge for enterprise networks) eases security policy management and reduces processing overheads on the existing security infrastructure.To realize MUD enforcement at the edge, there are mainly two challenges: (1) How to identify an IoT device at the edge so that enforcing device-specific MUD profile on the IoT traffic is possible. (2) How to scale MUD enforcement to a large network of IoT devices. In this paper, we present our approach to address these challenges and validate IoT device communication at the edge. In order to scale MUD enforcement to a large IoT network, we leverage multi-stage pipeline architecture and stateful ALUs of P4 programmable switch and process IoT traffic in the dataplane.},
booktitle = {Proceedings of the ACM SIGCOMM Workshop on Formal Foundations and Security of Programmable Network Infrastructures},
pages = {1–7},
numpages = {7},
keywords = {internet of things, manufacturer usage description, network security, programmable networks},
location = {Amsterdam, Netherlands},
series = {FFSPIN '22}
}

@inproceedings{10.1145/3406324.3410712,
author = {Paul, Partha Sarathi and Ghosh, Bishakh Chandra and Ghosh, Ankan and Saha, Sujoy and Nandi, Subrata and Chakraborty, Sandip},
title = {Aco-Wi : Acoustic Initiated Wi-Fi Peer-group Communication for Opportunistic Messaging},
year = {2021},
isbn = {9781450380522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406324.3410712},
doi = {10.1145/3406324.3410712},
abstract = {Super cyclone Amphan in May 2020, left the city of Kolkata without cellphone connectivity for hours, and without Internet connectivity for days. Rural areas were much worse affected. After any large-scale natural disaster, this disruption of conventional communication mediums is one of the most significant obstructions for rescue and relief operations. Smartphone messaging apps and social media-based tools depend on the Internet, making them unusable in the golden hour after large scale disasters like cyclones, earthquakes, etc. In such dire situations, to establish an alternate communication mechanism, we propose Aco-Wi, a Messaging application that uses opportunistic acoustic initiated Wi-Fi communication system that can work seamlessly through consumer smartphones. We are designing Aco-Wi keeping in mind the user experience, energy efficiency, and performance. The initial results and user feedback show that the above-mentioned technology is indeed promising.},
booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {24},
numpages = {5},
keywords = {Artifact or System, Disaster, Last Mile Connectivity, Mobile Devices: Smartphones/Tablets, Opportunistic Network},
location = {Oldenburg, Germany},
series = {MobileHCI '20}
}

@inproceedings{10.1145/3604915.3608762,
author = {Mahajan, Khushhall Chandra and Porobo Dharwadker, Amey and Gupta, Saurabh and Schumitsch, Brad},
title = {VideoRecSys 2023: First Workshop on Large-Scale Video Recommender Systems},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608762},
doi = {10.1145/3604915.3608762},
abstract = {The demand for personalized video recommendations has grown exponentially with the widespread use of video content across various domains, including entertainment, e-commerce, education and social media. The explosive growth of video content on the internet, combined with the ubiquitous availability of high-speed internet and advancements in mobile camera technology have made it easier than ever for users to create, access and consume videos. With the proliferation of online social media applications like Instagram, YouTube, Facebook and TikTok, the need for large-scale video recommendation systems which can provide users with personalized and relevant recommendations has increased. However, building effective and scalable video recommender systems for large-scale applications is a challenging task due to several technical and operational factors such as the huge volume of video content, the diversity of user preferences, noise and biases in underlying data and the need for real-time recommendations. At the same time, building a fair recommendation system which can distribute content from niche interests and emerging creators besides more popular content is very important to balance the demand and supply side of the ecosystem. This makes large-scale video recommendations an interesting, rapidly evolving and challenging problem which requires more discussions around various nuanced topics. The goal of this workshop is to provide a platform for researchers, practitioners, and industry experts to discuss and share insights on the latest trends, challenges and opportunities in the field of large-scale video recommendations, where the corpus of content to recommend from is in the order of hundreds of millions or even tens of billions. We hope that this workshop will foster collaborations in the growing recommender systems community and spark new ideas for future research in this exciting and rapidly evolving field.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1269–1271},
numpages = {3},
keywords = {Recommender system, Video recommendation},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3494322.3494325,
author = {Chabi Sika Boni, Abdel Kader and Hassan, Hassan and Drira, Khalil},
title = {Task Offloading in Autonomous IoT Systems using Deep Reinforcement Learning and ns3-gym},
year = {2022},
isbn = {9781450385664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494322.3494325},
doi = {10.1145/3494322.3494325},
abstract = {IoT systems grow quickly and are massively present in urban areas. Their successful deployment requires autonomy that can be built on automated learning technologies such as Deep Learning. The IoT applications require important computational resources, rarely available on devices. Autonomous IoT systems require the computation power available on the edge and cloud servers in order to offload some tasks related to the supported applications and the underlying platforms. Task offloading constitutes a big challenge in autonomous IoT systems due to the huge number of IoT devices for scenarios of the family of smart cities. Managing task offloading in such contexts requires adaptive strategies capable of taking into consideration the rapid evolution of available resources and proposing efficient offloading solutions to all received requests. In this paper we use a Deep Reinforcement Learning (DRL) approach capable of handling large state spaces, and resolve the optimization problem in this context, where other techniques can not scale efficiently. Our solution is based on a DRL agent that was developed in the ns3-gym framework and was tested on IoT system scenario implemented in the NS3 simulator. The results obtained show that the DRL agent can adapt quickly to resource evolution in the IoT system and can handle big number of demands fulfilling scalabilty requirements of autonomous IoT systems.},
booktitle = {Proceedings of the 11th International Conference on the Internet of Things},
pages = {17–24},
numpages = {8},
keywords = {Autonomous IoT systems, Deep Reinforcement Learning, Task offloading},
location = {St.Gallen, Switzerland},
series = {IoT '21}
}

@inproceedings{10.1145/3458336.3465280,
author = {Pemberton, Nathan and Schleier-Smith, Johann and Gonzalez, Joseph E.},
title = {The RESTless cloud},
year = {2021},
isbn = {9781450384384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458336.3465280},
doi = {10.1145/3458336.3465280},
abstract = {Cloud provider APIs have emerged as the de facto operating system interface for the warehouse scale computers that comprise the public cloud. Like single-server operating systems, they provide the resource allocation, protection, communication paths, naming, and scheduling for these large machines. Cloud provider APIs also provide all sorts of things that operating systems do not, things like big data analytics, machine learning model training, or factory automation. Somewhere, lurking within this menagerie of services, there is an operating system interface to a really big computer, the computer that today's application developers target. This computer works nothing like a single server, yet it also isn't a dispersed distributed system like the internet. It is something in-between. Now is the time to distill and refine a coherent "cloud system interface" from the multitude of cloud provider APIs, preferably a portable one. In this paper we discuss what goes in, what stays out, and the principles that inform these decisions.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {49–57},
numpages = {9},
location = {Ann Arbor, Michigan},
series = {HotOS '21}
}

@inproceedings{10.1145/3378936.3378979,
author = {Wenchi, Du and Niansong, Zhang and Aimin, Wang},
title = {Safety Production Process Hazard Situation Analysis System Based on Large-scale Data Real-time Processing},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378979},
doi = {10.1145/3378936.3378979},
abstract = {In order to give full play to the effects of big data in production safety monitoring and to meet the purpose of upgrading the safety management of enterprises, reducing personnel and increasing efficiency, this paper designs and implements a dangerous situation analysis system for production safety based on real-time processing of large-scale data. The platform adopts a layered architecture design method, and is constructed based on security parameter monitoring and monitoring technology, large-scale real-time data processing technology, dangerous situation assessment system, knowledge base, and SPC process analysis and early warning technology. The platform mainly covers three types of data source systems: people, things, and environment. It has basic information management, dynamic monitoring, big data analysis, and intelligent early warning. The results show that the platform can collect and share a large amount of data on safe production, summarize the rules of accidents through a big data analysis model, and finally achieve fine management of the safety of dangerous goods production.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {36–40},
numpages = {5},
keywords = {Big Data, Dangerous Goods Production, Layered Architecture, SPC, Safety Supervision System},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.5555/3581644.3581690,
author = {Tummula, Madhav and H, Manish Kausik and Saha, Sudipta},
title = {FlexiCast: A Structure-Adaptive Protocol for Efficient Data-Sharing in IoT},
year = {2023},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {IoT-technology is gaining a wide popularity over a large range of applications including not only monitoring of structures but also management and control of smart-systems. An IoT-system, in general, is composed of a number of IoT-devices which form a wireless decentralized setting as they get installed over a specific area to serve a particular purpose. The structure of the underlying wireless network depends on the structure of the target where the system gets deployed and hence, widely varies based on the exact application. Such structural variations often have an impact on the performance of the underlying IoT-protocols. Unfortunately most of the network protocols do not take care of such issues explicitly. For instance, although there have been quite significant development in the data-sharing protocols, especially with the advent of Synchronous-Transmission (ST), most of them are designed without considering the variation in the structural formation of the base networks. These protocols are tested over either in small scale simulated networks or in testbed settings bearing fixed/homogeneous structures. In this work, we demonstrate that the property of self-adaptability in an IoT-system can enable it not only to run faster but also save substantial energy which is an extremely important issue in the context of low-power system, in general. In particular, we design and implement a flexible and structure-adaptive many-to-many data-sharing protocol FlexiCast. Through extensive experiments under emulation-settings and IoT-testbeds we demonstrate that FlexiCast performs upto 49% faster and consumes upto 53% lesser energy compared to the case when it does not adapt to the network structure.},
booktitle = {Proceedings of the 18th International Conference on Network and Service Management},
articleno = {37},
numpages = {9},
keywords = {IoT, TDMA, WSN, capture effect, concurrent-transmission, many-to-many data sharing, self-adjusting protocol, time-varying schedule},
location = {Thessaloniki, Greece},
series = {CNSM '22}
}

@inproceedings{10.1145/3573428.3573565,
author = {Feng, Yun and Zhai, Feng and Liang, Xiaobing and Zhang, Liang},
title = {Design of L-IDAKA Protocol for Lightweight Access Authentication and Encrypted Transmission of New Power System-Aware Terminals Based on Hash Pre-authentication},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573565},
doi = {10.1145/3573428.3573565},
abstract = {The introduction of massive heterogeneous IoT terminals poses new challenges to the access security of the edge-aware side of the power IoT. Marked cipher technology can realize direct authentication based on device identity without relying on digital certificates, which significantly simplifies the difficulty of key management and authentication process and has natural advantages in the large-scale deployment of power IoT terminal scenario. However, identification cryptography is based on bilinear pair operation, which has high operational overhead and is difficult to apply to resource-constrained terminals. In this paper, we design a lightweight authentication key negotiation protocol based on hash pre-certification based on the four-layer architecture of power IoT application-platform-network-awareness for the characteristics of huge scale and resource-constrained power IoT devices, which reduces the number of bilinear pairs in the traditional SM9 algorithm from two to one and improves the protocol efficiency by more than 50%. Meanwhile, a security analysis of the protocol based on the classical Dolev-Yao threat model shows that this scheme can achieve efficient and secure applications in resource-constrained terminals of power IoT.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {766–771},
numpages = {6},
keywords = {identification cipher, key negotiation, lightweight, power IoT, resource-constrained terminals},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3628354.3629534,
author = {Fan, Xinxin and Xu, Lei},
title = {Towards a Rollup-Centric Scalable Architecture for Decentralized Physical Infrastructure Networks: A Position Paper},
year = {2024},
isbn = {9798400704390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628354.3629534},
doi = {10.1145/3628354.3629534},
abstract = {Decentralized physical infrastructure network (DePIN) is an emerging research topic in Web3 and blockchain. By combining blockchain, IoT and tokenomics, DePINs are expected to disrupt existing IoT business models and enable Web3 communities to build innovative, machine-driven and decentralized IoT networks and applications. Due to the characteristics of DePINs such as the large number of smart devices and network scale as well as the interactions with blockchain, scalability remains to be one of the key challenges. In this position paper, we outline the core ideas and components for building a rollup-centric scalable architecture for DePINs. The proposed architecture takes a modular design approach and leverages off-chain computing and zero-knowledge proofs to address the scalability challenge. This work is expected to highlight the importance of this new research direction and shed some light on potential solutions.},
booktitle = {Proceedings of the Fifth ACM International Workshop on Blockchain-Enabled Networked Sensor Systems},
pages = {9–12},
numpages = {4},
keywords = {Blockchain, DePIN, Scalability, System Architecture, zk-Rollup},
location = {<conf-loc>, <city>Istanbul</city>, <country>Turkiye</country>, </conf-loc>},
series = {BlockSys '23}
}

@inproceedings{10.1145/3479239.3485708,
author = {Banaszek, Mateusz and Dubiel, Wojciech and \L{}ysiak, Jacek and Dundefinedbski, Maciej and Kisiel, Maciej and \L{}azarczyk, Dawid and G\l{}ogowska, Ewa and Gumienny, Przemys\l{}aw and Si\l{}uszyk, Cezary and Cio\l{}kosz, Piotr and Paszkowska, Agnieszka and R\"{u}b, Inga and Matraszek, Maciej and Aceda\'{n}ski, Szymon and Horban, Przemys\l{}aw and Iwanicki, Konrad},
title = {1KT: A Low-Cost 1000-Node Low-Power Wireless IoT Testbed},
year = {2021},
isbn = {9781450390774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479239.3485708},
doi = {10.1145/3479239.3485708},
abstract = {Testbeds remain indispensable instruments for experimentally evaluating IoT-oriented low-power wireless networking solutions. With the evolution of the field, they are increasingly expected to match envisioned deployment conditions of such solutions, notably in terms of scale. However, large-scale testbeds are scarce, likely because they have been believed to be expensive. This paper argues that this belief need no longer be justified by presenting the architecture and basic properties of 1KT, our new smart-building IoT testbed for solutions utilizing IEEE 802.15.4 and Bluetooth Low Energy. It comprises 1000 experimental devices deployed directly in human spaces of 168 rooms on all 5 floors of a sizable building. At the same time, its cost is relatively low considering the scale.},
booktitle = {Proceedings of the 24th International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {109–113},
numpages = {5},
keywords = {1KT, IEEE 802.15.4, bluetooth low energy (BLE), experiment, experimental testbed, internet of things (IoT), large scale, low-power wireless network, smart building, testbed},
location = {Alicante, Spain},
series = {MSWiM '21}
}

@proceedings{10.1145/3426182,
title = {MPLR '20: Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to MPLR 2020, the 17th International Conference on Managed Programming Languages and Runtimes. MPLR is a successor to the conference series on Managed Languages and Runtimes (ManLang). It is a premier forum for presenting and discussing novel results in all aspects of managed programming languages and runtime systems, which serve as building blocks for some of the most important computing systems around, ranging from small-scale (embedded and real-time systems) to large-scale (cloud-computing and big-data platforms) and anything in between (mobile, IoT, and wearable applications).},
location = {Virtual, UK}
}

@inproceedings{10.1145/3551659.3559022,
author = {Courageux-Sudan, Cl\'{e}ment and Guegan, Lo\"{\i}c and Orgerie, Anne-C\'{e}cile and Quinson, Martin},
title = {A Flow-Level Wi-Fi Model for Large Scale Network Simulation},
year = {2022},
isbn = {9781450394826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551659.3559022},
doi = {10.1145/3551659.3559022},
abstract = {Wi-Fi networks are extensively used to provide Internet access to end-users and to deploy applications at the edge. By playing a major role in modern networking, Wi-Fi networks are getting bigger and denser. However, studying their performance at large-scale and in a reproducible manner remains a challenging task. Current solutions include real experiments and simulations. While the size of experiments is limited by their financial cost and potential disturbance of commercial networks, the simulations also lack scalability due to their models' granularity and computational runtime. In this paper, we introduce a new Wi-Fi model for large-scale simulations. This model, based on flow-level simulation, requires fewer computations than state-of-the-art models to estimate bandwidth sharing over a wireless medium, leading to better scalability. Comparing our model to the already existing Wi-Fi implementation of ns-3, we show that our approach yields to close performance evaluations while improving the runtime of simulations by several orders of magnitude. Using this kind of model could allow researchers to obtain reproducible results for networks composed of thousands of nodes much faster than previously.},
booktitle = {Proceedings of the 25th International ACM Conference on Modeling Analysis and Simulation of Wireless and Mobile Systems},
pages = {111–119},
numpages = {9},
keywords = {Wi-Fi networks, modeling and simulation, performance evaluation},
location = {Montreal, Quebec, Canada},
series = {MSWiM '22}
}

@inproceedings{10.1145/3531028.3531036,
author = {Penaflor Rey, William and Dugang Rey, Kieth Wilhelm Jan},
title = {Designing Secure and Scalable end-to-end Private Network Connections between sites via Overlay Tunnels},
year = {2022},
isbn = {9781450395847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531028.3531036},
doi = {10.1145/3531028.3531036},
abstract = {Today, a secure and scalable end-to-end private network connection between head office and branches is a fundamental requirement of modern-day businesses. There are multiple solutions available, but this research will focus on the wide availability of the technology in most areas. Internet is standard network connectivity that links different branches from a different location to the business headquarter. The widespread availability of the Internet and the compatibility of the Overlay tunnels make a cost-effective solution to connect businesses. The study location is an advertising firm in the Philippines, classified as a small-to-medium-sized enterprise that requires network architecture enhancements. The critical demand is to connect three remote offices with the headquarters office for more protected file and application sharing. The remote users or employees accessing the resources anytime and anywhere should also be considered. The study will use the Generic Routing Encapsulation (GRE) and the Internet Protocol Security (IPSec) as overlay tunnels to implement interconnectivity between sites at remote locations.Furthermore, in light of significant security breaches, the design will consider the use of new network security measures. The new network design enables secure file and application sharing through overlay tunnels between the headquarters and the other three provincial branches and remote users. Branch connectivity uses GRE over IPSEC tunnels, dynamic routing, and only authorized VPN connections to access HQ resources. The proposed network architecture operated successfully during the test validation (using ping, tracert, and traceroute utility commands). Finally, the suggested network design is secured from MAC address flooding, ping of death (DOS), and VPN snooping based on the vulnerability studies.},
booktitle = {Proceedings of the 2022 5th International Conference on Electronics, Communications and Control Engineering},
pages = {50–56},
numpages = {7},
keywords = {GRE over IPSec, PPDIOO, overlay tunnels, scalable network, virtual private network},
location = {Higashi-ku, Japan},
series = {ICECC '22}
}

@proceedings{10.1145/3475738,
title = {MPLR 2021: Proceedings of the 18th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
year = {2021},
isbn = {9781450386753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to MPLR 2021, the 18th International Conference on Managed Programming Languages and Runtimes. MPLR is a successor to the conference series on Managed Languages and Runtimes (ManLang) which originated as Principles and Practice of Programming in Java (PPPJ). This is a premier forum for presenting and discussing novel results in all aspects of managed programming languages and runtime systems, which serve as building blocks for many of the most important computing systems, ranging from small-scale (embedded and real-time systems) to large-scale (cloud-computing and big-data platforms) and anything in between (mobile, IoT, and wearable applications).},
location = {M\"{u}nster, Germany}
}

@inproceedings{10.1145/3569951.3597606,
author = {McIntosh, Tyler L and Verleye, Erick and Balch, Jennifer K and Cattau, Megan E and Ilangakoon, Nayani T and Korinek, Nathan and Nagy, R. Chelsea and Sanovia, James and Skidmore, Edwin and Swetnam, Tyson L and Tuff, Ty and Quarderer, Nathan and Wessman, Carol A},
title = {Cyberinfrastructure deployments on public research clouds enable accessible Environmental Data Science education},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569951.3597606},
doi = {10.1145/3569951.3597606},
abstract = {Modern science depends on computers, but not all scientists have access to the scale of computation they need. A digital divide separates scientists who accelerate their science using large cyberinfrastructure from those who do not, or who do not have access to the compute resources or learning opportunities to develop the skills needed. The exclusionary nature of the digital divide threatens equity and the future of innovation by leaving people out of the scientific process while over-amplifying the voices of a small group who have resources. However, there are potential solutions: recent advancements in public research cyberinfrastructure and resources developed during the open science revolution are providing tools that can help bridge this divide. These tools can enable access to fast and powerful computation with modest internet connections and personal computers. Here we contribute another resource for narrowing the digital divide: scalable virtual machines running on public cloud infrastructure. We describe the tools, infrastructure, and methods that enabled successful deployment of a reproducible and scalable cyberinfrastructure architecture for a collaborative data synthesis working group in February 2023. This platform enabled &nbsp;45 scientists with varying data and compute skills to leverage &nbsp;40,000 hours of compute time over a 4-day workshop. Our approach provides an open framework that can be replicated for educational and collaborative data synthesis experiences in any data- and compute-intensive discipline.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {367–373},
numpages = {7},
keywords = {cyberinfrastructure, digital equity, open science, team science},
location = {Portland, OR, USA},
series = {PEARC '23}
}

@inproceedings{10.1145/3485983.3494862,
author = {Kr\"{a}henb\"{u}hl, Cyrill and Tabaeiaghdaei, Seyedali and Gloor, Christelle and Kwon, Jonghoon and Perrig, Adrian and Hausheer, David and Roos, Dominik},
title = {Deployment and scalability of an inter-domain multi-path routing infrastructure},
year = {2021},
isbn = {9781450390989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485983.3494862},
doi = {10.1145/3485983.3494862},
abstract = {Path aware networking (PAN) is a promising approach that enables endpoints to participate in end-to-end path selection. PAN unlocks numerous benefits, such as fast failover after link failures, application-based path selection and optimization, and native interdomain multi-path. The utility of PAN hinges on the availability of a large number of high-quality path options. In an inter-domain context, two core questions arise. Can we deploy such an architecture natively in today's Internet infrastructure without creating an overlay relying on BGP? Can we build a scalable multi-path routing system that provides a large number of high-quality paths?We first report on the real-world native deployment of the SCION next-generation architecture, providing a usable PAN infrastructure operating in parallel to today's Internet. We then analyze the scalability of the architecture in an Internet-scale topology. Finally, we introduce a new routing approach to further improve scalability.},
booktitle = {Proceedings of the 17th International Conference on Emerging Networking EXperiments and Technologies},
pages = {126–140},
numpages = {15},
keywords = {BGP, SCION, control-plane algorithm design, deployment, inter-domain routing, multi-path, network simulation, next-generation internet architecture, scalability},
location = {Virtual Event, Germany},
series = {CoNEXT '21}
}

@proceedings{10.1145/3617651,
title = {MPLR 2023: Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to MPLR 2023, the 20th International Conference on Managed Programming Languages and Runtimes, held in Cacais, Portugal on Sunday 22 October 2023, co-located with SPLASH 2023. MPLR is a successor to the conference series on Managed Languages and Runtimes (ManLang). It is a premier forum for presenting and discussing novel results in all aspects of managed programming languages and runtime systems, which serve as building blocks for some of the most important computing systems around, ranging from small-scale (embedded and real-time systems) to large-scale (cloud-computing and big-data platforms) and anything in between (mobile, IoT, and wearable applications).},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3411498.3419967,
author = {Castiglione, Luca Maria and Lupu, Emil C.},
title = {Hazard Driven Threat Modelling for Cyber Physical Systems},
year = {2020},
isbn = {9781450380874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411498.3419967},
doi = {10.1145/3411498.3419967},
abstract = {Adversarial actors have shown their ability to infiltrate enterprise networks deployed around Cyber Physical Systems (CPSs) through social engineering, credential stealing and file-less infections. When inside, they can gain enough privileges to maliciously call legitimate APIs and apply unsafe control actions to degrade the system performance and undermine its safety. Our work lies at the intersection of security and safety, and aims to understand dependencies among security, reliability and safety in CPS/IoT. We present a methodology to perform hazard driven threat modelling and impact assessment in the context of CPSs. The process starts from the analysis of behavioural, functional and architectural models of the CPS. We then apply System Theoretic Process Analysis (STPA) on the functional model to highlight high-level abuse cases. We leverage a mapping between the architectural and the system theoretic(ST) models to enumerate those components whose impairment provides the attacker with enough privileges to tamper with or disrupt the data-flows. This enables us to find a causal connection between the attack surface (in the architectural model) and system level losses. We then link the behavioural and system theoretic representations of the CPS to quantify the impact of the attack. Using our methodology it is possible to compute a comprehensive attack graph of the known attack paths and to perform both a qualitative and quantitative impact assessment of the exploitation of vulnerabilities affecting target nodes. The framework and methodology are illustrated using a small scale example featuring a Communication Based Train Control (CBTC) system. Aspects regarding the scalability of our methodology and its application in real world scenarios are also considered. Finally, we discuss the possibility of using the results obtained to engineer both design time and real time defensive mechanisms.},
booktitle = {Proceedings of the 2020 Joint Workshop on CPS&amp;IoT Security and Privacy},
pages = {13–24},
numpages = {12},
keywords = {attack graphs, cyber physical systems security, resilience, system theoretic analysis, threat modelling},
location = {Virtual Event, USA},
series = {CPSIOTSEC'20}
}

@article{10.1145/3478118,
author = {Cheng, Tingyu and Li, Bu and Zhang, Yang and Li, Yunzhi and Ramey, Charles and Jung, Eui Min and Cui, Yepu and Swaminathan, Sai Ganesh and Do, Youngwook and Tentzeris, Manos and Abowd, Gregory D. and Oh, HyunJoo},
title = {Duco: Autonomous Large-Scale Direct-Circuit-Writing (DCW) on Vertical Everyday Surfaces Using A Scalable Hanging Plotter},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478118},
doi = {10.1145/3478118},
abstract = {Human environments are filled with large open spaces that are separated by structures like walls, facades, glass windows, etc. Most often, these structures are largely passive offering little to no interactivity. In this paper, we present Duco, a large-scale electronics fabrication robot that enables room-scale &amp; building-scale circuitry to add interactivity to vertical everyday surfaces. Duco negates the need for any human intervention by leveraging a hanging robotic system that automatically sketches multi-layered circuity to enable novel large-scale interfaces. The key idea behind Duco is that it achieves single-layer or multi-layer circuit fabrication on 2D surfaces as well as 2D cutouts that can be assembled into 3D objects by loading various functional inks (e.g., conductive, dielectric, or cleaning) to the wall-hanging drawing robot, as well as employing an optional laser cutting head as a cutting tool. Our technical evaluation shows that Duco's mechanical system works reliably on various surface materials with a wide range of roughness and surface morphologies. The system achieves superior mechanical tolerances (0.1mm XY axis resolution and 1mm smallest feature size). We demonstrate our system with five application examples, including an interactive piano, an IoT coffee maker controller, an FM energy-harvester printed on a large glass window, a human-scale touch sensor and a 3D interactive lamp.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {92},
numpages = {25},
keywords = {digital fabrication, large-scale circuit, printed electronics, smart environments, ubiquitous computing}
}

@inproceedings{10.1145/3458306.3460997,
author = {Farahani, Reza and Tashtarian, Farzad and Erfanian, Alireza and Timmerer, Christian and Ghanbari, Mohammad and Hellwagner, Hermann},
title = {ES-HAS: an edge- and SDN-assisted framework for HTTP adaptive video streaming},
year = {2021},
isbn = {9781450384353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458306.3460997},
doi = {10.1145/3458306.3460997},
abstract = {Recently, HTTP Adaptive Streaming (HAS) has become the dominant video delivery technology over the Internet. In HAS, clients have full control over the media streaming and adaptation processes. Lack of coordination among the clients and lack of awareness of the network conditions may lead to sub-optimal user experience and resource utilization in a pure client-based HAS adaptation scheme. Software Defined Networking (SDN) has recently been considered to enhance the video streaming process. In this paper, we leverage the capability of SDN and Network Function Virtualization (NFV) to introduce an edge- and SDN-assisted video streaming framework called ES-HAS. We employ virtualized edge components to collect HAS clients' requests and retrieve networking information in a time-slotted manner. These components then perform an optimization model in a time-slotted manner to efficiently serve clients' requests by selecting an optimal cache server (with the shortest fetch time). In case of a cache miss, a client's request is served (i) by an optimal replacement quality (only better quality levels with minimum deviation) from a cache server, or (ii) by the original requested quality level from the origin server. This approach is validated through experiments on a large-scale testbed, and the performance of our framework is compared to pure client-based strategies and the SABR system [12]. Although SABR and ES-HAS show (almost) identical performance in the number of quality switches, ES-HAS outperforms SABR in terms of playback bitrate and the number of stalls by at least 70% and 40%, respectively.},
booktitle = {Proceedings of the 31st ACM Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {50–57},
numpages = {8},
keywords = {dynamic adaptive streaming over HTTP (DASH), edge computing, network function virtualization (NFV), network-assisted video streaming, quality of experience (QOE), software defined networking (SDN)},
location = {Istanbul, Turkey},
series = {NOSSDAV '21}
}

@inproceedings{10.1145/3474085.3481541,
author = {Huang, Lianghua and Liu, Yu and Zhou, Xiangzeng and You, Ansheng and Li, Ming and Wang, Bin and Zhang, Yingya and Pan, Pan and Yinghui, Xu},
title = {Once and for All: Self-supervised Multi-modal Co-training on One-billion Videos at Alibaba},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481541},
doi = {10.1145/3474085.3481541},
abstract = {Videos grow to be one of the largest mediums on the Internet. E-commerce platforms like Alibaba need to process millions of video data across multimedia (e.g., visual, audio, image, and text) and on a variety of tasks (e.g., retrieval, tagging, and summary) every day. In this work, we aim to develop a once and for all pretraining technique for diverse modalities and downstream tasks. To achieve this, we make the following contributions: (1) We propose a self-supervised multi-modal co-training framework. It takes cross-modal pseudo-label consistency as the supervision and can jointly learn representations of multiple modalities. (2) We introduce several novel techniques (e.g., sliding-window subset sampling, coarse-to-fine clustering, fast spatial-temporal convolution and parallel data transmission and processing) to optimize the training process, making billion-scale stable training feasible. (3) We construct a large-scale multi-modal dataset consisting of 1.4 billion videos (~0.5 PB) and train our framework on it. The training takes only 4.6 days on an in-house 256 GPUs cluster, and it simultaneously produces pretrained video, audio, image, motion, and text networks. (4) Finetuning from our pretrained models, we obtain significant performance gains and faster convergence on diverse multimedia tasks at Alibaba. Furthermore, we also validate the learned representation on public datasets. Despite the domain gap between our commodity-centric pretraining and the action-centric evaluation data, we show superior results against state-of-the-arts.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1148–1156},
numpages = {9},
keywords = {co-training, multi-modal, once and for all, self-supervised learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3517745.3561434,
author = {Izhikevich, Liz and Akiwate, Gautam and Berger, Briana and Drakontaidis, Spencer and Ascheman, Anna and Pearce, Paul and Adrian, David and Durumeric, Zakir},
title = {ZDNS: a fast DNS toolkit for internet measurement},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561434},
doi = {10.1145/3517745.3561434},
abstract = {Active DNS measurement is fundamental to understanding and improving the DNS ecosystem. However, the absence of an extensible, high-performance, and easy-to-use DNS toolkit has limited both the reproducibility and coverage of DNS research. In this paper, we introduce ZDNS, a modular and open-source active DNS measurement framework optimized for large-scale research studies of DNS on the public Internet. We describe ZDNS's architecture, evaluate its performance, and present two case studies that highlight how the tool can be used to shed light on the operational complexities of DNS. We hope that ZDNS will enable researchers to better---and in a more reproducible manner---understand Internet behavior.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {33–43},
numpages = {11},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3447548.3467084,
author = {Huang, Yuzhen and Wei, Xiaohan and Wang, Xing and Yang, Jiyan and Su, Bor-Yiing and Bharuka, Shivam and Choudhary, Dhruv and Jiang, Zewei and Zheng, Hai and Langman, Jack},
title = {Hierarchical Training: Scaling Deep Recommendation Models on Large CPU Clusters},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467084},
doi = {10.1145/3447548.3467084},
abstract = {Neural network based recommendation models are widely used to power many internet-scale applications including product recommendation and feed ranking. As the models become more complex and more training data is required during training, improving the training scalability of these recommendation models becomes an urgent need. However, improving the scalability without sacrificing the model quality is challenging. In this paper, we conduct an in-depth analysis of the scalability bottleneck in existing training architecture on large scale CPU clusters. Based on these observations, we propose a new training architecture called Hierarchical Training, which exploits both data parallelism and model parallelism for the neural network part of the model within a group. We implement hierarchical training with a two-layer design: a tagging system that decides the operator placement and a net transformation system that materializes the training plans, and integrate hierarchical training into existing training stack. We propose several optimizations to improve the scalability of hierarchical training including model architecture optimization, communication compression, and various system-level improvements. Extensive experiments at massive scale demonstrate that hierarchical training can speed up distributed recommendation model training by 1.9x without model quality drop.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3050–3058},
numpages = {9},
keywords = {distributed training, optimization, system for machine learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3366424.3382722,
author = {Tobiyama, Shun and Hu, Bo and Kamiya, Kazunori and Takahashi, Kenji},
title = {Large-Scale Network-Traffic-Identification Method with Domain Adaptation},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382722},
doi = {10.1145/3366424.3382722},
abstract = {With the continuous evolution of the Internet, a variety of web applications, such as video streaming and social network services, are widely used, significantly increasing the amount of traffic. Classifying the types of network traffic in detail is becoming more important from the perspectives of resource management and security analysis. In an Internet service provider (ISP) level large-scale network, details of packet payload cannot be collected due to the huge amount of traffic. Alternatively, network flow data representing the statistics of a sequence of packets are collected from devices. However, due to the lack of packet details, the granularity of classification is limited to the protocol level. For better understanding of the types of traffic in detail, we propose an application-level traffic identification method combining both packet data of small-scale networks and flow data of large-scale networks. With our proposed method, we adapt an identification model trained from small-scale network data to large-scale network data with domain adaptation.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {109–110},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3486622.3493985,
author = {Li, Weijian and Kikuchi, Masato and Ozono, Tadachika},
title = {Product Information Browsing Support System Using Analytic Hierarchy Process},
year = {2022},
isbn = {9781450391153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486622.3493985},
doi = {10.1145/3486622.3493985},
abstract = {Large-scale e-commerce sites can collect and analyze a large number of user preferences and behaviors, and thus can recommend highly trusted products to users. However, it is very difficult for individuals or non-corporate groups to obtain large-scale user data. Therefore, we consider whether knowledge of the decision-making domain can be used to obtain user preferences and combine it with content-based filtering to design an information retrieval system. This study describes the process of building a product information browsing support system with high satisfaction based on product similarity and multiple other perspectives about products on the Internet. We present the architecture of the proposed system and explain the working principle of its constituent modules. Finally, we demonstrate the effectiveness of the proposed system through an evaluation experiment and a questionnaire.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {623–628},
numpages = {6},
keywords = {Analytic Hierarchy Process, Content-based filtering, Information Retrieval, Multiple-criteria decision-making, TF-IDF},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3588444.3591023,
author = {Burdinat, Christophe and Raulet, Mickael and Perrot, Pascal and Lemotheux, Julien and Angot, Patrice and Lhermitte, Richard and Cabarat, Pierre-Loup and Bui Do, Benoit},
title = {Sustainable TV Distribution by Delivering Universal DVB-I TV Services},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3591023},
doi = {10.1145/3588444.3591023},
abstract = {TV consumption patterns and delivery methods have been subject to major changes in the last decades. It has led to two heterogeneous ecosystems with, on one side the traditional broadcast making use of MPEG-TS applications, and, on the other one, streaming applications over the broadband internet, addressing enriched use cases such as customization and VoD. To reach their fragmented audience across a plurality of access networks, devices and usages, service providers and operators have to decline their services into many technically heterogeneous flavors. In parallel, the growing awareness of the streaming industry's impact on the climate has become a strong incentive to streamline the entire TV service delivery ecosystem. [1] proposed an end to end sustainable solution based on DVB-I addressing OTT, IPTV, and 5G mobile networks. DVB-I [2] is an emerging standard developed by DVB to harmonize discovery and consumption of TV services over the multiplicity of access networks. As a discovery mechanism, DVB-I can help prioritize the selection of network and service flavors based on energy savings criteria. This paper considers the set of key features from [1] aimed at reducing power consumption for video streaming, covering codec, packaging and transport aspects.Versatile Video Codec (VVC) [3], issued in mid-2021, provides around 50% bandwidth saving compared to its predecessor HEVC [4]. VCC is more complex and requires more power during the encoding and decoding phases but can provide large savings in transmission power arising from the reduced bandwidth demand. As indicated in [5], a large portion of energy consumption for video streaming is done by end devices. First measures on end devices for VVC were done in [6] and are complemented in this paper by a set of additional measurements. Subjective tests are performed to identify the best energy saving configuration (definition/codec) for mobile devices and TV sets: when the user experience is practically identical, the usage of smaller resolutions, in particular on small screens, is preferable. This paper investigates the impact of the codec choice (AVC/HEVC/VVC) on the energy consumption for the end to end streaming delivery system, from the head end to the end devices.The paper then analyzes the end to end impact of using the Common Media Application Format (CMAF [7]) for packaging the video streams. CMAF has been specified to be used with both the DASH [8] and HLS [9] protocols, reducing consequently the amount of media content to be cached or broadcasted: the media can be packaged once and stored once, reducing its footprint in the head-end and the CDN. The evolution of the smartphone fleet and the increasing portion of devices supporting CMAF confirms the relevance of this format.Finally, the paper considers the gains brought by using multicast for the distribution of live OTT services. First, at the CDN level, we can scale the delivery by leveraging DVB-MABR [10]. This can have a very significant impact of the CDN dimensioning, as it allows to absorb the consumption peaks during popular events. And then, at the access network level, by using point to multipoint transmissions over 5G using the new 5G Multicast Broadcast Services (5G MBS) feature [11].},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {124–125},
numpages = {2},
keywords = {DVB-I, DVB-MABR, CMAF, VVC, 5G MBS, sustainability},
location = {Denver, CO, USA},
series = {MHV '23}
}

@inproceedings{10.1145/3501409.3501431,
author = {Li, Xiang and Yang, Shulin and Huang, Yongliang and Peng, Jiao and Zhou, Meiqi},
title = {Research on Large Scale Data Concurrent Processing Scheme of Printing Equipment Fault Diagnosis System},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501431},
doi = {10.1145/3501409.3501431},
abstract = {With the development of intelligent printing equipment, equipment fault diagnosis based on Internet technology has become a trend. The existing platform uses the traditional relational database. With the increase of data volume and types, it is difficult to meet the needs of real-time and rapid response of large-scale data processing. In view of the above problems, this paper uses the distributed database HBase, combined with the parallel processing framework MapReduce, message middleware Kafka related technology, puts forward the large-scale data storage and concurrent processing scheme of printing equipment fault diagnosis system. The scheme solves the problem of large-scale data concurrent processing in fault diagnosis system of the printing equipment and has good feasibility.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {114–119},
numpages = {6},
keywords = {Big Data, Data Processing, HBase, MapReduce},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1145/3437963.3441663,
author = {Zheng, Da and Wang, Minjie and Gan, Quan and Song, Xiang and Zhang, Zheng and Karypis, George},
title = {Scalable Graph Neural Networks with Deep Graph Library},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441663},
doi = {10.1145/3437963.3441663},
abstract = {Learning from graph and relational data plays a major role in many applications including social network analysis, marketing, e-commerce, information retrieval, knowledge modeling, medical and biological sciences, engineering, and others. Recently, Graph Neural Networks (GNNs) have emerged as a promising new learning framework capable of bringing the power of deep representation learning to graph and relational data. This ever-growing body of research has shown that GNNs achieve state-of-the-art performance for problems such as link prediction, fraud detection, target-ligand binding activity prediction, knowledge-graph completion, and product recommendations. In practice, many of the real-world graphs are very large. It is urgent to have scalable solutions to train GNN on large graphs efficiently.The objective of this tutorial is twofold. First, it will provide an overview of the theory behind GNNs, discuss the types of problems that GNNs are well suited for, and introduce some of the most widely used GNN model architectures and problems/applications that are designed to solve. Second, it will introduce the Deep Graph Library (DGL), a scalable GNN framework that simplifies the development of efficient GNN-based training and inference programs at a large scale. To make things concrete, the tutorial will cover state-of-the-art training methods to scale GNN to large graphs and provide hands-on sessions to show how to use DGL to perform scalable training in different settings (multi-GPU training and distributed training). This hands-on part will start with basic graph applications (e.g., node classification and link prediction) to set up the context and move on to train GNNs on large graphs. It will provide tutorials to demonstrate how to apply the techniques in DGL to train GNNs for real-world applications.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {1141–1142},
numpages = {2},
keywords = {deep graph library, graph neural networks, scalability},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3394486.3406712,
author = {Zheng, Da and Wang, Minjie and Gan, Quan and Zhang, Zheng and Karypis, Geroge},
title = {Scalable Graph Neural Networks with Deep Graph Library},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406712},
doi = {10.1145/3394486.3406712},
abstract = {Learning from graph and relational data plays a major role in many applications including social network analysis, marketing, e-commerce, information retrieval, knowledge modeling, medical and biological sciences, engineering, and others. In the last few years, Graph Neural Networks (GNNs) have emerged as a promising new supervised learning framework capable of bringing the power of deep representation learning to graph and relational data. This ever-growing body of research has shown that GNNs achieve state-of-the-art performance for problems such as link prediction, fraud detection, target-ligand binding activity prediction, knowledge-graph completion, and product recommendations. In practice, many of the real-world graphs are very large. It is urgent to have scalable solutions to train GNN on large graphs efficiently.The objective of this tutorial is twofold. First, it will provide an overview of the theory behind GNNs, discuss the types of problems that GNNs are well suited for, and introduce some of the most widely used GNN model architectures and problems/applications that are designed to solve. Second, it will introduce the Deep Graph Library (DGL), a scalable GNN framework that simplifies the development of efficient GNN-based training and inference programs at a large scale. To make things concrete, the tutorial will cover state-of-the-art training methods to scale GNN to large graphs and provide hands-on sessions to show how to use DGL to perform scalable training in different settings (multi-GPU training and distributed training). This hands-on part will start with basic graph applications (e.g., node classification and link prediction) to set up the context and move on to train GNNs on large graphs. It will provide tutorials to demonstrate how to apply the techniques in DGL to train GNNs for real-world applications.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3521–3522},
numpages = {2},
keywords = {deep graph library, graph neural networks, scalability},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3548606.3560640,
author = {Jin, Xin and Manandhar, Sunil and Kafle, Kaushal and Lin, Zhiqiang and Nadkarni, Adwait},
title = {Understanding IoT Security from a Market-Scale Perspective},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560640},
doi = {10.1145/3548606.3560640},
abstract = {Consumer IoT products and services are ubiquitous; yet, a proper characterization of consumer IoT security is infeasible without an understanding of what IoT products are on the market, i.e., without a market-scale perspective. This paper seeks to close this gap by developing the IoTSpotter framework, which automatically constructs a market-scale snapshot of mobile-IoT apps, i.e., mobile apps that are used as companions or automation providers to IoT devices. IoTSpotter also extracts artifacts that allow us to examine the security of this snapshot in the IoT context (e.g., devices supported by apps, IoT-specific libraries). Using IoTSpotter, we identify 37,783 mobile-IoT apps from Google Play, the largest set of mobile-IoT apps so far, and uncover 7 key results in the process (ℛ1-ℛ7). We leverage this dataset to perform three key security analyses that lead to 10 impactful security findings (F1-F10) that demonstrate the current state of mobile-IoT apps. Our analysis uncovers severe cryptographic violations in 94.11% (863/917) mobile-IoT apps with &gt;1 million installs each, 65 vulnerable IoT-specific libraries affected by 79 unique CVEs, and used by 40 popular apps, and 7,887 apps that is affected by the Janus vulnerability. Finally, a case study with 18 popular mobile-IoT apps uncovers the critical impact of the vulnerabilities in them on important IoT artifacts and functions, motivating the development of mobile security analysis contextualized to IoT.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1615–1629},
numpages = {15},
keywords = {cryptographic api misuse, iot security, mobile-iot app identification, third-parity library vulnerability},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3610251.3610556,
author = {Teramoto, Kentaro and Hajdusek, Michal and Sasaki, Toshihiko and Van Meter, Rodney and Nagayama, Shota},
title = {RuleSet-based Recursive Quantum Internetworking},
year = {2023},
isbn = {9798400703065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610251.3610556},
doi = {10.1145/3610251.3610556},
abstract = {A scalable and robust infrastructure for arbitrary quantum communication and distributed quantum computation will realize enhanced as well as new applications that are beyond the reach of classical communication networks. Quantum internetworking, which achieves large-scale quantum networks by building recursively on smaller quantum networks, is a promising approach. However, there are two big difficulties: decomposing end-to-end quantum communication, which is actually a complex distributed computing spanning classical and quantum computing, into processing for each subnetwork; and supporting future network functionality sustainably. In this work, we propose design principles for a quantum internetworking protocol based on the quantum recursive network architecture (QRNA) utilizing the RuleSet-based approach. We design two distinct approaches for recursion in quantum networks, namely link- and node-recursion. We demonstrate how end-to-end Bell pairs are generated over our internetworking systems, and evaluate the scaling of inputs to the distributed processing, via implementation on the Quantum Internet Simulation Package (QuISP). Our subnetwork recursion demonstrates the long-term future scalability as the size of and demands made on the Quantum Internet.},
booktitle = {Proceedings of the 1st Workshop on Quantum Networks and Distributed Quantum Computing},
pages = {25–30},
numpages = {6},
keywords = {quantum repeater, quantum Internet, quantum communication},
location = {New York, NY, USA},
series = {QuNet '23}
}

@inproceedings{10.1145/3544902.3546240,
author = {Liu, Xin and Wu, Yixiong and Yu, Qingchen and Song, Shangru and Liu, Yue and Zhou, Qingguo and Zhuge, Jianwei},
title = {PG-VulNet: Detect Supply Chain Vulnerabilities in IoT Devices using Pseudo-code and Graphs},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546240},
doi = {10.1145/3544902.3546240},
abstract = {Background: With the boosting development of IoT technology, the supply chains of IoT devices become more powerful and sophisticated, and the security issues introduced by code reuse are becoming more prominent. Therefore, the detection and management of vulnerabilities through code similarity detection technology is of great significance for protecting the security of IoT devices. Aim: We aim to propose a more accurate, parallel-friendly, and realistic software supply chain vulnerability detection solution for IoT devices. Method: This paper presents PG-VulNet, standing for Vulnerability-detection Network based on Pseudo-code Graphs. It is a ”multi-model” cross-architecture vulnerability detection solution based on pseudo-code and Graph Matching Network (GMN). PG-VulNet extracts both behavioral and structural features of pseudo-code to build customized feature graphs and then uses GMN to detect supply chain vulnerabilities based on these graphs. Results: The experiments show that PG-VulNet achieves an average detection accuracy of 99.14%, significantly higher than existing approaches like Gemini, VulSeeker, FIT, and Asteria. In addition to this, PG-VulNet also excels in detection overhead and false alarms. In the real-world evaluation, PG-VulNet detected 690 known vulnerabilities in 1,611 firmwares. Conclusions: PG-VulNet can effectively detect the vulnerabilities introduced by software supply chain in IoT firmwares and is well suited for large-scale detection. Compared with existing approaches, PG-VulNet has significant advantages.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {205–215},
numpages = {11},
keywords = {Binary Code Similarity, Graph Neural Network, IoT Software Supply Chain, Vulnerability Detection},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1145/3581783.3612500,
author = {Lin, Hongbin and Chen, Bolin and Zhang, Zhichen and Lin, Jielian and Wang, Xu and Zhao, Tiesong},
title = {DeepSVC: Deep Scalable Video Coding for Both Machine and Human Vision},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612500},
doi = {10.1145/3581783.3612500},
abstract = {Nowadays, end-to-end video coding for both machine and human vision has become an emerging research topic. In complicated systems such as large-scale internet of video things (IoVT), feature streams and video streams can be separately encoded and delivered for machine judgement and human viewing. In this paper, we propose a deep scalable video codec (DeepSVC) to support three-layer scalability from machine to human vision. First, we design a semantic layer that encodes semantic features extracted from the captured video for machine analysis. This layer employs a conditional semantic compression (CSC) method to remove redundancies between semantic features. Second, we design a structure layer that can be combined with semantic layer to predict the captured video at a low quality. This layer effectively estimates video frames based on semantic layer with an interlayer frame prediction (IFP) network. Third, we design a texture layer that can be combined with the above two layers to reconstruct high-quality video signals. This layer also takes advantage of the IFP network to improve its coding efficiency. In large-scale IoVT systems, DeepSVC can deliver semantic layer for regular use and transmit the other layers on demand. Experimental results indicate that the proposed DeepSVC outperforms popular codecs for machine and human vision. Compared with scalable extension of H.265/HEVC (SHVC), the proposed DeepSVC reduces average bit-per-pixel (bpp) by 25.51%/27.63%/59.87% at the same mAP/PSNR/MS-SSIM. Sourcecode is available at: https://github.com/LHB116/DeepSVC.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9205–9214},
numpages = {10},
keywords = {deep video coding, scalable video coding, video coding for machines},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@article{10.1145/3529113.3529134,
author = {Chen, Xusheng and Zhao, Shixiong and Qi, Ji and Jiang, Jianyu and Song, Haoze and Wang, Cheng and On Li, Tsz and Hubert Chan, T-H. and Zhang, Fengwei and Luo, Xiapu and Wang, Sen and Zhang, Gong and Cui, Heming},
title = {Efficient and DoS-resistant Consensus for Permissioned Blockchains},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3529113.3529134},
doi = {10.1145/3529113.3529134},
abstract = {Existing permissioned blockchain systems designate a fixed and explicit group of committee nodes to run a consensus protocol that confirms the same sequence of blocks among all nodes. Unfortunately, when such a system runs on a large scale on the Internet, these explicit committee nodes can be easily turned down by denialof- service (DoS) or network partition attacks. Although recent studies proposed scalable BFT protocols that run on a larger number of committee nodes, these protocols' efficiency drops dramatically when only a small number of nodes are attacked.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {mar},
pages = {61–62},
numpages = {2}
}

@inproceedings{10.1145/3489525.3511672,
author = {Soltaniyeh, Mohammadreza and Lagrange Moutinho Dos Reis, Veronica and Bryson, Matt and Yao, Xuebin and Martin, Richard P. and Nagarakatte, Santosh},
title = {Near-Storage Processing for Solid State Drive Based Recommendation Inference with SmartSSDs®},
year = {2022},
isbn = {9781450391436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489525.3511672},
doi = {10.1145/3489525.3511672},
abstract = {Deep learning-based recommendation systems are extensively deployed in numerous internet services, including social media, entertainment services, and search engines, to provide users with the most relevant and personalized content. Production scale deep learning models consist of large embedding tables with billions of parameters. DRAM-based recommendation systems incur a high infrastructure cost and limit the size of the deployed models. Recommendation systems based on solid-state drives (SSDs) are a promising alternative for DRAM-based systems. Systems based on SSDs can offer ample storage required for deep learning models with large embedding tables. This paper proposes SmartRec, an inference engine for deep learning-based recommendation systems that utilizes Samsung SmartSSD, an SSD with an on-board FPGA that can process data in-situ. We evaluate SmartRec with state-of-the-art recommendation models from Facebook and compare its performance and energy efficiency to a DRAM-based system on a CPU. We show SmartRec improves the energy efficiency of the recommendation inference task up to 10x in comparison to the baseline CPU implementation. In addition, we propose a novel application-specific caching system for SmartSSDs that allows the kernel on the FPGA to use its DRAM as a cache to minimize high latency SSD accesses. Finally, we demonstrate the scalability of our design by offloading the computation to multiple SmartSSDs to further improve performance.},
booktitle = {Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering},
pages = {177–186},
numpages = {10},
keywords = {deep learning, fpga, near-storage computation, recommendation systems, smartssd},
location = {Beijing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3392561.3397577,
author = {Cannanure, Vikram Kamath and Brown, Timothy X and Ogan, Amy},
title = {DIA: A Human AI Hybrid Conversational Assistant for Developing Contexts},
year = {2020},
isbn = {9781450387620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3392561.3397577},
doi = {10.1145/3392561.3397577},
abstract = {Social media messaging applications(i.e. WhatsApp, Facebook) have reached 2.3 billion users in 2019, with the majority of users emerging from developing countries. The high usage among emergent users opens the possibility of designing text-based interventions for social change but such interventions rely on experts (i.e. doctors, educators, and moderators) knowledge which is scarce in developing contexts. Expert knowledge can be scaled up using chatbots but more research is needed to support emergent users who need context-specific support such as local language interventions or may not have regular internet connectivity. Therefore to support the design of chatbot based interventions in low resource contexts, we built DIA a chatbot architecture for low resource contexts to scale expert knowledge and support localization. DIA is a human-chatbot (humbot) hybrid system that organically learns topic-specific knowledge and local language from user interactions. We built a preliminary version of DIA on WhatsApp and deployed it to mentor 38 teachers in a rural context of C\^{o}te d'Ivoire. Through our preliminary deployment, we show that DIA can help (1) build a data-set of a topic and language-specific dialogues (2) understand users' online smartphone usage through chat logs and (3) collect survey data for through conversational interaction.},
booktitle = {Proceedings of the 2020 International Conference on Information and Communication Technologies and Development},
articleno = {24},
numpages = {5},
location = {Guayaquil, Ecuador},
series = {ICTD '20}
}

@article{10.1145/3625294,
author = {Li, Siyuan and Wang, Yongpan and Dong, Chaopeng and Yang, Shouguo and Li, Hong and Sun, Hao and Lang, Zhe and Chen, Zuxin and Wang, Weijie and Zhu, Hongsong and Sun, Limin},
title = {LibAM: An Area Matching Framework for Detecting Third-Party Libraries in Binaries},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3625294},
doi = {10.1145/3625294},
abstract = {Third-party libraries (TPLs) are extensively utilized by developers to expedite the software development process and incorporate external functionalities. Nevertheless, insecure TPL reuse can lead to significant security risks. Existing methods, which involve extracting strings or conducting function matching, are employed to determine the presence of TPL code in the target binary. However, these methods often yield unsatisfactory results due to the recurrence of strings and the presence of numerous similar non-homologous functions. Furthermore, the variation in C/C++ binaries across different optimization options and architectures exacerbates the problem. Additionally, existing approaches struggle to identify specific pieces of reused code in the target binary, complicating the detection of complex reuse relationships and impeding downstream tasks. And, we call this issue the poor interpretability of TPL detection results.In this article, we observe that TPL reuse typically involves not just isolated functions but also areas encompassing several adjacent functions on the Function Call Graph (FCG). We introduce LibAM, a novel Area Matching framework that connects isolated functions into function areas on FCG and detects TPLs by comparing the similarity of these function areas, significantly mitigating the impact of different optimization options and architectures. Furthermore, LibAM is the first approach capable of detecting the exact reuse areas on FCG and offering substantial benefits for downstream tasks. To validate our approach, we compile the first TPL detection dataset for C/C++ binaries across various optimization options and architectures. Experimental results demonstrate that LibAM outperforms all existing TPL detection methods and provides interpretable evidence for TPL detection results by identifying exact reuse areas. We also evaluate LibAM’s scalability on large-scale, real-world binaries in IoT firmware and generate a list of potential vulnerabilities for these devices. Our experiments indicate that the Area Matching framework performs exceptionally well in the TPL detection task and holds promise for other binary similarity analysis tasks. Last but not least, by analyzing the detection results of IoT firmware, we make several interesting findings, for instance, different target binaries always tend to reuse the same code area of TPL. The datasets and source code used in this article are available at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {52},
numpages = {35},
keywords = {Static binary analysis, third-party library detection, software component analysis}
}

@inproceedings{10.1145/3534678.3542633,
author = {Rabhi, Sara and Ak, Ronay and Romeijn, Marc and Moreira, Gabriel De Souza Pereira and Schifferer, Benedikt D.},
title = {Reducing the Friction for Building Recommender Systems with Merlin},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542633},
doi = {10.1145/3534678.3542633},
abstract = {Recommender Systems (RecSys) are the engine of the modern internet and the catalyst for human decisions. The goal of a recommender system is to generate relevant recommendations for users from a collection of items or services that might interest them. Building a recommendation system is challenging because it requires multiple stages (item retrieval, filtering, ranking, ordering) to work together seamlessly and efficiently during training and inference. The biggest challenges faced by new practitioners are the lack of understanding around what RecSys look like in the real world and the difficulty in transitioning from the simple Matrix Factorization (MF) to more complex deep learning architectures with multiple input features, neural components and prediction heads.To address these challenges on building recommender systems, NVIDIA developed an open source framework, called Merlin. Merlin consists of a set of libraries and tools to help RecSys practitioners build models and pipelines easily and more efficiently. Merlin Models provides modularized building blocks that can be easily connected to build classic and state-of-the-art models. It offers flexibility at each stage: multiple input processing/representation modules, different layers for designing the model's architecture, prediction heads, loss functions, negative sampling techniques, among others.In this hands-on tutorial, participants will start with data preparation using NVTabular an open-source feature engineering and preprocessing library designed to quickly and easily manipulate large scale datasets. Participants will then work on modeling with Merlin Models library, building the fundamental recommendation models such as MF and then transitioning to more complex deep learning-based models for candidate retrieval. In each iteration, we will demonstrate the seamless integration between data preparation and model training. Over the span of this tutorial, participants will learn the fundamentals of recommender systems modeling and how to build a two-stage recommender system easily using open source Merlin libraries.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4816–4817},
numpages = {2},
keywords = {deep learning, etl, personalization, recommendation, recommender systems, scaling, two-stage recommender systems},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3485447.3512245,
author = {Lee, Sangyup and An, Jaeju and Woo, Simon S.},
title = {BZNet: Unsupervised Multi-scale Branch Zooming Network for Detecting Low-quality Deepfake Videos},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512245},
doi = {10.1145/3485447.3512245},
abstract = {Generating a deep learning-based fake video has become no longer rocket science. The advancement of automated Deepfake (DF) generation tools that mimic certain targets has rendered society vulnerable to fake news or misinformation propagation. In real-world scenarios, DF videos are compressed to low-quality (LQ) videos, taking up less storage space and facilitating dissemination through the web and social media. Such LQ DF videos are much more challenging to detect than high-quality (HQ) DF videos. To address this challenge, we rethink the design of standard deep learning-based DF detectors, specifically exploiting feature extraction to enhance the features of LQ images. We propose a novel LQ DF detection architecture, multi-scale Branch Zooming Network (BZNet), which adopts an unsupervised super-resolution (SR) technique and utilizes multi-scale images for training. We train our BZNet only using highly compressed LQ images and experiment under a realistic setting, where HQ training data are not readily accessible. Extensive experiments on the FaceForensics++ LQ and GAN-generated datasets demonstrate that our BZNet architecture improves the detection accuracy of existing CNN-based classifiers by 4.21% on average. Furthermore, we evaluate our method against a real-world Deepfake-in-the-Wild dataset collected from the internet, which contains 200 videos featuring 50 celebrities worldwide, outperforming the state-of-the-art methods by 4.13%.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3500–3510},
numpages = {11},
keywords = {Deepfake Detection, Forensics, Low-quality Deepfakes, Multi-scale Learning, Unsupervised Super-Resolution},
location = {<conf-loc>, <city>Virtual Event, Lyon</city>, <country>France</country>, </conf-loc>},
series = {WWW '22}
}

@inproceedings{10.1145/3510450.3517274,
author = {Taraghi, Babak and Bentaleb, Abdelhak and Timmerer, Christian and Zimmermann, Roger and Hellwagner, Hermann},
title = {CAdViSE or how to find the sweet spots of ABR systems},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517274},
doi = {10.1145/3510450.3517274},
abstract = {With the recent surge in Internet multimedia traffic, the enhancement and improvement of media players, specifically Dynamic Adaptive Streaming over HTTP (DASH) media players happened at an incredible rate. DASH Media players take advantage of adapting a media stream to the network fluctuations by continuously monitoring the network and making decisions in near real-time. The performance of algorithms that are in charge of making such decisions was often difficult to be evaluated and objectively assessed from an End-to-end or holistic perspective [1].CAdViSE provides a Cloud-based Adaptive Video Streaming Evaluation framework for the automated testing of adaptive media players [4]. We will introduce the CAdViSE framework, its application, and propose the benefits and advantages that it can bring to every web-based media player development pipeline. To demonstrate the power of CAdViSE in evaluating Adaptive Bitrate (ABR) algorithms we will exhibit its capabilities when combined with objective Quality of Experience (QoE) models. Our team at Bitmovin Inc. and ATHENA laboratory has selected the ITU-T P.1203 (mode 1) quality evaluation model in order to assess the experiments and calculate the Mean Opinion Score (MOS), and better understand the behavior of a set of well-known ABR algorithms in a real-life setting [2]. We will display how we tested and deployed our framework using a modular architecture into a cloud infrastructure. This method yields a massive growth to the number of concurrent experiments and the number of media players that can be evaluated and compared at the same time, thus enabling maximum potential scalability. In our team's most recent experiments, we used Amazon Web Services (AWS) for demonstration purposes. Another awesome feature of CAdViSE that will be discussed here is the ability to shape the test network with endless network profiles. To do so, we used a fluctuation network profile and a real LTE network trace based on the recorded internet usage of a bicycle commuter in Belgium.CAdViSE produces comprehensive logs for each experimental session. These logs can then be applied against different goals, such as objective evaluation or to stitch back media segments and conduct subjective evaluations. In addition, startup delays, stall events, and other media streaming defects can be imitated exactly as they happened during the experimental streaming sessions [3].},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {94},
numpages = {1},
keywords = {ABR algorithms, HTTP adaptive streaming, quality of experience},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3377713.3377728,
author = {Zhang, Tongda and Qian, Jun and Sun, Xiao and Guo, Daqiang},
title = {Personalized Recommendation using Similarity Powered Pairwise Amplifier Network},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377728},
doi = {10.1145/3377713.3377728},
abstract = {Online advertising, one typical application of recommendation system, calls for effective and accurate recommendations of keywords. Extreme sparse and large scale data makes online advertising a challenging problem. To achieve better performance and accuracy of the recommendation, a better model with a short turnaround time is needed. In this paper, we address the problem of personalized online advertising for extreme sparse and large scale data. We develop a novel machine learning model (Similarity Powered Pairwise Amplifier Network, SPPAN for short). The complexity of this model (a.k.a. the number of parameters) grows with the amount of observed data, which makes it suitable to extremely sparse data. The training algorithm based on gradient descent makes it easy to parallelize. The similarity model combines the user neighborhood and item neighborhood ideas in collaborative filtering smartly, obtaining a cost-effective way to handle large scale data. The proposed framework is evaluated on a large set of real-world data set from a large internet company (expressed by "Company A"). The experiment results demonstrate that the proposed SPPAN model can greatly improve the prediction and recommendation accuracy on that extreme sparse data set compared with existing approaches.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {138–146},
numpages = {9},
keywords = {Neural network, Personalized recommendation, Similarity},
location = {Sanya, China},
series = {ACAI '19}
}

@inproceedings{10.1145/3573428.3573434,
author = {Yu, Houhui and Chen, Taowei and Wang, Jingyi},
title = {A blockchain-based Access Control Mechanism for IOT},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573434},
doi = {10.1145/3573428.3573434},
abstract = {Because of lightweight, large-scale and dynamic access characteristics in IoT devices, the current centralized access control mechanism leads to many security problems such as failure of single point, privacy leakage and user authentication. With the advent of blockchain technology, it could be a better way to address these problems by decentralized, transparent and immutability properties of blockchain. Thus, a blockchain-based access control mechanism for IOT is proposed based on the ABAC access control model. Firstly, the model architecture and access control workflow are developed in detail, and the data format of transactions in the blockchain is defined so as to facilitate the release, update, and revocation of attributes and policies. And then, the smart contracts on chain are programed to ensure that the conditions they set on access and sharing will be enforced. Finally, experimental simulation demonstrates that the proposed scheme is efficient in policy-making and policy-querying. At the same time, the use of decentralized ABAC makes it much secure for decision-makers to control variables that ensure a fine-grained approach to access control.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {25–30},
numpages = {6},
keywords = {ABAC model, Access control, Blockchain, IOT, Smart contract},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3576842.3582386,
author = {Gupta, Tanmaey and Handa, Shubhankar and Nambi, Akshay},
title = {Verified Telemetry: A General, Easy to use, Scalable and Robust Fault Detection SDK for IoT Sensors},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3582386},
doi = {10.1145/3576842.3582386},
abstract = {With the proliferation of IoT sensors, the reliance on sensor telemetry has never been greater. Today numerous applications from smart agriculture to smart buildings and cities, rely on IoT telemetry to provide insights and to take decisions. However, due to the characteristics of these IoT deployments (in the wild, harsh conditions), sensors are prone to failures, leading to the generation of bad/dirty data. Hitherto, data-centric algorithms were used to determine the quality of the sensed data, which has several limitations and relies on additional contextual information or sensor redundancy. Recently, system-centric approaches based on sensor fingerprinting has shown to detect sensor faults reliably without any additional information. However, the sensor fingerprinting approach is validated only for specific sensors, is not robust to real-world conditions, and cannot scale to large-scale deployments. To this end, we propose a general, easy-to-use, scalable, and robust fault detection SDK called Verified Telemetry (VT) SDK. VT SDK builds on the sensor fingerprinting approach and can work with a wide variety of sensors (both analog and digital) and IoT devices with very minimal changes. We propose improved sensor fingerprinting algorithms that are robust to signal variations, sensor circuitry, and real-world conditions. VT SDK is implemented across numerous devices and we show its usage on several practical applications. Finally, VT SDK is made available for the community to address sensor fault detection in IoT deployments (https://aka.ms/verifiedtelemetry).},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {381–395},
numpages = {15},
keywords = {Fault detection and isolation, reliability, sensor faults, verified telemetry},
location = {<conf-loc>, <city>San Antonio</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {IoTDI '23}
}

@inproceedings{10.1145/3495018.3495481,
author = {Huhemandula and Bai, Jie and Ji, Wenhui},
title = {Implementation of Parallel Algorithms for Liquid Metal Solidification Molecular Dynamics Based on Big Data},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495481},
doi = {10.1145/3495018.3495481},
abstract = {With the advancement of science and technology and the popularization of the Internet, large amounts of data and information will be generated from people's daily work and life to the state of molecular motion trajectories. How to efficiently mine the hidden information of these massive data information is the key to scientific and social development, which requires big data analysis technology. The solidification process of metals is a very complex process, including both macroscopic and microscopic processes. Molecular dynamics can simulate the process of liquid metal solidification from a microscopic scale. In molecular dynamics simulation, the more particles the simulation system contains, the more accurate the data obtained. However, the molecular dynamics serial program runs slowly, and the number of simulated atoms is limited, which makes the simulation results far from the real situation. The rise of parallel computing provides conditions for large-scale molecular dynamics parallelization and promotes the rapid development of molecular dynamics parallelization. The purpose of this paper is to study the dynamic parallel algorithm of liquid metal solidification molecules based on big data. In this paper, through in-depth research on big data, analysis of the current research results of parallel computing of liquid metal solidification molecules, and combined with the current research status of liquid metal solidification molecules in my country, to discuss the research of liquid metal solidification molecular dynamics parallel algorithm under big data. Research shows that the spatial domain decomposition method is more general, and its communication cost is relatively small. It reflects that it can obtain relatively high parallel performance and computing efficiency in local communication, especially in large-scale simulation systems with a large number of atoms.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1769–1773},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3387939.3388614,
author = {Sakizloglou, Lucas and Ghahremani, Sona and Brand, Thomas and Barkowsky, Matthias and Giese, Holger},
title = {Towards highly scalable runtime models with history},
year = {2020},
isbn = {9781450379625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387939.3388614},
doi = {10.1145/3387939.3388614},
abstract = {Advanced systems such as IoT comprise many heterogeneous, interconnected, and autonomous entities operating in often highly dynamic environments. Due to their large scale and complexity, large volumes of monitoring data are generated and need to be stored, retrieved, and mined in a time- and resource-efficient manner. Architectural self-adaptation automates the control, orchestration, and operation of such systems. This can only be achieved via sophisticated decision-making schemes supported by monitoring data that fully captures the system behavior and its history.Employing model-driven engineering techniques we propose a highly scalable, history-aware approach to store and retrieve monitoring data in form of enriched runtime models. We take advantage of rule-based adaptation where change events in the system trigger adaptation rules. We first present a scheme to incrementally check model queries in the form of temporal logic formulas which represent the conditions of adaptation rules against a runtime model with history. Then we equip the model with the capability to retain only information that is relevant to queries. Finally, we demonstrate the scalability of our approach via experiments on a simulated smart healthcare system employing a real-world medical guideline.},
booktitle = {Proceedings of the IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {188–194},
numpages = {7},
keywords = {IoT, architectural self-adaptation, history-awareness, memory efficiency, runtime models, scalability, temporal graph conditions},
location = {Seoul, Republic of Korea},
series = {SEAMS '20}
}

@article{10.1145/3451964.3451982,
author = {Mackenzie, Joel M.},
title = {Managing tail latency in large scale information retrieval systems},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3451964.3451982},
doi = {10.1145/3451964.3451982},
abstract = {As both the availability of internet access and the prominence of smart devices continue to increase, data is being generated at a rate faster than ever before. This massive increase in data production comes with many challenges, including efficiency concerns for the storage and retrieval of such large-scale data. However, users have grown to expect the sub-second response times that are common in most modern search engines, creating a problem --- how can such large amounts of data continue to be served efficiently enough to satisfy end users?This dissertation investigates several issues regarding tail latency in large-scale information retrieval systems. Tail latency corresponds to the high percentile latency that is observed from a system --- in the case of search, this latency typically corresponds to how long it takes for a query to be processed. In particular, keeping tail latency as low as possible translates to a good experience for all users, as tail latency is directly related to the worst-case latency and hence, the worst possible user experience. The key idea in targeting tail latency is to move from questions such as "what is the median latency of our search engine?" to questions which more accurately capture user experience such as "how many queries take more than 200ms to return answers?" or "what is the worst case latency that a user may be subject to, and how often might it occur?"While various strategies exist for efficiently processing queries over large textual corpora, prior research has focused almost entirely on improvements to the average processing time or cost of search systems. As a first contribution, we examine some state-of-the-art retrieval algorithms for two popular index organizations, and discuss the trade-offs between them, paying special attention to the notion of tail latency. This research uncovers a number of observations that are subsequently leveraged for improved search efficiency and effectiveness.We then propose and solve a new problem, which involves processing a number of related query variations together, known as multi-queries, to yield higher quality search results. We experiment with a number of algorithmic approaches to efficiently process these multi-queries, and report on the cost, efficiency, and effectiveness trade-offs present with each.Finally, we examine how predictive models can be used to improve the tail latency and end-to-end cost of a commonly used multi-stage retrieval architecture without impacting result effectiveness. By combining ideas from numerous areas of information retrieval, we propose a prediction framework which can be used for training and evaluating several efficiency/effectiveness trade-off parameters, resulting in improved trade-offs between cost, result quality, and tail latency.},
journal = {SIGIR Forum},
month = {feb},
articleno = {18},
numpages = {2}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00103,
author = {Makhshari, Amir and Mesbah, Ali},
title = {IoT development in the wild: bug taxonomy and developer challenges},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00103},
doi = {10.1109/ICSE-Companion52605.2021.00103},
abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {225–226},
numpages = {2},
keywords = {empirical study, internet of things, mining software repositories, software engineering},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3534678.3539448,
author = {Ghosh, Rahul and Renganathan, Arvind and Tayal, Kshitij and Li, Xiang and Khandelwal, Ankush and Jia, Xiaowei and Duffy, Christopher and Nieber, John and Kumar, Vipin},
title = {Robust Inverse Framework using Knowledge-guided Self-Supervised Learning: An application to Hydrology},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539448},
doi = {10.1145/3534678.3539448},
abstract = {Machine Learning is beginning to provide state-of-the-art performance in a range of environmental applications such as streamflow prediction in a hydrologic basin. However, building accurate broad-scale models for streamflow remains challenging in practice due to the variability in the dominant hydrologic processes, which are best captured by sets of process-related basin characteristics. Existing basin characteristics suffer from noise and uncertainty, among many other things, which adversely impact model performance. To tackle the above challenges, in this paper, we propose a novel Knowledge-guided Self-Supervised Learning (KGSSL) inverse framework to extract system characteristics from driver(input) and response(output) data. This first-of-its-kind framework achieves robust performance even when characteristics are corrupted or missing. We evaluate the KGSSL framework in the context of stream flow modeling using CAMELS (Catchment Attributes and MEteorology for Large-sample Studies) which is a widely used hydrology benchmark dataset. Specifically, KGSSL outperforms baseline by 16% in predicting missing characteristics. Furthermore, in the context of forward modelling, KGSSL inferred characteristics provide a 35% improvement in performance over a standard baseline when the static characteristic are unknown.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {465–474},
numpages = {10},
keywords = {forward modeling, inverse modeling, self-supervised learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3575813.3595192,
author = {Tabaeiaghdaei, Seyedali and Scherrer, Simon and Kwon, Jonghoon and Perrig, Adrian},
title = {Carbon-Aware Global Routing in Path-Aware Networks},
year = {2023},
isbn = {9798400700323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575813.3595192},
doi = {10.1145/3575813.3595192},
abstract = {The growing energy consumption of Information and Communication Technology (ICT) has raised concerns about its environmental impact. However, the carbon footprint of data transmission over the Internet has so far received relatively modest attention. This carbon footprint can be reduced by sending traffic over carbon-efficient inter-domain paths. However, challenges in estimating and disseminating carbon intensity of inter-domain paths have prevented carbon-aware path selection from becoming a reality. In this paper, we take advantage of path-aware network architectures to overcome these challenges. In particular, we design&nbsp;CIRo, a system for forecasting the carbon intensity of inter-domain paths and disseminating them across the Internet. We implement a proof of concept for&nbsp;CIRo on the codebase of the SCION path-aware Internet architecture and test it on the SCIONLab global research testbed. Further, through large-scale simulations, we demonstrate the potential of&nbsp;CIRo for reducing the carbon footprint of endpoints and end domains: With&nbsp;CIRo, half of domain pairs can reduce the carbon intensity of their inter-domain traffic by at least 47%, and 87% of end domains can reduce their carbon footprint of Internet use by at least 50%.},
booktitle = {Proceedings of the 14th ACM International Conference on Future Energy Systems},
pages = {144–158},
numpages = {15},
keywords = {Carbon-Aware Routing, Green Networking, Inter-Domain Routing, Internet Carbon-Emission Modeling and Measurement, SCION},
location = {Orlando, FL, USA},
series = {e-Energy '23}
}

@inproceedings{10.1145/3579856.3595806,
author = {Sivakumaran, Pallavi and Zuo, Chaoshun and Lin, Zhiqiang and Blasco, Jorge},
title = {Uncovering Vulnerabilities of Bluetooth Low Energy IoT from Companion Mobile Apps with Ble-Guuide},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3595806},
doi = {10.1145/3579856.3595806},
abstract = {Increasingly, with embedded intelligence and control, IoT devices are being adopted faster than ever. However, the IoT landscape and its security implications are not yet fully understood. This paper seeks to shed light on this by focusing on a particular type of IoT devices, namely the ones using Bluetooth Low Energy (BLE). Our contributions are two-fold: First, we present Ble-Guuide, a framework for performing mobile app-centric security issue identification. We exploit Universally Unique Identifiers (UUIDs), which underpin data transmissions in BLE, to glean rich information regarding device functionality and the underlying security issues. We combine this with information from app descriptions and BLE libraries, to identify the corresponding security vulnerabilities in BLE devices and determine the security or privacy impact they could have depending on the device functionality. Second, we present a large-scale analysis of 17,243 free, BLE-enabled Android APKs, systematically crawled from the official Google Play store. By applying Ble-Guuide to this dataset, we uncover that more than 70% of these APKs contain at least one security vulnerability. We also obtain insights into the identified security vulnerabilities and their impact.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {1004–1015},
numpages = {12},
keywords = {Android., BLE, IoT Security, UUID},
location = {<conf-loc>, <city>Melbourne</city>, <state>VIC</state>, <country>Australia</country>, </conf-loc>},
series = {ASIA CCS '23}
}

@inproceedings{10.1145/3443467.3443869,
author = {Gu, Huijie and Hu, Jieping and Gu, Mengdie and Yuan, Ming},
title = {Research and Design of Digital Content Management System Based on Microservice},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443869},
doi = {10.1145/3443467.3443869},
abstract = {With the advent of the Internet plus era, the traditional information management system is not only increasingly large and complex, but also difficult to cope with the diversity of information. In view of the traditional digital content management system, which has the problems of low efficiency of digital content processing, trivial and huge data, this paper uses the unique organizational structure of microservice technology, distributed service registration technology and Spring Cloud microservice framework to solve them, and proposes a digital content management system based on microservice. Microservice technology has the characteristics of independent deployment, single responsibility, easy maintenance and easy upgrade, which makes the system have better expansibility, large-scale deployment and flexible configuration, and provides new ideas and methods for information system.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {865–869},
numpages = {5},
keywords = {Digital content management, Distributed service registration technology, Microservice, Spring Cloud},
location = {Xiamen, China},
series = {EITCE '20}
}

@inproceedings{10.1145/3520084.3520095,
author = {Xinhong, Yin and Hailong, Su and Jidong, Bao and Fei, Liu},
title = {Construction of large-scale equipment sharing cloud service platform based on Wechat Mini Program: Construction of large-scale equipment,etc.},
year = {2022},
isbn = {9781450395519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520084.3520095},
doi = {10.1145/3520084.3520095},
abstract = {With the continuous progress of "Internet +", the development of mobile apps has facilitated people's travel and life. The Wechat Mini Program is simple, fast, and lightweight, allowing users to open the desired application by scanning or searching. Therefore, based on the Wechat Mini Program vantWeapp framework and so on, this paper adopts WeChat developer tools to implement a Wechat Mini Program of large-scale instrument sharing cloud service platform. Rapid appointment of large-scale equipment and scientific and technological services is realized through mobile internet technology, and a full coverage pattern from PC to mobile terminal of scientific instrument resource sharing service is formed to realize "technical services at your fingertips".},
booktitle = {Proceedings of the 2022 5th International Conference on Software Engineering and Information Management},
pages = {68–75},
numpages = {8},
keywords = {Large equipment, Science and technology resources, Wechat Mini Program},
location = {Yokohama, Japan},
series = {ICSIM '22}
}

@inproceedings{10.5555/3400306.3400309,
author = {Delbruel, St\'{e}phane and Small, Nicolas and Aras, Emekcan and Oostvogels, Jonathan and Hughes, Danny},
title = {Tackling Contention Through Cooperation: A Distributed Federation in LoRaWAN Space},
year = {2020},
isbn = {9780994988645},
publisher = {Junction Publishing},
address = {USA},
abstract = {Low-Power Wide Area Networks (LPWAN) play a key role in the IoT marketplace wherein LoRaWAN is considered a leading solution. Despite the traction of LoRaWAN, research shows that the current contention management mechanisms of LoRaWAN do not scale. This paper tackles contention on LoRaWAN by introducing FLIP, a fully distributed and open architecture for LoRaWAN that fundamentally rethinks how LoRa gateways should be managed and coordinated. FLIP transforms LoRa gateways into a federated network that provides inherent support for roaming while tackling contention using consensus-driven load balancing. FLIP offers identical security guarantees to LoRaWAN, is compatible with existing gateway hardware and requires no updates to end-device hardware or firmware. These features ensure the practicality of FLIP and provide a path to its adoption. We evaluate the performance of FLIP in a large-scale real-world deployment and demonstrate that FLIP delivers scalable roaming and improved contention management in comparison to LoRaWAN. FLIP achieves these benefits within the resource constraints of conventional LoRa gateways and requires no server hardware.},
booktitle = {Proceedings of the 2020 International Conference on Embedded Wireless Systems and Networks},
pages = {13–24},
numpages = {12},
keywords = {Architecture, Contention, Distributed, Federation, LoRaWAN, Scalability},
location = {Lyon, France},
series = {EWSN '20}
}

@article{10.1145/3520441,
author = {Shudrenko, Yevhenii and Pl\"{o}ger, Daniel and Kuladinithi, Koojana and Timm-Giel, Andreas},
title = {A Novel Approach to Enhance the End-to-End Quality of Service for Avionic Wireless Sensor Networks},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3520441},
doi = {10.1145/3520441},
abstract = {Going wireless is one of the key industrial trends, which assists the emergence of new manufacturing and maintenance processes by reducing the complexity and cost of physical equipment. However, the adoption of Wireless Sensor Networks (WSNs) in production environments is limited due to the strict Quality of Service (QoS) requirements of industrial applications. In particular, Wireless Avionics Intra-Communication (WAIC) systems operating in 4.3 GHz band are designed for intra-aircraft use cases with considerable restrictions on the transmission power of sensors, which results in multi-hop topologies, complicating a guaranteed QoS. The Internet Engineering Task Force (IETF) has developed the protocol stack IPv6 over the Time Slotted Channel Hopping (TSCH) mode of IEEE 802.15.4 (6TiSCH) based on the IEEE 802.15.4 Standard for Low-Rate Wireless Networks, which combines the TSCH reliability with ubiquitous IPv6 connectivity and with the robust Routing Protocol for Low-Power and Lossy Networks (RPL). The Scheduling Function (SF) is a core IPv6 over the TSCH mode of IEEE 802.15.4 (6TiSCH) component, but the specification of the SF is an open research topic: numerous scientific articles investigated how QoS for a wide range of applications can be met by developing specialized SFs. However, no full-scale information exchange between the layers of the 6TiSCH stack was considered to optimize the SFs and to improve the network performance. In this work, we propose a novel solution named 6TiSCH-CLX to satisfy demanding QoS requirements using cross-layer communication. It is an extension of the 6TiSCH framework at the network and Medium Access Control (MAC) layers, addressing latency and reliability challenges agnostic of the physical layer. 6TiSCH-CLX is evaluated both analytically and in simulations for several safety-critical avionic intra-communication use cases in WAIC. Preliminary results indicate considerable improvements to latency, while maintaining almost 100% Packet Delivery Ratio (PDR) without retransmissions and they highlight the capability of the cross-layer approach compared to existing solutions.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {95},
numpages = {29},
keywords = {WAIC, 6TiSCH, MSF, WSN, IEEE 802.15.4, TSCH, IPv6, RPL, scheduling function, end-to-end QoS, avionics}
}

@inproceedings{10.1109/ICSE43902.2021.00051,
author = {Makhshari, Amir and Mesbah, Ali},
title = {IoT Bugs and Development Challenges},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00051},
doi = {10.1109/ICSE43902.2021.00051},
abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We collected 5,565 bug reports from 91 representative IoT project repositories and categorized a random sample of 323 based on the observed failures, root causes, and the locations of the faulty components. In addition, we conducted nine interviews with IoT experts to uncover more details about IoT bugs and to gain insight into IoT developers' challenges. Lastly, we surveyed 194 IoT developers to validate our findings and gain further insights. We propose the first bug taxonomy for IoT systems based on our results. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {460–472},
numpages = {13},
keywords = {Empirical Study, Internet of Things, Mining Software Repositories, Software Engineering},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3507909,
author = {Ricci, Alessandro and Croatti, Angelo and Mariani, Stefano and Montagna, Sara and Picone, Marco},
title = {Web of Digital Twins},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3507909},
doi = {10.1145/3507909},
abstract = {In recent years, digital twins have been pervading different application domains—from manufacturing to healthcare—as an approach for virtualising different kinds of physical entities (things, products, machines). The dominant view developed in the literature so far is about the virtualisation of individual physical assets in a closed-system perspective. In this article, we introduce and explore a broader perspective that we call Web of Digital Twins (WoDT), in which the digital twin paradigm is exploited for the pervasive softwarisation of possibly large-scale interrelated physical realities. A WoDT can be conceived as an open, distributed and dynamic ecosystem of connected digital twins, functioning as an interoperable service-oriented layer for applications running on top, especially smart applications and multiagent systems. The article introduces an abstract model and architecture aimed to capture key aspects of the idea not bound to any specific application domains or implementing technologies and discusses their adoption in engineering real-world systems. To this purpose, two concrete case studies are considered, in the context of healthcare and smart mobility. Finally, the article includes a discussion of a selected set of research directions.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {101},
numpages = {30},
keywords = {Digital twins, web, agents, MAS, WoDT}
}

@article{10.1145/3604611,
author = {Yang, Shouguo and Dong, Chaopeng and Xiao, Yang and Cheng, Yiran and Shi, Zhiqiang and Li, Zhi and Sun, Limin},
title = {Asteria-Pro: Enhancing Deep Learning-based Binary Code Similarity Detection by Incorporating Domain Knowledge},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3604611},
doi = {10.1145/3604611},
abstract = {Widespread code reuse allows vulnerabilities to proliferate among a vast variety of firmware. There is an urgent need to detect these vulnerable codes effectively and efficiently. By measuring code similarities, AI-based binary code similarity detection is applied to detecting vulnerable code at scale. Existing studies have proposed various function features to capture the commonality for similarity detection. Nevertheless, the significant code syntactic variability induced by the diversity of IoT hardware architectures diminishes the accuracy of binary code similarity detection. In our earlier study and the tool Asteria, we adopted a Tree-LSTM network to summarize function semantics as function commonality, and the evaluation result indicates an advanced performance. However, it still has utility concerns due to excessive time costs and inadequate precision while searching for large-scale firmware bugs.To this end, we propose a novel deep learning-enhancement architecture by incorporating domain knowledge-based pre-filtration and re-ranking modules, and we develop a prototype named Asteria-Pro based on Asteria. The pre-filtration module eliminates dissimilar functions, thus reducing the subsequent deep learning-model calculations. The re-ranking module boosts the rankings of vulnerable functions among candidates generated by the deep learning model. Our evaluation indicates that the pre-filtration module cuts the calculation time by 96.9%, and the re-ranking module improves MRR and Recall by 23.71% and 36.4%, respectively. By incorporating these modules, Asteria-Pro outperforms existing state-of-the-art approaches in the bug search task by a significant margin. Furthermore, our evaluation shows that embedding baseline methods with pre-filtration and re-ranking modules significantly improves their precision. We conduct a large-scale real-world firmware bug search, and Asteria-Pro manages to detect 1,482 vulnerable functions with a high precision 91.65%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {1},
numpages = {40},
keywords = {Binary code similarity detection, pre-fitering, re-ranking, abstract syntactic tree, graph neural network}
}

@article{10.1145/3366372,
author = {Mordacchini, Matteo and Conti, Marco and Passarella, Andrea and Bruno, Raffaele},
title = {Human-centric Data Dissemination in the IoP: Large-scale Modeling and Evaluation},
year = {2020},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/3366372},
doi = {10.1145/3366372},
abstract = {Data management using Device-to-Device (D2D) communications and opportunistic networks (ONs) is one of the main focuses of human-centric pervasive Internet services. In the recently proposed “Internet of People” paradigm, accessing relevant data dynamically generated in the environment nearby is one of the key services. Moreover, personal mobile devices become proxies of their human users while exchanging data in the cyber world and, thus, largely use ONs and D2D communications for exchanging data directly. Recently, researchers have successfully demonstrated the viability of embedding human cognitive schemes in data dissemination algorithms for ONs. In this article, we consider one such scheme based on the recognition heuristic, a human decision-making scheme used to efficiently assess the relevance of data. While initial evidence about its effectiveness is available, the evaluation of its behaviour in large-scale settings is still unsatisfactory. To overcome these limitations, we have developed a novel hybrid modeling methodology that combines an analytical model of data dissemination within small-scale communities of mobile users, with detailed simulations of interactions between different communities. This methodology allows us to evaluate the algorithm in large-scale city- and countrywide scenarios. Results confirm the effectiveness of cognitive data dissemination schemes, even when content popularity is very heterogenous.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {feb},
articleno = {10},
numpages = {25},
keywords = {Internet of people, data dissemination, human cognitive heuristic, hybrid simulation, opportunistic networks}
}

@inproceedings{10.1145/3551349.3556901,
author = {Yin, Zijing and Xu, Yiwen and Zhou, Chijin and Jiang, Yu},
title = {Empirical Study of System Resources Abused by IoT Attackers},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556901},
doi = {10.1145/3551349.3556901},
abstract = {IoT devices have been under frequent attacks in recent years, causing severe impacts. Previous research has shown the evolution and features of some specific IoT malware families or stages of IoT attacks through offline sample analysis. However, we still lack a systematic observation of various system resources abused by active attackers and the malicious intentions behind these behaviors. This makes it difficult to design appropriate protection strategies to defend against existing attacks and possible future variants. In this paper, we fill this gap by analyzing 117,862 valid attack sessions captured by our dedicated high-interaction IoT honeypot, HoneyAsclepius, and further discover the intentions in our designed workflow. HoneyAsclepius enables high capture capability as well as continuous behavior monitoring during active attack sessions in real-time. Through a large-scale deployment, we collected 11,301,239 malicious behaviors originating from 50,594 different attackers. Based on this information, we further separate the behaviors in different attack sessions targeting distinct categories of system resources, estimate the temporal relations and summarize their malicious intentions behind. Inspired by such investigations, we present several key insights about abusive behaviors of the file, network, process, and special capability resources, and further propose practical defense strategies to better protect IoT devices.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {39},
numpages = {13},
keywords = {Behavior Intention, IoT Attack, System Resource Abuse},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1145/3542637.3543705,
author = {Yu, Bo and Zhang, Yongyi and Liu, Runhao and Sheng, Zhoushi},
title = {A Component Vulnerability Matching Approach for IoT Firmware},
year = {2023},
isbn = {9781450397483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542637.3543705},
doi = {10.1145/3542637.3543705},
abstract = {Component vulnerability matching offers an approach for discovering vulnerabilities existing in IoT firmware. In this work, A component composition analysis and reliability assessment (C2ARA) is developed to improve the component vulnerability matching. The C2ARA method employs a knowledge graph for discovering the components and their relationships from the extracted file system of the firmware. The key to the proposed method is to discover vulnerabilities from the component composition extracted from IoT firmware file systems, rather than only the information provided by CVE databases and firmware vendor. The results of the experiment with a large-scale dataset demonstrate the effectiveness of the C2ARA method.},
booktitle = {Proceedings of the 6th Asia-Pacific Workshop on Networking},
pages = {95–96},
numpages = {2},
keywords = {Component composition, IoT firmware, Reliability assessment, Vulnerability matching},
location = {<conf-loc>, <city>Fuzhou</city>, <country>China</country>, </conf-loc>},
series = {APNet '22}
}

@inproceedings{10.1145/3638550.3643625,
author = {Ghosh, Ushasi and Jain, Ish Kumar and Bharadia, Dinesh and Shakkottai, Srinivas},
title = {Poster: Tiny-twin: A Lightweight and Verifiable Digital Twin for NextG Cellular Networks},
year = {2024},
isbn = {9798400704970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638550.3643625},
doi = {10.1145/3638550.3643625},
abstract = {The next generation of cellular networks, driven by applications like large-scale autonomy, extended reality, and IoT-based control, demands high network performance in terms of throughput, latency, and jitter, along with quick network adaptability. Developing these applications and corresponding network algorithms on commercial radio access networks (RAN) or through large-scale frameworks such as [1, 2] is often prohibitively expensive and time-consuming. Currently, the norm is to use simpler, simulated environments for initial testing, followed by real-world network evaluations. This approach simplifies testing but may not accurately capture the cellular network's complexities that can significantly impact application performance. Can we optimize both?},
booktitle = {Proceedings of the 25th International Workshop on Mobile Computing Systems and Applications},
pages = {145},
numpages = {1},
keywords = {digital twin, cellular networks, multimedia applications},
location = {<conf-loc>, <city>San Diego</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {HOTMOBILE '24}
}

@inproceedings{10.1145/3503161.3548145,
author = {Qin, An and Xiao, Mengbai and Huang, Ben and Zhang, Xiaodong},
title = {Maze: A Cost-Efficient Video Deduplication System at Web-scale},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548145},
doi = {10.1145/3503161.3548145},
abstract = {With the advancement and dominant service of Internet videos, the content-based video deduplication system becomes an essential and dependent infrastructure for Internet video service. However, the explosively growing video data on the Internet challenges the system design and implementation for its scalability in several ways. (1) Although the quantization-based indexing techniques are effective for searching visual features at a large scale, the costly re-training over the complete dataset must be done periodically. (2) The high-dimensional vectors for visual features demand increasingly large SSD space, degrading I/O performance. (3) Videos crawled from the Internet are diverse, and visually similar videos are not necessarily the duplicates, increasing deduplication complexity. (4) Most videos are edited ones. The duplicate contents are more likely discovered as clips inside the videos, demanding processing techniques with close attention to details.To address above-mentioned issues, we propose Maze, a full-fledged video deduplication system. Maze has an ANNS layer that indexes and searches the high dimensional feature vectors. The architecture of the ANNS layer supports efficient reads and writes and eliminates the data migration caused by re-training. Maze adopts the CNN-based feature and the ORB feature as the visual features, which are optimized for the specific video deduplication task. The features are compact and fully reside in the memory. Acoustic features are also incorporated in Maze so that the visually similar videos but having different audio tracks are recognizable. A clip-based matching algorithm is developed to discover duplicate contents at a fine granularity. Maze has been deployed as a production system for two years. It has indexed 1.3 billion videos and is indexing ~800 thousand videos per day. For the ANNS layer, the average read latency is 4 seconds and the average write latency is at most 4.84 seconds. The re-training over the complete dataset is no longer required no matter how many new data sets are added, eliminating the costly data migration between nodes. Maze recognizes the duplicate live streaming videos with both the similar appearance and the similar audio at a recall of 98%. Most importantly, Maze is also cost-effective. For example, the compact feature design helps save 5800 SSDs and the computation resources devoted to running the whole system decrease to 250K standard cores per billion videos.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3163–3172},
numpages = {10},
keywords = {scalable search engine architecture, video deduplication, video retrieval},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@article{10.1145/3477540,
author = {Lu, Feng and Li, Wei and Lin, Song and Peng, Chengwangli and Wang, Zhiyong and Qian, Bin and Ranjan, Rajiv and Jin, Hai and Zomaya, Albert Y.},
title = {Multi-scale Features Fusion for the Detection of Tiny Bleeding in Wireless Capsule Endoscopy Images},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3477540},
doi = {10.1145/3477540},
abstract = {Wireless capsule endoscopy is a modern non-invasive Internet of Medical Imaging Things that has been increasingly used in gastrointestinal tract examination. With about one gigabyte image data generated for a patient in each examination, automatic lesion detection is highly desirable to improve the efficiency of the diagnosis process and mitigate human errors. Despite many approaches for lesion detection have been proposed, they mainly focus on large lesions and are not directly applicable to tiny lesions due to the limitations of feature representation. As bleeding lesions are a common symptom in most serious gastrointestinal diseases, detecting tiny bleeding lesions is extremely important for early diagnosis of those diseases, which is highly relevant to the survival, treatment, and expenses of patients. In this article, a method is proposed to extract and fuse multi-scale deep features for detecting and locating both large and tiny lesions. A feature extracting network is first used as our backbone network to extract the basic features from wireless capsule endoscopy images, and then at each layer multiple regions could be identified as potential lesions. As a result, the features maps of those potential lesions are obtained at each level and fused in a top-down manner to the fully connected layer for producing final detection results. Our proposed method has been evaluated on a clinical dataset that contains 20,000 wireless capsule endoscopy images with clinical annotation. Experimental results demonstrate that our method can achieve 98.9% prediction accuracy and 93.5%  score, which has a significant performance improvement of up to 31.69% and 22.12% in terms of recall rate and  score, respectively, when compared to the state-of-the-art approaches for both large and tiny bleeding lesions. Moreover, our model also has the highest AP and the best medical diagnosis performance compared to state-of-the-art multi-scale models.},
journal = {ACM Trans. Internet Things},
month = {oct},
articleno = {2},
numpages = {19},
keywords = {Wireless Capsule Endoscopy, deep learning, bleeding lesion detection}
}

@inproceedings{10.1145/3387514.3405875,
author = {Harchol, Yotam and Bergemann, Dirk and Feamster, Nick and Friedman, Eric and Krishnamurthy, Arvind and Panda, Aurojit and Ratnasamy, Sylvia and Schapira, Michael and Shenker, Scott},
title = {A Public Option for the Core},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405875},
doi = {10.1145/3387514.3405875},
abstract = {This paper is focused not on the Internet architecture - as defined by layering, the narrow waist of IP, and other core design principles - but on the Internet infrastructure, as embodied in the technologies and organizations that provide Internet service. In this paper we discuss both the challenges and the opportunities that make this an auspicious time to revisit how we might best structure the Internet's infrastructure. Currently, the tasks of transit-between-domains and last-mile-delivery are jointly handled by a set of ISPs who interconnect through BGP. In this paper we propose cleanly separating these two tasks. For transit, we propose the creation of a "public option" for the Internet's core backbone. This public option core, which complements rather than replaces the backbones used by large-scale ISPs, would (i) run an open market for backbone bandwidth so it could leverage links offered by third-parties, and (ii) structure its terms-of-service to enforce network neutrality so as to encourage competition and reduce the advantage of large incumbents.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {377–389},
numpages = {13},
keywords = {Internet infrastructure, Internet transit, Network neutrality},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3474085.3475486,
author = {Xu, Jiahua and Li, Jing and Zhou, Xingguang and Zhou, Wei and Wang, Baichao and Chen, Zhibo},
title = {Perceptual Quality Assessment of Internet Videos},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475486},
doi = {10.1145/3474085.3475486},
abstract = {With the fast proliferation of online video sites and social media platforms, user, professionally and occupationally generated content (UGC, PGC, OGC) videos are streamed and explosively shared over the Internet. Consequently, it is urgent to monitor the content quality of these Internet videos to guarantee the user experience. However, most existing modern video quality assessment (VQA) databases only include UGC videos and cannot meet the demands for other kinds of Internet videos with real-world distortions. To this end, we collect 1,072 videos from Youku, a leading Chinese video hosting service platform, to establish the Internet video quality assessment database (Youku-V1K). A special sampling method based on several quality indicators is adopted to maximize the content and distortion diversities within a limited database, and a probabilistic graphical model is applied to recover reliable labels from noisy crowdsourcing annotations. Based on the properties of Internet videos originated from Youku, we propose a spatio-temporal distortion-aware model (STDAM). First, the model works blindly which means the pristine video is unnecessary. Second, the model is familiar with diverse contents by pre-training on the large-scale image quality assessment databases. Third, to measure spatial and temporal distortions, we introduce the graph convolution and attention module to extract and enhance the features of the input video. Besides, we leverage the motion information and integrate the frame-level features into video-level features via a bi-directional long short-term memory network. Experimental results on the self-built database and the public VQA databases demonstrate that our model outperforms the state-of-the-art methods and exhibits promising generalization ability.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1248–1257},
numpages = {10},
keywords = {database, internet videos, model, perceptual quality},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3623652.3623667,
author = {Brun, Lelio and Hasuo, Ichiro and Ono, Yasushi and Sekiyama, Taro},
title = {Automated Security Analysis for Real-World IoT Devices},
year = {2023},
isbn = {9798400716232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623652.3623667},
doi = {10.1145/3623652.3623667},
abstract = {Automatic security protocol analysis is a fruitful research topic that demonstrates the application of formal methods to security analysis. Several endeavors in the last decades successfully verified security properties of large-scale network protocols like TLS, sometimes unveiling unknown vulnerabilities. In this work, we show how to apply these techniques to the domain of IoT, where security is a critical aspect. While most existing security analyses for IoT tackle individually either protocols, firmware or applications, our goal is to treat IoT systems as a whole. We focus our work on a case study, the Armadillo-IoT&nbsp;G4 device, highlighting the specific challenges we must tackle to analyze the security of a typical IoT device. We propose a model using the Tamarin prover, that allows us to state certain key security properties about the device and to prove them automatically.},
booktitle = {Proceedings of the 12th International Workshop on Hardware and Architectural Support for Security and Privacy},
pages = {29–37},
numpages = {9},
keywords = {Internet of Things, cryptographic protocols, formal verification},
location = {<conf-loc>, <city>Toronto</city>, <country>Canada</country>, </conf-loc>},
series = {HASP '23}
}

@inproceedings{10.1145/3384419.3430448,
author = {Kawasaki, Takafumi and Okoshi, Tadashi and Nakazawa, Jin},
title = {A mobility-aware pub/sub architecture for short-lived data in smart cities: poster abstract},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430448},
doi = {10.1145/3384419.3430448},
abstract = {With an increase in the number of IoT devices, the amount of data transferred between the devices and applications is becoming huge. To ensure that these data are properly used, a new IoT data transfer system is needed to better control the timing and the content of the data to transmit. One of the promising means for such a large-scale city-data transfer is the publish/subscribe messaging model, which can separate data senders and receivers so that they can run independently. However, existing pub/sub systems cannot cope well with the mobility of senders and receivers, thereby limiting its applicability to real-world uses. In concrete, they don't consider Time-to-live (TTL) of data. Users can use the data anytime and within TTL of it. IoT platforms can improve controlling data transmission by used to this characteristic. In this paper, we focus on the Time-to-Live of data (data-TTL). Our system can control data transmission by using data-TTL and combine with the user's movement information. We have constructed a system, that is capable of control data transmission for mobility aware.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {645–646},
numpages = {2},
keywords = {data transfer, internet of things, mobility, smart city},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@article{10.1145/3583571.3583577,
author = {Zhang, Junbo and Soltanaghai, Elahe and Balanuta, Artur and Grimsley, Reese and Kumar, Swarun and Rowe, Anthony},
title = {PLatter: On the Feasibility of Building-Scale Power Line Backscatter},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {2375-0529},
url = {https://doi.org/10.1145/3583571.3583577},
doi = {10.1145/3583571.3583577},
abstract = {Can we read ultra-low-power sensors in a large industrial or commercial building with a single reader using the power line system? As the manufacturing industry becomes more and more automated, IoT sensors are also being widely deployed inside industrial buildings. Given the significant cost associated with retrofitting an industrial building, a wired network for IoT installation might not be desirable. On the other hand, long-range wireless networks are either power-hungry (e.g., Wi-Fi or cellular), or support only a low data rate (e.g., LoRa). In this paper, we explore an alternative approach: leveraging the power line infrastructure to enable building-scale wireless backscatter communication.},
journal = {GetMobile: Mobile Comp. and Comm.},
month = {feb},
pages = {19–22},
numpages = {4}
}

@inproceedings{10.1145/3372278.3390722,
author = {Aslam, Asra and Curry, Edward},
title = {Reducing Response Time for Multimedia Event Processing using Domain Adaptation},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3390722},
doi = {10.1145/3372278.3390722},
abstract = {The Internet of Multimedia Things (IoMT) is an emerging concept due to the large amount of multimedia data produced by sensing devices. Existing event-based systems mainly focus on scalar data, and multimedia event-based solutions are domain-specific. Multiple applications may require handling of numerous known/unknown concepts which may belong to the same/different domains with an unbounded vocabulary. Although deep neural network-based techniques are effective for image recognition, the limitation of having to train classifiers for unseen concepts will lead to an increase in the overall response-time for users. Since it is not practical to have all trained classifiers available, it is necessary to address the problem of training of classifiers on demand for unbounded vocabulary. By exploiting transfer learning based techniques, evaluations showed that the proposed framework can answer within ~0.01 min to ~30 min of response-time with accuracy ranges from 95.14% to 98.53%, even when all subscriptions are new/unknown.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {261–265},
numpages = {5},
keywords = {domain adaptation, event-based systems, internet of multimedia things, machine learning, multimedia stream processing, object detection, online training, smart cities, transfer learning},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@inproceedings{10.1145/3555776.3577598,
author = {Fernandez Blanco, David and Le Mouel, Frederic and Lin, Trista and Ponge, Julien},
title = {An Energy-efficient FaaS Edge Computing platform over IoT Nodes: Focus on Consensus Algorithm},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577598},
doi = {10.1145/3555776.3577598},
abstract = {With the increasing computing needs of the new systems and applications, cloud offloading has become a popular choice for constructors to keep the prices of their devices affordable. However, this solution only shifts the scaling problem from the end devices to the cloud, increasingly enhancing the capacities of cloud infrastructures. As a way to reinforce the cloud capabilities on the edge without needing to add extra computing resources, we propose PyCloudIoT, a collaborative energy-efficient Function-as-a-Service (FaaS) computing platform (pltf.) with low-to-medium availability targeting the execution of punctual stateless functions over the already deployed IoTs and gateways. As these resources are extremely dynamic, with intermittent availability, heterogeneity and faultiness, the addition of strong control mechanisms is key to efficient operation. In this paper, we discuss the PyCloudIoT Consensus Model (PCM), which enables the coordination and orchestration of resources dynamically and compensates for the faults of the IoT computing farm. Compared to SOTA, PCM shows promising results with a performance and energy consumption improvement of 20% and 66% and 37% and 65% respectively compared to the best configurations of Raft and Pirogue (4+1 quorum), achieving at the same time a slightly stronger fault tolerance level.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {661–670},
numpages = {10},
keywords = {consensus algorithms, FaaS edge computing, internet of things, distributed computing, resource orchestration},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3460426.3463618,
author = {Li, Jiao and Sun, Jialiang and Xu, Xing and Yu, Wei and Shen, Fumin},
title = {Cross-Modal Image-Recipe Retrieval via Intra- and Inter-Modality Hybrid Fusion},
year = {2021},
isbn = {9781450384636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460426.3463618},
doi = {10.1145/3460426.3463618},
abstract = {In recent years, the Internet has stimulated the explosion of multimedia data. Food-related cooking videos, images, and recipes promote the rapid development of food computing. Image-recipe retrieval is an important sub-task in the field of cross-modal retrieval, which focuses on the measurement of the association between food image and recipe (title, ingredients, instructions). Although the existing methods have proposed some feasible solutions to achieve the goal of Image-recipe retrieval, there are still the following issues: 1) complex model structure and time-consuming training process. 2) the lack of information interaction within modalities and information integration between images and recipes. To this end, we propose a novel lightweight framework namedIntra- and Inter-Modality Hybrid Fusion (IMHF). Our IMHF model abandons a separate deep vision encoder and utilizes the transformer module to unify the visual and text features. In this way, valuable information from images and recipes can be condensed and the direct information interaction between the two modalities can be promoted. Both the intra- and inter-modality fusion can be realized. Extensive experiment results on the large-scale benchmark dataset Recipe1M demonstrate that our model IMHF with a lightweight architecture is superior to the state-of-the-art approaches.},
booktitle = {Proceedings of the 2021 International Conference on Multimedia Retrieval},
pages = {173–182},
numpages = {10},
keywords = {attention mechanism, fusion strategy, image-recipe retrieval, transformer},
location = {Taipei, Taiwan},
series = {ICMR '21}
}

@inproceedings{10.1145/3517745.3561422,
author = {Vermeulen, Kevin and Gurmericliler, Ege and Cunha, Italo and Choffnes, David and Katz-Bassett, Ethan},
title = {Internet scale reverse traceroute},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561422},
doi = {10.1145/3517745.3561422},
abstract = {Knowledge of Internet paths allows operators and researchers to better understand the Internet and troubleshoot problems. Paths are often asymmetric, so measuring just the forward path only gives partial visibility. Despite the existence of Reverse Traceroute, a technique that captures reverse paths (the sequence of routers traversed by traffic from an arbitrary, uncontrolled destination to a given source), this technique did not fulfill the needs of operators and the research community, as it had limited coverage, low throughput, and inconsistent accuracy. In this paper we design, implement and evaluate revtr 2.0, an Internet-scale Reverse Traceroute system that combines novel measurement approaches and studies with a large-scale deployment to improve throughput, accuracy, and coverage, enabling the first exploration of reverse paths at Internet scale. revtr 2.0 can run 15M reverse traceroutes in one day. This scale allows us to open the system to external sources and users, and supports tasks such as traffic engineering and troubleshooting.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {694–715},
numpages = {22},
keywords = {internet measurements, internet scale, reverse traceroute},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3425269.3425277,
author = {Costa, Diego Ivo Campos and Filho, Eduardo Pereira e Silva and Silva, Reginaldo Florencio da and de C. Quaresma Gama, Thiago Dias and Cort\'{e}s, Mariela I.},
title = {Microservice Architecture: A Tertiary Study},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425277},
doi = {10.1145/3425269.3425277},
abstract = {Context. The large-scale use of microservices and their increasing adoption in the industry in recent years has motivated researches on the most diverse aspects related to microservice-based development. However, as it is a relatively new topic, there is still no consolidated body of knowledge in the area. Objective. The present work intends to investigate the current state of research on microservices based on the formulation of six research questions covering fundamental aspects, such as: main interest topics and adopted standards, techniques and tools have been used and application areas. Method. From four digital libraries, 22 secondary studies were selected as a data source, which were analyzed and synthesized in the present study following the proposed research protocol. Results. Among the main topics of interest addressed, we highlight researches related to the applicability of microservice architecture, both by industry and academia. Results indicated that standards focus on challenges related to communication have been the most commonly considered by researchers of the area. Finally, the predominance in the use of the Docker container and the presence of DevOps practices in the automation of operations are noteworthy. Conclusions. The present mapping study points to some directions of research based on the identified gaps, such as modeling and testing of microservice applications, and addressing security aspects. Another promising point to be explored involves the combined use of microservice architecture with other related concepts such as IoT, smart cities, FOG computing and reactive systems, in order to reinforce the use of microservices, as well as creating new solutions and challenges to be researched.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {61–70},
numpages = {10},
keywords = {Academia, Arquitetura de software, Industria, Mapeamento sistem\'{a}tico, Microsservi\c{c}os, Padr\~{o}es de arquitetura},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1145/3449360,
author = {Zeng, Shaoning and Zhang, Bob and Gou, Jianping and Xu, Yong and Huang, Wei},
title = {Fast and Robust Dictionary-based Classification for Image Data},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3449360},
doi = {10.1145/3449360},
abstract = {Dictionary-based classification has been promising in knowledge discovery from image data, due to its good performance and interpretable theoretical system. Dictionary learning effectively supports both small- and large-scale datasets, while its robustness and performance depends on the atoms of the dictionary most of the time. Empirically, using a large number of atoms is helpful to obtain a robust classification, while robustness cannot be ensured when setting a small number of atoms. However, learning a huge dictionary dramatically slows down the speed of classification, which is especially worse on the large-scale datasets. To address the problem, we propose a Fast and Robust Dictionary-based Classification (FRDC) framework, which fully utilizes the learned dictionary for classification by staging - and -norms to obtain a robust sparse representation. The new objective function, on the one hand, introduces an additional -norm term upon the conventional -norm optimization, which generates a more robust classification. On the other hand, the optimization based on both - and -norms is solved in two stages, which is much easier and faster than current solutions. In this way, even when using a limited size of dictionary, which makes sure the classification runs very fast, it still can gain higher robustness for multiple types of image data. The optimization is then theoretically analyzed in a new formulation, close but distinct to elastic-net, to prove it is crucial to improve the performance under the premise of robustness. According to our extensive experiments conducted on four image datasets for face and object classification, FRDC keeps generating a robust classification no matter whether using a small or large number of atoms. This guarantees a fast and robust dictionary-based image classification. Furthermore, when simply using deep features extracted via some popular pre-trained neural networks, it outperforms many state-of-the-art methods on the specific datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {may},
articleno = {97},
numpages = {22},
keywords = {Image classification, regularization, sparse representation, dictionary learning, SVD}
}

@inproceedings{10.1145/3615984.3616503,
author = {Cao, Bryan Bo and Alali, Abrar and Liu, Hansi and Meegan, Nicholas and Gruteser, Marco and Dana, Kristin and Ashok, Ashwin and Jain, Shubham},
title = {ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time Measurements},
year = {2023},
isbn = {9798400703645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615984.3616503},
doi = {10.1145/3615984.3616503},
abstract = {Tracking subjects in videos is one of the most widely used functions in camera-based IoT applications such as security surveillance, smart city traffic safety enhancement, vehicle to pedestrian communication and so on. In computer vision domain, tracking is usually achieved by first detecting subjects, then associating detected bounding boxes across video frames. Typically, frames are transmitted to a remote site for processing, incurring high latency and network costs. To address this, we propose ViFiT, a transformer-based model that reconstructs vision bounding box trajectories from phone data (IMU and Fine Time Measurements). It leverages a transformer's ability of better modeling long-term time series data. ViFiT is evaluated on Vi-Fi Dataset, a large-scale multimodal dataset in 5 diverse real world scenes, including indoor and outdoor environments. Results demonstrate that ViFiT outperforms the state-of-the-art approach for cross-modal reconstruction in LSTM Encoder-Decoder architecture X-Translator and achieves a high frame reduction rate as 97.76% with IMU and Wi-Fi data.},
booktitle = {Proceedings of the 3rd ACM MobiCom Workshop on Integrated Sensing and Communications Systems},
pages = {13–18},
numpages = {6},
keywords = {Efficient Video System, IMU, Multimodal Learning, Multimodal Reconstruction, Object Detection, Tracking, Transformer},
location = {Madrid, Spain},
series = {ISACom '23}
}

@inbook{10.1145/3570361.3614074,
author = {Milani, Stefano and Garlisi, Domenico and Di Fraia, Matteo and Pisani, Patrizio and Chatzigiannakis, Ioannis},
title = {Enabling Edge processing on LoRaWAN architecture},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3614074},
abstract = {LoRaWAN is a wireless technology that enables high-density deployments of IoT devices. Designed for Low Power Wide Area Networks (LPWAN), LoRaWAN employs large cells to service a potentially extremely high number of devices. The technology enforces a centralized architecture, directing all data generated by the devices to a single network server for data processing. End-to-end encryption is used to guarantee the confidentiality and security of data. In this demo, we present Edge2LoRa, a system architecture designed to incorporate edge processing in LoRaWAN without compromising the security and confidentiality of data. Edge2LoRa maintains backward compatibility and addresses scalability issues arising from handling large amounts of data sourced from a diverse range of devices. The demo provides evidence of the advantages in terms of reduced latency, lower network bandwidth requirements, higher scalability, and improved security and privacy resulting from the application of the Edge processing paradigm to LoRaWAN.},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {103},
numpages = {3}
}

@inproceedings{10.1145/3585088.3589377,
author = {Mannila, Linda and Skog, Mia},
title = {"Look at Our Smart Shoe" - a Scalable Online Concept for Introducing Design as Part of Computational Thinking in Grades 1-6},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3589377},
doi = {10.1145/3585088.3589377},
abstract = {While programming is a process covering many stages, many of the tasks K-12 students meet at school are small with little need for, e.g., analysis or design. These earlier phases are, however, important to let children meet open-ended problems, brainstorm solutions and ideate their own creative designs. In this paper we present a model for an online, scalable and scaffolded design workshop for covering such aspects at K-12 level. Through a case study with 1200 students and 60 teachers on IoT and smart things, we describe the workshop and the resulting designs. While the students managed to design their own artifacts, more time had been needed for covering ethical aspects related to technology design. The results suggest creating separate workshops for different grade levels, and also for design and ethical aspects respectively. Moreover, additional resources could support teachers in continuing the discussion with the students after the workshop.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {222–232},
numpages = {11},
keywords = {Case study, Computational thinking, Design, K-12 education, Online workshop, Programming},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@inproceedings{10.1145/3583120.3589567,
author = {Yu, Xiaofan},
title = {PhD Forum Abstract: Intelligence beyond the Edge in IoT},
year = {2023},
isbn = {9798400701184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583120.3589567},
doi = {10.1145/3583120.3589567},
abstract = {Along with the recent advancements of lightweight machine learning and powerful systems and hardware platforms, intelligence beyond the edge has become the next tide of IoT. However, multiple barriers exist from data, algorithm, network and hardware perspectives. In this abstract, I provide an overview of my PhD research which aims at closing the gap towards deploying edge intelligence for large-scale and real-world IoT applications. I further introduce our recent contributions and the work planned ahead.},
booktitle = {Proceedings of the 22nd International Conference on Information Processing in Sensor Networks},
pages = {344–345},
numpages = {2},
keywords = {Edge Computing, IoT, On-Device Learning, Sensor Networks},
location = {San Antonio, TX, USA},
series = {IPSN '23}
}

@article{10.1145/3490391,
author = {Marinelli, Tommaso and P\'{e}rez, Jos\'{e} Ignacio G\'{o}mez and Tenllado, Christian and Komalan, Manu and Gupta, Mohit and Catthoor, Francky},
title = {Microarchitectural Exploration of STT-MRAM Last-level Cache Parameters for Energy-efficient Devices},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3490391},
doi = {10.1145/3490391},
abstract = {As the technology scaling advances, limitations of traditional memories in terms of density and energy become more evident. Modern caches occupy a large part of a CPU physical size and high static leakage poses a limit to the overall efficiency of the systems, including IoT/edge devices. Several alternatives to CMOS SRAM memories have been studied during the past few decades, some of which already represent a viable replacement for different levels of the cache hierarchy. One of the most promising technologies is the spin-transfer torque magnetic RAM (STT-MRAM), due to its small basic cell design, almost absent static current and non-volatility as an added value. However, nothing comes for free, and designers will have to deal with other limitations, such as the higher latencies and dynamic energy consumption for write operations compared to reads. The goal of this work is to explore several microarchitectural parameters that may overcome some of those drawbacks when using STT-MRAM as last-level cache (LLC) in embedded devices. Such parameters include: number of cache banks, number of miss status handling registers (MSHRs) and write buffer entries, presence of hardware prefetchers. We show that an effective tuning of those parameters may virtually remove any performance loss while saving more than 60% of the LLC energy on average. The analysis is then extended comparing the energy results from calibrated technology models with data obtained with freely available tools, highlighting the importance of using accurate models for architectural exploration.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jan},
articleno = {3},
numpages = {20},
keywords = {Energy efficiency, gem5, last-level cache, nvsim, spec cpu2017, stt-mram}
}

@inproceedings{10.1145/3404835.3462878,
author = {Li, Houyi and Chen, Zhihong and Li, Chenliang and Xiao, Rong and Deng, Hongbo and Zhang, Peng and Liu, Yongchao and Tang, Haihong},
title = {Path-based Deep Network for Candidate Item Matching in Recommenders},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462878},
doi = {10.1145/3404835.3462878},
abstract = {The large-scale recommender system mainly consists of two stages: matching and ranking. The matching stage (also known as the retrieval step) identifies a small fraction of relevant items from billion-scale item corpus in low latency and computational cost. Item-to-item collaborative filtering (item-based CF) and embedding-based retrieval (EBR) have been long used in the industrial matching stage owing to its efficiency. However, item-based CF is hard to meet personalization, while EBR has difficulty in satisfying diversity. In this paper, we propose a novel matching architecture, Path-based Deep Network (named PDN), through incorporating both personalization and diversity to enhance matching performance. Specifically, PDN is comprised of two modules: Trigger Net and Similarity Net. PDN utilizes Trigger Net to capture the user's interest in each of his/her interacted item. Similarity Net is devised to evaluate the similarity between each interacted item and the target item based on these items' profile and CF information. The final relevance between the user and the target item is calculated by explicitly considering user's diverse interests, ie aggregating the relevance weights of the related two-hop paths (one hop of a path corresponds to user-item interaction and the other to item-item relevance). Furthermore, we describe the architecture design of the proposed PDN in a leading real-world E-Commerce service (Mobile Taobao App). Based on offline evaluations and online A/B test, we show that PDN outperforms the existing solutions for the same task. The online results also demonstrate that PDN can retrieve more personalized and more diverse items to significantly improve user engagement. Currently, PDN system has been successfully deployed at Mobile Taobao App and handling major online traffic.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1493–1502},
numpages = {10},
keywords = {deep learning, recommendation systems},
location = {<conf-loc>, <city>Virtual Event</city>, <country>Canada</country>, </conf-loc>},
series = {SIGIR '21}
}

@inproceedings{10.1145/3576915.3624373,
author = {Yu, Jihyeon and Kim, Juhwan and Yun, Yeohoon and Yun, Joobeom},
title = {Poster: Combining Fuzzing with Concolic Execution for IoT Firmware Testing},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3624373},
doi = {10.1145/3576915.3624373},
abstract = {The supply of IoT devices is increasing year by year. Even in industries that demand sophistication, such as unmanned driving, construction, and robotics industry, IoT devices are being utilized. However, the security of IoT devices is lagging behind this development due to their diverse types and challenging firmware execution environments. The existing methods, such as direct device connectivity or partial emulation, are used to solve this. However, full system emulation is better suited for the large-scale analysis, because it can test many firmwares without requiring devices. Therefore, recent studies have integrated emulation and software testing techniques such as fuzzing, but they are still unsuitable for testing various firmware and inefficient. In this poster, we propose FirmColic, which combines fuzzing with concolic execution to mitigate these limitations. FirmColic is a type of augmented process emulation, which improves the effectiveness of fuzzing using keyword extraction based on concolic execution. Also, we apply five arbitration techniques in an augmented process emulation environment for the high success rates of the emulation. We prove that FirmColic has faster detection, more crash detection, and a higher code coverage than the previous studies.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3564–3566},
numpages = {3},
keywords = {firmware, fuzzing, iot devices, keyword extraction},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@inproceedings{10.1145/3581791.3596861,
author = {Garg, Nakul and Roy, Nirupam},
title = {Sirius: A Self-Localization System for Resource-Constrained IoT Sensors},
year = {2023},
isbn = {9798400701108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581791.3596861},
doi = {10.1145/3581791.3596861},
abstract = {Low-power sensor networks are transforming large-scale sensing in precision farming, livestock tracking, climate-monitoring and surveying. Accurate and robust localization in such low-power sensor nodes has never been as crucial as it is today. This paper presents, Sirius, a self-localization system using a single receiver for low-power IoT nodes. Traditionally, systems have relied on antenna arrays and tight synchronization to estimate angle-of-arrival (AoA) and time-of-flight with known access points. While these techniques work well for regular mobile systems, low-power IoT nodes lack the resources to support these complex systems. Sirius explores the use of gain-pattern reconfigurable antennas with passive envelope detector-based radios to perform AoA estimation without requiring any kind of synchronization. It shows a technique to embed direction specific codes to the received signals which are transparent to regular communication channel but carry AoA information with them. Sirius embeds these direction-specific codes by using reconfigurable antennas and fluctuating the gain pattern of the antenna. Our prototype demonstrates a median error of 7 degrees in AoA estimation and 2.5 meters in localization, which is similar to state-of-the-art antenna array-based systems. Sirius opens up new possibilities for low-power IoT nodes.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services},
pages = {289–302},
numpages = {14},
keywords = {low-power sensing, IoT, ultra-low-power localization, embedded AI, low-power antenna},
location = {Helsinki, Finland},
series = {MobiSys '23}
}

@inproceedings{10.1145/3565287.3617610,
author = {Pongr\'{a}cz, Gergely and Mih\'{a}ly, Attila and G\'{o}dor, Istv\'{a}n and Laki, S\'{a}ndor and Nanos, Anastasios and Papagianni, Chrysa},
title = {Towards extreme network KPIs with programmability in 6G},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565287.3617610},
doi = {10.1145/3565287.3617610},
abstract = {6G's superpower must be simplicity, which should not be viewed as a constraint, but rather as the organic outcome of using the most advanced technologies at our disposal. Programmability in the data plane, cloud-native features like automatic scaling and failover, transparent acceleration of both network functions and applications and AI-driven optimizations are already present. We only need to integrate these different innovations into a consistent architecture and offer a simple yet powerful solution for the very different applications that would use future mobile networks. The application space is getting more and more heterogeneous, e.g., legacy Internet-based services still using the good old TCP protocol, future media services relying on multipath transport - always utilizing the best available connection, or control applications of robots or drones requiring extreme low and stable latency. The different applications will require very different Key Performance Indicators (KPIs) from the network. In this paper, we present a novel architecture called DESIRE6G (D6G) architecture that aims to fulfill these requirements by integrating the key technological innovations mentioned above. Besides supporting the diverse KPIs of future applications, the novel architecture should also simplify the mobile network itself by promoting modularity and service-based network function selection which can replace traditional control plane centric solutions, e.g., for handover.},
booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {340–345},
numpages = {6},
keywords = {programmable networks, 6G, manageability, efficiency, resiliency, extreme network KPIs},
location = {Washington, DC, USA},
series = {MobiHoc '23}
}

@article{10.1145/3376919,
author = {Pal, Amitangshu and Kant, Krishna},
title = {Exploiting Proxy Sensing for Efficient Monitoring of Large-Scale Sensor Networks},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3376919},
doi = {10.1145/3376919},
abstract = {Large networks of IoT devices, each consisting of one or more sensors, are being increasingly deployed for comprehensive real-time monitoring of cyber-physical systems. Such networks form an essential component of the emerging edge computing paradigm and are expected to increase in complexity and size. The physical phenomenon sensed by different sensors (within the same or different IoT devices in close proximity) often have relationships that makes them correlated. This is a form of proxy sensing that can be exploited for achieving better energy efficiency and higher robustness in monitoring. In this article, we explore how a set of sensors can optimize its data collection rates efficiently in a semi-distributed manner and yet provide the advantages of autonomy, relative isolation, and distributed control that is essential in a large-scale network.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {14},
numpages = {31},
keywords = {Edge computing, adaptive sampling, heterogeneous sensing, proxy sensing, rate adaptation}
}

@inproceedings{10.5555/3408352.3408670,
author = {Cheng, Yuan and Huang, Guangtai and Zhen, Peining and Liu, Bin and Chen, Hai-Bao and Wong, Ngai and Yu, Hao},
title = {An anomaly comprehension neural network for surveillance videos on terminal devices},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Anomaly comprehension in surveillance videos is more challenging than detection. This work introduces the design of a lightweight and fast anomaly comprehension neural network. For comprehension, a spatio-temporal LSTM model is developed based on the structured, tensorized time-series features extracted from surveillance videos. Deep compression of network size is achieved by tensorization and quantization for the implementation on terminal devices. Experiments on large-scale video anomaly dataset UCF-Crime demonstrate that the proposed network can achieve an impressive inference speed of 266 FPS on a GTX-1080Ti GPU, which is 4.29 faster than ConvLSTM-based method; a 3.34% AUC improvement with 5.55% accuracy niche versus the 3D-CNN based approach; and at least 15k\texttimes{} parameter reduction and 228\texttimes{} storage compression over the RNN-based approaches. Moreover, the proposed framework has been realized on an ARM-core based IOT board with only 2.4W power consumption.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1396–1401},
numpages = {6},
keywords = {AI on IOT, anomaly comprehension, surveillance videos},
location = {<conf-loc>, <city>Grenoble</city>, <country>France</country>, </conf-loc>},
series = {DATE '20}
}

@inproceedings{10.1145/3460418.3480402,
author = {Li, Guodong and Wu, Zhiyuan and Liu, Ning and Liu, Xinyu and Wang, Yue and Zhang, Lin},
title = {Blind Calibration by Maximizing Correlation},
year = {2021},
isbn = {9781450384612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460418.3480402},
doi = {10.1145/3460418.3480402},
abstract = {In large-scale IoT systems, blind calibration problem becomes increasingly prominent for sensor calibration without ground truth reference. Most of the existing blind calibration methods adopt either a handcrafted spatio-temporal model or a specific drift mechanism assumption. However, these assumptions may be over-simplified or introduce inappropriate bias, and therefore lead to great performance degradation in the real-world deployment. In this paper, we present a novel generative framework for blind calibration problems without specific data correlation or drift model assumption. We extract the most informative feature that maximizes correlation between reference data and target data using soft-HGR maximal correlation regression. Therefore, our method can be used in different blind calibration tasks especially where data correlation or drift model is unknown or deviated. Besides, our method can be conveniently augmented with a reliable drift model to further improve performance on specific tasks. We conduct comprehensive evaluations over a three-month real-world air pollution sensing dataset collected in Foshan, China. Results show our method can obtain the best performance compared to previous blind calibration methods in the absence of accurate drift model knowledge.},
booktitle = {Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers},
pages = {637–642},
numpages = {6},
keywords = {Air Pollution, Blind Calibration, Maximal Correlation},
location = {Virtual, USA},
series = {UbiComp/ISWC '21 Adjunct}
}

@inproceedings{10.5555/3400306.3400329,
author = {Ma, Xiaoyuan and Li, Dan and Yang, Fengxu and Boano, Carlo Alberto and Tian, Pei and Wei, Jianming},
title = {Poster: Chirpbox - A Low-Cost LoRa Testbed Solution},
year = {2020},
isbn = {9780994988645},
publisher = {Junction Publishing},
address = {USA},
abstract = {Low-Power Wide Area Networks (LP-WANs) are becoming a key enabler of many practical IoT applications. Although LoRa is one of the most representative LP-WAN technologies, only a few large-scale LoRa testbeds are publicly open to network researchers, which limits the ability to test the real-world performance of LoRa-based solutions. To enable a large-scale testing on real hardware, in this poster we present Chirpbox, a low-cost LoRa testbed solution. Chirpbox aims to evaluate the different LoRa protocols rather than to debug a system in detail. By only using the LoRa radio transceiver on each testbed node, Chirpbox is able to: (i) send back the evaluation results, (ii) schedule (i.e., start and stop) experiments, as well as (iii) disseminate a firmware under test without the need of additional infrastructure, such as local area networks. Each node in Chirpbox can get synchronized with a GPS module and is able to compute time-related statistics such as end-to-end latency on its own. Moreover, in order to easily deploy a LoRa network over a large area, a battery set is integrated in each Chirpbox node, as there is no need for energy-hungry observers.},
booktitle = {Proceedings of the 2020 International Conference on Embedded Wireless Systems and Networks},
pages = {166–167},
numpages = {2},
location = {Lyon, France},
series = {EWSN '20}
}

@inproceedings{10.1145/3340531.3412705,
author = {Vasudevan, Sriram and Kenthapadi, Krishnaram},
title = {LiFT: A Scalable Framework for Measuring Fairness in ML Applications},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412705},
doi = {10.1145/3340531.3412705},
abstract = {Many internet applications are powered by machine learned models, which are usually trained on labeled datasets obtained through user feedback signals or human judgments. Since societal biases may be present in the generation of such datasets, it is possible for the trained models to be biased, thereby resulting in potential discrimination and harms for disadvantaged groups. Motivated by the need to understand and address algorithmic bias in web-scale ML systems and the limitations of existing fairness toolkits, we present the LinkedIn Fairness Toolkit (LiFT), a framework for scalable computation of fairness metrics as part of large ML systems. We highlight the key requirements in deployed settings, and present the design of our fairness measurement system. We discuss the challenges encountered in incorporating fairness tools in practice and the lessons learned during deployment at LinkedIn. Finally, we provide open problems based on practical experience.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2773–2780},
numpages = {8},
keywords = {distributed computation, fairness-aware machine learning, linkedin fairness toolkit, scalable framework},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3486001.3486237,
author = {Gandhi, Himanshu and Mehra, Misha and Ribeiro, Vinay},
title = {BOND: Efficient and Frugal DL Model Co-design for Botnet detection on IoT Gateways},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486237},
doi = {10.1145/3486001.3486237},
abstract = {A botnet is a network of devices infected by the same malware, acting as a single entity and controlled by a botmaster. They are the biggest cybersecurity threat to carry out large-scale attacks from spamming, ransomware, data exfiltration, and denial-of-service attacks. Lightweight IoT devices without traditional security mechanisms have become favorite victims and agents to carry out botnet attacks. In our work, we seek to detect botnet-infected IoT nodes. This paper presents BOND, a frugal Deep Learning analysis of network traffic for detecting IoT devices infected with botnet(s), correctly classifying Zero-Day attacks and newer benign traffic. BOND is designed considering the constraints of IoT gateways and betters the F1 score of standard benchmark ML algorithms and State-of-The-Art method - Kitsune, by at least 10%, with under 1 millisecond inference time and less than 150 KB of model memory. This paper also presents labeled data-set 27-Botnet spanning 27 IoT botnet families and ten different IoT devices. We believe, it is the first data set with a separate zero-day component.},
booktitle = {Proceedings of the First International Conference on AI-ML Systems},
articleno = {17},
numpages = {7},
keywords = {Botnet Detection, Datasets, Internet of Things, Neural Networks},
location = {Bangalore, India},
series = {AIMLSystems '21}
}

@inproceedings{10.1145/3459637.3482026,
author = {Nakov, Preslav and Da San Martino, Giovanni},
title = {Fake News, Disinformation, Propaganda, and Media Bias},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482026},
doi = {10.1145/3459637.3482026},
abstract = {The rise of Internet and social media changed not only how we consume information, but it also democratized the process of content creation and dissemination, thus making it easily available to anybody. Despite the hugely positive impact, this situation has the downside that the public was left unprotected against biased, deceptive, and disinformative content, which could now travel online at breaking-news speed and allegedly influence major events such as political elections, or disturb the efforts of governments and health officials to fight the ongoing COVID-19 pandemic. The research community responded to the issue, proposing a number of inter-connected research directions such as fact-checking, disinformation, misinformation, fake news, propaganda, and media bias detection. Below, we cover the mainstream research, and we also pay attention to less popular, but emerging research directions, such as propaganda detection, check-worthiness estimation, detecting previously fact-checked claims, and multimodality, which are of interest to human fact-checkers and journalists. We further cover relevant topics such as stance detection, source reliability estimation, detection of persuasion techniques in text and memes, and detecting malicious users in social media. Moreover, we discuss large-scale pre-trained language models, and the challenges and opportunities they offer for generating and for defending against neural fake news. Finally, we explore some recent efforts aiming at flattening the curve of the COVID-19 infodemic.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4862–4865},
numpages = {4},
keywords = {disinformation, fact-checking, factuality, fake news, media bias, misinformation, propaganda, veracity},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1145/3549543,
author = {Sangar, Yaman and Biradavolu, Yoganand and Krishnaswamy, Bhuvana},
title = {A Novel Time-Interval Based Modulation for Large-Scale, Low-Power, Wide-Area-Networks},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3549543},
doi = {10.1145/3549543},
abstract = {Wireless communication over long distances has become the bottleneck for battery-powered, large-scale deployments. Low-power protocols like Zigbee and Bluetooth Low Energy have limited communication range, whereas long-range communication strategies like cellular and satellite networks are power-hungry. Technologies that use narrow-band communication like LoRa, SigFox, and NB-IoT have low spectral efficiency, leading to scalability issues. The goal of this work is to develop a communication framework that is energy efficient, long-range, and scalable. We propose, design, and prototype WiChronos, a communication paradigm that encodes information in the time interval between two narrowband symbols to drastically reduce the energy consumption in a wide area network with large number of senders. We leverage the low data-rate and relaxed latency requirements of such applications to achieve the desired features identified above. We design and implement chirp spread spectrum transmitter and receiver using off-the-shelf components to send the narrowband symbols. Based on our prototype, WiChronos achieves an impressive 60% improvement in battery life compared to state-of-the-art LPWAN technologies in transmission of payloads less than 10 bytes at experimentally verified distances of over 4 km. We also show that more than 1,000 WiChronos senders can co-exist with less than 5% collision probability under low traffic conditions.},
journal = {ACM Trans. Sen. Netw.},
month = {nov},
articleno = {68},
numpages = {30},
keywords = {Low-power wireless, timing interval modulation, scalability}
}

@inproceedings{10.1145/3438872.3439085,
author = {Wu, Yi and Song, Yan and Yang, Hongshan},
title = {Intelligent Distributed Web Crawler Based on Attention Mechanism},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439085},
doi = {10.1145/3438872.3439085},
abstract = {With the rapid development of the Internet, webpages' content has become the central platform for people to publish and retrieve information. Recently, web crawlers could quickly and accurately find the information users need from the massive network information resources. There have been many different types of web crawlers in the literature, developed for data retrieval. However, most of the existing web crawlers have significant limitations. For example, they focus on the effective overall architecture instead of paying attention to the actual data's complexity. Moreover, the advertising links in the news and the public platform's promotional content have become ubiquitous noise. The existing web crawler collection strategy lacks sufficient identification of advertising information. The degree of automation to detect advertisements is low, so it isn't easy to form a complete and deployable large-scale distributed data crawling system. Therefore, the research and improvement of distributed web crawlers that intelligently distinguish advertisements is a work of practical significance. The distributed intelligent web crawler system designed and implemented in this paper solves low manual crawler efficiency and poor data quality. The crawler system can effectively identify and eliminate advertising information and significantly improve the automatically extracted data in the distributed crawler system from the experimental results.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {229–233},
numpages = {5},
keywords = {Artificial Intelligence, Deep Learning, Distributed Framework, Intelligent Web Crawler},
location = {Shanghai, China},
series = {RICAI '20}
}

@inproceedings{10.1145/3543507.3583417,
author = {Tian, Lin and Zhang, Xiuzhen and Lau, Jey Han},
title = {MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583417},
doi = {10.1145/3543507.3583417},
abstract = {State-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. Existing troll detection models are developed based on training data for known campaigns (e.g. the influence campaign by Russia’s Internet Research Agency on the 2016 US Election), and they fall short when dealing with novel campaigns with new targets. We propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce campaign-specific transformer adapters to MetaTroll to “memorise” campaign-specific knowledge so as to tackle catastrophic forgetting, where a model “forgets” how to detect trolls from older campaigns due to continual adaptation. Our experiments demonstrate that MetaTroll substantially outperforms baselines and state-of-the-art few-shot text classification models. Lastly, we explore simple approaches to extend MetaTroll to multilingual and multimodal detection. Source code for MetaTroll is available at: https://github.com/ltian678/metatroll-code.git},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1743–1753},
numpages = {11},
keywords = {adapter, continual learning, few-shot learning, multilingual, multimodal, troll detection},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.1145/3546931,
author = {Kleppmann, Martin and Alvaro, Peter},
title = {Convergence: Research for Practice reboot},
year = {2022},
issue_date = {May/June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/3546931},
doi = {10.1145/3546931},
abstract = {It is with great pride and no small amount of excitement that I announce the reboot of acmqueue's Research for Practice column. For three years, beginning at its inception in 2016, Research for Practice brought both seminal and cutting-edge research - via careful curation by experts in academia - within easy reach for practitioners who are too busy building things to manage the deluge of scholarly publications. We believe the series succeeded in its stated goal of sharing "the joy and utility of reading computer science research" between academics and their counterparts in industry. We know our readers have missed it, and we are delighted to rekindle the flame after a three-year hiatus. For this first installment, we invited Dr. Martin Kleppmann, research fellow and affiliated lecturer at the University of Cambridge, to curate a selection of recent research papers in a perennially interesting domain: convergent or "eventual consistent" replicated systems. His expert analysis circles the topic, viewing it through the lens of recent work in four distinct research domains: systems, programming languages, human-computer interaction, and data management. Along the way, readers will be exposed to a variety of data structures, algorithms, proof techniques, and programming models (each described in terms of a distinct formalism), all of which attempt to make programming large-scale distributed systems easier. I hope you enjoy his column as much as I did.},
journal = {Queue},
month = {jul},
pages = {88–95},
numpages = {8}
}

@inproceedings{10.1145/3377713.3377756,
author = {Plikynas, Darius and Miliauskas, Arunas and Lau\v{z}ikas, Rimvydas},
title = {Simulation of Social Capital Dynamics: Impact of Cultural Events},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377756},
doi = {10.1145/3377713.3377756},
abstract = {In the modern multicultural, globalizing, and simultaneously locally radicalizing world, evident limitations of understanding the cultural impact on even basic social capital processes open up a complex and challenging research frontier. Cultural policy experiments are time-consuming, too costly, and risky. Thus, there is a clear need to have feasible simulation models, which could help to foresee some consequences of social and cultural policies. However, there is a lack of available metrics, conceptual approaches, not to mention simulation models. In this paper, we present a scaled-down simulation of stylized cultural events' impact on social capital dynamics using NetLogo agent-based simulation model. The presented abstract model of social capital dynamics is based on well-known principles of Axelrod's model for culture dissemination, which we expanded for the agents' interaction not only in the physical space but also in the cultural features' space as well. Due to the added extensions, the presented simulation model is capable of simulating the impact of broadcasted cultural events. It is aimed to model not only neighborhood (pair-to-pair) interactions between simulated agents but also larger-scale social networking and mass media impact. Simulation results, among other things, reveal a plausible mechanism of culture impact to social capital, and cultural conditions for the emergence of various social cohesion states like globalization, polarization, or radicalization.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {315–319},
numpages = {5},
keywords = {Agent-based simulation, complex social system, cultural events, social capital},
location = {Sanya, China},
series = {ACAI '19}
}

@inproceedings{10.1145/3372224.3380898,
author = {Sangar, Yaman and Krishnaswamy, Bhuvana},
title = {WiChronos: energy-efficient modulation for long-range, large-scale wireless networks},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3380898},
doi = {10.1145/3372224.3380898},
abstract = {Wireless communication over long distances has become the bottleneck for battery-powered, large-scale deployments. Currently used low-power protocols such as Zigbee and Bluetooth Low Energy have limited communication range, whereas long-range communication strategies used in cellular and satellite networks are heavy on energy consumption. Methods that use narrow-band communication such as LoRa, SigFox, and NB-IoT have low spectral efficiency, leading to scalability issues. The goal of this work is to develop a communication framework that can satisfy the following requirements: (1) Increased battery life, (2) Longer communication range, (3) Scalability in a wireless network. In this work, we propose, design, and prototype WiChronos, a communication paradigm that encodes information in the time interval between two narrowband symbols in order to drastically reduce the energy consumption in a wide area network with a large number of senders. We leverage the low data-rate and relaxed latency requirements of such applications to achieve the desired features identified above. Based on our prototype using off-the-shelf components, WiChronos achieves an impressive 60% improvement in battery life compared to state-of-the-art LPWAN technologies at distances of over 800 meters. We also show that more than 1000 WiChronos senders can co-exist with less than 5% probability of collisions under low traffic conditions.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {21},
numpages = {14},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@inproceedings{10.1145/3491418.3530759,
author = {Boubin, Jayson and Banerjee, Avishek and Yun, Jihoon and Qi, Haiyang and Fang, Yuting and Chang, Steve and Srinivasan, Kannan and Ramnath, Rajiv and Arora, Anish},
title = {PROWESS: An Open Testbed for Programmable Wireless Edge Systems},
year = {2022},
isbn = {9781450391610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491418.3530759},
doi = {10.1145/3491418.3530759},
abstract = {Edge computing is a growing paradigm where compute resources are provisioned between data sources and the cloud to decrease compute latency from data transfer, lower costs, comply with security policies, and more. Edge systems are as varied as their applications, serving internet services, IoT, and emerging technologies. Due to the tight constraints experienced by many edge systems, research computing testbeds have become valuable tools for edge research and application benchmarking. Current testbed infrastructure, however, fails to properly emulate many important edge contexts leading to inaccurate benchmarking. Institutions with broad interests in edge computing can build testbeds, but prior work suggests that edge testbeds are often application or sensor specific. A general edge testbed should include access to many of the sensors, software, and accelerators on which edge systems rely, while slicing those resources to fit user-defined resource footprints. PROWESS is an edge testbed that answers this challenge. PROWESS provides access across an institution to sensors, compute resources, and software for testing constrained edge applications. PROWESS runs edge workloads as sets of containers with access to sensors and specialized hardware on an expandable cluster of light-weight edge nodes which leverage institutional networks to decrease implementation cost and provide wide access to sensors. We implemented a multi-node PROWESS deployment connected to sensors across Ohio State University’s campus. Using three edge-native applications, we demonstrate that PROWESS is simple to configure, has a small resource footprint, scales gracefully, and minimally impacts institutional networks. We also show that PROWESS closely approximates native execution of edge workloads and facilitates experiments that other systems testbeds can not.},
booktitle = {Practice and Experience in Advanced Research Computing},
articleno = {11},
numpages = {9},
location = {Boston, MA, USA},
series = {PEARC '22}
}

@inproceedings{10.1145/3418653.3418671,
author = {Yu, Kangning},
title = {The Impact of Internet Finance on Commercial Banks' Personal Wealth Management},
year = {2020},
isbn = {9781450387972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418653.3418671},
doi = {10.1145/3418653.3418671},
abstract = {With the development of China's economy and society since the 21st century, the residents&nbsp;'&nbsp;disposable income has increased dramatically, the demand for wealth preservation and appreciation has been constantly increasing, the scale of personal financial services of commercial banks has expanded, and the number of different types of personal financial products has increased. However, with the rapid development of this business, it also shows the shortcomings of high purchase threshold, high transaction cost and complex purchase procedures. The demand of small customers&nbsp;can not&nbsp;be effectively met, the development and popularization of Internet technology, in this context, Internet finance emerged at the historic moment. This paper combines qualitative and quantitative methods, based on the study of relevant literature, analyses the impact of Internet Finance on personal finance business of commercial banks, and puts forward relevant suggestions and opinions according to the analysis results. Firstly, this paper discusses the personal financial services&nbsp;'&nbsp;development process, product types and characteristics, and theoretically analyses the impact of internal factors such as total assets scale and return on net assets on personal financial services of commercial&nbsp;banks. Then&nbsp;it analyses the Internet finance's definition, development status, competitive advantage and disadvantage, and the impact of various elements of Internet finance, such as the size of Internet financial users, the scale of third-party Internet payment, the Internet financial index and the annual return rate of Internet financial products. Then, 16 listed banks are selected as the research object, and the impact of each factor on the sales scale of individual financial business of commercial banks is empirically analyzed by using econometric regression model. The analysis results show that the scale of third -party Internet payment, the scale&nbsp;of Internet users and the increase of Internet financial index have optimistic influence on the personal financial services of commercial banks. The expansion of the scale of internal total assets and the decrease of the return on net assets will also have the positive impact on the personal financial services of banks, and the annual return on Internet financial products represented by Yu&nbsp;Ebao&nbsp;has formed a certain pressure on the commercial banks&nbsp;'&nbsp;business, but at the same time, in the fierce market competition, it has brought opportunities for the transformation of commercial banks' personal financial services. Finally, based on the above analysis results, this paper puts forward some suggestions for the development&nbsp;of commercial&nbsp;banks&nbsp;'&nbsp;personal financial services.},
booktitle = {2020 The 4th International Conference on Business and Information Management},
pages = {63–70},
numpages = {8},
keywords = {Commercial Bank, Internet Finance, Personal Finance Business},
location = {Rome, Italy},
series = {ICBIM 2020}
}

@inproceedings{10.1145/3565472.3595630,
author = {Van\v{c}ura, Vojt\v{e}ch},
title = {Scalable and Explainable Linear Shallow Autoencoders for Collaborative Filtering from Industrial Perspective},
year = {2023},
isbn = {9781450399326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565472.3595630},
doi = {10.1145/3565472.3595630},
abstract = {The popularity of linear shallow autoencoders for collaborative filtering is growing in the research community, and internet industry providers of Recommender Systems are also taking notice. However, despite their simplicity and accuracy, these models often cannot be used in real-world industrial recommender systems due to their inability to scale to very large interaction matrices. Our research aims to address this issue by developing a scalable, explainable, and accurate shallow linear autoencoder method for collaborative filtering that meets the demands of real-world recommenders. In this paper, we present our industrial Ph.D. research project, which includes: (1) the development of a scalable method called ELSA and the adaptation of the method to a large real-world recommender and (2) the creation of a framework to visualize the recommender systems insights based on modeling the distribution of retrieval metrics in latent user space. We discuss the current status of our project, the key steps to finish the project, and the possible future extensions after the dissertation.},
booktitle = {Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
pages = {290–295},
numpages = {6},
keywords = {Distribution analysis, Implicit feedback recommendation, Linear models, Recommender systems, Shallow autoencoders, User simulation},
location = {Limassol, Cyprus},
series = {UMAP '23}
}

@inproceedings{10.1145/3443467.3443829,
author = {Zhou, Chaoran and Zhao, Jianping and Ni, Min},
title = {An Internet Resource Extracting Relation Model based on Distant Supervision and Deep Learning},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443829},
doi = {10.1145/3443467.3443829},
abstract = {Relational extraction of large-scale text data is one of the key issues in natural language processing. Facing large-scale Internet data resources, this paper proposes a relationship extraction model based on distant supervision and deep learning. The model build entity-to-package based on distant supervision to solve the high computational complexity and time cost of Internet large-scale data. Model's input embeddings based on BERT and entity position, which can improve the text feature representation ability. And the model utilized attention mechanism to optimize the entity-to-package tag attention parameters to alleviate the wrong label problems arising from distant supervision learning. In experiments, the performance of the proposed model exceeds that of other baseline models. The results demonstrate the positive contribution of our model in Internet large-scale relation extraction.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {647–652},
numpages = {6},
keywords = {Attention mechanism, Deep learning, Distant supervision, Large scale data, Relation extraction, Wrong label processing},
location = {Xiamen, China},
series = {EITCE '20}
}

@inproceedings{10.1145/3603269.3604852,
author = {Miao, Congcong and Xiao, Yunming and Canini, Marco and Dai, Ruiqiang and Zheng, Shengli and Wang, Jilong and Bu, Jiwu and Kuzmanovic, Aleksandar and Wang, Yachen},
title = {TENSOR: Lightweight BGP Non-Stop Routing},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3604852},
doi = {10.1145/3603269.3604852},
abstract = {As the solitary inter-domain protocol, BGP plays an important role in today's Internet. Its failures threaten network stability and will usually result in large-scale packet losses. Thus, the non-stop routing (NSR) capability that protects inter-domain connectivity from being disrupted by various failures, is critical to any Autonomous System (AS) operator. Replicating the BGP and underlying TCP connection status is key to realizing NSR. But existing NSR solutions, which heavily rely on OS kernel modifications, have become impractical due to providers' adoption of virtualized network gateways for better scalability and manageability.In this paper, we tackle this problem by proposing TENSOR, which incorporates a novel kernel-modification-free replication design and lightweight architecture. More concretely, the kernel-modification-free replication design mitigates the reliance on OS kernel modification and hence allows the virtualization of the network gateway. Meanwhile, lightweight virtualization provides strong performance guarantees and improves system reliability. Moreover, TENSOR provides a solution to the split-brain problem that affects NSR solutions. Through extensive experiments, we show that TENSOR realizes NSR while bearing little overhead compared to open-source BGP implementations. Further, our two-year operational experience on a fleet of 400 servers controlling over 31,000 BGP peering connections demonstrates that TENSOR reduces the development, deployment, and maintenance costs significantly - at least by factors of 20, 5, and 10, respectively, while retaining the same SLA with the NSR-enabled routers.},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {108–121},
numpages = {14},
keywords = {border gateway protocol, fault tolerance, lightweight virtualization},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@inproceedings{10.1145/3575879.3575961,
author = {Kalogeras, Georgios and Tsakanikas, Vassilios and Ballas, Ioannis and Aggelopoulos, Vassilios and Tampakas, Vassilios},
title = {Community Detection at scale: A comparison study among Apache Spark and Neo4j},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575961},
doi = {10.1145/3575879.3575961},
abstract = {The proliferation of data generation devices, including IoT and edge computing has led to the big data paradigm, which has considerably placed pressure on well-established relational databases during the last decade. Researchers have proposed several alternative database models in order to model the captured data more efficiently. Among these approaches, graph databases seem the most promising candidate to supplement relational schemes. Within this study, a comparison is performed among Neo4j, one of the leading graph databases, and Apache Spark, a unified engine for distributed large-scale data processing environment, in terms of processing limits. More specifically, the two frameworks are compared on their capacity to execute community detection algorithms.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {21–26},
numpages = {6},
keywords = {Apache Spark, Label Propagation algorithm, Neo4j, graph database},
location = {<conf-loc>, <city>Athens</city>, <country>Greece</country>, </conf-loc>},
series = {PCI '22}
}

@inproceedings{10.1145/3511808.3557148,
author = {Zhou, Jun and Qi, Feng and Hua, Zhigang and Jian, Daohong and Liu, Ziqi and Wu, Hua},
title = {A Practical Distributed ADMM Solver for Billion-Scale Generalized Assignment Problems},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557148},
doi = {10.1145/3511808.3557148},
abstract = {Assigning items to owners is a common problem found in various real-world applications, for example, audience-channel matching in marketing campaigns, borrower-lender matching in loan management, and shopper-merchant matching in e-commerce. Given an objective and multiple constraints, an assignment problem can be formulated as a constrained optimization problem. Such assignment problems are usually NP-hard [21], so when the number of items or the number of owners is large, solving for exact solutions becomes challenging. In this paper, we are interested in solving constrained assignment problems with hundreds of millions of items. Thus, with just tens of owners, the number of decision variables is at billion-scale. This scale is usually seen in the internet industry, which makes decisions for large groups of users. We relax the possible integer constraint, and formulate a general optimization problem that covers commonly seen assignment problems. Its objective function is convex. Its constraints are either linear, or convex and separable by items. We study to solve our generalized assignment problems in the Bregman Alternating Direction Method of Multipliers (BADMM) framework where we exploit Bregman divergence to transform the Augmented Lagrangian into a separable form, and solve many subproblems in parallel. The entire solution can thus be implemented using a MapReduce-style distributed computation framework. We present experiment results on both synthetic and real-world datasets to verify its accuracy and scalability.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {3715–3724},
numpages = {10},
keywords = {ADMM, assignment problem, distributed algorithms, large-scale optimization},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3366423.3380216,
author = {Wang, Zhihao and Li, Qiang and Song, Jinke and Wang, Haining and Sun, Limin},
title = {Towards IP-based Geolocation via Fine-grained and Stable Webcam Landmarks},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380216},
doi = {10.1145/3366423.3380216},
abstract = {IP-based geolocation is essential for various location-aware Internet applications, such as online advertisement, content delivery, and online fraud prevention. Achieving accurate geolocation enormously relies on the number of high-quality (i.e., the fine-grained and stable over time) landmarks. However, the previous efforts of garnering landmarks have been impeded by the limited visible landmarks on the Internet and manual time cost. In this paper, we leverage the availability of numerous online webcams that are used to monitor physical surroundings as a rich source of promising high-quality landmarks for serving IP-based geolocation. In particular, we present a new framework called GeoCAM, which is designed to automatically generate qualified landmarks from online webcams, providing IP-based geolocation services with high accuracy and wide coverage. GeoCAM periodically monitors websites that are hosting live webcams and uses the natural language processing technique to extract the IP addresses and latitude/longitude of webcams for generating landmarks at large-scale. We develop a prototype of GeoCAM and conduct real-world experiments for validating its efficacy. Our results show that GeoCam can detect 282,902 live webcams hosted in webpages with 94.2% precision and 90.4% recall, and then generate 16,863 stable and fine-grained landmarks, which are two orders of magnitude more than the landmarks used in prior works. Thus, by correlating a large scale of landmarks, GeoCAM is able to provide a geolocation service with high accuracy and wide coverage.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1422–1432},
numpages = {11},
keywords = {Data Mining, IP Geolocation, Information Extraction, Internet of Things, Landmarks, Webcam},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3561074.3561082,
author = {Jiang, Harry and Zhang, Xiaoxi and Joe-Wong, Carlee},
title = {DOLL: Distributed OnLine Learning Using Preemptible Cloud Instances},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3561074.3561082},
doi = {10.1145/3561074.3561082},
abstract = {Most large-scale ML implementations scale to large amounts of data by utilizing multiple servers or virtual machines (VMs) that iteratively compute model updates on local data that are periodically synchronized. Due to the complexity of managing the resulting computing infrastructure, many companies run their ML jobs on external cloud providers' servers. However, cloud resources can be expensive, particularly for large ML jobs with long runtimes.A particularly popular method to limit the costs of training ML jobs is to utilize preemptible cloud instances. These may be interrupted at the cloud provider's discretion, but they are significantly (up to 90%) cheaper than conventional on-demand instances. Most studies of these ML methods, however, assume the availability of large datasets at training time. In practice, training data may arrive at irregular intervals and models may be trained online as new data samples arrive, e.g., when monitoring data from IoT sensors. While some software frameworks like Apache Kafka can feed online data arrivals to ML algorithms, they provide little insight into the resulting costs of ML training. We extend prior work on provisioning preemptible instances to analyze available pools of data in order to run online ML on incoming datastreams, which presents new challenges due to the need to carefully handle data arrivals. We design, analyze, and optimize DOLL, which to the best of our knowledge is the first system that provides provable performance guarantees for Distributed OnLine Learning over preemptible instances.Research Challenges and Our Contributions: When pools of data are readily available, the bottleneck to distributed ML training often lies in the time required for each VM to compute its model updates. In our scenario, however, the arrival rate of incoming data may also bottleneck data processing. An intuitive strategy would then be for each VM to process each data point as it arrives. However, since arrivals at different VMs may not be coordinated, synchronizing the model parameters at each VM between data arrivals may introduce additional delays, while asynchronous SGD methods can lead to slow convergence [1]. DOLL uses a batching and grouping process to limit the synchronization delay, which naturally realizes traditional mini-batch SGD so as to provide provable model convergence guarantees.Handling online data arrivals becomes particularly challenging when we use preemptible instances to compute model updates. Existing methods utilizing preemptible instances for ML jobs largely focus on mitigating training interruptions [2] and their effects on model convergence [3]. When used on datastreams, we face an additional challenge of interruptions pausing the data arrival process, which impedes the rate at which we can compute model updates and thus model convergence. Thus, one should ensure that preemptions do not happen "too often," e.g., by computing some updates on on-demand instances. Our work is the first to optimize the number of preemptible VMs used and demonstrate that we can meet ML convergence guarantees.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {aug},
pages = {21–23},
numpages = {3}
}

@inproceedings{10.1145/3576842.3589162,
author = {Wang, Zhaohui and Luo, Bo and Li, Fengjun},
title = {Poster Abstract: SmartAppZoo: a Repository of SmartThings Apps for IoT Benchmarking},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3589162},
doi = {10.1145/3576842.3589162},
abstract = {A well-organized SmartApps dataset provides a valuable resource for researchers to evaluate their work on smart home automation systems. The IoTBench dataset created by Celik et al. 1 is a significant contribution to the IoT research community [1]. However, due to the fast growth of SmartApps and the retirement of some old apps, the IoTBench dataset becomes outdated. The research community is in need of a new large-scale and carefully cleaned benchmarking dataset. In this poster, we present a new repository, namely, SmartAppZoo, which contains 3,526 SmartApps collected from GitHub repositories, including 184 SmartThings official apps, 468 third-party apps from IoTBench, and 2,874 new third-party apps. SmartAppZoo&nbsp;is a manually-verified, comprehensive, clean, and diverse IoT benchmarking dataset. SmartAppZoo&nbsp;is available at: https://github.com/SmartAppZoo/SmartAppZoo.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {448–449},
numpages = {2},
keywords = {Dataset, IoT, SmartApps, SmartThings},
location = {<conf-loc>, <city>San Antonio</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {IoTDI '23}
}

@article{10.1145/3640466,
author = {Neekhara, Paarth and Hussain, Shehzeen and Zhang, Xinqiao and Huang, Ke and McAuley, Julian and Koushanfar, Farinaz},
title = {FaceSigns: Semi-Fragile Watermarks for Media Authentication},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3640466},
doi = {10.1145/3640466},
abstract = {Manipulated media is becoming a prominent threat due to the recent advances in realistic image and video synthesis techniques. There have been several attempts at detecting synthetically tampered media using machine learning classifiers. However, such classifiers do not generalize well to black-box image synthesis techniques and have been shown to be vulnerable to adversarial examples. To address these challenges, we introduce FaceSigns — a deep learning based semi-fragile watermarking technique that allows media authentication by verifying an invisible secret message embedded in the image pixels. Instead of identifying and detecting manipulated media using visual artifacts, we propose to proactively embed a semi-fragile watermark into a real image or video so that we can prove its authenticity when needed. FaceSigns is designed to be fragile to malicious manipulations or tampering while being robust to benign operations such as image/video compression, scaling, saturation, contrast adjustments etc. This allows images and videos shared over the internet to retain the verifiable watermark as long as a malicious modification technique is not applied. We demonstrate that our framework can embed a 128 bit secret as an imperceptible image watermark that can be recovered with a high bit recovery accuracy at several compression levels, while being non-recoverable when unseen malicious manipulations are applied. For a set of unseen benign and malicious manipulations studied in our work, our framework can reliably detect manipulated content with an AUC score of 0.996 which is significantly higher than prior image watermarking and steganography techniques.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
keywords = {media forensics, Deepfakes, watermarking, semi-fragile watermarking, video watermarking}
}

@article{10.1145/3552432,
author = {\'{A}lamos, Jos\'{e} and Kietzmann, Peter and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {DSME-LoRa: Seamless Long-range Communication between Arbitrary Nodes in the Constrained IoT},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3552432},
doi = {10.1145/3552432},
abstract = {Long-range radio communication is preferred in many IoT deployments, as it avoids the complexity of multi-hop wireless networks. LoRa is a popular, energy-efficient wireless modulation but its networking substrate LoRaWAN introduces severe limitations to its users. In this article, we present and thoroughly analyze DSME-LoRa, a system design of LoRa with IEEE 802.15.4 Deterministic Synchronous Multichannel Extension (DSME) as a MAC layer. DSME-LoRa offers the advantage of seamless client-to-client communication beyond the pure gateway-centric transmission of LoRaWAN. We evaluate its feasibility via a full-stack implementation on the popular RIOT operating system, assess its steady-state packet flows in an analytical stochastic Markov model, and quantify its scalability in massive communication scenarios using large-scale network simulations. Our findings indicate that DSME-LoRa is indeed a powerful approach that opens LoRa to standard network layers and outperforms LoRaWAN in many dimensions.},
journal = {ACM Trans. Sen. Netw.},
month = {nov},
articleno = {69},
numpages = {43},
keywords = {Internet of Things, wireless, LPWAN, MAC layer, network experimentation}
}

@article{10.1145/3544492,
author = {Song, Yihang and Song, Chao and Lu, Li and Yang, Shen and Li, Songfan and Zhang, Chong and Meng, Qianhe and Shao, Xiandong and Wang, Haili},
title = {Chipnet: Enabling Large-scale Backscatter Network with Processor-free Devices},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3544492},
doi = {10.1145/3544492},
abstract = {Differing from tremendous existing works that mainly focus on optimizing backscatter communication, Radio-to-Bus (R2B) communication utilizes backscatter to offload processors from IoT devices to the gateway, achieving processor-free devices of significantly reduced power and hardware cost. However, R2B communication is not suitable for large-scale backscatter networks, since R2B cannot support parallel and long-range communication between the gateway and hundreds of R2B devices. In this article, we present Chipnet, a network that supports hundreds of long-range and concurrent connections between the gateway and multiple processor-free devices. The high-level design of Chipnet includes a parallel frequency-division uplink mechanism that can work on processor-free devices and a processor-free MAC layer protocol that supports gateway to broadcast downlink data and individually manage each processor-free device. This design addresses practical issues facing the processor-free device architecture, such as synchronizing hundreds of processor-free devices, assigning unique channel frequencies to every device, and realizing power-efficient processor-free signal conversion. The results demonstrate that a Chipnet network can achieve a task throughput of 2,400 tasks/s with a latency of 72.23 ms. Compared with the R2B network, Chipnet achieves 3\texttimes{}–5\texttimes{} improvements in network coverage range and two orders of magnitude improvement in both network throughput and network latency.},
journal = {ACM Trans. Sen. Netw.},
month = {nov},
articleno = {61},
numpages = {26},
keywords = {Backscatter network, IoT device, long-range backscatter, network latency, network throughput, parallel communication, processor-free devices}
}

@inproceedings{10.1145/3512388.3512394,
author = {He, Haoming and Yang, Jiadong and Chen, Qiang},
title = {Multi-scale Semantic Representation and Supervision for Remote Sensing Change Detection},
year = {2022},
isbn = {9781450395465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512388.3512394},
doi = {10.1145/3512388.3512394},
abstract = {Deep Convolutional Neural Networks have been adopted for remote sensing change detection that focused on how to migrate semantic segmentation networks designed for a single image to remote sensing change detection tasks. These networks tend to have an accuracy of the large region rather than boundary quality and small region quality. In this paper, we propose a novel architecture, Siamese Change Detection Network (SCD-Net), and a new hybrid loss, Multi-Scale Perceptual (MSP) Loss, for bi-temporal remote sensing change detection. Specifically, the architecture is composed of a densely supervised Encoder-Decoder network in which, unlike existing work, we add an up-sampling path to the encoder in charge of building multi-level strong semantic feature maps. In this way, the comparison of low-level feature maps is based on global information prior instead of only local information. The Multi-Scale Perceptual (MSP) Loss consists of Tversky loss and a variant of Focal loss. It is applied to the output results of the network at different scales to be able to learn the changed regions at different scales effectively. Equipped with MSP loss, the proposed SCD-Net can effectively segment the change regions and accurately predict the fine structures with accurate boundaries. Experimental results on two public datasets show that our method outperforms the state-of-the-art methods in F1 score.},
booktitle = {Proceedings of the 2022 5th International Conference on Image and Graphics Processing},
pages = {37–44},
numpages = {8},
keywords = {Multi-Scale Perceptual Loss, Remote sensing change detection, Siamese Network},
location = {Beijing, China},
series = {ICIGP '22}
}

@inproceedings{10.1145/3386901.3388947,
author = {Lee, Seulki and Nirjon, Shahriar},
title = {Fast and scalable in-memory deep multitask learning via neural weight virtualization},
year = {2020},
isbn = {9781450379540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386901.3388947},
doi = {10.1145/3386901.3388947},
abstract = {This paper introduces the concept of Neural Weight Virtualization - which enables fast and scalable in-memory multitask deep learning on memory-constrained embedded systems. The goal of neural weight virtualization is two-fold: (1) packing multiple DNNs into a fixed-sized main memory whose combined memory requirement is larger than the main memory, and (2) enabling fast in-memory execution of the DNNs. To this end, we propose a two-phase approach: (1) virtualization of weight parameters for fine-grained parameter sharing at the level of weights that scales up to multiple heterogeneous DNNs of arbitrary network architectures, and (2) in-memory data structure and run-time execution framework for in-memory execution and context-switching of DNN tasks. We implement two multitask learning systems: (1) an embedded GPU-based mobile robot, and (2) a microcontroller-based IoT device. We thoroughly evaluate the proposed algorithms as well as the two systems that involve ten state-of-the-art DNNs. Our evaluation shows that weight virtualization improves memory efficiency, execution time, and energy efficiency of the multitask learning systems by 4.1x, 36.9x, and 4.2x, respectively.},
booktitle = {Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services},
pages = {175–190},
numpages = {16},
keywords = {deep neural network, in-memory, multitask learning, virtualization},
location = {Toronto, Ontario, Canada},
series = {MobiSys '20}
}

@inproceedings{10.1145/3622896.3622917,
author = {Xiao, Hongzhao and Tang, Jie and Song, Hongjian and Hu, Juncheng},
title = {MSTCNet: Parallel Multi-Scale Network For Medical Image Segmentation},
year = {2023},
isbn = {9798400708190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622896.3622917},
doi = {10.1145/3622896.3622917},
abstract = {Transformer-like architectures, which are the model of choice in the field of natural language processing, have recently been adapted to computer vision (CV) fields and demonstrated remarkable effectiveness on various CV tasks. However, current transformer-based methods require large-scale datasets, which are usually unavailable in medical image analysis, thus resulting in adverse achievement. To this end, we propose a novel segmentation model, named MSTCNet, which constructs a parallel multi-scale transformer (MST) encoder in U-Net. In MST, we devise multi-scale patch partition and multi-scale mix attention to perform multi-scale long-range dependencies modeling. The U-Net encoder paralleled with MST alleviates the burden of large-scale datasets and extract local features supplementarily. We also propose Feature Fusion Head to narrow the gap between convolutional features and transformer features. Sufficient experiments demonstrate that our MSTCNet outperforms state-of-the-art methods on GlaS and ISIC18 datasets and is more suitable for medical image segmentation with small-scale datasets.},
booktitle = {Proceedings of the 2023 4th International Conference on Control, Robotics and Intelligent System},
pages = {123–127},
numpages = {5},
keywords = {attention mechanism, encoder-decoder, medical image segmentation, multi scale, vison transformer},
location = {<conf-loc>, <city>Guangzhou</city>, <country>China</country>, </conf-loc>},
series = {CCRIS '23}
}

@article{10.1145/3607187,
author = {Liu, Xuanzhe and Yang, Chengxu and Li, Ding and Zhou, Yuhan and Li, Shaofei and Chen, Jiali and Chen, Zhenpeng},
title = {Adonis: Practical and Efficient Control Flow Recovery through OS-level Traces},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607187},
doi = {10.1145/3607187},
abstract = {Control flow recovery is critical to promise the software quality, especially for large-scale software in production environment. However, the efficiency of most current control flow recovery techniques is compromised due to their runtime overheads along with deployment and development costs. To tackle this problem, we propose a novel solution, Adonis, which harnesses Operating System (OS)-level traces, such as dynamic library calls and system call traces, to efficiently and safely recover control flows in practice. Adonis operates in two steps: It first identifies the call-sites of trace entries, and then it executes a pairwise symbolic execution to recover valid execution paths. This technique has several advantages. First, Adonis does not require the insertion of any probes into existing applications, thereby minimizing runtime cost. Second, given that OS-level traces are hardware-independent, Adonis can be implemented across various hardware configurations without the need for hardware-specific engineering efforts, thus reducing deployment cost. Third, as Adonis is fully automated and does not depend on manually created logs, it circumvents additional development cost. We conducted an evaluation of Adonis on representative desktop applications and real-world IoT applications. Adonis can faithfully recover the control flow with 86.8% recall and 81.7% precision. Compared to the state-of-the-art log-based approach, Adonis can not only cover all the execution paths recovered but also recover 74.9% of statements that cannot be covered. In addition, the runtime cost of Adonis is 18.3\texttimes{} lower than the instrument-based approach; the analysis time and storage cost (indicative of the deployment cost) of Adonis is 50\texttimes{} smaller and 443\texttimes{} smaller than the hardware-based approach, respectively. To facilitate future replication and extension of this work, we have made the code and data publicly available.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {2},
numpages = {27},
keywords = {Control flow recovery, os-level traces, reverse engineering}
}

@article{10.1145/3538227,
author = {Wang, Yongge},
title = {Byzantine Fault Tolerance For Distributed Ledgers Revisited},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3538227},
doi = {10.1145/3538227},
abstract = {The problem of Byzantine Fault Tolerance (BFT) has received a lot of attention in the last 30 years. Due to the popularity of Proof of Stake (PoS) blockchains in recent years, several BFT protocols have been deployed in the large scale of Internet environment. We analyze several popular BFT protocols such as Capser FFG/CBC-FBC for Ethereum 2.0 and GRANDPA for Polkadot. Our analysis shows that the security models for these BFT protocols are slightly different from the models commonly accepted in the academic literature. For example, we show that, if the adversary has a full control of the message delivery order in the underlying network, then none of the BFT protocols for Ethereum blockchain 2.0 and Polkadot blockchain could achieve liveness even in a synchronized network. Though it is not clear whether a practical adversary could actually control and re-order the underlying message delivery system (at Internet scale) to mount these attacks, it raises an interesting question on security model gaps between academic BFT protocols and deployed BFT protocols in the Internet scale. With these analysis, this article proposes a Casper CBC-FBC style binary BFT protocol and shows its security in the traditional academic security model with complete asynchronous networks. For partial synchronous networks, we propose a multi-value BFT protocol BDLS based on the seminal DLS protocol and show that it is one of the most efficient practical BFT protocols at large scale networks in the traditional academic BFT security model. The implementation of BDLS is available at . Finally, we propose a multi-value BFT protocol XP for complete asynchronous networks and show its security in the traditional academic BFT security model.},
journal = {Distrib. Ledger Technol.},
month = {sep},
articleno = {2},
numpages = {26},
keywords = {Byzantine fault tolerance, distributed computing, partial synchronous networks, security models, blockchain}
}

@inproceedings{10.1145/3549206.3549312,
author = {Chaudhary, Anav and Talwar, Maanas and Goel, Avil and Singal, Gaurav and Kushwaha, Riti},
title = {De-Fence: LoRa based Hop-to-Hop Communication},
year = {2022},
isbn = {9781450396752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549206.3549312},
doi = {10.1145/3549206.3549312},
abstract = {The need for a low-power reliable form of communication is ever-present in a multitude of fields. Our paper aims to develop and explore a security-oriented application of LoRa-based hop-to-hop communication, which provides low-power, large-scale, and long-range solutions to our current safety needs. It utilizes all the components of an IoT-based implementation, in to develop a network, which consists of different types of nodes. It takes input from the external environment through sensors, transports it via several intermediate nodes, using an effective routing algorithm, and provides output through the means of an actuator. The system achieved henceforth is highly scalable, reliable, portable, cheap, and easy to maintain, and provides a fresh outlook on the contemporary need of modernizing the security infrastructure using a growing non-cellular form of communication, LoRa technology.},
booktitle = {Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing},
pages = {629–637},
numpages = {9},
keywords = {Ad-hoc Network, Hop-to-Hop Communication, LoRa},
location = {Noida, India},
series = {IC3-2022}
}

@article{10.1145/3397328,
author = {Wu, Di and Xiao, Tao and Liao, Xuewen and Luo, Jie and Wu, Chao and Zhang, Shigeng and Li, Yong and Guo, Yike},
title = {When Sharing Economy Meets IoT: Towards Fine-grained Urban Air Quality Monitoring through Mobile Crowdsensing on Bike-share System},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3397328},
doi = {10.1145/3397328},
abstract = {Air pollution is a serious global issue impacting public health and social economy. In particular, exposure to small particulate matter of 2.5 microns or less in diameter (PM2.5) can cause cardiovascular and respiratory diseases, and cancer. Fine-grained urban air quality monitoring is crucial yet difficult to achieve. In this paper, we present the design, implementation, and evaluation of an ambient environment aware system, namely UbiAir, which can support fine-grained urban air quality monitoring through mobile crowdsensing on a bike-sharing system. We have built specific IoT box configured with multiple pollutant sensors and attached on shared bikes to sample micro-scale air quality data in the monitoring space that is split by a scalable grid structure. Both hardware and software data calibration methods are exploited in UbiAir to make the sampled data reliable. Then, we use Bayesian compressive sensing (BCS) as an inference model that leverages the calibrated samples to recover data points without direct measurements and reconstruct an accurate air quality map covering the entire monitoring space. In addition, red envelope based incentive schemes and differential rewarding strategies have been designed in UbiAir, and an adaptive BCS algorithm is proposed to deploy the red envelopes at the most informative positions to facilitate data sampling and inference. We have tested our system on campus with over 100k data measurements collected by 36 students through 18 days. Our real-world experiments show that UbiAir is a light-weight, low-cost, accurate and scalable system for fine-grained air quality monitoring, as compared with other solutions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jun},
articleno = {61},
numpages = {26},
keywords = {Internet of things, air quality monitoring, mobile crowdsensing, sharing economy, urban computing}
}

@inproceedings{10.1145/3405669.3405824,
author = {Hall, Matthew Nance and Liu, Guyue and Durairajan, Ramakrishnan and Sekar, Vyas},
title = {Fighting Fire with Light: Tackling Extreme Terabit DDoS Using Programmable Optics},
year = {2020},
isbn = {9781450380416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405669.3405824},
doi = {10.1145/3405669.3405824},
abstract = {Distributed denial-of-service (DDoS) attacks are a clear and present threat to both today's and future network infrastructures. Attacks are constantly growing in sophistication with new threats emerging and likely amplified with other technology trends (e.g., amplification, IoT botnets, 5G connectivity). While great progress has been made in devising many types of mitigation strategies, they are found wanting in light of advanced large-scale attacks and our ability to minimize the impact of the attacks on legitimate services.In this work, we explore a new opportunity for bolstering our DDoS defense arsenal by leveraging recent advances in programmable optics. We envision ONSET: an Optics-enabled In-Network defenSe for Extreme Terabit DDoS attacks. Our approach seeks to isolate and steer attack traffic by dynamic reconfiguration of (backup) wavelengths. This physical isolation of attack traffic enables finer-grained handling of suspicious flows and offers better performance for legitimate traffic in the face of large-scale attacks. In this position paper, we demonstrate the preliminary promise of this vision and identify several open problems at the intersection of security, optical, and systems communities.},
booktitle = {Proceedings of the Workshop on Secure Programmable Network Infrastructure},
pages = {42–48},
numpages = {7},
keywords = {Programmable optics, Terabit DDoS defense},
location = {Virtual Event, USA},
series = {SPIN '20}
}

@inproceedings{10.1145/3591365.3592945,
author = {Hosen, A. S. M. Sanwar and Sharma, Pradip Kumar and Puthal, Deepak and Ra, In-Ho and Cho, Gi Hwan},
title = {SECBlock-IIoT: A Secure Blockchain-enabled Edge Computing Framework for Industrial Internet of Things},
year = {2023},
isbn = {9798400701825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591365.3592945},
doi = {10.1145/3591365.3592945},
abstract = {The IoT is widely used in a number of industries and generates large amounts of data. The data are processed, computed, and stored through distributed computing for analytical purposes. This invokes serious security and privacy concerns, and presents scalability issues. This paper describes a secure P2P and group communication supportive edge computing framework for IIoT systems, a consortium blockchain, and IPFS-based immutable data storage system, and an intelligent threat detection model to protect confidential data and identify cyber-attacks. Secure communications were ensured using a hybrid security scheme that included modified ECC, PUF, and Lagrange interpolation. We utilized a modified PoV consensus algorithm to resolve latency issues due to overhead and point of failure errors during block mining. The threat intelligence model used an autoencoder to transform data into a new format which was then fed into an RNN-DL to identify cyber-attacks. The model detected normal and anomalous activity, and then identified the category of detected malicious activity. We evaluated the framework according to various metrics and compared it with ECC, PoV, and ML-based classifiers. The results showed that the proposed system demonstrated a higher efficiency and improved scalability than conventional frameworks.},
booktitle = {Proceedings of the Third International Symposium on Advanced Security on Software and Systems},
articleno = {1},
numpages = {14},
keywords = {Blockchain, Edge computing, Industrial internet of things (IIoT), Intelligent threat detection (ITD), Security and privacy},
location = {Melbourne, VIC, Australia},
series = {ASSS '23}
}

@article{10.1109/TNET.2021.3073926,
author = {Li, Qiang and Wang, Zhihao and Tan, Dawei and Song, Jinke and Wang, Haining and Sun, Limin and Liu, Jiqiang},
title = {GeoCAM: An IP-Based Geolocation Service Through Fine-Grained and Stable Webcam Landmarks},
year = {2021},
issue_date = {Aug. 2021},
publisher = {IEEE Press},
volume = {29},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3073926},
doi = {10.1109/TNET.2021.3073926},
abstract = {IP-based geolocation is essential for various location-aware Internet applications, such as online advertisement, content delivery, and online fraud prevention. Achieving accurate geolocation enormously relies on the number of high-quality (i.e., the fine-grained and stable over time) landmarks. However, the previous efforts of garnering landmarks have been impeded by the limited visible landmarks on the Internet and manual time cost. In this paper, we leverage the availability of numerous online webcams used to monitor physical surroundings as a rich source of promising high-quality landmarks for serving IP-based geolocation. In particular, we present a new framework called &lt;italic&gt;GeoCAM&lt;/italic&gt;, which is designed to automatically generate qualified landmarks from online webcams, providing an IP-based geolocation service with high accuracy and wide coverage. GeoCAM periodically monitors websites hosting live webcams and uses the natural language processing technique to extract the IP addresses and latitude/longitude of webcams for generating landmarks at a large-scale. Given latency and topology constraints among webcam landmarks, GeoCAM uses the maximum likelihood estimation to approximately pinpoint the geolocation of a target host. We develop a prototype of GeoCAM and conduct real-world experiments for validating its efficacy. Our results show that GeoCam can detect 282,902 live webcams hosted in webpages with 94.2% precision and 90.4% recall, and then generate 16,863 stable and fine-grained landmarks, which are two orders of magnitude more than the landmarks used in prior works. To demonstrate the superiority of using large-scale webcams as landmarks, we implement four different geolocation algorithms and compare their performance between webcam landmarks and open-source landmarks. The evaluation results show that all the algorithms can significantly improve geolocation accuracy by using webcam landmarks.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {1798–1812},
numpages = {15}
}

@inproceedings{10.1145/3543507.3583215,
author = {Xie, Zhe and Xu, Haowen and Chen, Wenxiao and Li, Wanxue and Jiang, Huai and Su, Liangfei and Wang, Hanzhang and Pei, Dan},
title = {Unsupervised Anomaly Detection on Microservice Traces through Graph VAE},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583215},
doi = {10.1145/3543507.3583215},
abstract = {The microservice architecture is widely employed in large Internet systems. For each user request, a few of the microservices are called, and a trace is formed to record the tree-like call dependencies among microservices and the time consumption at each call node. Traces are useful in diagnosing system failures, but their complex structures make it difficult to model their patterns and detect their anomalies. In this paper, we propose a novel dual-variable graph variational autoencoder (VAE) for unsupervised anomaly detection on microservice traces. To reconstruct the time consumption of nodes, we propose a novel dispatching layer. We find that the inversion of negative log-likelihood (NLL) appears for some anomalous samples, which makes the anomaly score infeasible for anomaly detection. To address this, we point out that the NLL can be decomposed into KL-divergence and data entropy, whereas lower-dimensional anomalies can introduce an entropy gap with normal inputs. We propose three techniques to mitigate this entropy gap for trace anomaly detection: Bernoulli &amp; Categorical Scaling, Node Count Normalization, and Gaussian Std-Limit. On five trace datasets from a top Internet company, our proposed TraceVAE achieves excellent F-scores.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2874–2884},
numpages = {11},
keywords = {anomaly detection, deep learning, graph vae, microservice trace, variational autoencoder},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3485730.3493375,
author = {Ranathunga, Tharindu and McGibney, Alan and Rea, Susan},
title = {The convergence of Blockchain and Machine Learning for Decentralized Trust Management in IoT Ecosystems},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3493375},
doi = {10.1145/3485730.3493375},
abstract = {The EU data strategy postulates that by 2025 there will be a paradigm shift towards more decentralized intelligence and data processing at the edge. The convergence of a large number of nodes at the IoT edge along with multiple service providers and network operators exposes data owners and resource providers to potential threats. To address cloud-edge risks, trust-based decentralized management is needed. Blockchain technology has created an opportunity to decentralize IoT ecosystems, through its intrinsic properties and together with machine learning (ML) it can be used to provide a trusted backbone for managing IoT ecosystems to support automated and adaptive trust management. This paper presents a novel approach for crosslayer intelligent trust computation modelling leveraging ML and Blockchain for decentralized trust management in IoT ecosystems. The effectiveness of the proposed approach for flow-based trust assessment is demonstrated using the Hyperledger Framework and the Cooja-based simulation environment. Finally, an initial evaluation is presented to understand the performance in terms of scalability and trust convergence of the proposed model.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {499–504},
numpages = {6},
keywords = {Blockchain, Hyperledger, Internet of things, IoT Ecosystems, Machine Learning, Trust},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{10.1145/3444370.3444558,
author = {Fang, Fuyang and Zhang, Daojuan and Wang, Chonghua and Zhang, De and Zhang, Yuanfei and Li, Chao and Luo, Xi and Fu, Ye},
title = {Identification of important nodes on large-scale Internet based on unsupervised learning},
year = {2021},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444558},
doi = {10.1145/3444370.3444558},
abstract = {In recent years, scholars have conducted in-depth researches on the robustness, structural vulnerability, and detection and identification of devices in cyberspace from different perspectives such as complex networks and cyberspace resource mapping. Aiming at the problem of identifying important nodes on a large-scale Internet, a CETCRank algorithm for identifying important Internet nodes based on unsupervised learning is proposed. When the algorithm analyzes the attributes of each cyberspace equipment, it not only considers the graph structure characteristics based on the network topology, but also integrates the threat metric of cyberspace equipment. Based on the hypothesis of the cyber attack model, the effective identification of important nodes in the Internet can be realized by integrating the node attributes into the constructed Markov chain model. Experiments show that the time and space complexity of the CETCRank algorithm is suitable for analyzing large-scale Internet, and the recognition performance of important nodes is better than the PageRank algorithm.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {121–129},
numpages = {9},
keywords = {CETCRank Algorithm, Important Nodes, Large-scale Internet, Unsupervised Learning},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@inproceedings{10.1145/3372297.3417892,
author = {Campobasso, Michele and Allodi, Luca},
title = {Impersonation-as-a-Service: Characterizing the Emerging Criminal Infrastructure for User Impersonation at Scale},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417892},
doi = {10.1145/3372297.3417892},
abstract = {In this paper we provide evidence of an emerging criminal infrastructure enabling impersonation attacks at scale. Impersonation-as-a-Service (IMPaaS) allows attackers to systematically collect and enforce user profiles (consisting of user credentials, cookies, device and behavioural fingerprints, and other metadata) to circumvent risk-based authentication system and effectively bypass multi-factor authentication mechanisms. We present the IMPaaS model and evaluate its implementation by analysing the operation of a large, invite-only, Russian IMPaaS platform providing user profiles for more than 260,000 Internet users worldwide. Our findings suggest that the IMPaaS model is growing, and provides the mechanisms needed to systematically evade authentication controls across multiple platforms, while providing attackers with a reliable, up-to-date, and semi-automated environment enabling target selection and user impersonation against Internet users as scale.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1665–1680},
numpages = {16},
keywords = {impersonation attacks, impersonation-as-a-service, threat modeling, user profiling},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/3564625.3564649,
author = {Anwar, Afsah and Chen, Yi Hui and Hodgman, Roy and Sellers, Tom and Kirda, Engin and Oprea, Alina},
title = {A Recent Year On the Internet: Measuring and Understanding the Threats to Everyday Internet Devices},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564625.3564649},
doi = {10.1145/3564625.3564649},
abstract = {An effective way to improve resilience to cyber attacks is to measure and understand the adversary’s capabilities. Gaining insights into the threats we are exposed to helps us build better defenses, share findings with practitioners, and identify the perpetrators to limit their impact. Honeypot interactions have been widely studied in the past to measure cyber attacks, but the focus of more recent honeypot studies has been on IoT-based threats. Hence, classic threats studied by honeypots in depth a decade ago, such as desktop malware and web threats, have lately received much less attention. In this paper, we perform a measurement study on a large-scale honeypot data collected between July 2020 and June 2021 by a large cybersecurity company. We measure a set of 7 billion connections to extract 806 million alerts raised by 662 endpoints (honeypots) distributed globally. For this study, we create a framework that leverages Open Source Cyber Threat Intelligence (OSCTI) to generate high-level attack classification and malware campaign inferences. One of the main findings of our work is that some networks involved in rogue activities that were reported in literature more than a decade ago&nbsp;[59] are still involved in malicious activity. Also, we find that 17 vulnerabilities disclosed more than a decade ago, even as early as 1999, are still used to launch cyber attacks. At the same time, the threat landscape has evolved. We discover that a large fraction of recent campaigns (63.4%) are Stealers or Keyloggers, new attack vectors such as the SMB EternalBlue vulnerability enable rapid self-propagation of malware across the globe, and infection strategies are shared among multiple campaigns (e.g., 10K alerts for Gafgyt, Trickbot, Freakout, and Hajime utilize the infection strategy of Mirai or muBot).},
booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
pages = {251–266},
numpages = {16},
location = {<conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
series = {ACSAC '22}
}

@article{10.1145/3533432,
author = {Dong, Yucheng and Ran, Qin and Chao, Xiangrui and Li, Congcong and Yu, Shui},
title = {Personalized Individual Semantics Learning to Support a Large-Scale Linguistic Consensus Process},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3533432},
doi = {10.1145/3533432},
abstract = {When making decisions, individuals often express their preferences linguistically. The computing with words methodology is a key basis for supporting linguistic decision making, and the words in that methodology may mean different things to different individuals. Thus, in this article, we propose a continual personalized individual semantics learning model to support a consensus-reaching process in large-scale linguistic group decision making. Specifically, we first derive personalized numerical scales from the data of linguistic preference relations. We then perform a clustering ensemble method to divide large-scale group and conduct consensus management. Finally, we present a case study of intelligent route optimization in shared mobility to illustrate the usability of our proposed model. We also demonstrate its effectiveness and feasibility through a comparative analysis.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {26},
numpages = {27},
keywords = {Computing with words, large-scale linguistic group decision making, personalized individual semantics, consensus process, Internet of Things}
}

@inproceedings{10.1145/3426746.3434064,
author = {Asif, Sana and Jun, Byungjin},
title = {Leveraging Demand Data as a Proxy for Understanding Large-scale Events},
year = {2020},
isbn = {9781450381833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426746.3434064},
doi = {10.1145/3426746.3434064},
abstract = {Although, the internet opens an exceptional possibility for researchers conducting empirical studies on large-scale events by lowering the cost of collecting data and increasing the amount of information, there is a need to be cautious about not breaching the privacy of the users involved. Some types of internet data such as demand on networked systems (demand data) alleviate privacy concerns to some degree. To gauge the utility of demand data in understanding community behaviors during large scale events, we investigate its use in understanding quarantine compliance during Covid-19 pandemic. We analyse CDN demand data, google mobility data, and daily infection cases to see the pattern of correlation, and degree of influence of one variable on the others.},
booktitle = {Proceedings of the Student Workshop},
pages = {25–26},
numpages = {2},
location = {Barcelona, Spain},
series = {CoNEXT'20}
}

@inproceedings{10.1145/3618257.3625001,
author = {Gouel, Matthieu and Darwich, Omar and Mouchet, Maxime and Vermeulen, Kevin},
title = {Poster: Towards a Publicly Available Framework to Process Traceroutes with MetaTrace},
year = {2023},
isbn = {9798400703829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618257.3625001},
doi = {10.1145/3618257.3625001},
abstract = {The objective of this research is to contribute towards the development of an open-source framework for processing large-scale traceroute datasets. By providing such a framework, we aim to benefit the community by saving time in everyday traceroute analysis and enabling the design of new scalable reactive measurements [1], where prior traceroute measurements are leveraged to make informed decisions for future ones[8, 12].It is important to clarify that our goal is not to surpass proprietary solutions like BigQuery, which are utilized by CDNs for processing billions of traceroutes [6, 10]. These proprietary solutions are not freely accessible to the public, whereas our focus is on creating an open and freely available framework for the wider community.Our contributions include (1) sharing the ideas and thinking process behind building MetaTrace, which efficiently utilizes ClickHouse features for traceroute processing; and (2) providing an open-source implementation of MetaTrace.We evaluated MetaTrace using two types of queries: predicate queries for filtering traceroutes based on conditions, and aggregate queries for computing metrics on traceroutes. Our results show that MetaTrace is significantly faster compared to alternative solutions. For predicate queries, it outperforms a multiprocessed Rust solution by a factor of 552 and is 3.4 times faster than ClickHouse without MetaTrace optimizations. For aggregate queries, MetaTrace processes 202 million traceroutes in 11 seconds, with its performance scaling linearly with traceroute volume. Notably, on a single server, MetaTrace can perform a predicate query on a 6-year dataset of 6 billion traceroutes in just 240 seconds.Furthermore, MetaTrace is resource-efficient, making it accessible for research groups with limited resources to conduct Internet-scale traceroute studies.},
booktitle = {Proceedings of the 2023 ACM on Internet Measurement Conference},
pages = {728–729},
numpages = {2},
keywords = {active internet measurements, traceroute},
location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
series = {IMC '23}
}

@inproceedings{10.1145/3487552.3487834,
author = {Philip, Adithya Abraham and Ware, Ranysha and Athapathu, Rukshani and Sherry, Justine and Sekar, Vyas},
title = {Revisiting TCP congestion control throughput models &amp; fairness properties at scale},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487834},
doi = {10.1145/3487552.3487834},
abstract = {Much of our understanding of congestion control algorithm (CCA) throughput and fairness is derived from models and measurements that (implicitly) assume congestion occurs in the last mile. That is, these studies evaluated CCAs in "small scale" edge settings at the scale of tens of flows and up to a few hundred Mbps bandwidths. However, recent measurements show that congestion can also occur at the core of the Internet on inter-provider links, where thousands of flows share high bandwidth links. Hence, a natural question is: Does our understanding of CCA throughput and fairness continue to hold at the scale found in the core of the Internet, with 1000s of flows and Gbps bandwidths?Our preliminary experimental study finds that some expectations derived in the edge setting do not hold at scale. For example, using loss rate as a parameter to the Mathis model to estimate TCP NewReno throughput works well in edge settings, but does not provide accurate throughput estimates when thousands of flows compete at high bandwidths. In addition, BBR - which achieves good fairness at the edge when competing solely with other BBR flows - can become very unfair to other BBR flows at the scale of the core of the Internet. In this paper, we discuss these results and others, as well as key implications for future CCA analysis and evaluation.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {96–103},
numpages = {8},
keywords = {BBR, RENO, TCP, computer networks, congestion control, cubic, fairness, throughput},
location = {Virtual Event},
series = {IMC '21}
}

@article{10.1145/3466696,
author = {Lin, Changyuan and Khazaei, Hamzeh and Walenstein, Andrew and Malton, Andrew},
title = {Autonomic Security Management for IoT Smart Spaces},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3466696},
doi = {10.1145/3466696},
abstract = {Embedded sensors and smart devices have turned the environments around us into smart spaces that could automatically evolve, depending on the needs of users, and adapt to the new conditions. While smart spaces are beneficial and desired in many aspects, they could be compromised and expose privacy, security, or render the whole environment a hostile space in which regular tasks cannot be accomplished anymore. In fact, ensuring the security of smart spaces is a very challenging task due to the heterogeneity of devices, vast attack surface, and device resource limitations. The key objective of this study is to minimize the manual work in enforcing the security of smart spaces by leveraging the autonomic computing paradigm in the management of IoT environments. More specifically, we strive to build an autonomic manager that can monitor the smart space continuously, analyze the context, plan and execute countermeasures to maintain the desired level of security, and reduce liability and risks of security breaches. We follow the microservice architecture pattern and propose a generic ontology named Secure Smart Space Ontology (SSSO) for describing dynamic contextual information in security-enhanced smart spaces. Based on SSSO, we build an autonomic security manager with four layers that continuously monitors the managed spaces, analyzes contextual information and events, and automatically plans and implements adaptive security policies.As the evaluation, focusing on a current BlackBerry customer problem, we deployed the proposed autonomic security manager to maintain the security of a smart conference room with 32 devices and 66 services. The high performance of the proposed solution was also evaluated on a large-scale deployment with over 1.8 million triples.},
journal = {ACM Trans. Internet Things},
month = {aug},
articleno = {27},
numpages = {20},
keywords = {Autonomic security management, IoT, ontology, smart spaces}
}

@inproceedings{10.1145/3510450.3517271,
author = {Biatek, Thibaud and Abdoli, Mohsen and Burdinat, Christophe and Toullec, Eric and Gregory, Lucas and Raulet, Mickael},
title = {Live OTT services delivery with Ad-insertion using VVC, CMAF-LL and ROUTE: an end-to-end chain},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517271},
doi = {10.1145/3510450.3517271},
abstract = {During the past decade, broadcasting has been challenged by OTT services, offering a more personalized and flexible way of experiencing the content. The on-demand features paved the way for new media delivery paradigms, replacing traditional MPEG-TS stream-based approach by file-based IP protocols. With those new protocols came new ways of monetizing the content. While traditional TV leveraged advertisements to increase broadcasters' revenues for decades, it has never approached the level of personalization offered by OTT, until the release of ATSC-3.0 [8] and its IP stack in 2016. However, OTT services have some drawbacks compared to broadcasting: video quality, latency, and scaling capabilities when millions of viewers want to access a content at the same time. In this paper, we describe how the several recent technologies can be exploited to address these drawbacks. An end-to-end chain is also demonstrated, bringing significant improvement over existing approaches in terms of bandwidth, latency and experience.The Versatile Video Coding (VVC) [1] standard has been released in mid-2020 by ISO/IEC MPEG and ITU-T VCEG. VVC has been designed to address a wide range of applications and video formats, while providing a substantial bandwidth saving (around 50%), compared to its predecessor, High Efficiency Video Coding (HEVC) [10], at an equivalent perceived video quality [12]. In this paper, VVC is used to reduce the bandwidth occupied by video over the network, which is a key issue in 2021, knowing that more than 80% of the internet traffic is used to deliver video content [2]. A live software implementation of VVC provided by ATEME is used in the headend before packaging and distribution, producing ISO/IEC Base Media File Format (ISOBMFF) output files [6].The Common Media Application Format in its Low-Latency profile (CMAF-LL) is then used to deliver the video [7], leveraging HTTP chunk transfer encoding to deliver Low-Latency DASH (LL-DASH) [3]. To further reduce the bandwidth, the DASH-ROUTE server is leveraged to deliver multicast instead of redundant unicast sessions [11]. The ATEME packager, origin server and DASH-ROUTE multicast server are used for that purpose. Combined with VVC efficient source-coding, this paper provides a bandwidth efficient and low-latency manner of delivering OTT services at scale.This paper adopts multi-period DASH manifests for ad insertion using XLink to signal ad-server URL. Typical streams coming from broadcasting studios, embedding SCTE-35 splicing event are used as input and interpreted by ATEME pre-processing engine to trigger the ad-insertion and multi-period events in the DASH manifest. The ad-server is then provisioned with VVC-encoded ad-clip for reducing the CDN cost of those files. To demonstrate such advanced features, the GPAC Framework [9] is used with real-time VVC software decoder [4, 5]. The multicast gateway and player from GPAC embeds the real-time decoder libraries to demonstrate both the ROUTE demuxing and ad-replacement within the player.The benefits of using VVC and ROUTE has been measured. It is verified that the proposed solution enables similar latency as typical terrestrial broadcast services with a high-level of quality providing around 3 seconds glass-to-glass latency leveraging CMAF with HTTP chunk transfer encoding. An accurate description of encoding, packaging and delivery setting will be presented in the poster. Finally, the interoperability is demonstrated by integrating it with other CDN-providers and devices.In summary, this paper describes implementation and demonstration of a state-of-the-art complete end-to-end and transmission chain for OTT, That aims to address well-known OTT drawbacks, such as latency and scaling capability, by combining the VVC, CMAF-LL and DASH-ROUTE. The proposed solution demonstrates significant benefits in terms of latency reduction, bandwidth saving and network traffic reduction, while enabling flexible and customized ad-insertion solutions for live OTT services. The interoperability of the proposed components is also demonstrated by integrating the chain with 3rd party CDN-providers and integrating it within broadcast environment. As a result, this paper shows the good degree of maturity of emerging technologies such as VVC, when combined with CMAF and ROUTE.},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {79–80},
numpages = {2},
keywords = {CMAF, DASH, OTT, ROUTE, VVC, low-latency},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3538393.3544937,
author = {Dayalan, Udhaya Kumar and Fezeu, Rostand A. K. and Salo, Timothy J. and Zhang, Zhi-Li},
title = {Kaala: scalable, end-to-end, IoT system simulator},
year = {2022},
isbn = {9781450393928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538393.3544937},
doi = {10.1145/3538393.3544937},
abstract = {We introduce Kaala, a scalable, hybrid, end-to-end IoT system simulator that can integrate with diverse, real-world IoT cloud services. Many IoT simulators run in isolation and do not interface with real-world IoT cloud systems or servers. This isolation makes it difficult for experiments to fully replicate the diversity that exists in end-to-end, real-world systems. Kaala is intended to bridge the gap between IoT simulation experiments and the real world. The simulator can interact with cloud IoT services, such as those offered by Amazon, Microsoft and Google. Kaala leverages vendor-provided software development kits (SDKs) to implement the vendor-specific protocols that are necessary permit simulated IoT devices and gateways to seamlessly communicate with real-world cloud IoT systems. Kaala has the ability to simulate a large number of diverse IoT devices, as well as to simulate events that may simultaneously affect several sensors. Evaluation results show that Kaala is able to, with minimal overhead, seamlessly connect simulated IoT devices to real-world cloud IoT systems.},
booktitle = {Proceedings of the ACM SIGCOMM Workshop on Networked Sensing Systems for a Sustainable Society},
pages = {33–38},
numpages = {6},
keywords = {5G, IoT cloud, IoT devices, IoT gateway SDK, IoT simulator, network},
location = {Amsterdam, Netherlands},
series = {NET4us '22}
}

@inproceedings{10.1145/3544109.3544166,
author = {Yu, Lejie},
title = {Design of Network Public Opinion Management System Based on Big Data},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544166},
doi = {10.1145/3544109.3544166},
abstract = {With the development of IT technology and the popularization of Internet applications, the scale and value of information carried and disseminated on the Internet are increasing. It has become one of the most important sources of information for all walks of life, institutions and individuals. The core of the network public opinion system is to use intelligent mining, machine learning and other computer technologies to identify, mine and analyze the public opinion information existing on the network, and solve the problem of the inability to realize the timely supervision of massive and dynamic Internet content by manual means. This paper takes network public opinion management as the research object, and constructs a public opinion management system model under the big data network environment. In the context of big data, this paper focuses on the work flow of the public opinion management system, and designs the system components and implementation methods in detail. The public opinion management system based on big data designed in this paper can meet the needs of users from many aspects. The system can efficiently mine and identify public opinion information from a large amount of data, thus providing a solution for government and enterprise public opinion monitoring.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {309–313},
numpages = {5},
keywords = {Big data, Hadoop, Information resource management, Network public opinion},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/3408127.3408173,
author = {Chen, Shihua and Chen, Jia and Chen, Jing},
title = {A Deep Reinforcement Learning based Network Management System in Smart Identifier Network},
year = {2020},
isbn = {9781450376877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408127.3408173},
doi = {10.1145/3408127.3408173},
abstract = {As a new large-scale deployment network, Smart Identifier Network (SINET) points out the identity/location binding, which is one of the root causes of current Internet's problem. In order to ensure the controllable manageability and large-scale deployment of the SINET, a network management system is of great essence. And considering the single point of failure of centralized network management, this paper proposes a Deep Reinforcement Learning (DRL) based domain network management system to manage the devices and achieve the reasonable allocation of management resources, where we consider the problem as a Markov Decision Process (MDP), including how to settle the new device and the change of the number of devices in each management domain at each time. By quantifying the cost function, we want to minimize it in a period of time, that is, to maximize the long-term expected reward value. The experiments show that our agent can automatically learn the environment, and the results will gradually reach convergence after certain iterations.},
booktitle = {Proceedings of the 2020 4th International Conference on Digital Signal Processing},
pages = {268–273},
numpages = {6},
keywords = {Deep Reinforcement Learning, Markov Decision Process, Network Management System, Smart Identifier Network},
location = {Chengdu, China},
series = {ICDSP '20}
}

@inproceedings{10.1145/3405656.3418708,
author = {Ghasemi, Chavoosh and Yousefi, Hamed and Zhang, Beichuan},
title = {Far Cry: Will CDNs Hear NDN's Call?},
year = {2020},
isbn = {9781450380409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405656.3418708},
doi = {10.1145/3405656.3418708},
abstract = {Content Delivery Networks (CDNs) have become indispensable to Internet content distribution. As they evolve to meet the ever-increasing demands, they are also facing challenges such as system complexity, resource footprint, and content security. In this paper, we look at CDNs once again, but this time from the eyes of a young networking technology called named-data networking (NDN). NDN supports content distribution without requiring an overlay service to bridge the gap between network services and application needs. Therefore, it can realize content distribution at large scale with an arguably simpler system design.We conducted real-world experiments to compare the standard deployment of NDN (i.e., the global NDN testbed) and two leading CDNs (Akamai and Fastly) in terms of caching and retrieving static contents through streaming videos from four different continents over these networks for two weeks. We found that although NDN can provide a satisfactory quality of service in most cases, it falls behind CDNs mainly due to its lack of hardware infrastructure and software/protocol immaturity. Nevertheless, NDN outperforms CDNs in terms of server workload and failure resiliency due to its ubiquitous in-network caching and adaptive forwarding plane. Besides, NDN comes with built-in content security, but it needs an efficient solution for content privacy. NDN's architectural advantages make it a natural fit for Internet content distribution in the long run. That said, in terms of forthcoming goals, this paper reveals several limitations of the current NDN deployment and discusses why the future of NDN hinges on addressing those limitations.},
booktitle = {Proceedings of the 7th ACM Conference on Information-Centric Networking},
pages = {89–98},
numpages = {10},
keywords = {Content Delivery Networks, Information-Centric Networks, Named Data Networking, Performance Evaluation},
location = {Virtual Event, Canada},
series = {ICN '20}
}

@inproceedings{10.1145/3386901.3396604,
author = {Bansal, Atul and Kumar, Swarun and Iannucci, Bob},
title = {Does ambient RF energy suffice to power battery-free IoT?},
year = {2020},
isbn = {9781450379540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386901.3396604},
doi = {10.1145/3386901.3396604},
abstract = {Recent years have witnessed novel designs of battery-free IoT tags using RF backscatter. Traditionally, they require a dedicated transmitter to excite the tag. However, such a deployment is infeasible at large scale. To counter this problem, researchers have proposed using ambient RF energy to power up the battery-free tag. In this poster, we evaluate if this ambient RF energy is sufficient to meet the power requirements of a battery-free tag in today's urban and rural areas. We also compare available ambient RF energy across different frequencies. Finally, we discuss open challenges in realising ambient backscatter systems in real-world.},
booktitle = {Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services},
pages = {470–471},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {MobiSys '20}
}

@inproceedings{10.1145/3503161.3548108,
author = {Huang, Xinyu and Zhang, Youcai and Cheng, Ying and Tian, Weiwei and Zhao, Ruiwei and Feng, Rui and Zhang, Yuejie and Li, Yaqian and Guo, Yandong and Zhang, Xiaobo},
title = {IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548108},
doi = {10.1145/3503161.3548108},
abstract = {Vision-Language Pre-training (VLP) with large-scale image-text pairs has demonstrated superior performance in various fields. However, the image-text pairs co-occurrent on the Internet typically lack explicit alignment information, which is suboptimal for VLP. Existing methods proposed to adopt an off-the-shelf object detector to utilize additional image tag information. However, the object detector is time-consuming and can only identify the pre-defined object categories, limiting the model capacity. Inspired by the observation that the texts incorporate incomplete fine-grained image information, we introduce IDEA, which stands for increasing text diversity via online multi-label recognition for VLP. IDEA shows that multi-label learning with image tags extracted from the texts can be jointly optimized during VLP. Moreover, IDEA can identify valuable image tags online to provide more explicit textual supervision. Comprehensive experiments demonstrate that IDEA can significantly boost the performance on multiple downstream datasets with a small extra computational cost.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4573–4583},
numpages = {11},
keywords = {multi-label recognition, natural language supervision, vision-language intelligence, vision-language pre-training},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3394171.3413688,
author = {Gupta, Akash and Panda, Rameswar and Paul, Sujoy and Zhang, Jianming and Roy-Chowdhury, Amit K.},
title = {Adversarial Knowledge Transfer from Unlabeled Data},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413688},
doi = {10.1145/3394171.3413688},
abstract = {While machine learning approaches to visual recognition offer great promise, most of the existing methods rely heavily on the availability of large quantities of labeled training data. However, in the vast majority of real-world settings, manually collecting such large labeled datasets is infeasible due to the cost of labeling data or the paucity of data in a given domain. In this paper, we present a novel Adversarial Knowledge Transfer (AKT) framework for transferring knowledge from internet-scale unlabeled data to improve the performance of a classifier on a given visual recognition task. The proposed adversarial learning framework aligns the feature space of the unlabeled source data with the labeled target data such that the target classifier can be used to predict pseudo labels on the source data. An important novel aspect of our method is that the unlabeled source data can be of different classes from those of the labeled target data, and there is no need to define a separate pretext task, unlike some existing approaches. Extensive experiments well demonstrate that models learned using our approach hold a lot of promise across a variety of visual recognition tasks on multiple standard datasets. Project page is at texttthttps://agupt013.github.io/akt.html.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2175–2183},
numpages = {9},
keywords = {adversarial learning, feature alignment, knowledge transfer},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3590837.3590867,
author = {Selvaraj, Arun Prasath and Annamalai, Suresh},
title = {Towards Reliable Medical Transactions for Blockchain based Healthcare Systems using Hybrid Consensus Method},
year = {2023},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590837.3590867},
doi = {10.1145/3590837.3590867},
abstract = {Blockchain technology opens the door to securing and protecting massive amounts of IoT data storage, improving decentralized storage applications, getting rid of centralized trust servers, and facilitating data accountability and traceability. A hybrid blockchain can be used to build a network where sensors communicate with a device that performs smart contracts and records all events on the blockchain. Actual clinical observation and treatment choices would be provided by alerting patients and medical professionals and keeping a safe history of who started these acts. The proposed work aggrandize blockchain consensus algorithm that combines the benefits of the PoW and PoA algorithms. These PoW-PoA hybrid consensus methods are simulated with Merkle tree creation focusing conditional contrast large item set tree. The PoW mining mechanism is effectively utilized in proposed system to control the block creation time. With PoA consensus technique, the user generates the real blocks. The validating nodes, which are arbitrary chosen as reliable entities, protect PoA blockchain. The Proof of Authority concept is a highly scalable system since it only requires a small number of block validators. Participants who have been pre-approved serve as the system's moderators and confirm blocks and transactions. According to results, the proposed method performs better than conventional consensus algorithm in terms of scalability, network throughput, and the quantity of blocks generated.},
booktitle = {Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
articleno = {30},
numpages = {8},
keywords = {Blockchain, Consensus Algorithm, Healthcare Applications, Hybrid Method, IoT, Smart contract},
location = {Jaipur, India},
series = {ICIMMI '22}
}

@inproceedings{10.1145/3508230.3508245,
author = {Jin, Kai and Liu, Wuying},
title = {Scored and Error-annotated Essay Dataset of Chinese EFL/ESL Learners},
year = {2022},
isbn = {9781450387354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508230.3508245},
doi = {10.1145/3508230.3508245},
abstract = {A certain scale of finely annotated essay dataset of EFL/ESL (English as a foreign language or the second language) learners is not only an important language resource for language research and teaching, but also contributing materials for language-related computing science. Unfortunately, this type of data open on the Internet are not only of small quantity but also of uneven quality, especially such data of Chinese learners. We collected 147 essays of Chinese EFL/ESL learners and had four teachers score them under the same criteria and one teacher annotate major errors, and have them scored in Pigai scoring system. We then structured the score file, error-annotated files, essay files together with context information, and built the Scored and Error-annotated Essay Dataset of Chinese EFL/ESL Learners (SeedCel) which is open on the Internet and will be incrementally updated. This paper explains how SeedCel is constructed, what the details of SeedCel are, and where SeedCel will be used.},
booktitle = {Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval},
pages = {102–108},
numpages = {7},
keywords = {Chinese leaners, EFL/ESL Learners, Error-annotated, Essay Dataset, Scored},
location = {Sanya, China},
series = {NLPIR '21}
}

@inproceedings{10.1145/3359852.3359878,
author = {Baptista, Sofia and Azevedo, Jos\'{e}},
title = {Analysing Interactive Documentaries and Narrative: A Proposal for Categorization},
year = {2020},
isbn = {9781450372503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359852.3359878},
doi = {10.1145/3359852.3359878},
abstract = {This paper proposes a categorization model for interactive documentaries based on six dichotomic poles, in order to identify narrative and storytelling challenges in i-docs. This dual structure helps to identify key strategies that allow us to profit from both interactivity and narrative, producing i-docs with stronger stories and more effective messages to the audience, crossing the inherent characteristics of interactive documentaries with the importance of maintaining functional narratives.We identified six antagonistic poles, ordered by interference with storytelling and user experience: Technological innovation VS Filmmaking, Database VS Narrative, Participation VS Authorship, Interactivity VS AV Content, Gameplay VS Poetics, Immersion VS Fluidity. From these concepts, we developed four key strategies.Then, we created a database with eligible i-docs and we established evaluation grids to analyse them. Our goal is to establish a general panorama of the i-docs production and to provide a new form of categorization based on the perspective of Narrative.We developed graphs to position i-docs within a dual-variable scale according to the opposite poles. On the horizontal axis, there are fundamental aspects of Documentary; on the vertical axis, there are characteristics of the introduction of the new medium - the internet - and its features: connectivity and interactivity.Finally, we applied this model to five random i-docs in order to verify its applicability for each criterion under analysis.As this proposal is part of an ongoing research project about the relationship of interactive documentaries with narrative and storytelling, this categorization proposal is still under development and will be refined along with the other steps of the project, namely the analysis of the i-docs database and interviews with authors.},
booktitle = {Proceedings of the 9th International Conference on Digital and Interactive Arts},
articleno = {49},
numpages = {9},
keywords = {Authorship, Documentary, Interactivity, Narrative, Storytelling},
location = {Braga, Portugal},
series = {ARTECH 2019}
}

@article{10.1145/3530691,
author = {Khatun, Aisha and Rahman, Anisur and Islam, Md Saiful and Chowdhury, Hemayet Ahmed and Tasnim, Ayesha},
title = {Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3530691},
doi = {10.1145/3530691},
abstract = {Authorship Attribution is the task of creating an appropriate characterization of text that captures the authors’ writing style to identify the original author of a given piece of text. With increased anonymity on the internet, this task has become increasingly crucial in various security and plagiarism detection fields. Despite significant advancements in other languages such as English, Spanish, and Chinese, Bangla lacks comprehensive research in this field due to its complex linguistic feature and sentence structure. Moreover, existing systems are not scalable with the increasing number of authors, and performance drops with the small number of samples per author. In this paper, we propose the use of Average-Stochastic Gradient Descent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an effective transfer learning approach that addresses the problem of complex linguistic features extraction and scalability for authorship attribution in Bangla Literature (AABL). We analyze the effect of different tokenization, such as word, sub-word, and character level tokenization, and demonstrate the effectiveness of these tokenizations in the proposed model. Moreover, we introduce the publicly available Bangla Authorship Attribution Dataset of 16 authors (BAAD16) containing 17,966 sample texts and 13.4+ million words to solve the standard dataset scarcity problem and release six variations of pre-trained language models for use in any Bangla NLP downstream task. For evaluation, we used our developed BAAD16 dataset as well as other publicly available datasets. Empirically, our proposed model outperformed state-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset. Furthermore, we showed that the proposed system scales much better with the increasing number of authors, and performance remains steady even with few training samples.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
keywords = {Authorship attribution, Transfer Learning, Language model, AWD-LSTM, Bangla}
}

@article{10.1145/3478087,
author = {Fang, Zhihan and Yang, Yu and Yang, Guang and Xian, Yikuan and Zhang, Fan and Zhang, Desheng},
title = {CellSense: Human Mobility Recovery via Cellular Network Data Enhancement},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478087},
doi = {10.1145/3478087},
abstract = {Data from the cellular network have been proved as one of the most promising way to understand large-scale human mobility for various ubiquitous computing applications due to the high penetration of cellphones and low collection cost. Existing mobility models driven by cellular network data suffer from sparse spatial-temporal observations because user locations are recorded with cellphone activities, e.g., calls, text, or internet access. In this paper, we design a human mobility recovery system called CellSense to take the sparse cellular billing data (CBR) as input and outputs dense continuous records to recover the sensing gap when using cellular networks as sensing systems to sense the human mobility. There is limited work on this kind of recovery systems at large scale because even though it is straightforward to design a recovery system based on regression models, it is very challenging to evaluate these models at large scale due to the lack of the ground truth data. In this paper, we explore a new opportunity based on the upgrade of cellular infrastructures to obtain cellular network signaling data as the ground truth data, which log the interaction between cellphones and cellular towers at signal levels (e.g., attaching, detaching, paging) even without billable activities. Based on the signaling data, we design a system CellSense for human mobility recovery by integrating collective mobility patterns with individual mobility modeling, which achieves the 35.3% improvement over the state-of-the-art models. The key application of our recovery model is to take regular sparse CBR data that a researcher already has, and to recover the missing data due to sensing gaps of CBR data to produce a dense cellular data for them to train a machine learning model for their use cases, e.g., next location prediction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {100},
numpages = {22},
keywords = {Cellular Network, Human Mobility, Signaling}
}

@inproceedings{10.1145/3369740.3372752,
author = {Patel, Yashwant Singh and Banerjee, Sourasekhar and Misra, Rajiv and Das, Sajal K.},
title = {Low-Latency Energy-Efficient Cyber-Physical Disaster System Using Edge Deep Learning},
year = {2020},
isbn = {9781450377515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369740.3372752},
doi = {10.1145/3369740.3372752},
abstract = {Reported works on cyber-physical disaster systems (CPDS) deal with the assessment of loss and damage aftermath of a large-scale disaster such as earthquake, wildfire, and cyclone, etc. involves collecting data from the IoT devices sent to the cloud data centers for analysis, often causes high bandwidth usage with substantial delay. In our work, we have shown to eliminate bandwidth cost and reducing latency substantially suitable for post-disaster response for rescue operations. We propose a low-latency and energy-efficient CPDS applying cloud-IoT-edge by bringing intelligence and infer-encing proximity to the disaster site to detect the disaster events in real-time and inform to the rescue teams. The edge computing model of CPDS uses convolutional neural network (CNN) with MobileNetV2 lightweight model and gradient weighted class activation mapping (Grad-CAM++) to locate and quantify degree of the damage into classes- severe, mild, and no damage. We implemented CPDS on a real-world laboratory testbed that comprises resource-constrained edge devices (Raspberry Pi, smartphones, and PCs) and docker-based containerization of deep learning models and analyzed the computational complexity. With the rigorous experiments of the proposed approach, we evaluated the performance in terms of classification accuracy, energy saving, and end-to-end (E2E) delay comparing with the current state-of-the-art approaches.},
booktitle = {Proceedings of the 21st International Conference on Distributed Computing and Networking},
articleno = {34},
numpages = {6},
keywords = {Containerization, Cyber-physical systems, Deep learning, Disaster damage assessment, Edge computing, Energy efficiency},
location = {Kolkata, India},
series = {ICDCN '20}
}

@inproceedings{10.1145/3442442.3451140,
author = {Zhang, Lei and Liu, Jie and Zhang, Fuquan and Mao, Yu},
title = {Distributed Fog Computing Based on Improved LT codes for Deep Learning in Web of Things},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451140},
doi = {10.1145/3442442.3451140},
abstract = {With the rapid development of the Web of Things, there have been a lot of sensors deployed. Advanced knowledge can be achieved by deep learning method and easier integration with open Web standards. A large number of the data generated by sensors required extra processing resources due to the limited resources of the sensors. Due to the limitation of bandwidth or requirement of low latency, it is impossible to transfer such large amounts of data to cloud servers for processing. Thus, the concept of distributed fog computing has been proposed to process such big data into knowledge in real-time. Large scale fog computing system is built using cheap devices, denotes as fog nodes. Therefore, the resiliency to fog node failures should be considered in design of distributed fog computing. LT codes (LTC) have important applications in the design of modern distributed computing, which can reduce the latency of the computing tasks, such as matrix multiplication in deep learning methods. In this paper, we consider that fog nodes may be failure, and an improved LT codes are applied to matrix multiplication of distributed fog computing process to reduce latency. Numerical results show that the improved LTC based scheme can reduce average overhead and degree simultaneously, which reduce the latency and computation complexity of distributed fog computing.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {57–62},
numpages = {6},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3482632.3483067,
author = {Yu, Wenjun},
title = {Application of Big Data Technology in the Innovation of University Education Management Work},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483067},
doi = {10.1145/3482632.3483067},
abstract = {With the development of higher education in China, the scale of colleges and universities is expanding rapidly and the information of teaching management is developing rapidly. Facing the growing management information, it is inevitable to carry out education management innovation to meet the needs of the times. The use of teachers' resources and the assessment of teachers' classroom teaching quality must be solved by modern means to achieve the rational use and optimal allocation of teaching resources and improve the effectiveness of teaching management. The purpose of this paper is to study the application of big data(BD) technology in the innovation of university education management. This paper uses case study and literature research methods, combines data warehousing and data mining technologies in BD technology for evaluating teaching data with data warehousing as an organizational tool, and analyzes the current situation and future development trends of database technology data application in information management of educational administration construction in universities. The experimental results show that the ID3 algorithm performs better when there are fewer number of things and is about 10 seconds faster than the Apriori algorithm. But when the number of things are much more, the Apriori algorithm is significantly better than the ID3 algorithm. Moreover, the Apriori algorithm is more suitable for mining the deep laws of ungrasped objective things, and reveals unknown dependencies between data for rule mining with fuzzy concepts. Therefore, through analysis of data warehouse, data mining techniques and mining algorithms, by data mining of a large amount of assessment data, this paper finds that combining decision trees and association rule algorithms in data mining algorithms with teaching systems, data mining techniques introduced in educational assessment systems for assessment, not only improves the scientific nature of educational management, but also improves the effectiveness of digital education construction.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {988–992},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3418094.3418118,
author = {Udo, Ifiok J. and Ekpenyong, Moses E.},
title = {Improving Emergency Healthcare Response using Real-Time Collaborative Technology},
year = {2020},
isbn = {9781450377768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418094.3418118},
doi = {10.1145/3418094.3418118},
abstract = {Harnessing geospatial technologies (GPS, GIS, internet mapping and remote sensing) can promote collaboration of humans, environmental, and healthcare resources-for real-time decision support in emergency healthcare scenarios. Oftentimes, high death rates are recorded by emergency rescue workers due to infrastructural deficit and sparsity of ambulatory services within the patients' location. Also, acute shortage of health personnel may pose great risk to patients requiring emergency healthcare services, as limited knowledge of healthcare data can prevent proactive, just-in-time response to emergency, and dearth of appropriate healthcare policies. This study therefore proposes a geospatial recommender framework that connects patients with healthcare providers and other relevant stakeholders; to exploit and filter in context, emergency healthcare information and knowledge-for enhanced response strategy that minimizes fatalities among patients. We examine case study data within the African context and discuss infrastructural challenges, opportunities and design implications, to demonstrate the feasibility of our framework. The immediate benefit of the framework is enhanced specialty care for home-bound patients or in emergency cases where distance appears prohibitive. Our strategy will certainly scale up complex healthcare interventions to large populations, as a new paradigm for evidence-based intervention with state-of-the-art technology for optimized services is guaranteed.},
booktitle = {Proceedings of the 4th International Conference on Medical and Health Informatics},
pages = {165–173},
numpages = {9},
keywords = {Emergency response, collaborative technology, geospatial recommender, healthcare decision},
location = {Kamakura City, Japan},
series = {ICMHI '20}
}

@inproceedings{10.1145/3400302.3415673,
author = {Wang, Tianyu and Zhu, Wenbin and Ma, Qun and Shen, Zhaoyan and Shao, Zili},
title = {ABACUS: address-partitioned bloom filter on address checking for UniquenesS in IoT blockchain},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415673},
doi = {10.1145/3400302.3415673},
abstract = {DAG-based blockchain systems have been deployed to enable trustworthy peer-to-peer transactions for IoT devices. Unique address checking, as a key part of transaction generation for privacy and security protection in DAG-based blockchain systems, incurs big latency overhead and degrades system throughput.In this paper, we propose a Bloom-filter-based approach, called ABACUS to optimize the unique address checking process. We partition the large address space into multiple small subspaces and apply one Bloom filter to perform uniqueness checking for all addresses in a subspace. Specifically, we propose a two-level address space mechanism so as to strike a balance between the checking efficiency and the memory/storage space overhead of the Bloom filter design. A bucket-based scalable Bloom filter design is proposed to address the growth of used addresses and provide the checking latency guarantee with efficient I/O access through storing all sub-Bloom-filters together in one bucket. To further reduce disk I/Os, ABACUS incorporates an in-memory write buffer and a read-only cache.We have implemented ABACUS into IOTA, one of the most widely used DAG-based blockchain systems, and conducted a series of experiments on a private IOTA system. The experimental results show that ABACUS can significantly reduce the transaction generation time by up to four orders of magnitude while achieving up to 3X boost on the system throughput, compared with the original design.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {10},
numpages = {7},
keywords = {DAG-based blockchain, bloom filter, performance boost},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1145/3468266,
author = {Roy, Satyaki and Ghosh, Preetam and Ghosh, Nirnay and Das, Sajal K.},
title = {Transcriptional Regulatory Network Topology with Applications to Bio-inspired Networking: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3468266},
doi = {10.1145/3468266},
abstract = {The advent of the edge computing network paradigm places the computational and storage resources away from the data centers and closer to the edge of the network largely comprising the heterogeneous IoT devices collecting huge volumes of data. This paradigm has led to considerable improvement in network latency and bandwidth usage over the traditional cloud-centric paradigm. However, the next generation networks continue to be stymied by their inability to achieve adaptive, energy-efficient, timely data transfer in a dynamic and failure-prone environment—the very optimization challenges that are dealt with by biological networks as a consequence of millions of years of evolution. The transcriptional regulatory network (TRN) is a biological network whose innate topological robustness is a function of its underlying graph topology. In this article, we survey these properties of TRN and the metrics derived therefrom that lend themselves to the design of smart networking protocols and architectures. We then review a body of literature on bio-inspired networking solutions that leverage the stated properties of TRN. Finally, we present a vision for specific aspects of TRNs that may inspire future research directions in the fields of large-scale social and communication networks.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {166},
numpages = {36},
keywords = {Robustness, motifs, gene interaction, energy efficiency, IoT}
}

@inproceedings{10.1145/3551662.3560926,
author = {Fardin, Ivan and Milani, Stefano and Cuomo, Francesca and Chatzigiannakis, Ioannis},
title = {Enabling Edge Computing over LoRaWAN: A Device-Gateway Coordination Protocol},
year = {2022},
isbn = {9781450394826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551662.3560926},
doi = {10.1145/3551662.3560926},
abstract = {The freedom of the LoRaWAN license-free ad-hoc deployment model can significantly reduce the complexity of network deployment, however, it introduces certain problems that hinder network scalability and performance. First, Network Gateways simply forward frames to the centralised Network Server without the possibility of becoming Edge computing processing elements. Second, Network Gateways may have overlapping areas of network coverage (a) resulting in an increase of network traffic at the network backbone as frames are relayed to the Network Server multiple times, (b) may cause unexpected frame collisions and duty-cycle exhaustion. In this paper, a novel decentralised algorithm is presented that assigns each IoT Device to a single Network Gateway so that (a) duplicate message deliveries are completely avoided, (b) Network Gateways can become an intermediate operations layer between the IoT devices and the Network Server providing computational and storage resources. The proposed protocol is implemented in the OMNeT++ simulator and evaluated based on datasets collected from long-term real-world deployments. To improve the accuracy of the experiments in large-scale and dense urban deployments the OMNeT++ LoRa interference model is extended by considering the non-perfect orthogonality of the LoRa Spreading Factors and the channel adjacency. The performance evaluation highlights the benefits of the distributed approach proposed here and provides valuable indications on the overall performance of the network.},
booktitle = {Proceedings of the 12th ACM International Symposium on Design and Analysis of Intelligent Vehicular Networks and Applications},
pages = {23–30},
numpages = {8},
keywords = {contention, performance evaluation, protocol design, scalability},
location = {Montreal, Quebec, Canada},
series = {DIVANet '22}
}

@inproceedings{10.1145/3445969.3450426,
author = {Shakarami, Mehrnoosh and Sandhu, Ravi},
title = {Role-Based Administration of Role-Based Smart Home IoT},
year = {2021},
isbn = {9781450383196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445969.3450426},
doi = {10.1145/3445969.3450426},
abstract = {Using role-based access control (RBAC) to manage RBAC is among RBAC's attractive benefits, contributing to its long-standing dominance in practice. Administrative models facilitate management of (mostly configuration) changes in the underlying operational models. Overall system security is crucially dependent on both the administrative and operational models. In this paper, we develop an RBAC administrative model to manage authorization assignments in the EGRBAC (enhanced generalized role-based access control) operational model for smart home IoT. We design the administrative model based on pairwise disjoint Administrative Units, each of which contains a uniquely assigned administrative role and a set of administrative tasks. Administrative tasks determine the administrative permissions available to manage the operational model assignments. We begin with a model containing a single administrative unit and then extend it to include additional units. Multiple administrative units enable decentralized administration which could be adapted to provide scalability in inherently distributed and large-scale environments beyond smart home, such as smart buildings or smart campuses. We provide formalism of our proposed model and illustrate it by specifying operational and administrative use cases. Although, the model is proposed based on a specific smart home operational model, our approach could be applied to environments with similar dynamics.},
booktitle = {Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {49–58},
numpages = {10},
keywords = {RBAC administrative model, decentralized administration, smart home},
location = {Virtual Event, USA},
series = {SAT-CPS '21}
}

@inproceedings{10.1145/3397536.3422266,
author = {Mellou, Konstantina and Marshall, Luke and Chintalapudi, Krishna and Jaillet, Patrick and Menache, Ishai},
title = {Optimizing Onsite Food Services at Scale},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422266},
doi = {10.1145/3397536.3422266},
abstract = {Large food-service companies typically support a wide range of operations (catering, vending machines, repairs), each with different operational characteristics (manpower, vehicles, tools, timing constraints, etc.). While the advances in Internet-based technologies facilitate the adoption of automated scheduling systems, the complexity and heterogeneity of the different operations hinders the design of comprehensive optimization solutions. Indeed, our collaboration with Compass Group, one of the largest food-service companies in the world, reveals that many of its workforce assignments are done manually due to the lack of scheduling solutions that can accommodate the complexity of operational constraints. Further, the diversity in the nature of operations prevents collaboration and sharing of resources among various services such as catering and beverage distribution, leading to an inflated fleet size.To address these challenges, we design a unified optimization framework, which can be applied to various food-service operations. Our design combines neighborhood search methods and Linear Programming techniques. We test our framework on real food-service request data from a large Compass Group customer, the Puget-Sound Microsoft Campus. Our results show that our approach scales well while yielding fleet size reductions of around 2x. Further, using our unified framework to simultaneously schedule the operations of two different divisions (catering, water distribution) yields 20% additional savings.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {618–629},
numpages = {12},
keywords = {planning, route optimization, workforce scheduling},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/3603216.3624960,
author = {Lopes, Daniela and Castro, Daniel and Barradas, Diogo and Santos, Nuno},
title = {TIGER: Tor Traffic Generator for Realistic Experiments},
year = {2023},
isbn = {9798400702358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603216.3624960},
doi = {10.1145/3603216.3624960},
abstract = {Tor is the most widely adopted anonymity network, helping safeguard the privacy of Internet users, including journalists and human rights activists. However, effective attacks aimed at deanonymizing Tor users' remains a significant threat. Unfortunately, evaluating the impact such attacks by collecting realistic Tor traffic without gathering real users' data poses a significant challenge.  This paper introduces TIGER (Tor traffIc GEnerator for Realistic experiments), a novel framework that automates the generation of realistic Tor traffic datasets towards improving our understanding of the robustness of Tor's privacy mechanisms. To this end, TIGER allows researchers to design large-scale testbeds and collect data on the live Tor network while responsibly avoiding the need to collect real users' traffic. We motivate the usefulness of TIGER by collecting a preliminary dataset with applicability to the evaluation of traffic confirmation attacks and defenses.},
booktitle = {Proceedings of the 22nd Workshop on Privacy in the Electronic Society},
pages = {147–152},
numpages = {6},
keywords = {dataset generation, tor, traffic analysis, web privacy},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {WPES '23}
}

@inproceedings{10.1145/3396851.3397687,
author = {Lukuyu, June and Muhebwa, Aggrey and Taneja, Jay},
title = {Fish and Chips: Converting Fishing Boats for Electric Mobility to Serve as Minigrid Anchor Loads},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3397687},
doi = {10.1145/3396851.3397687},
abstract = {Though electricity access remains out of reach for roughly one billion primarily rural and low-income people, crucial strides have been made in developing new pathways for connecting households and businesses to electricity supplies. Among these, decentralized minigrids - typically comprised of generation, storage, and a medium- and low-voltage distribution network - have considerable technical promise for balancing recent advances in decentralized generation as well as grid sensing and communication systems with the overwhelming economies-of-scale enjoyed by electricity grids. However, low revenues and, in response, high tariffs necessary for cost recovery stifle the widespread development of this promising pathway for electrification.In this paper, we study techniques for addressing the principal challenge for sustainable minigrids: demand stimulation among rural customers. Specifically, we evaluate the potential for conversion of diesel-based fishing boats in Lake Victoria to electric motor and battery-based systems that can provide a crucial anchor load for a nascent 650 kWp hybrid solar-battery-diesel minigrid. We conduct a survey among fishing boat operators (n = 69) to characterize the target population and deploy a custom tracking system to measure fishing boat movement patterns. Using these primary data along with secondary data on customer consumption, we select a candidate electric mobility system, create synthetic loads of residential and business customers, and construct technical and financial models of the complete minigrid system. We then use these models to evaluate the excess capacity on the minigrid for electric boats, evaluate the tradeoffs among electric mobility and manufacturing on the minigrid, and assess the impacts of demand response capabilities for charging the boats. We find that electric boat charging contributes to at least 17% more consumption per day resulting in substantial technical as well as financial value to the minigrid system, though perhaps at the cost of additional use of the system's backup diesel generator. We find that adding shifting capabilities to electric boat charging can save up to 6% of diesel expenditures at little to no impact on the system Net Present Value. We combine these minigrid-scale evaluations with design considerations for a future boat tracking system, providing guidance for minigrid designers and operators to incorporate the potentially attractive load class of electric mobility systems.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {208–219},
numpages = {12},
keywords = {demand stimulation, flexible load, minigrid},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1145/3320269.3384755,
author = {Sisodia, Devkishen and Li, Jun and Jiao, Lei},
title = {In-Network Filtering of Distributed Denial-of-Service Traffic with Near-Optimal Rule Selection},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384755},
doi = {10.1145/3320269.3384755},
abstract = {A recent trend to mitigate large-scale distributed denial-of-service (DDoS) attacks is in-network filtering, where victims can deploy traffic-filtering rules in networks other than their own. However, given multiple constraints, such as the number of rules a victim can afford to deploy, the set of rules that DDoS defense entities allow a victim to deploy, and the amount of collateral damage to limit, the selection of rules has a large impact on the efficacy of an in-network filtering solution.In this paper, we introduce a new, offer-based operational model for in-network DDoS defense and formulate the NP-hard rule selection problem for this model. We then design an algorithm that overcomes the fundamental limitations of the classical ACO framework and transform it with several key changes to make it applicable to the domain of in-network DDoS defense. Finally, we use a real-world-based Internet routing topology and two real-world DDoS traces, along with one synthetic trace that follows the attack distribution of the recent Mirai DDoS attack, to evaluate the efficacy and runtime of our algorithm against four other rule selection algorithms, and show our algorithm is near-optimal.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {153–164},
numpages = {12},
keywords = {DDoS-filtering rule selection, distributed denial-of-service (DDoS), in-network DDoS filtering, rule selection optimization},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@article{10.1145/3629979,
author = {Chen, Jeffrey and Jun, Sang-Woo and Hong, Sehwan and He, Warrick and Moon, Jinyeong},
title = {Eciton: Very Low-power Recurrent Neural Network Accelerator for Real-time Inference at the Edge},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3629979},
doi = {10.1145/3629979},
abstract = {This article presents Eciton, a very low-power recurrent neural network accelerator for time series data within low-power edge sensor nodes, achieving real-time inference with a power consumption of 17&nbsp;mW under load. Eciton reduces memory and chip resource requirements via 8-bit quantization and hard sigmoid activation, allowing the accelerator as well as the recurrent neural network model parameters to fit in a low-cost, low-power Lattice iCE40 UP5K FPGA.We evaluate Eciton on multiple, established time-series classification applications including predictive maintenance of mechanical systems, sound classification, and intrusion detection for IoT nodes. Binary and multi-class classification edge models are explored, demonstrating that Eciton can adapt to a variety of deployable environments and remote use cases. Eciton demonstrates real-time processing at a very low power consumption with minimal loss of accuracy on multiple inference scenarios with differing characteristics, while achieving competitive power efficiency against the state-of-the-art of similar scale. We show that the addition of this accelerator actually reduces the power budget of the sensor node by reducing power-hungry wireless transmission. The resulting power budget of the sensor node is small enough to be powered by a power harvester, potentially allowing it to run indefinitely without a battery or periodic maintenance.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {feb},
articleno = {16},
numpages = {25},
keywords = {LSTM, RNN, predictive maintenance, binary classification, multi-class classification, quantization, low power, edge IoT, CPS, neural networks, FPGA, iCE40}
}

@article{10.1145/3444692,
author = {Varghese, Blesson and Wang, Nan and Bermbach, David and Hong, Cheol-Ho and Lara, Eyal De and Shi, Weisong and Stewart, Christopher},
title = {A Survey on Edge Performance Benchmarking},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3444692},
doi = {10.1145/3444692},
abstract = {Edge computing is the next Internet frontier that will leverage computing resources located near users, sensors, and data stores to provide more responsive services. Therefore, it is envisioned that a large-scale, geographically dispersed, and resource-rich distributed system will emerge and play a key role in the future Internet. However, given the loosely coupled nature of such complex systems, their operational conditions are expected to change significantly over time. In this context, the performance characteristics of such systems will need to be captured rapidly, which is referred to as performance benchmarking, for application deployment, resource orchestration, and adaptive decision-making. Edge performance benchmarking is a nascent research avenue that has started gaining momentum over the past five years. This article first reviews articles published over the past three decades to trace the history of performance benchmarking from tightly coupled to loosely coupled systems. It then systematically classifies previous research to identify the system under test, techniques analyzed, and benchmark runtime in edge performance benchmarking.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {66},
numpages = {33},
keywords = {Edge computing, benchmark runtime, edge performance benchmarking, system under test, techniques analyzed}
}

@inproceedings{10.1145/3416010.3423227,
author = {Mendula, Matteo and Khodadadeh, Siavash and Bacanli, Salih Safa and Zehtabian, Sharare and Sheikh, Hassam Ullah and B\"{o}l\"{o}ni, Ladislau and Turgut, Damla and Bellavista, Paolo},
title = {Interaction and Behaviour Evaluation for Smart Homes: Data Collection and Analytics in the ScaledHome Project},
year = {2020},
isbn = {9781450381178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416010.3423227},
doi = {10.1145/3416010.3423227},
abstract = {The smart home concept can significantly benefit from predictive models that take proactive management operations on home actuators, based on users' behavior evaluation. In this paper, we use a small-scale physical model, the ScaledHome-2 testbed, to experiment with the evolution of measurements in a suburban home under different environmental scenarios. We start from the observation that, for a home to become smart, in addition to IoT sensors and actuators, we also need a predictive model of how actions taken by inhabitants and home actuators affect the internal environment of the home, reflected in the sensor readings. In this paper, we propose a technique to create such a predictive model through machine learning in various simulated weather scenarios. This paper also contributes to the literature in the field by quantitatively comparing several machine learning algorithms (K-nearest neighbor, regression trees, Support Vector Machine regression, and Long Short Term Memory deep neural networks) in their ability to create accurate and generalizable predictive models for smart homes.},
booktitle = {Proceedings of the 23rd International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {225–233},
numpages = {9},
keywords = {energy, iot, optimization, scaling, simulation},
location = {Alicante, Spain},
series = {MSWiM '20}
}

@inproceedings{10.1145/3421766.3421830,
author = {Huang, Jun and Wu, Miao and Huang, Yangshan},
title = {Research and Application of eID Digital Identity},
year = {2020},
isbn = {9781450375535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421766.3421830},
doi = {10.1145/3421766.3421830},
abstract = {At present, Internet applications generally use "name and citizen ID number" to identify the online users. Although it plays a unique role in distinguishing subjects, it also causes large-scale personal information disclosure, which frequently leads to identity theft, account theft, telecommunication and network fraud. To deal with these problems, the article proposes to use eID digital identity to build a national unified digital identity system, and elaborates the design principle and application of eID digital identity.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {266–270},
numpages = {5},
keywords = {data circulation, eID digital identity, identity authentication, privacy protection},
location = {Manchester, United Kingdom},
series = {AIAM2020}
}

@inproceedings{10.1145/3613424.3614266,
author = {Amar, Saar and Chisnall, David and Chen, Tony and Filardo, Nathaniel Wesley and Laurie, Ben and Liu, Kunyan and Norton, Robert and Moore, Simon W. and Tao, Yucong and Watson, Robert N. M. and Xia, Hongyan},
title = {CHERIoT: Complete Memory Safety for Embedded Devices},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614266},
doi = {10.1145/3613424.3614266},
abstract = {The ubiquity of embedded devices is apparent. The desire for increased functionality and connectivity drives ever larger software stacks, with components from multiple vendors and entities. These stacks should be replete with isolation and memory safety technologies, but existing solutions impinge upon development, unit cost, power, scalability, and/or real-time constraints, limiting their adoption and production-grade deployments. As memory safety vulnerabilities mount, the situation is clearly not tenable and a new approach is needed. To slake this need, we present a novel adaptation of the CHERI capability architecture, co-designed with a green-field, security-centric RTOS. It is scaled for embedded systems, is capable of fine-grained software compartmentalization, and provides affordances for full inter-compartment memory safety. We highlight central design decisions and offloads and summarize how our prototype RTOS uses these to enable memory-safe, compartmentalized applications. Unlike many state-of-the-art schemes, our solution deterministically (not probabilistically) eliminates memory safety vulnerabilities while maintaining source-level compatibility. We characterize the power, performance, and area microarchitectural impacts, run microbenchmarks of key facilities, and exhibit the practicality of an end-to-end IoT application. The implementation shows that full memory safety for compartmentalized embedded systems is achievable without violating resource constraints or real-time guarantees, and that hardware assists need not be expensive, intrusive, or power-hungry.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {641–653},
numpages = {13},
location = {<conf-loc>, <city>Toronto</city>, <state>ON</state>, <country>Canada</country>, </conf-loc>},
series = {MICRO '23}
}

@inproceedings{10.1145/3320269.3384723,
author = {Huber, Manuel and Hristozov, Stefan and Ott, Simon and Sarafov, Vasil and Peinado, Marcus},
title = {The Lazarus Effect: Healing Compromised Devices in the Internet of Small Things},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384723},
doi = {10.1145/3320269.3384723},
abstract = {We live in a time when billions of IoT devices are being deployed and increasingly relied upon. This makes ensuring their availability and recoverability in case of a compromise a paramount goal. The large and rapidly growing number of deployed IoT devices make manual recovery impractical, especially if the devices are dispersed over a large area. Thus, there is a need for a reliable and scalable remote recovery mechanism that works even after attackers have taken full control over devices, possibly misusing them or trying to render them useless.To tackle this problem, we present Lazarus, a system that enables the remote recovery of compromised IoT devices. With Lazarus, an IoT administrator can remotely control the code running on IoT devices unconditionally and within a guaranteed time bound. This makes recovery possible even in case of severe corruption of the devices' software stack. We impose only minimal hardware requirements, making Lazarus applicable even for low-end constrained off-the-shelf IoT devices. We isolate Lazarus's minimal recovery trusted computing base from untrusted software both in time and by using a trusted execution environment. The temporal isolation prevents secrets from being leaked through side-channels to untrusted software. Inside the trusted execution environment, we place minimal functionality that constrains untrusted software at runtime.We implement Lazarus on an ARM Cortex-M33-based microcontroller in a full setup with an IoT hub, device provisioning and secure update functionality. Our prototype can recover compromised embedded OSs and bare-metal applications and prevents attackers from bricking devices, for example, through flash wear out. We show this at the example of FreeRTOS, which requires no modifications but only a single additional task. Our evaluation shows negligible runtime performance impact and moderate memory requirements.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {6–19},
numpages = {14},
keywords = {availability, cyber resilience, dominance primitive, embedded security, recovery, trusted computing},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@inproceedings{10.1145/3433174.3433603,
author = {Krundyshev, Vasiliy},
title = {Neural network approach to assessing cybersecurity risks in large-scale dynamic networks},
year = {2021},
isbn = {9781450387514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433174.3433603},
doi = {10.1145/3433174.3433603},
abstract = {The problem, the solution of which the paper is devoted to, is an assessment of the cybersecurity risks of large-scale dynamic networks of smart city (IoT, IIoT, MANET, VANET) in the current situation of cyber threats. Analysis of relevant cyber threats for such systems showed that the main threats are those aimed at disrupting the management processes of enterprises, urban infrastructure, and not at stealing personal data. As a result of a detailed review of existing risk assessment methods, it was found that traditional cyber-risk analysis strategies developed for static computer networks do not meet the requirements and cannot be directly applied in the construction and evaluation of heterogeneous, flexible, reconfigurable information systems based on dynamic networks. In this paper, the author proposes a neural network approach to assessing cybersecurity risks, based on a quantitative assessment and takes into account the use of a large number of devices and their constant interaction. A three-layer perceptron was chosen as a model of a neural network, datasets were generated synthetically using a network simulator NS-3. Experimental results showed that the neural network model proposed by the author allows for rapidly changing conditions to unambiguously and reasonably assess the risks of cybersecurity. The developed risk assessment method is comprehensive and adapted to various types of objects in large dynamic networks of a smart city.},
booktitle = {13th International Conference on Security of Information and Networks},
articleno = {32},
numpages = {8},
keywords = {ANN, Cybersecurity, Large-Scale Network, Machine Learning, Neural Network, Perceptron, Quantitative Risk, Risk Assessment, Smart City},
location = {Merkez, Turkey},
series = {SIN 2020}
}

@inproceedings{10.1145/3485983.3494868,
author = {Deccio, Casey and Yadav, Tarun and Bennett, Nathaniel and Hilton, Alden and Howe, Michael and Norton, Tanner and Rohde, Jacob and Tan, Eunice and Taylor, Bradley},
title = {Measuring email sender validation in the wild},
year = {2021},
isbn = {9781450390989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485983.3494868},
doi = {10.1145/3485983.3494868},
abstract = {Email is a critical Internet application, and its security is important. The Sender Policy Framework (SPF), DomainKeys Identified Mail (DKIM), and Domain-based Message Authentication, Reporting, and Conformance (DMARC) were developed to enable mail servers to detect and reject email coming from fraudulent sources. In this paper we study the state of SPF, DKIM, and DMARC validation across a large number of mail servers, the first such study at scale that we know of. We consider two behaviors of sender-validating mail servers: behavior when an email with a valid sender is received and behavior when an email from a invalid sender is received. Our techniques allow us to elicit SPF, DKIM, and DMARC validation behavior of the servers without spam. We find that as many as 85% of mail servers are deploying SPF validation, and over half are deploying all three mechanisms: SPF, DKIM, and DMARC. We also observe there are some nuanced behaviors with regard to adherence to the SPF specification.},
booktitle = {Proceedings of the 17th International Conference on Emerging Networking EXperiments and Technologies},
pages = {230–242},
numpages = {13},
keywords = {DKIM, DMARC, DNS, SPF, email, measurement, security},
location = {Virtual Event, Germany},
series = {CoNEXT '21}
}

@article{10.14778/3611540.3611559,
author = {Shen, Chunhui and Ouyang, Qianyu and Li, Feibo and Liu, Zhipeng and Zhu, Longcheng and Zou, Yujie and Su, Qing and Yu, Tianhuan and Yi, Yi and Hu, Jianhong and Zheng, Cen and Wen, Bo and Zheng, Hanbang and Xu, Lunfan and Pan, Sicheng and Wu, Bin and He, Xiao and Li, Ye and Tan, Jian and Wang, Sheng and Pei, Dan and Zhang, Wei and Li, Feifei},
title = {Lindorm TSDB: A Cloud-Native Time-Series Database for Large-Scale Monitoring Systems},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611559},
doi = {10.14778/3611540.3611559},
abstract = {Internet services supported by large-scale distributed systems have become essential for our daily life. To ensure the stability and high quality of services, diverse metric data are constantly collected and managed in a time-series database to monitor the service status. However, when the number of metrics becomes massive, existing time-series databases are inefficient in handling high-rate data ingestion and queries hitting multiple metrics. Besides, they all lack the support of machine learning functions, which are crucial for sophisticated analysis of large-scale time series. In this paper, we present Lindorm TSDB, a distributed time-series database designed for handling monitoring metrics at scale. It sustains high write throughput and low query latency with massive active metrics. It also allows users to analyze data with anomaly detection and time series forecasting algorithms directly through SQL. Furthermore, Lindorm TSDB retains stable performance even during node scaling. We evaluate Lindorm TSDB under different data scales, and the results show that it outperforms two popular open-source time-series databases on both writing and query, while executing time-series machine learning tasks efficiently.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3715–3727},
numpages = {13}
}

@inproceedings{10.1145/3534678.3542612,
author = {Wen, Qingsong and Yang, Linxiao and Zhou, Tian and Sun, Liang},
title = {Robust Time Series Analysis and Applications: An Industrial Perspective},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542612},
doi = {10.1145/3534678.3542612},
abstract = {Time series analysis is ubiquitous and important in various areas, such as Artificial Intelligence for IT Operations (AIOps) in cloud computing, AI-powered Business Intelligence (BI) in E-commerce, Artificial Intelligence of Things (AIoT), etc. In real-world scenarios, time series data often exhibit complex patterns with trend, seasonality, outlier, and noise. In addition, as more time series data are collected and stored, how to handle the huge amount of data efficiently is crucial in many applications. We note that these significant challenges exist in various tasks like forecasting, anomaly detection, and fault cause localization. Therefore, how to design effective and efficient time series models for different tasks, which are robust to address the aforementioned challenging patterns and noise in real-world scenarios, is of great theoretical and practical interests. In this tutorial, we provide a comprehensive and organized tutorial on the state-of-the-art algorithms of robust time series analysis, ranging from traditional statistical methods to the most recent deep learning based methods. We will not only introduce the principle of time series algorithms, but also provide insights into how to apply them effectively in practical real-world industrial applications. Specifically, we organize the tutorial in a bottom-up framework. We first present preliminaries from different disciplines including robust statistics, signal processing, optimization, and deep learning. Then, we identify and discuss those most-frequently processing blocks in robust time series analysis, including periodicity detection, trend filtering, seasonal-trend decomposition, and time series similarity. Lastly, we discuss recent advances in multiple time series tasks including forecasting, anomaly detection, fault cause localization, and autoscaling, as well as practical lessons of large-scale time series applications from an industrial perspective.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4836–4837},
numpages = {2},
keywords = {anomaly detection, forecasting, robustness, root cause analysis, time series},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3384943.3409430,
author = {Lu, Xin and Guan, Zhitao},
title = {A Blockchain-based Trading Matching Scheme in Energy Internet},
year = {2020},
isbn = {9781450376105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384943.3409430},
doi = {10.1145/3384943.3409430},
abstract = {With the advent of the Industry 4.0 era, the energy industry system revolution will also face new challenges. Against the backdrop of the global energy crisis, renewable energy has received great attention from countries around the world. Due to the characteristics of geographical dispersion, randomness, volatility and uncontrollability of renewable energy, the centralized traditional electrical energy networks cannot meet the management requirements of large-scale access to renewable energy. The emergence of the energy internet has enabled the effective use of renewable energy. As a distributed trading and data management technology, blockchain solves the problem of distributed electrical energy trading in the energy Internet. However, the distributed energy trading based on blockchain cannot satisfy the rational matching of trading objects. Therefore, we propose a blockchain-based electrical energy trading system BC-TMS Blockchain-based trading matching system, which can reasonably match trading objects according to the user's personalized needs and maximize the benefits of both parties to the trading. At the same time, the system also provides a certain degree of privacy protection. Finally, through security and application prospect analysis, we prove the usability and practicability of our proposed scheme.},
booktitle = {Proceedings of the 2nd ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {142–150},
numpages = {9},
keywords = {blockchain, energy internet, privacy protection, trading matching},
location = {Taipei, Taiwan},
series = {BSCI '20}
}

@inproceedings{10.1145/3589806.3600032,
author = {Rosendo, Daniel and Keahey, Kate and Costan, Alexandru and Simonin, Matthieu and Valduriez, Patrick and Antoniu, Gabriel},
title = {KheOps: Cost-effective Repeatability, Reproducibility, and Replicability of Edge-to-Cloud Experiments},
year = {2023},
isbn = {9798400701764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589806.3600032},
doi = {10.1145/3589806.3600032},
abstract = {Distributed infrastructures for computation and analytics are now evolving towards an interconnected ecosystem allowing complex scientific workflows to be executed across hybrid systems spanning from IoT Edge devices to Clouds, and sometimes to supercomputers (the Computing Continuum). Understanding the performance trade-offs of large-scale workflows deployed on such complex Edge-to-Cloud Continuum is challenging. To achieve this, one needs to systematically perform experiments, to enable their reproducibility and allow other researchers to replicate the study and the obtained conclusions on different infrastructures. This breaks down to the tedious process of reconciling the numerous experimental requirements and constraints with low-level infrastructure design choices. To address the limitations of the main state-of-the-art approaches for distributed, collaborative experimentation, such as Google Colab, Kaggle, and Code Ocean, we propose KheOps, a collaborative environment specifically designed to enable cost-effective reproducibility and replicability of Edge-to-Cloud experiments. KheOps is composed of three core elements: (1) an experiment repository; (2) a notebook environment; and (3) a multi-platform experiment methodology. We illustrate KheOps with a real-life Edge-to-Cloud application. The evaluations explore the point of view of the authors of an experiment described in an article (who aim to make their experiments reproducible) and the perspective of their readers (who aim to replicate the experiment). The results show how KheOps helps authors to systematically perform repeatable and reproducible experiments on the Grid5000 + FIT IoT LAB testbeds. Furthermore, KheOps helps readers to cost-effectively replicate authors experiments in different infrastructures such as Chameleon Cloud + CHI@Edge testbeds, and obtain the same conclusions with high accuracies (&gt; 88% for all performance metrics).},
booktitle = {Proceedings of the 2023 ACM Conference on Reproducibility and Replicability},
pages = {62–73},
numpages = {12},
keywords = {Cloud Computing, Computing Continuum, Edge Computing, Repeatability, Replicability, Reproducibility, Workflows},
location = {Santa Cruz, CA, USA},
series = {ACM REP '23}
}

@article{10.1145/3506696,
author = {Lin, Zehao and Li, Guodun and Zhang, Jingfeng and Deng, Yue and Zeng, Xiangji and Zhang, Yin and Wan, Yao},
title = {XCode: Towards Cross-Language Code Representation with Large-Scale Pre-Training},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3506696},
doi = {10.1145/3506696},
abstract = {Source code representation learning is the basis of applying artificial intelligence to many software engineering tasks such as code clone detection, algorithm classification, and code summarization. Recently, many works have tried to improve the performance of source code representation from various perspectives, e.g., introducing the structural information of programs into latent representation. However, when dealing with rapidly expanded unlabeled cross-language source code datasets from the Internet, there are still two issues. Firstly, deep learning models for many code-specific tasks still suffer from the lack of high-quality labels. Secondly, the structural differences among programming languages make it more difficult to process multiple languages in a single neural architecture.To address these issues, in this article, we propose a novel Cross-language Code representation with a large-scale pre-training (XCode) method. Concretely, we propose to use several abstract syntax trees and ELMo-enhanced variational autoencoders to obtain multiple pre-trained source code language models trained on about 1.5 million code snippets. To fully utilize the knowledge across programming languages, we further propose a Shared Encoder-Decoder (SED) architecture which uses the multi-teacher single-student method to transfer knowledge from the aforementioned pre-trained models to the distilled SED. The pre-trained models and SED will cooperate to better represent the source code. For evaluation, we examine our approach on three typical downstream cross-language tasks, i.e., source code translation, code clone detection, and code-to-code search, on a real-world dataset composed of programming exercises with multiple solutions. Experimental results demonstrate the effectiveness of our proposed approach on cross-language code representations. Meanwhile, our approach performs significantly better than several code representation baselines on different downstream tasks in terms of multiple automatic evaluation metrics.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {52},
numpages = {44},
keywords = {Deep learning, neural networks, code representation, cross-language, pre-training}
}

@inproceedings{10.1145/3543507.3583291,
author = {Qian, Shengsheng and Chen, Hong and Xue, Dizhan and Fang, Quan and Xu, Changsheng},
title = {Open-World Social Event Classification},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583291},
doi = {10.1145/3543507.3583291},
abstract = {With the rapid development of Internet and the expanding scale of social media, social event classification has attracted increasing attention. The key to social event classification is effectively leveraging the visual and textual semantics for classification. However, most of the existing approaches may suffer from the following limitations: (1) Most of them just simply concatenate the image features and text features to get the multimodal features and ignore the fine-grained semantic relationship between modalities. (2) The majority of them hold the closed-world assumption that all classes in test are already seen in training, while this assumption can be easily broken in real-world applications. In practice, new events on Internet may not belong to any existing/seen class, and therefore cannot be correctly identified by closed-world learning algorithms. To tackle these challenges, we propose an Open-World Social Event Classifier (OWSEC) model in this paper. Firstly, we design a multimodal mask transformer network to capture cross-modal semantic relations and fuse fine-grained multimodal features of social events while masking redundant information. Secondly, we design an open-world classifier and propose a cross-modal event mixture mechanism with a novel open-world classification loss to capture the potential distribution space of the unseen class. Extensive experiments on two public datasets demonstrate the superiority of our proposed OWSEC model for open-world social event classification.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1562–1571},
numpages = {10},
keywords = {Social event classification, multimodal learning, open-world learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3341105.3374046,
author = {Balteanu, Vasile-Daniel and Neculai, Alexandru and Negru, Catalin and Pop, Florin and Stoica, Adrian},
title = {Near real-time scheduling in cloud-edge platforms},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374046},
doi = {10.1145/3341105.3374046},
abstract = {As Cloud-Edge architectures are becoming more and more popular, due to their improvement on the battery life of the IoT devices and the high availability of data from the Cloud, this approach also creates new problems. As data gathered from the Edge has to be transferred to the Cloud in order to be processed, the result will be a decreased responsiveness of the system. Also, devices might have to process data by themselves, as the Cloud could be unreachable at random moments in time, resulting in a reduction in battery life. Therefore, we propose an architecture that solves these problems, by introducing an intermediate layer, called Fog, which uses a task scheduling algorithm to send data received from Edge to another device that has enough resources and the required hardware and software to complete the task. In addition, the architecture is based on microservices, hence improving scalability and flexibility. In the performance analysis, we used different values to find the best node that should receive the data for processing. In addition, we compared the microservice based architecture with a monolithic one in order to see how the throughput and responsiveness of the system are affected.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1264–1271},
numpages = {8},
keywords = {cloud computing, edge computing, task scheduling},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3630138.3630486,
author = {Wan, Fujun and Zhou, Xingyao and Zhang, Yuchen},
title = {Design and Research of Road Rescue Service Monitoring System Based on Driving Vehicles},
year = {2024},
isbn = {9781450399951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630138.3630486},
doi = {10.1145/3630138.3630486},
abstract = {In the era of data explosion, with the rapid development of Internet information technology and the gradual expansion of enterprise scale, most enterprises need to have their own task scheduling system to deal with a variety of complex data business work. More of them are scheduled tasks, driving vehicles road rescue services. In the past, the task requirements were relatively simple, and the deployment of a single server node could complete the scheduling. However, with the increasing application requirements of scheduled data services, the requirements of many scheduling systems have also increased. In the face of large-scale task requirements, single-server scheduling is slow in processing efficiency, prone to single point of failure and serious resource contention. Therefore, a multi-server cluster distributed task scheduling system is needed, and the subsequent problem is how to implement the on-line scheduling of tasks, the triggered execution of tasks, the abnormal error correction of tasks and the security management of the system. Based on this, this paper establishes a mixed integer programming model considering job conflict for road rescue operation scenarios, and adopts static scheduling to design a dynamic task chain based simulated annealing algorithm to optimize the solution of the model, and verifies the effectiveness of the algorithm through simulation and comparison with other algorithms.},
booktitle = {Proceedings of the 2023 International Conference on Power, Communication, Computing and Networking Technologies},
articleno = {68},
numpages = {5},
keywords = {Horizontal scheduling, job conflict, road rescue, simulated annealing algorithm},
location = {<conf-loc>, <city>Wuhan</city>, <country>China</country>, </conf-loc>},
series = {PCCNT '23}
}

@inproceedings{10.1145/3460319.3464795,
author = {Herrera, Adrian and Gunadi, Hendra and Magrath, Shane and Norrish, Michael and Payer, Mathias and Hosking, Antony L.},
title = {Seed selection for successful fuzzing},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464795},
doi = {10.1145/3460319.3464795},
abstract = {Mutation-based greybox fuzzing---unquestionably the most widely-used fuzzing technique---relies on a set of non-crashing seed inputs (a corpus) to bootstrap the bug-finding process. When evaluating a fuzzer, common approaches for constructing this corpus include: (i) using an empty file; (ii) using a single seed representative of the target's input format; or (iii) collecting a large number of seeds (e.g., by crawling the Internet). Little thought is given to how this seed choice affects the fuzzing process, and there is no consensus on which approach is best (or even if a best approach exists).  To address this gap in knowledge, we systematically investigate and evaluate how seed selection affects a fuzzer's ability to find bugs in real-world software. This includes a systematic review of seed selection practices used in both evaluation and deployment contexts, and a large-scale empirical evaluation (over 33 CPU-years) of six seed selection approaches. These six seed selection approaches include three corpus minimization techniques (which select the smallest subset of seeds that trigger the same range of instrumentation data points as a full corpus).  Our results demonstrate that fuzzing outcomes vary significantly depending on the initial seeds used to bootstrap the fuzzer, with minimized corpora outperforming singleton, empty, and large (in the order of thousands of files) seed sets. Consequently, we encourage seed selection to be foremost in mind when evaluating/deploying fuzzers, and recommend that (a) seed choice be carefully considered and explicitly documented, and (b) never to evaluate fuzzers with only a single seed.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {230–243},
numpages = {14},
keywords = {corpus minimization, fuzzing, software testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3460418.3479323,
author = {Kim, Dongwoo and Min, Chulhong and Kang, Seungwoo},
title = {Towards Automatic Recognition of Perceived Level of Understanding on Online Lectures using Earables},
year = {2021},
isbn = {9781450384612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460418.3479323},
doi = {10.1145/3460418.3479323},
abstract = {The COVID-19 pandemic has seriously impacted education and forced the whole education system to shift to online learning. Such a transition has been readily made by virtue of today’s Internet technology and infrastructure, but online learning also has limitations compared to traditional face-to-face lectures. One of the biggest hurdles is that it is challenging for teachers to instantly keep track of students’ learning status. In this paper, we envision earables as an opportunity to automatically estimate learner’s understanding of learning material for effective learning and teaching, e.g., to pinpoint the part for which learners need to put more effort to understand. To this end, we conduct a small-scale exploratory study with 8 participants for 24 lectures in total and investigate learner’s behavioral characteristics that indicate the level of understanding. We demonstrate that those behaviors can be captured from a motion signal on earables. We discuss challenges that need to be further addressed to realize our vision.},
booktitle = {Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers},
pages = {158–164},
numpages = {7},
keywords = {Automatic Recognition, Earable, Online Learning, Understanding Level},
location = {Virtual, USA},
series = {UbiComp/ISWC '21 Adjunct}
}

@inproceedings{10.1145/3358960.3379140,
author = {Hashemian, Raoufehsadat and Carlsson, Niklas and Krishnamurthy, Diwakar and Arlitt, Martin},
title = {Contention Aware Web of Things Emulation Testbed},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379140},
doi = {10.1145/3358960.3379140},
abstract = {Since the advent of the Web, new Web benchmarking tools have frequently been introduced to keep up with evolving workloads and environments. The introduction of Web of Things (WoT) marks the beginning of another important paradigm that requires new benchmarking tools and testbeds. Such a WoT benchmarking testbed can enable the comparison of different WoT application configurations and workload scenarios under assumptions regarding WoT application resource demands and WoT device network characteristics. The powerful computational capabilities of modern commodity multicore servers along with the limited resource consumption footprints of WoT devices suggest the feasibility of a benchmarking testbed that can emulate the application behaviour of a large number of WoT devices on just a single multicore server. However, to obtain test results that reflect the true performance of the system being emulated, care must be exercised to detect and consider the impact of testbed bottlenecks on performance results. For example, if too many WoT devices are emulated then performance metrics obtained from a test run, e.g., WoT device response times, would only reflect contention among emulated devices for shared multicore server resources instead of providing a true indication of the performance of the WoT system being emulated. We develop a testbed that helps a user emulate a system consisting of multiple WoT devices on a single multicore server by exploiting Docker containers. Furthermore, we devise a novel mechanism for the user to check whether shared resource contention in the testbed has impacted the integrity of test results. Our solution allows for careful scaling of experiments and enables resource efficient evaluation of a wide range of WoT systems, architectures, application characteristics, workload scenarios, and network conditions.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {246–256},
numpages = {11},
keywords = {Linux containers, emulation testbed, internet of things, web of things},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/3474085.3475576,
author = {Zhu, Hongyuan and Niu, Ye and Fu, Di and Wang, Hao},
title = {MusicBERT: A Self-supervised Learning of Music Representation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475576},
doi = {10.1145/3474085.3475576},
abstract = {Music recommendation has been one of the most used information retrieval services on internet. Finding suitable music for users' demands from tens of millions of music relies on the understanding of music content. Traditional studies usually focus on music representation based on massive user behavioral data and music meta-data, which ignore the audio characteristic of music. However, it is found that the melodic characteristics of music themselves can be further used to understand music. Moreover, how to utilize large-scale audio data to learn music representation is not well explored. To this end, we propose a self-supervised learning model for music representation. We firstly utilize a beat-level music pre-training model to learn the structure of music. Then, we use a multi-task learning framework to model music self-representation and co-relations between music, concurrently. Besides, we propose several downstream tasks to evaluate music representation, including music genre classification, music highlight, and music similarity retrieval. Extensive experiments on multiple music datasets demonstrate our model's superiority over baselines on learning music representation.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3955–3963},
numpages = {9},
keywords = {music information retrieval, music representation, pre-training},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3600006.3613156,
author = {Seemakhupt, Korakit and Stephens, Brent E. and Khan, Samira and Liu, Sihang and Wassel, Hassan and Yeganeh, Soheil Hassas and Snoeren, Alex C. and Krishnamurthy, Arvind and Culler, David E. and Levy, Henry M.},
title = {A Cloud-Scale Characterization of Remote Procedure Calls},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613156},
doi = {10.1145/3600006.3613156},
abstract = {The global scale and challenging requirements of modern cloud applications have led to the development of complex, widely distributed, service-oriented applications. One enabler of such applications is the remote procedure call (RPC), which provides location-independent communication and hides the myriad of cloud communication complexities and requirements within the RPC stack. Understanding RPCs is thus one key to understanding the behavior of cloud applications. While there have been numerous studies of RPCs in distributed systems, as well as attempts to optimize RPC overheads with both software and hardware, there is still a lack of knowledge about the characteristics of RPCs "in the wild" in the modern cloud environment.To address this gap, we present, to the best of our knowledge, the first large-scale fleet-wide study of RPCs. Our study is conducted at Google, where we measured the infrastructure supporting Google's user-facing, billion-user web services, such as Google Search, Gmail, Maps, and YouTube, and the information and data management systems that support them. To carry out the study, we examined over 10,000 different RPC methods sampled from over one billion traces, along with statistics collected every 30 minutes over a period of nearly two years. Among other things, we consider the volume, throughput and growth rate of RPCs in the datacenter, the latency of RPCs and their components (the "RPC latency tax"), and the structure of RPC call chains. Our analysis shows that the characteristics, scope and complexity of RPCs at hyperscale differ significantly from the assumptions made in prior research. Overall, our work provides new insights into RPC usage and characteristics at the largest scale and motivates further research on optimizing the diverse behavior of this crucial communication mechanism.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {498–514},
numpages = {17},
keywords = {remote procedure call, cloud computing, distributed computing, communications systems},
location = {<conf-loc>, <city>Koblenz</city>, <country>Germany</country>, </conf-loc>},
series = {SOSP '23}
}

@article{10.1109/TASLP.2021.3069193,
author = {Sato, Ryotaro and Niwa, Kenta and Kobayashi, Kazunori},
title = {Ambisonic Signal Processing DNNs Guaranteeing Rotation, Scale and Time Translation Equivariance},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3069193},
doi = {10.1109/TASLP.2021.3069193},
abstract = {We propose a novel framework to design Ambisonic signal processing deep neural networks (DNNs) that guarantee physical symmetries. In general, spatial acoustic signal processing DNNs for, e.g., sound event detection, ought to perform with the equivalent accuracy regardless of the directions of arrival of sound sources. This property is well known as rotation symmetry in natural science. However, in most conventional multichannel signal processing DNNs, rotation symmetry has not been explicitly incorporated into the model structure, and pseudo rotation symmetry has been acquired by training models with a large amount of signal datasets arriving from various directions. Therefore, the conventional methods will not perform sufficiently when the training dataset is relatively small scale or statistically biased, e.g., the distribution of the arriving directions of the sound events is inhomogeneous. Furthermore, in order to efficiently handle acoustic signals in DNNs, it is necessary to consider several additional symmetries, such as amplitude scaling and time translation of the signals. In this paper, we integratedly formulate these symmetry assumptions, which are called equivariance, in the form of constraints for our targeted DNN design. We propose a new DNN design method called Clebsch-Gordan Nets with Scale and Time translation Symmetry (CGNets-STS), which guarantees to simultaneously satisfy three types of equivariance (3D rotation, amplitude scaling, and time translation). As an instance of this method, we design a DNN model for sound event localization and detection tasks from Ambisonic signals. Experimental results show that this model is highly robust against spatial rotations for input data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1449–1462},
numpages = {14}
}

@inproceedings{10.1145/3460417.3482970,
author = {Liang, Teng and Xia, Zhongda and Tang, Guoming and Zhang, Yu and Zhang, Beichuan},
title = {NDN in large LEO satellite constellations: a case of consumer mobility support},
year = {2021},
isbn = {9781450384605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460417.3482970},
doi = {10.1145/3460417.3482970},
abstract = {Large low Earth orbit (LEO) satellite constellations are intended to provide global low-latency high-bandwidth Internet connectivity. Due to their large scale and high mobility nature, networking is a big challenge. In this paper, we investigate applying Named Data Networking (NDN) to this scenario. Specifically, we discuss that NDN's architectural benefits, such as adaptive forwarding, in-network caching, off-the-grid communication, data mule service, in-network/edge computing, mobility support, and data-centric security, make it a promising candidate. Moreover, we focus on studying NDN's consumer mobility support. Specifically, NDN's in-network Interest retransmission can quickly react to satellite handovers. However, we make an observation that Interest routing paths before and after satellite handover may not overlap, hence underusing NDN's in-network caching. Therefore, we direct retransmitted Interests due to handovers to the previous connected satellite via forwarding hint. Simulation results show that the studied approaches can decently improve the consumers' performance and reduce the network traffic, achieving better consumer mobility support.},
booktitle = {Proceedings of the 8th ACM Conference on Information-Centric Networking},
pages = {1–12},
numpages = {12},
keywords = {ICN, NDN, consumer mobility, large LEO satellite constellations, mobility management, satellite networks},
location = {Paris, France},
series = {ICN '21}
}

@inproceedings{10.1145/3495018.3495483,
author = {Zhang, Hanyue},
title = {A Comparative Algorithm of the Similarity in “Blank” Concept for Chinese and Western Poetics in the Context of Internet},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495483},
doi = {10.1145/3495018.3495483},
abstract = {Traditional cross-language similarity assessment techniques mostly rely on the theories of linguistics and pragmatics, which is also bound up with the natural features of "natural language". This paper mainly studies the similarity comparison algorithm of "blank" concept in Chinese and Western poetics under the background of Internet. In this paper, a sentence-level cross-language similarity assessment framework (SCLSE) is proposed. The framework is based on word embedding as the underlying vector representation, which is used to learn the semantic representation of sentences through the fusion of various neural network structures, and finally outputs the similarity score of sentences. In this paper, we also divide the short text into paragraphs and treat the paragraphs as long sentences as sequence input to realize the iterative calculation of similarity on a larger scale. In this paper, we set up different comparative experiments to verify the effectiveness and application value of SCLSE framework in the cross-language text similarity assessment task under different text unit granularity.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1778–1782},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1145/3512908,
author = {TeBlunthuis, Nathan and Kiene, Charles and Brown, Isabella and Levi, Laura (Alia) and McGinnis, Nicole and Hill, Benjamin Mako},
title = {No Community Can Do Everything: Why People Participate in Similar Online Communities},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512908},
doi = {10.1145/3512908},
abstract = {Large-scale quantitative analyses have shown that individuals frequently talk to each other about similar things in different online spaces. Why do these overlapping communities exist? We provide an answer grounded in the analysis of 20 interviews with active participants in clusters of highly related subreddits. Within a broad topical area, there are a diversity of benefits an online community can confer. These include (a) specific information and discussion, (b) socialization with similar others, and (c) attention from the largest possible audience. A single community cannot meet all three needs. Our findings suggest that topical areas within an online community platform tend to become populated by groups of specialized communities with diverse sizes, topical boundaries, and rules. Compared with any single community, such systems of overlapping communities are able to provide a greater range of benefits.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {61},
numpages = {25},
keywords = {ecology, interviews, multiple communities, online communities, reddit}
}

@inproceedings{10.1145/3419635.3419729,
author = {Tianmei, Bao},
title = {Research on English Vocabulary Translation Technology Based on Dynamic Bilingual Corpus},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419729},
doi = {10.1145/3419635.3419729},
abstract = {The goal of this study is to use modern Internet technology to collect dynamic English-language contrastive corpora in a large-scale information database, build a bilingual dynamic corpus in Chinese and English, and use bilingual translation memory to complete bilingual assistance based on computer-assisted translation Construction of translation search engine system. The main content of this article mainly includes the design of constructing the corpus and the implementation of the translation search engine system. In terms of building a dynamic corpus, according to the general process of Web content mining, the paper first uses web spiders to search and collect raw data in the Internet's massive information database, and then preprocesses and rearranges them. After purification and identification, the Two-page bilingual texts and two-page bilingual texts are used for word segmentation matching and web page structure matching, respectively. Chinese-English bilingual translation corpora are extracted and stored in the database. The deployment of the auxiliary translation search engine system is based on the construction of a dynamic corpus. The indexing tool is used to index the corpus, and manual review and user feedback modules are added to provide users with Chinese-English bilingual auxiliary translation services.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {288–291},
numpages = {4},
keywords = {Dynamic Bilingual Corpus, English Vocabulary Translation, Web Content Mining},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@inproceedings{10.1145/3382494.3410692,
author = {Hassan, Foyzul and Bansal, Chetan and Nagappan, Nachiappan and Zimmermann, Thomas and Awadallah, Ahmed Hassan},
title = {An Empirical Study of Software Exceptions in the Field using Search Logs},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410692},
doi = {10.1145/3382494.3410692},
abstract = {Background: Software engineers spend a substantial amount of time using Web search to accomplish software engineering tasks. Such search tasks include finding code snippets, API documentation, seeking help with debugging, etc. While debugging a bug or crash, one of the common practices of software engineers is to search for information about the associated error or exception traces on the internet. Aims: In this paper, we analyze query logs from Bing to carry out a large scale study of software exceptions. To the best of our knowledge, this is the first large scale study to analyze how Web search is used to find information about exceptions. Method: We analyzed about 1 million exception related search queries from a random sample of 5 billion web search queries. To extract exceptions from unstructured query text, we built a novel machine learning model. With the model, we extracted exceptions from raw queries and performed popularity, effort, success, query characteristic and web domain analysis. We also performed programming language-specific analysis to give a better view of the exception search behavior. Results: Using the model with an F1-score of 0.82, our study identifies most frequent, most effort-intensive, or less successful exceptions and popularity of community Q&amp;A sites. Conclusion: These techniques can help improve existing methods, documentation and tools for exception analysis and prediction. Further, similar techniques can be applied for APIs, frameworks, etc.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {4},
numpages = {12},
keywords = {debugging, machine learning, software engineering, web search},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.1145/3392153,
author = {Gopalan, Aditya and Sankararaman, Abishek and Walid, Anwar and Vishwanath, Sriram},
title = {Stability and Scalability of Blockchain Systems},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3392153},
doi = {10.1145/3392153},
abstract = {The blockchain paradigm provides a mechanism for content dissemination and distributed consensus on Peer-to-Peer (P2P) networks. While this paradigm has been widely adopted in industry, it has not been carefully analyzed in terms of its network scaling with respect to the number of peers. Applications for blockchain systems, such as cryptocurrencies and IoT, require this form of network scaling. In this paper, we propose a new stochastic network model for a blockchain system. We identify a structural property called one-endedness, which we show to be desirable in any blockchain system as it is directly related to distributed consensus among the peers. We show that the stochastic stability of the network is sufficient for the one-endedness of a blockchain. We further establish that our model belongs to a class of network models, called monotone separable models. This allows us to establish upper and lower bounds on the stability region. The bounds on stability depend on the connectivity of the P2P network through its conductance and allow us to analyze the scalability of blockchain systems on large P2P networks. We verify our theoretical insights using both synthetic data and real data from the Bitcoin network.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {jun},
articleno = {35},
numpages = {35},
keywords = {distributed consensus, monotone separability, peer-to-peer, queueing}
}

@inproceedings{10.1145/3487552.3487853,
author = {Ziv, Maya and Izhikevich, Liz and Ruth, Kimberly and Izhikevich, Katherine and Durumeric, Zakir},
title = {ASdb: a system for classifying owners of autonomous systems},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487853},
doi = {10.1145/3487552.3487853},
abstract = {While Autonomous Systems (ASes) are crucial for routing Internet traffic, organizations that own them are little understood. Regional Internet Registries (RIRs) inconsistently collect, release, and update basic AS organization information (e.g., website), and prior work provides only coarse-grained classification. Bootstrapping from RIR WHOIS data, we build ASdb, a system that uses data from established business intelligence databases and machine learning to accurately categorize ASes at scale. ASdb achieves 96% coverage of ASes, and 93% and 75% accuracy on 17 industry categories and 95 sub-categories, respectively. ASdb creates a more rich, accurate, comprehensive, and maintainable dataset cataloging AS-owning organizations. This system, and resulting dataset, will allow researchers to better understand who owns the Internet, and perform new forms of meaningful analysis and interpretation at scale.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {703–719},
numpages = {17},
location = {Virtual Event},
series = {IMC '21}
}

@inproceedings{10.1145/3436369.3436492,
author = {Ren, Li and Xian, Weifu and Tang, Hao and Jiang, Yadong and Jia, Haitao and Li, Jing},
title = {Pedestrian and Face Detection with Low Resolution Based on Improved MTCNN},
year = {2021},
isbn = {9781450387835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436369.3436492},
doi = {10.1145/3436369.3436492},
abstract = {In recent years, the application of deep learning based on deep convolutional neural networks has gained great success in face detection. However, the large visual variations of pedestrians and faces, such as large pose variations and dark lightings, resulting in lower resolution targets, impose great challenges for these tasks in real-world applications. To solve this problem, we present a conceptually simple, end-to-end, and general framework for pedestrian and face detection. Our approach efficiently detects both pedestrian and face in an image. First, an efficient improved P-Net is developed to detect a pedestrian. Then an efficient improved R-Net1 is developed to filter pedestrian targets in the second level, and improved R-Net2 carries out the preliminary detection of face targets in the remaining pedestrian targets. In order to improve the face detection rate on a small scale, improved R-Net2 introduces a multi-level feature fusion mechanism. Last, an improved O-Net is proposed to identify pedestrian and face regions. Compared to state-of-the-art face detection methods such as Multiscale Cascade CNN、 Faceness、 Two-stage CNN、 MTCNN, the proposed method achieves promising performance on WIDER FACE benchmarks, our method also reaches promising results on the Caltech benchmarks.},
booktitle = {Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition},
pages = {174–180},
numpages = {7},
keywords = {Pedestrian and face detection, improved O-Net, improved P-Net, improved R-Net},
location = {Xiamen, China},
series = {ICCPR '20}
}

@inproceedings{10.1145/3482898.3483366,
author = {Apostolaki, Maria and Singla, Ankit and Vanbever, Laurent},
title = {Performance-Driven Internet Path Selection},
year = {2021},
isbn = {9781450390842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482898.3483366},
doi = {10.1145/3482898.3483366},
abstract = {Internet routing can often be sub-optimal, with the chosen routes providing worse performance than other available policy-compliant routes. This stems from the lack of visibility into route performance at the network layer. While this is an old problem, we argue that recent advances in programmable hardware finally open up the possibility of performance-aware routing in a deployable, BGP-compatible manner.We introduce RouteScout, a hybrid hardware/software system supporting performance-based routing at ISP scale. In the data plane, RouteScoutleverages P4-enabled hardware to monitor performance across policy-compliant route choices for each destination, at line-rate and with a small memory footprint. RouteScout'scontrol plane then asynchronously pulls aggregated performance metrics to synthesize a performance-aware forwarding policy.We show that RouteScoutcan monitor performance across most of an ISP's traffic, using only 4 MB of memory. Further, its control can flexibly satisfy a variety of operator objectives, with sub-second operating times.},
booktitle = {Proceedings of the ACM SIGCOMM Symposium on SDN Research (SOSR)},
pages = {41–53},
numpages = {13},
location = {Virtual Event, USA},
series = {SOSR '21}
}

@inproceedings{10.1145/3537797.3537806,
author = {Loh Chee Wyai, Gary and Zaman, Tariq and Hamid, Khairuddin Ab},
title = {Unleashing Indigenous Iban Values For Collective Technology Design},
year = {2022},
isbn = {9781450396813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537797.3537806},
doi = {10.1145/3537797.3537806},
abstract = {The digital divide continues to be a problem on a global scale. Since the pandemic began, there has been widespread media coverage of people being unable to work from home or conduct online studies due to the lack of internet access. Many development and design approaches have been used to accomplish specific rural community objectives, including user-centred, participatory and co-design. Each of these strategies is effective at accomplishing certain specific objectives. The purpose of this research is to investigate the cultural and values dimension of the Iban indigenous community in Malaysian Borneo, Sarawak, using a value sensitive design approach. Domestic and cultural probes techniques were used to conduct design workshops with members of the community to explore indigenous communal values and finally, the card sorting method was used to reflect and create meaning from the information gathered. Our processes of engagement and co-design targets establishing a pluriversal design space for building trust, dialogue and co-existence despite the differences between academic and indigenous knowledge systems. In future, the primary discovery regarding collective values will be incorporated into the design and development of Community Networks as a solution bridging the digital divide for the indigenous community in Sarawak.},
booktitle = {Proceedings of the Participatory Design Conference 2022 - Volume 2},
pages = {39–46},
numpages = {8},
keywords = {Community-Based Co-Design, Design Paradigms, Value Sensitive Design},
location = {Newcastle upon Tyne, United Kingdom},
series = {PDC '22}
}

@article{10.1145/3374136,
author = {Chua, Mark Yep-Kui and Yee, George O. M. and Gu, Yuan Xiang and Lung, Chung-Horng},
title = {Threats to Online Advertising and Countermeasures: A Technical Survey},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3374136},
doi = {10.1145/3374136},
abstract = {Online advertising, also known as web advertising or Internet marketing, is the means and process of promoting products and services on the Internet, and it has been one of the important business models for the Internet. Due to its lucrative nature and its large scale of adoption, it has also been a target for malicious parties with various attack aims such as getting a cut of online advertising revenues, obtaining a user’s privacy, and spreading malware. Over the years, a great deal of research has been conducted on online advertising. Recently, the health of the online advertising ecosystem has become more of a concern for both advertisers and regular Internet users. Advertising budgets have been abused, and Internet users’ privacy and security have been infringed. In this article, we broadly study threats to online advertising and trace the root causes from a systems point of view. Existing threat mitigation strategies are also reviewed and analyzed. To protect online advertising, which has been an essential funding source of many free Internet services, several challenges still need to be addressed, including the need for transparency of the advertising ecosystem and software vulnerabilities on the client-side. To overcome these challenges, we conclude by brainstorming some innovative ideas on some potentially interesting and useful research directions.},
journal = {Digital Threats},
month = {may},
articleno = {11},
numpages = {27},
keywords = {IoT, Online advertising, blockchain, privacy, security, software protection}
}

@inproceedings{10.1145/3360774.3360833,
author = {Gad-Elrab, Ahmed. A. A. and Noaman, Amin. Y.},
title = {Fuzzy clustering-based task allocation approach using bipartite graph in cloud-fog environment},
year = {2020},
isbn = {9781450372831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360774.3360833},
doi = {10.1145/3360774.3360833},
abstract = {Recently, due to the limitations in using cloud computing services for the recent advances IoTs applications, a newly distributed computing architecture is established called cloud-fog paradigm by exploiting the cooperation between fog and cloud entities. Fog nodes are used to reduce monetary cost and transferring latency for cloud resources, while for offloading of large-scale applications cloud servers are used. In this paradigm, The main problem is task allocation which aims to select the optimal nodes among cloud and fog nodes for each task to minimize makespan, monetary and energy costs. In this paper, to solve this problem a new task allocation approach called bipartite graph with fuzzy clustering task allocation approach is proposed and it uses a hybrid DAG for representing independent and dependent tasks. Also, it uses fuzzy clustering and bipartite graph to solve the uncertainty executing problem and find the maximum bipartite matching, respectively. The conducted simulation results show that the proposed approach can achieve a higher performance int terms of makespan, total cost, and cost-makespan tradeoff than existing approaches.},
booktitle = {Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {454–463},
numpages = {10},
keywords = {bipartite graph, cloud computing, energy consumption, fog computing, fuzzy clustering, internet of things, task allocation},
location = {Houston, Texas, USA},
series = {MobiQuitous '19}
}

@inproceedings{10.1145/3450614.3464469,
author = {Komninos, Nikos},
title = {Preserving Privacy of Data with Efficient Attribute-based Encryption Schemes&nbsp;},
year = {2021},
isbn = {9781450383677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450614.3464469},
doi = {10.1145/3450614.3464469},
abstract = {Attribute-based encryption (ABE) schemes and their variations are often applied to preserve the privacy of data. In particular, ABE schemes proposals are resilient to multiple attacks, including attacks in interception, interruption, modification, fabrication, unauthorized authentication, and access of data. Existing proposals have several limitations, such as the generation, verification, and distribution of digital certificates incur extra computation and communication overhead which are not suitable for resource-constrained computing. Furthermore, in most of the ABE schemes, a certification authority (CA) generates the public/secret keys according to a set of attributes. However, the compromise of CA can endanger the secret keys, therefore, the secrecy of encrypted messages. Some of the existing ABE schemes are based on bilinear pairing that requires large security parameters, which make ABE schemes unsuitable for resource-constrained computing devices.&nbsp;The current ABE proposals [1, 2, 3, 4] are complex because they require implementing large-number security parameters (i.e., 2048-bit or 4096-bit size) to achieve 2128 security. Besides that, those ABE schemes consider a CA with an active role in the application process. The CA generates and distributes secret keys to devices or users. Nonetheless, sharing private attributes with the CA can risk data and user privacy, since the CA can also decrypt messages, depending on the application scenario, and retrieve the data. Moreover, the compromise of CA poses a risk to the communication secrecy between the sender and the receiver. In addition, some studies propose symmetric key schemes for resource-constrained devices. However, in large-scale networked systems, the symmetric key management becomes very complex and inefficient. The symmetric-key deployment often requires a separate protocol for session key agreement and generation. In IoT networks where mostly short-sized data is exchanged, symmetric key encryption schemes are often subject to ciphertext-only attacks.&nbsp;In this paper, we will discuss how we can generate efficient ABE schemes based on elliptic curve cryptographic (ECC) techniques without the use of a CA. ECC-based ABE schemes require smaller-number security parameters (i.e., 256 or 512 bits) for achieving at least 2128 computational security that makes them efficient in resource-constrained computing devices. In particular, efficient ABE schemes, such as [5], are based on the computational Diffie-Hellman assumptions and their derivatives. Using such an assumption, we can perform elliptic curve operations, addition and multiplication, in a group without compromising the security of the ABE scheme. In other words, an adversary or oracle cannot efficiently “guess” or “find” the secret asymmetric key.ABE schemes are usually enclosed by assumptions that are necessary for the deployed systems. In addition, ABE schemes consist of the&nbsp;Key pair generation,&nbsp;encryption, and&nbsp;decryption&nbsp;algorithms, even though few ABE proposals additionally include&nbsp;key pair update&nbsp;and&nbsp;key pair revocation&nbsp;algorithms [5]. The&nbsp;key pair generation&nbsp;algorithm considers the secret attributes of a device or user. The algorithm takes as input a security parameter λ and a set of secret attributes AS. λ consists of a long string of 1s in a chosen finite field that defines the access structure and the length of the secret keys and messages. It outputs the public/secret key pair (PK, SK) that is either offline or online and is distributed to the involved entities, i.e., devices or users.&nbsp;The encryption algorithm takes as input the public key of a device PK, the access policy P, and a message M. It outputs a ciphertext CT that is exchanged in a hostile environment. On the contrary, the decryption algorithm takes as input the secret key SK and ciphertext CT and outputs the plaintext message M. Obviously, the key pair generation algorithm sets the mathematical foundations that connect the key pair, whereas encryption and decryption algorithms conceal and reveal the actual data during online transmission and exposure.If the secret or shared attributes of an entity are changed, for any reason, then all the keys should be updated. The&nbsp;key pair update algorithm&nbsp;will regenerate the public/secret (PK/SK) key pair. In the key pair update procedure, the updated secret attributes AS are considered as input to the algorithm and new key pair (PK ́/ SK ́) is regenerated. In the&nbsp;key pair revocation&nbsp;algorithm, the keys are revoked by an entity or due to the malicious behaviour of some users/devices. Three cases for key revocation have been identified:&nbsp;(1)&nbsp;Legitimate revoke: in this case, the key pair can be revoked due to a system update, expiration date, and scheduled maintenance of the networked system. (2)&nbsp;Malicious activity: in this case, the key revocation may take place due to the malicious behaviour which might be observed and/or reported by an entity of the networked system. (3)&nbsp;Attribute update: in this case, the change in the attribute set can trigger a new key revocation procedure.&nbsp;In literature, the ABE algorithms are theoretically assessed by proving that the mathematical foundations hold in a malicious environment. Likewise, the ABE algorithms are assessed by their computational and memory complexity as well as by their practical implementation in real devices or the simulated network. In principle, the security of the ABE schemes is carefully analysed to ensure security against popular attacks: (i) Computing the secret key SK from the public key PK, (ii) Computing the secret key SK from multiple ciphertexts (i.e., chosen ciphertext attack), (iii) it can be shown by a reduction that the computational problem in an ABE scheme is at least as hard as the discrete logarithm problem (DLP) and (iv) the ABE scheme is secure against an adversary A with knowledge of the shared attribute set AK for deriving the secret key SK by a collision attack.&nbsp;The security and privacy challenges posed by resource-constrained systems affect the heterogeneous nature of devices with varying degrees of computation and storage capacity. It is therefore essential to find lightweight solutions which eliminate the need for applying different security schemes per system. The existing public-key encryption and attribute-based encryption schemes are often computationally expensive, therefore, not suitable for resource-constrained devices. Moreover, the sharing of attributes with a certification authority risks the privacy of devices if CA has been compromised. New ABE schemes should not endanger the secrecy of messages among devices if CA is compromised.&nbsp;It is also essential for researchers to propose schemes based on mathematical constructions that are proven to be secure and light, such as elliptic curve cryptography which supports smaller key sizes and is highly suitable for resource-constrained devices.&nbsp;},
booktitle = {Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {366–367},
numpages = {2},
keywords = {Attribute based encryption, key revocation, key update, privacy preservation},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@article{10.1145/3474385,
author = {Tosch, Emma and Bakshy, Eytan and Berger, Emery D. and Jensen, David D. and Moss, J. Eliot B.},
title = {PlanAlyzer: assessing threats to the validity of online experiments},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3474385},
doi = {10.1145/3474385},
abstract = {Online experiments are an integral part of the design and evaluation of software infrastructure at Internet firms. To handle the growing scale and complexity of these experiments, firms have developed software frameworks for their design and deployment. Ensuring that the results of experiments in these frameworks are trustworthy---referred to as internal validity---can be difficult. Currently, verifying internal validity requires manual inspection by someone with substantial expertise in experimental design.We present the first approach for checking the internal validity of online experiments statically, that is, from code alone. We identify well-known problems that arise in experimental design and causal inference, which can take on unusual forms when expressed as computer programs: failures of randomization and treatment assignment, and causal sufficiency errors. Our analyses target PLANOUT, a popular framework that features a domain-specific language (DSL) to specify and run complex experiments. We have built PLANALYZER, a tool that checks PLANOUT programs for threats to internal validity, before automatically generating important data for the statistical analyses of a large class of experimental designs. We demonstrate PLANALYZER'S utility on a corpus of PLANOUT scripts deployed in production at Facebook, and we evaluate its ability to identify threats on a mutated subset of this corpus. PLANALYZER has both precision and recall of 92% on the mutated corpus, and 82% of the contrasts it generates match hand-specified data.},
journal = {Commun. ACM},
month = {aug},
pages = {108–116},
numpages = {9}
}

@inproceedings{10.1145/3474085.3475268,
author = {Yin, Yifang and Zhang, Ying and Liu, Zhenguang and Liang, Yuxuan and Wang, Sheng and Shah, Rajiv Ratn and Zimmermann, Roger},
title = {Learning Multi-context Aware Location Representations from Large-scale Geotagged Images},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475268},
doi = {10.1145/3474085.3475268},
abstract = {With the ubiquity of sensor-equipped smartphones, it is common to have multimedia documents uploaded to the Internet that have GPS coordinates associated with them. Utilizing such geotags as an additional feature is intuitively appealing for improving the performance of location-aware applications. However, raw GPS coordinates are fine-grained location indicators without any semantic information. Existing methods on geotag semantic encoding mostly extract hand-crafted, application-specific location representations that heavily depend on large-scale supplementary data and thus cannot perform efficiently on mobile devices. In this paper, we present a machine learning based approach, termed GPS2Vec+, which learns rich location representations by capitalizing on the world-wide geotagged images. Once trained, the model has no dependence on the auxiliary data anymore so it encodes geotags highly efficiently by inference. We extract visual and semantic knowledge from image content and user-generated tags, and transfer the information into locations by using geotagged images as a bridge. To adapt to different application domains, we further present an attention-based fusion framework that estimates the importance of the learnt location representations under different contexts for effective feature fusion. Our location representations yield significant performance improvements over the state-of-the-art geotag encoding methods on image classification and venue annotation.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {899–907},
numpages = {9},
keywords = {attention-based fusion, geo-aware applications, location representations, pre-trained neural networks},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3544109.3544138,
author = {Yuan, Min},
title = {Modeling Analysis of the Influence of Seoul City Image on Tourists' Willingness to Revisit Based on Parallel Computing},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544138},
doi = {10.1145/3544109.3544138},
abstract = {Tourism is closely related to people's lives today, and the development level of tourism informatization is an important indicator to measure the modern tourism industry. At present, more and more data on the Internet is released in the form of linked data, which reduces the complexity of the integration of distributed, heterogeneous or autonomous data, and at the same time promotes the application of linked data. The purpose of this article is to model the influence of Seoul's city image on tourists' willingness to revisit based on parallel computing. This paper studies the similarity calculation efficiency in the data set of passenger-related passenger revisiting intention resources. According to the established tourist tourism ontology, this paper adopts the MapReduce parallel computing framework to design a parallel computing method of related data similarity to improve the discovery efficiency of the related data model of the willingness of large-scale tourists to revisit. Experimental research shows that this article analyzes the difference in perceptions of various image factors by tourists of different ages, and finds that the Р value in the single-factor analysis of variance table is greater than 0.05, indicating that tourists of different ages do not have significant perceptions of each image factor.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {164–168},
numpages = {5},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/3640912.3640951,
author = {Li, Yimeng and Li, Xi and Zhang, Yichao},
title = {Traffic Classification of Network Security Agents Based on Feature Fusion Convolutional Neural Network},
year = {2024},
isbn = {9798400716683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640912.3640951},
doi = {10.1145/3640912.3640951},
abstract = {With the rapid development of the Internet, the number of network attacks has increased, and network security has gradually become a focus of attention. Network security agent traffic classification refers to the analysis and detection of network security issues based on useful information in network traffic so that network operations can perform well. This paper proposes a security agent in network classification system based on point convolutional neural network. This mode combines various functions to improve the accuracy of the car parts list. First, various attributes are scaled down to minimize their impact on benefit allocation. Second, the neural network solution is used to learn and predict different types of features, thereby improving the classification accuracy of the machine list. After all, this kind of effectiveness and efficiency is confirmed by social media testing.},
booktitle = {Proceedings of the 2023 International Conference on Communication Network and Machine Learning},
pages = {196–200},
numpages = {5},
location = {<conf-loc>, <city>Zhengzhou</city>, <country>China</country>, </conf-loc>},
series = {CNML '23}
}

@inproceedings{10.1145/3534678.3539045,
author = {Wu, Tailin and Wang, Qinchen and Zhang, Yinan and Ying, Rex and Cao, Kaidi and Sosic, Rok and Jalali, Ridwan and Hamam, Hassan and Maucec, Marko and Leskovec, Jure},
title = {Learning Large-scale Subsurface Simulations with a Hybrid Graph Network Simulator},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539045},
doi = {10.1145/3534678.3539045},
abstract = {Subsurface simulations use computational models to predict the flow of fluids (e.g., oil, water, gas) through porous media. These simulations are pivotal in industrial applications such as petroleum production, where fast and accurate models are needed for high-stake decision making, for example, for well placement optimization and field development planning. Classical finite difference numerical simulators require massive computational resources to model large-scale real-world reservoirs. Alternatively, streamline simulators and data-driven surrogate models are computationally more efficient by relying on approximate physics models, however they are insufficient to model complex reservoir dynamics at scale.Here we introduce Hybrid Graph Network Simulator (HGNS), which is a data-driven surrogate model for learning reservoir simulations of 3D subsurface fluid flows. To model complex reservoir dynamics at both local and global scale, HGNS consists of a subsurface graph neural network (SGNN) to model the evolution of fluid flows, and a 3D-U-Net to model the evolution of pressure. HGNS is able to scale to grids with millions of cells per time step, two orders of magnitude higher than previous surrogate models, and can accurately predict the fluid flow for tens of time steps (years into the future).Using an industry-standard subsurface flow dataset (SPE-10) with 1.1 million cells, we demonstrate that HGNS is able to reduce the inference time up to 18 times compared to standard subsurface simulators, and that it outperforms other learning-based models by reducing long-term prediction errors by up to 21%.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4184–4194},
numpages = {11},
keywords = {hybrid graph neural network, large-scale, multi-scale, subsurface simulations},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3450444,
author = {Flores, Marcel and Kahn, Andrew and Warrior, Marc and Mislove, Alan and Kuzmanovic, Aleksandar},
title = {Utilizing Web Trackers for Sybil Defense},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/3450444},
doi = {10.1145/3450444},
abstract = {User tracking has become ubiquitous practice on the Web, allowing services to recommend behaviorally targeted content to users. In this article, we design Alibi, a system that utilizes such readily available personalized content, generated by recommendation engines in real time, as a means to tame Sybil attacks. In particular, by using ads and other tracker-generated recommendations as implicit user “certificates,” Alibi is capable of creating meta-profiles that allow for rapid and inexpensive validation of users’ uniqueness, thereby enabling an Internet-wide Sybil defense service.We demonstrate the feasibility of such a system, exploring the aggregate behavior of recommendation engines on the Web and demonstrating the richness of the meta-profile space defined by such inputs. We further explore the fundamental properties of such meta-profiles, i.e., their construction, uniqueness, persistence, and resilience to attacks. By conducting a user study, we show that the user meta-profiles are robust and show important scaling effects. We demonstrate that utilizing even a moderate number of popular Web sites empowers Alibi to tame large-scale Sybil attacks.},
journal = {ACM Trans. Web},
month = {apr},
articleno = {8},
numpages = {19},
keywords = {Sybil attacks, User tracking, recommendation engines}
}

@inproceedings{10.1145/3539618.3591972,
author = {Ren, Yimo and Wang, Jinfa and Li, Hong and Zhu, Hongsong and Sun, Limin},
title = {DeviceGPT: A Generative Pre-Training Transformer on the Heterogenous Graph for Internet of Things},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591972},
doi = {10.1145/3539618.3591972},
abstract = {Recently, Graph neural networks (GNNs) have been adopted to model a wide range of structured data from academic and industry fields. With the rapid development of Internet technology, there are more and more meaningful applications for Internet devices, including device identification, geolocation and others, whose performance needs improvement. To replicate the several claimed successes of GNNs, this paper proposes DeviceGPT based on a generative pre-training transformer on a heterogeneous graph via self-supervised learning to learn interactions-rich information of devices from its large-scale databases well. The experiments on the dataset constructed from the real world show DeviceGPT could achieve competitive results in multiple Internet applications.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1929–1933},
numpages = {5},
keywords = {graph representation learning, internet of things, pre-training, self-supervised},
location = {<conf-loc>, <city>Taipei</city>, <country>Taiwan</country>, </conf-loc>},
series = {SIGIR '23}
}

@inproceedings{10.1145/3451400.3451413,
author = {Lau, Chi Yat and Yuen, Man-Ching and Yiu, Kam-Sum and Lau, Hoi-Shan and Lo, Chun-Ting},
title = {Cat's Eye: Media Insights Analyzer},
year = {2021},
isbn = {9781450389389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3451400.3451413},
doi = {10.1145/3451400.3451413},
abstract = {Different media are used to report news happening all around the world to the public. The news from all over the world is a big data source on the Internet. Besides, the news media have their own writing style and stands towards the news which may cause great influence towards readers’ opinion. It is valuable to analyze the insights of various media. The existing works on sentiment analysis for the news data do not have a user-friendly interface to present their work and the news data are not retrieved regularly for continuous data analysis. To address the above problems, we propose Cat's Eye Project which aims to identify the stands of different news media through the usage of the words of those media, forecast the trend of some social issues, find out whether there is any indication before a new social issue formed and present our findings by using a user-friendly interface. We build our system on Google Cloud and use the built-in machine learning algorithms on Google Cloud to develop our learning model. By using cloud technology, the system implementation period is highly reduced, and it also guarantees the scalability of our system. The output finding can be used as interesting insights. Our pilot study showed that the media can affect the readers’ opinion towards social issues.},
booktitle = {Proceedings of the 2021 4th International Conference on Big Data and Education},
pages = {80–87},
numpages = {8},
keywords = {Data analysis, News},
location = {London, United Kingdom},
series = {ICBDE '21}
}

@inproceedings{10.1145/3485447.3511986,
author = {Zhang, Wentao and Shen, Yu and Lin, Zheyu and Li, Yang and Li, Xiaosen and Ouyang, Wen and Tao, Yangyu and Yang, Zhi and Cui, Bin},
title = {PaSca: A Graph Neural Architecture Search System under the Scalable Paradigm},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511986},
doi = {10.1145/3485447.3511986},
abstract = {Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based tasks. However, as mainstream GNNs are designed based on the neural message passing mechanism, they do not scale well to data size and message passing steps. Although there has been an emerging interest in the design of scalable GNNs, current researches focus on specific GNN design, rather than the general design space, limiting the discovery of potential scalable GNN models. This paper proposes PaSca, a new paradigm and system that offers a principled approach to systemically construct and explore the design space for scalable GNNs, rather than studying individual designs. Through deconstructing the message passing mechanism, PaSca presents a novel Scalable Graph Neural Architecture Paradigm (SGAP), together with a general architecture design space consisting of 150k different designs. Following the paradigm, we implement an auto-search engine that can automatically search well-performing and scalable GNN architectures to balance the trade-off between multiple criteria (e.g., accuracy and efficiency) via multi-objective optimization. Empirical studies on ten benchmark datasets demonstrate that the representative instances (i.e., PaSca-V1, V2, and V3) discovered by our system achieve consistent performance among competitive baselines. Concretely, PaSca-V3 outperforms the state-of-the-art GNN method JK-Net by 0.4% in terms of predictive accuracy on our large industry dataset while achieving up to 28.3 \texttimes{} training speedups.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1817–1828},
numpages = {12},
keywords = {Design Space, Graph Neural Networks, Scalable Graph Learning},
location = {<conf-loc>, <city>Virtual Event, Lyon</city>, <country>France</country>, </conf-loc>},
series = {WWW '22}
}

@inproceedings{10.1145/3468691.3468698,
author = {Chai, Xiaohu and Cai, Gaoyang and Li, Hui},
title = {aSIT: An Interface-oriented Distributed Automated Test System},
year = {2021},
isbn = {9781450389693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468691.3468698},
doi = {10.1145/3468691.3468698},
abstract = {Modern Internet services are often complex, large-scale distributed systems that consist of multiple software modules which may be developed by different teams using different programming languages. This multi-team collaboration and multiple development languages have brought new challenges to distributed program testing tools. Traditional manual testing cannot meet the testing requirements of large-scale distributed systems, and automated testing tools are usually used to complete testing tasks. This paper believes that enterprises with separate development and testing teams are more suitable to apply interface testing to realize automatic testing, and have designed and implemented a set of interface-oriented distributed automated testing systems—aSIT. Existing automated testing tools cannot well support scenarios such as internal heterogeneous service integration, multi-team collaboration, environmental network isolation, and massive test case execution. aSIT combined with actual engineering practice has carried out some specific designs to solve the above problems, so that it can be applied in the enterprise and improve the efficiency of the test team. After nearly 10 months of continuous using within the enterprise, testers can focus on use case design, the defect verification time was reduced to 1% of manual testing, the number of regression test cases increased by 30 times, and the average requirement delivery time was reduced by 63%.},
booktitle = {Proceedings of the 2021 2nd International Conference on Computing, Networks and Internet of Things},
articleno = {7},
numpages = {7},
keywords = {Automated testing, Distributed system testing, Interface testing},
location = {Beijing, China},
series = {CNIOT '21}
}

@inproceedings{10.1145/3383583.3398498,
author = {Si, Luo},
title = {Natural Language Technologies for Internet Applications},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398498},
doi = {10.1145/3383583.3398498},
abstract = {Natural Language Processing (NLP) and related technologies are critical for the success of many Internet applications such as digital libraries, e-commerce and customer service.This talk presents some recent research efforts and trends of four sets of NLP technologies for Internet applications. First, neural language model has been a very popular research direction in the last a few years that serves as the foundation of many NLP technologies and has significantly improved the performance of many applications; Second, machine translation techniques have been substantially advanced to better bridge the language barriers for many Internet applications; Third, the identification of inappropriate Internet text information (e.g., pornographic content) is a challenging research topic due to the diversified text representation; Fourth, machine reading comprehension has been become an important question and answering technology to directly satisfy information needs of many Internet users. These technologies will be discussed with examples from large-scale real-world applications.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {5},
numpages = {1},
keywords = {customer service, digital library, e-commerce, machine reading comprehension, machine translation, natural language processing, neural language model},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3495243.3560542,
author = {Rostami, Mohammad and Sundaresan, Karthikeyan},
title = {Enabling high accuracy pervasive tracking with ultra low power UWB tags},
year = {2022},
isbn = {9781450391818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495243.3560542},
doi = {10.1145/3495243.3560542},
abstract = {With the recent popularity of active ultra-wideband (UWB) tracking beacons, the next era of low-power IoT aims to offer tracking as a core feature in numerous enterprise and industrial applications. However, generating high-bandwidth tracking signals on cost-effective, low-power tags in a large scale deployment, poses fundamental challenges in energy-cost-performance trade-offs, not afforded by existing solutions that fall short in one or more critical dimensions.To this end, we present a novel tag design, WaveTag, whose innovation lies in taking low frequency, but high fidelity clock signals that can be generated at a very low energy footprint, and transforming them using ultra low power non-linear circuits into standards-compliant UWB preambles at desired super high frequencies (3--10 GHz UWB channels) - all without compromising on their ultra low energy footprint. We present WaveTag's systematic design, whose components address numerous technical challenges in realizing this principle in practice. Its implementation and extensive evaluation highlight WaveTag's ability to be tracked by UWB chips in our commodity devices, while operating upto 8m ranges with a tracking accuracy of under 50cm, and tracking support for upto tens of tags concurrently, each with a lifetime of about 8 years.},
booktitle = {Proceedings of the 28th Annual International Conference on Mobile Computing And Networking},
pages = {459–472},
numpages = {14},
location = {Sydney, NSW, Australia},
series = {MobiCom '22}
}

@inproceedings{10.1145/3404835.3462965,
author = {Li, Jiao and Xu, Xing and Yu, Wei and Shen, Fumin and Cao, Zuo and Zuo, Kai and Shen, Heng Tao},
title = {Hybrid Fusion with Intra- and Cross-Modality Attention for Image-Recipe Retrieval},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462965},
doi = {10.1145/3404835.3462965},
abstract = {Image-recipe retrieval, which aims at retrieving the relevant recipe from a food image and vice versa, is now attracting widespread attention, since sharing food-related images and recipes on the Internet has become a popular trend. Existing methods have formulated this problem as a typical cross-modal retrieval task by learning the image-recipe similarity. Though these methods have made inspiring achievements for image-recipe retrieval, they may still be less effective to jointly incorporate the three crucial points: (1) the association between ingredients and instructions, (2) fine-grained image information, and (3) the latent alignment between recipes and images. To this end, we propose a novel framework namedHybrid Fusion with Intra- and Cross-Modality Attention (HF-ICMA) to learn accurate image-recipe similarity. Our HF-ICMA model adopts an intra-recipe fusion module to focus on the interaction between ingredients and instructions within a recipe, and further enriches the expressions of the two separate embeddings. Meanwhile, an image-recipe fusion module is devised to explore the potential relationship between fine-grained image regions and ingredients from the recipe, which jointly forms the final image-recipe similarity from both the local and global aspects. Extensive experiments on the large-scale benchmark dataset Recipe1M show that our model significantly outperforms the state-of-the-art approaches on various image-recipe retrieval scenarios.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {244–254},
numpages = {11},
keywords = {attention mechanism, cross-modal retrieval, fusion strategy, image-recipe retrieval},
location = {<conf-loc>, <city>Virtual Event</city>, <country>Canada</country>, </conf-loc>},
series = {SIGIR '21}
}

@article{10.1145/3526213,
author = {Nielsen, Michael B. and Bojsen-Hansen, Morten and Stamatelos, Konstantinos and Bridson, Robert},
title = {Physics-Based Combustion Simulation},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3526213},
doi = {10.1145/3526213},
abstract = {We propose a physics-based combustion simulation method for computer graphics that extends the mathematical models of previous efforts to automatically capture more realistic flames as well as temperature and soot distributions. Our method includes mathematical models for the thermodynamic properties of real-world fuels which enables, for example, the prediction of adiabatic flame temperatures. We couple this with a model of heat transfer that includes convection, conduction as well as both radiative cooling and heating. This facilitates among other things ignition at a distance without heating up the intermediate air. We model the combustion as infinitely fast chemistry and couple this with the thin flame model, spatially varying laminar burning velocities based on local species and empirical measurements, physically validated soot formation and oxidation as well as water vapor production and condensation. We implement this on adaptive octree-like grids with collocated state variables, a new SBDF2-derived semi-Lagrangian time integrator for velocity, and a multigrid scheme used for multiple solver components. In combination, these models enable us to simulate deflagration phenomena ranging from small scale premixed and diffusion flames to fireballs and subsonic explosions which we demonstrate by several examples. In addition, we validate several of the results based on reference footage and measurements and discuss the relation of prevalent heuristic techniques arising in visual effects production to some of the physics-based models we propose.},
journal = {ACM Trans. Graph.},
month = {may},
articleno = {176},
numpages = {21},
keywords = {Fluid simulation, smoke simulation, fire simulation, combustion}
}

@article{10.1145/3377551,
author = {Pui, Chak-Wa and Young, Evangeline F. Y.},
title = {Lagrangian Relaxation-Based Time-Division Multiplexing Optimization for Multi-FPGA Systems},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3377551},
doi = {10.1145/3377551},
abstract = {&lt;?tight?&gt;To increase the resource utilization in multi-FPGA (field-programmable gate array) systems, time-division multiplexing (TDM) is a widely used technique to accommodate a large number of inter-FPGA signals. However, with this technique, the delay imposed by the inter-FPGA connections becomes significant. Previous research has shown that the TDM ratios of signals can greatly affect the performance of a system. In this article, to minimize the system clock period and support more practical constraints in modern multi-FPGA systems, we propose an analytical framework to optimize the TDM ratios of inter-FPGA nets. A Lagrangian relaxation-based method first gives a continuous result under relaxed constraints. A binary search--based discretization algorithm is then used to assign the TDM ratio of each net such that the resulting maximum displacement is optimal and all the constraints are satisfied. Finally, a swapping-based post refinement is performed to further optimize the TDM ratios. For comparison, we also solve the problem using linear programming (LP)--based methods, which have guaranteed error bounds to the optimal solutions. Experimental results show that our framework can achieve similar quality with much shorter runtime compared to the LP-based methods. Moreover, our framework scales for designs with over 45,000 inter-FPGA nets while the runtime and memory usage of the LP-based methods will increase dramatically as the design scale becomes larger.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {feb},
articleno = {21},
numpages = {23},
keywords = {EDA, FPGA, Lagrangian relaxation, routing, time-division multiplexing}
}

@article{10.1145/3380990,
author = {Fang, Zhihan and Fu, Boyang and Qin, Zhou and Zhang, Fan and Zhang, Desheng},
title = {PrivateBus: Privacy Identification and Protection in Large-Scale Bus WiFi Systems},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3380990},
doi = {10.1145/3380990},
abstract = {Recently, the ubiquity of mobile devices leads to an increasing demand of public network services, e.g., WiFi hot spots. As a part of this trend, modern transportation systems are equipped with public WiFi devices to provide Internet access for passengers as people spend a large amount of time on public transportation in their daily life. However, one of the key issues in public WiFi spots is the privacy concern due to its open access nature. Existing works either studied location privacy risk in human traces or privacy leakage in private networks such as cellular networks based on the data from cellular carriers. To the best of our knowledge, none of these work has been focused on bus WiFi privacy based on large-scale real-world data. In this paper, to explore the privacy risk in bus WiFi systems, we focus on two key questions how likely bus WiFi users can be uniquely re-identified if partial usage information is leaked and how we can protect users from the leaked information. To understand the above questions, we conduct a case study in a large-scale bus WiFi system, which contains 20 million connection records and 78 million location records from 770 thousand bus WiFi users during a two-month period. Technically, we design two models for our uniqueness analyses and protection, i.e., a PB-FIND model to identify the probability a user can be uniquely re-identified from leaked information; a PB-HIDE model to protect users from potentially leaked information. Specifically, we systematically measure the user uniqueness on users' finger traces (i.e., connection URL and domain), foot traces (i.e., locations), and hybrid traces (i.e., both finger and foot traces). Our measurement results reveal (i) 97.8% users can be uniquely re-identified by 4 random domain records of their finger traces and 96.2% users can be uniquely re-identified by 5 random locations on buses; (ii) 98.1% users can be uniquely re-identified by only 2 random records if both their connection records and locations are leaked to attackers. Moreover, the evaluation results show our PB-HIDE algorithm protects more than 95% users from the potentially leaked information by inserting only 1.5% synthetic records in the original dataset to preserve their data utility.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {9},
numpages = {23},
keywords = {bus WiFi, finger traces, foot traces, hybrid traces, uniqueness}
}

@inproceedings{10.1145/3571306.3571438,
author = {Banerjee, Sourasekhar and Patel, Yashwant Singh and Kumar, Pushkar and Bhuyan, Monowar},
title = {Towards Post-disaster Damage Assessment using Deep Transfer Learning and GAN-based Data Augmentation},
year = {2023},
isbn = {9781450397964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571306.3571438},
doi = {10.1145/3571306.3571438},
abstract = {Cyber-physical disaster systems (CPDS) are a new cyber-physical application that collects physical realm measurements from IoT devices and sends them to the edge for damage severity analysis of impacted sites in the aftermath of a large-scale disaster. However, the lack of effective machine learning paradigms and the data and device heterogeneity of edge devices pose significant challenges in disaster damage assessment (DDA). To address these issues, we propose a generative adversarial network (GAN) and a lightweight, deep transfer learning-enabled, fine-tuned machine learning pipeline to reduce overall sensing error and improve the model’s performance. In this paper, we applied several combinations of GANs (i.e., DCGAN, DiscoGAN, ProGAN, and Cycle-GAN) to generate fake images of the disaster. After that, three pre-trained models: VGG19, ResNet18, and DenseNet121, with deep transfer learning, are applied to classify the images of the disaster. We observed that the ResNet18 is the most pertinent model to achieve a test accuracy of 88.81%. With the experiments on real-world DDA applications, we have visualized the damage severity of disaster-impacted sites using different types of Class Activation Mapping (CAM) techniques, namely Grad-CAM++, Guided Grad-Cam, &amp; Score-CAM. Finally, using k-means clustering, we have obtained the scatter plots to measure the damage severity into no damage, mild damage, and severe damage categories in the generated heat maps.},
booktitle = {Proceedings of the 24th International Conference on Distributed Computing and Networking},
pages = {372–377},
numpages = {6},
keywords = {Class Activation Mapping, Clustering., Cyber-Physical Systems, Damage Assessment, Deep Learning, Generative Adversarial Networks},
location = {<conf-loc>, <city>Kharagpur</city>, <country>India</country>, </conf-loc>},
series = {ICDCN '23}
}

@inproceedings{10.1145/3339825.3393581,
author = {Taraghi, Babak and Zabrovskiy, Anatoliy and Timmerer, Christian and Hellwagner, Hermann},
title = {CAdViSE: cloud-based adaptive video streaming evaluation framework for the automated testing of media players},
year = {2020},
isbn = {9781450368452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339825.3393581},
doi = {10.1145/3339825.3393581},
abstract = {Attempting to cope with fluctuations of network conditions in terms of available bandwidth, latency and packet loss, and to deliver the highest quality of video (and audio) content to users, research on adaptive video streaming has attracted intense efforts from the research community and huge investments from technology giants. How successful these efforts and investments are, is a question that needs precise measurements of the results of those technological advancements. HTTP-based Adaptive Streaming (HAS) algorithms, which seek to improve video streaming over the Internet, introduce video bitrate adaptivity in a way that is scalable and efficient. However, how each HAS implementation takes into account the wide spectrum of variables and configuration options, brings a high complexity to the task of measuring the results and visualizing the statistics of the performance and quality of experience. In this paper, we introduce CAdViSE, our Cloud-based Adaptive Video Streaming Evaluation framework for the automated testing of adaptive media players. The paper aims to demonstrate a test environment which can be instantiated in a cloud infrastructure, examines multiple media players with different network attributes at defined points of the experiment time, and finally concludes the evaluation with visualized statistics and insights into the results.},
booktitle = {Proceedings of the 11th ACM Multimedia Systems Conference},
pages = {349–352},
numpages = {4},
keywords = {HTTP adaptive streaming, MPEG-DASH, automated testing, media players, network emulation, quality of experience},
location = {Istanbul, Turkey},
series = {MMSys '20}
}

@inproceedings{10.1145/3489048.3522640,
author = {Jin, Lin and Hao, Shuai and Wang, Haining and Cotton, Chase},
title = {Understanding the Practices of Global Censorship through Accurate, End-to-End Measurements},
year = {2022},
isbn = {9781450391412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489048.3522640},
doi = {10.1145/3489048.3522640},
abstract = {It is challenging to conduct a large scale Internet censorship measurement, as it involves triggering censors through artificial requests and identifying abnormalities from corresponding responses. Due to the lack of ground truth on the expected responses from legitimate services, previous studies typically require a heavy, unscalable manual inspection to identify false positives while still leaving false negatives undetected. In this paper, we propose Disguiser, a novel framework that enables end-to-end measurement to accurately detect the censorship activities and reveal the censor deployment without manual efforts. The core of Disguiser is a control server that replies with a static payload to provide the ground truth of server responses. As such, we send requests from various types of vantage points across the world to our control server, and the censorship activities can be recognized if a vantage point receives a different response. In particular, we design and conduct a cache test to pre-exclude the vantage points that could be interfered by cache proxies along the network path. Then we perform application traceroute towards our control server to explore censors' behaviors and their deployment. With Disguiser, we conduct 58 million measurements from vantage points in 177 countries. We observe 292 thousand censorship activities that block DNS, HTTP, or HTTPS requests inside 122 countries, achieving a 106 false positive rate and zero false negative rate. Furthermore, Disguiser reveals the censor deployment in 13 countries.},
booktitle = {Abstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {17–18},
numpages = {2},
keywords = {censorship, disguiser, end-to-end measurements},
location = {Mumbai, India},
series = {SIGMETRICS/PERFORMANCE '22}
}

@inproceedings{10.1145/3510547.3517927,
author = {Hudani, Danish and Haseeb, Muhammad and Taufiq, Muhammad and Umer, Muhammad Azmi and Kandasamy, Nandha Kumar},
title = {A Data-Centric Approach to Generate Invariants for a Smart Grid Using Machine Learning},
year = {2022},
isbn = {9781450392297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510547.3517927},
doi = {10.1145/3510547.3517927},
abstract = {Cyber-Physical Systems (CPS) have gained popularity due to the increased requirements on their uninterrupted connectivity and process automation. Due to their connectivity over the network including intranet and internet, dependence on sensitive data, heterogeneous nature, and large-scale deployment, they are highly vulnerable to cyber-attacks. Cyber-attacks are performed by creating anomalies in the normal operation of the systems with a goal either to disrupt the operation or destroy the system completely. The study proposed here focuses on detecting those anomalies which could be the cause of cyber-attacks. This is achieved by deriving the rules that govern the physical behavior of a process within a plant. These rules are called Invariants. We have proposed a Data-Centric approach (DaC) to generate such invariants. The entire study was conducted using the operational data of a functional smart power grid which is also a living lab.},
booktitle = {Proceedings of the 2022 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {31–36},
numpages = {6},
keywords = {anomaly detection, association rule mining, critical infrastructure, cyber-physical attack, cyber-physical system, electrical power plant, industrial control system, machine learning},
location = {Baltimore, MD, USA},
series = {Sat-CPS '22}
}

@article{10.1145/3491055,
author = {Jin, Lin and Hao, Shuai and Wang, Haining and Cotton, Chase},
title = {Understanding the Practices of Global Censorship through Accurate, End-to-End Measurements},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3491055},
doi = {10.1145/3491055},
abstract = {It is challenging to conduct a large scale Internet censorship measurement, as it involves triggering censors through artificial requests and identifying abnormalities from corresponding responses. Due to the lack of ground truth on the expected responses from legitimate services, previous studies typically require a heavy, unscalable manual inspection to identify false positives while still leaving false negatives undetected. In this paper, we propose Disguiser, a novel framework that enables end-to-end measurement to accurately detect the censorship activities and reveal the censor deployment without manual efforts. The core of Disguiser is a control server that replies with a static payload to provide the ground truth of server responses. As such, we send requests from various types of vantage points across the world to our control server, and the censorship activities can be recognized if a vantage point receives a different response. In particular, we design and conduct a cache test to pre-exclude the vantage points that could be interfered by cache proxies along the network path. Then we perform application traceroute towards our control server to explore censors' behaviors and their deployment. With Disguiser, we conduct 58 million measurements from vantage points in 177 countries. We observe 292 thousand censorship activities that block DNS, HTTP, or HTTPS requests inside 122 countries, achieving a 10^-6 false positive rate and zero false negative rate. Furthermore, Disguiser reveals the censor deployment in 13 countries.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {dec},
articleno = {43},
numpages = {25},
keywords = {censorship, disguiser, end-to-end measurements}
}

@inproceedings{10.1145/3563766.3564099,
author = {Galstyan, Narek and McCauley, James and Farid, Hany and Ratnasamy, Sylvia and Shenker, Scott},
title = {Global content revocation on the internet: a case study in technology ecosystem transformation},
year = {2022},
isbn = {9781450398992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563766.3564099},
doi = {10.1145/3563766.3564099},
abstract = {Common wisdom holds that once personal content such as photographs have been shared on the Internet, they will stay there forever. This paper explores how we could allow users to reclaim some degree of their privacy by "revoking" previously shared photographs, hindering (but not eliminating) any subsequent viewing or sharing by others. Our goal is not to build a system that can withstand determined efforts to subvert it, but rather to give well-intentioned users the ability to respect the privacy wishes of others. Achieving this goal at scale will eventually require the participation of large content aggregators, and they are unlikely (putting it mildly) to find our proposal compelling. We therefore propose an approach we call technology ecosystem transformation (TET) that begins with a transitional and more easily deployable (but not fully scalable) design that does not require the participation of large incumbents but is designed to change user and societal expectations enough so that these companies would find it in their interest to adopt the approach we propose here. The intellectual challenge in this TET approach is finding transitional designs that (i) have parties willing to deploy it and (ii) once deployed, would change the incentives for the incumbents so that they would be willing to adopt the proposal.},
booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
pages = {115–121},
numpages = {7},
keywords = {social aspects of privacy, security, privacy},
location = {Austin, Texas},
series = {HotNets '22}
}

@inproceedings{10.1145/3474085.3475336,
author = {Qiu, Haonan and He, Pan and Liu, Shuchun and Shao, Weiyuan and Zhang, Feiyun and Wang, Jiajun and He, Liang and Wang, Feng},
title = {Ego-Deliver: A Large-Scale Dataset For Egocentric Video Analysis},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475336},
doi = {10.1145/3474085.3475336},
abstract = {The egocentric video provides a unique view of event participants to show their attention, vision, and interaction with objects. In this paper, we introduce Ego-Deliver, a new large-scale egocentric video benchmark recorded by takeaway riders about their daily work. To the best of our knowledge, Ego-Deliver presents the first attempt in understanding activities from the takeaway delivery process while being one of the largest egocentric video action datasets to date. Our dataset provides a total of 5,360 videos with more than 139,000 multi-track annotations and 45 different attributes, which we believe is pivotal to future research in this area. We introduce the FS-Net architecture, a new anchor-free action detection approach handling extreme variations of action durations. We partition videos into fragments and build dynamic graphs over fragments, where multi-fragment context information is aggregated to boost fragment classification. A splicing and scoring module is applied to obtain final action proposals. Our experimental evaluation confirms that the proposed framework outperforms existing approaches on the proposed Ego-Deliver benchmark and is competitive on other popular benchmarks. In our current version, Ego-Deliver is used to make a comprehensive comparison between algorithms for activity detection. We also show its application to action recognition with promising results. The dataset, toolkits and baseline results will be made available at: https://egodeliver.github.io/EgoDeliver_Dataset/},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1847–1855},
numpages = {9},
keywords = {video action localization, food delivery, egocentric vision},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3555041.3589360,
author = {Alonso, Gustavo and Ailamaki, Natassa and Krishnamurthy, Sailesh and Madden, Sam and Sivasubramanian, Swami and Ramakrishnan, Raghu},
title = {Future of Database System Architectures},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555041.3589360},
doi = {10.1145/3555041.3589360},
abstract = {Over the past two decades, we have experienced major technology disruptions on multiple fronts, none bigger than the emergence of cloud computing, which has led to fundamental changes in how database software is architected. We are seeing several new trends that are similarly shaping the future of data management.With the demise of Moore's Law, we are now seeing a lot of interest (and start-ups with significant investments) in hardware database accelerators, exploring FPGAs, GPUs, and more. Economies of scale in the cloud make it possible to move to hardware many things that were done in software, the trend will continue and increase.Modern data estates are spread across data located on premises, on the edge and in one or more public clouds, spread across various sources like multiple relational databases, file and storage systems, and no-SQL systems, both operational and analytic. This phenomenon is referred to as data sprawl.We are also seeing the emergence of many novel data workloads. For example, rich data pipelines are an increasingly common workload. And finally, Machine Learning is having a rapidly increasing role in every aspect of the database software lifecycle.This SIGMOD panel will discuss the impact of the above changes and trends on database hardware and software architectures. How will these changes impact DB system design, how will DB systems look like in the near future? Where are the hardest research challenges? What learnings from the past will guide us through these disruptions?},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {261–262},
numpages = {2},
keywords = {security, privacy, parallel databases, machine learning, lakehouses, hardware acceleration, distributed computing, database architecture, data warehouses, data governance, cloud computing},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@inproceedings{10.1145/3487552.3487821,
author = {McQuistin, Stephen and Karan, Mladen and Khare, Prashant and Perkins, Colin and Tyson, Gareth and Purver, Matthew and Healey, Patrick and Iqbal, Waleed and Qadir, Junaid and Castro, Ignacio},
title = {Characterising the IETF through the lens of RFC deployment},
year = {2021},
isbn = {9781450391290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487552.3487821},
doi = {10.1145/3487552.3487821},
abstract = {Protocol standards, defined by the Internet Engineering Task Force (IETF), are crucial to the successful operation of the Internet. This paper presents a large-scale empirical study of IETF activities, with a focus on understanding collaborative activities, and how these underpin the publication of standards documents (RFCs). Using a unique dataset of 2.4 million emails, 8,711 RFCs and 4,512 authors, we examine the shifts and trends within the standards development process, showing how protocol complexity and time to produce standards has increased. With these observations in mind, we develop statistical models to understand the factors that lead to successful uptake and deployment of protocols, deriving insights to improve the standardisation process.},
booktitle = {Proceedings of the 21st ACM Internet Measurement Conference},
pages = {137–149},
numpages = {13},
keywords = {request for comments, protocol standardisation, IETF},
location = {Virtual Event},
series = {IMC '21}
}

@inproceedings{10.1145/3474124.3474153,
author = {Choudhary, Monika and Chouhan, Satyendra Singh and Pilli, Emmanuel S.},
title = {CbPIS: Cyberbullying Profile Identification System with Users in Loop},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474153},
doi = {10.1145/3474124.3474153},
abstract = {The internet has given us access to vast amounts of information and enabled us to communicate globally. However, posting uncensored comments on internet has raised several concerns. Cyberbullying, provoking and pestering are some of the growing issues on social media. People try to belittle others on public forums for fun. There is no suitable and scalable way to establish the authenticity and reliability of information and users. In this paper, we propose Cyberbullying Profile Identification System (CbPIS), to identify bullying profile involved in cyberbullying. In the proposed work, we first evaluate and compare existing state-of-the-art machine learning and deep learning techniques, then select the suitable technique for the system development. Moreover, CbPIS takes Users’ feedback for validation purpose and based on the feedback, if needed, CbPIS retrains itself to improve the predictions. Experimental results show that CbPIS with user feedback gives effective results.},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {203–208},
numpages = {6},
keywords = {Youtube comments, Machine Learning, Cyberbullying, Bullying profiles},
location = {Noida, India},
series = {IC3-2021}
}

@inproceedings{10.1145/3448891.3448893,
author = {Elmi, Sayda and Kian-Lee, Tan},
title = {Learned Taxi Fare for real-life trip trajectories via Temporal ResNet Exploration},
year = {2021},
isbn = {9781450388405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448891.3448893},
doi = {10.1145/3448891.3448893},
abstract = {Accurate taxi fare forecasting in complex and crowded scenarios is an important building block to enabling intelligent transportation systems in a smart city. Given the observation, increasing popularity of taxi services such as Uber and Didi Chuxing in China, unable to collect large-scale taxi fare data continuously. Traditional taxi fare prediction methods mostly rely on time series forecasting techniques, which fail to model the complex non-linear spatial and temporal relations. To address those issues, we propose a Deep Multi-View Network called Temporal ResNet (TRES-Net) framework. Specifically, our proposed model consists of three views: (i) temporal view: modeling correlations between future taxi fare values with near time points, (ii) spatial view: to model deep spatial correlations, we further introduce a spatial similarity matrix that can learn from spatially similar taxi trips and capture the multi-modality of the motion patterns, and (iii) semantic view: to extract more taxi fare patterns, we integrate more factors such as trip distance, travel time, passenger count, tolls amount, tip amount, etc.. Extensive experiments on more than 700 millions NYC trips over several fare prediction benchmarks demonstrate that our method is able to predict taxi fare in complex scenarios and achieves state-of-the-art performance. Our large scale evaluation demonstrates that our system is (a) accurate—with the mean fare error under 1 US dollar and (b) capable of real-time performance.},
booktitle = {MobiQuitous 2020 - 17th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {86–95},
numpages = {10},
keywords = {Trip Spatial Similarity, Taxi Trip trajectory, Taxi Fare Prediction, Deep Learning},
location = {Darmstadt, Germany},
series = {MobiQuitous '20}
}

@inproceedings{10.1145/3407023.3407041,
author = {Rivera, Sean and Gurbani, Vijay K. and Lagraa, Sofiane and Iannillo, Antonio Ken and State, Radu},
title = {Leveraging eBPF to preserve user privacy for DNS, DoT, and DoH queries},
year = {2020},
isbn = {9781450388337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407023.3407041},
doi = {10.1145/3407023.3407041},
abstract = {The Domain Name System (DNS), a fundamental protocol that controls how users interact with the Internet, inadequately provides protection for user privacy. Recently, there have been advancements in the field of DNS privacy and security in the form of the DNS over TLS (DoT) and DNS over HTTPS (DoH) protocols. The advent of these protocols and recent advancements in large-scale data processing have drastically altered the threat model for DNS privacy. Users can no longer rely on traditional methods, and must instead take active steps to ensure their privacy. In this paper, we demonstrate how the extended Berkeley Packet Filter (eBPF) can assist users in maintaining their privacy by leveraging eBPF to provide privacy across standard DNS, DoH, and DoT communications. Further, we develop a method that allows users to enforce application-specific DNS servers. Our method provides users with control over their DNS network traffic and privacy without requiring changes to their applications while adding low overhead.},
booktitle = {Proceedings of the 15th International Conference on Availability, Reliability and Security},
articleno = {78},
numpages = {10},
location = {Virtual Event, Ireland},
series = {ARES '20}
}

@inproceedings{10.1145/3503161.3548128,
author = {Li, Zhangming and Qian, Shengsheng and Cao, Jie and Fang, Quan and Xu, Changsheng},
title = {Adaptive Transformer-Based Conditioned Variational Autoencoder for Incomplete Social Event Classification},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548128},
doi = {10.1145/3503161.3548128},
abstract = {With the rapid development of the Internet and the expanding scale of social media, incomplete social event classification has increasingly become a challenging task. The key for incomplete social event classification is to accurately leverage the image-level and text-level information. However, most of the existing approaches may suffer from the following limitations: (1) Most Generative Models use the available features to generate the incomplete modality features for social events classification while ignoring the rich semantic label information. (2) The majority of existing multi-modal methods just simply concatenate the coarse-grained image features and text features of the event to get the multi-modal features to classify social events, which ignores the irrelevant multi-modal features and limits their modeling capabilities. To tackle these challenges, in this paper, we propose an Adaptive Transformer-Based Conditioned Variational Autoencoder Network (AT-CVAE) for incomplete social event classification. In the AT-CVAE, we propose a novel Transformer-based Conditioned Variational Autoencoder to jointly model the textual information, visual information and label information into a unified deep model, which can generate more discriminative latent features and enhance the performance of incomplete social event classification. Furthermore, the Mixture-of-Experts Mechanism is utilized to dynamically acquire the weights of each multi-modal information, which can better filter out the irrelevant multi-modal information and capture the vitally important information. Extensive experiments are conducted on two public event datasets, demonstrating the superior performance of our AT-CVAE method.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1698–1707},
numpages = {10},
keywords = {social event classification, multi-modal learning, incomplete data learning},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3433996.3434000,
author = {Chen, Changdong and Zhang, Bo},
title = {Research on Behavior of Blocking Internet Advertising Regulated by Economic Law},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434000},
doi = {10.1145/3433996.3434000},
abstract = {With the advent of big data and Internet era, the scale of network economy is getting larger and larger. Blocking online advertisements is practical for users, but it also does harm to certain interests of advertisers. The antagonistic relationship has led to the debate on the regulations of Economic Law concerning blocking the internet advertisements between relevant circles of theory and practice.At present, the mainstream view analyzes the behavior from two aspects. One is to measure the behavior from the Anti-Unfair Competition Law, the other is to judge the behavior from the perspective of Copyright Law. According to the current judicial practice in China, the act of blocking online advertisements harms the economic environment that advertisers depend on, destroys the economic order of the online market, and goes against the formation of long-term interests of the Internet. However, there are loopholes in legal regulation of China's Anti-Unfair Competition Law and Copyright Law, which makes it impossible to evaluate the nature of the behavior of blocking online advertising according to the existing legal provisions.Facing the situation, we should constantly improve and perfect the economic legal system of advertising blocking behavior in China, strengthen the protection of market economic order, and provide a strong legal basis for the measurement of blocking online advertising behavior.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {14–18},
numpages = {5},
keywords = {internet advertising, blocking, Rules and regulations, Economic Law},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3565473.3569191,
author = {Wang, Ta-Yang and Zhou, Hongkuan and Kannan, Rajgopal and Swami, Ananthram and Prasanna, Viktor},
title = {Throughput optimization in heterogeneous MIMO networks: a GNN-based approach},
year = {2022},
isbn = {9781450399333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565473.3569191},
doi = {10.1145/3565473.3569191},
abstract = {With the development of 5G and IoT networks, Device-to-Device (D2D) communication has become a major paradigm in wireless communication. Most existing approaches for D2D resource allocation are usually time consuming and demand a high computational budget, especially in heterogeneous deployments where the D2D links have different configurations (i.e., different number of transmit and receive antennas). Recently, Graph neural networks (GNNs) have been proposed to solve many problems in the networking domain and have significantly outperformed traditional algorithms, including throughput optimization problems in D2D networks. However, existing throughput optimization works either only apply to MISO or SISO D2D networks or require extremely long runtime on MIMO D2D networks, which makes it hard to apply them in real-world D2D applications. In this paper, we consider the throughput prediction problem across a fixed association of transmitters and receivers to maximize the total throughput in heterogeneous MIMO D2D networks. We model the interference between different link types as heterogeneous edges and learn the optimal beamforming policy using a heterogeneous GNN. Simulation results show that our proposed GNN-based approach achieves a significant speedup compared with the state-of-the-art algorithm, while providing robust performance on large-scale synthetic datasets.},
booktitle = {Proceedings of the 1st International Workshop on Graph Neural Networking},
pages = {42–47},
numpages = {6},
keywords = {resource allocation, heterogeneous network, graph neural network, device-to-device communications, beamforming},
location = {<conf-loc>, <city>Rome</city>, <country>Italy</country>, </conf-loc>},
series = {GNNet '22}
}

@inproceedings{10.1145/3571306.3571431,
author = {Sikarwar, Himani and Das, Debasis},
title = {SMMAP: Secure MAC-based Mutual Authentication Protocol for IoV},
year = {2023},
isbn = {9781450397964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571306.3571431},
doi = {10.1145/3571306.3571431},
abstract = {Internet of Vehicles (IoV) is the emerging domain in the Intelligent Transportation System (ITS), which facilitate synergic decision from the human being involved in the network and has been a focus of research in the past few years. IoV has the capabilities of large-scale data sensing, collection, and sharing of information between vehicles to the roadside unit and other smart nodes in the environment, allowing dynamic and spontaneous communication. However, these communications in IoV are more prone to security attacks and still face enormous challenges in the aspect of security and privacy. This paper presents an effective and lightweight Secure MAC-based Mutual Authentication Protocol (SMMAP) for IoV and a mechanism for secure communication between vehicles and other smart units of IoV. We provide a rigorous proof of security and authenticity for the proposed SMMAP scheme using the BAN logic. Qualitative and quantitative analysis has been concluded in the paper, which depicts that the SMMAP has better performance than other existing state-of-the-art schemes.},
booktitle = {Proceedings of the 24th International Conference on Distributed Computing and Networking},
pages = {330–335},
numpages = {6},
keywords = {security in IoV, network security, mutual authentication in IoV, batch verification, MAC-based authentication},
location = {<conf-loc>, <city>Kharagpur</city>, <country>India</country>, </conf-loc>},
series = {ICDCN '23}
}

@article{10.1145/3505226,
author = {Zhang, Ning and Ebrahimi, Mohammadreza and Li, Weifeng and Chen, Hsinchun},
title = {Counteracting Dark Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3505226},
doi = {10.1145/3505226},
abstract = {Automated monitoring of dark web (DW) platforms on a large scale is the first step toward developing proactive Cyber Threat Intelligence (CTI). While there are efficient methods for collecting data from the surface web, large-scale dark web data collection is often hindered by anti-crawling measures. In particular, text-based CAPTCHA serves as the most prevalent and prohibiting type of these measures in the dark web. Text-based CAPTCHA identifies and blocks automated crawlers by forcing the user to enter a combination of hard-to-recognize alphanumeric characters. In the dark web, CAPTCHA images are meticulously designed with additional background noise and variable character length to prevent automated CAPTCHA breaking. Existing automated CAPTCHA breaking methods have difficulties in overcoming these dark web challenges. As such, solving dark web text-based CAPTCHA has been relying heavily on human involvement, which is labor-intensive and time-consuming. In this study, we propose a novel framework for automated breaking of dark web CAPTCHA to facilitate dark web data collection. This framework encompasses a novel generative method to recognize dark web text-based CAPTCHA with noisy background and variable character length. To eliminate the need for human involvement, the proposed framework utilizes Generative Adversarial Network (GAN) to counteract dark web background noise and leverages an enhanced character segmentation algorithm to handle CAPTCHA images with variable character length. Our proposed framework, DW-GAN, was systematically evaluated on multiple dark web CAPTCHA testbeds. DW-GAN significantly outperformed the state-of-the-art benchmark methods on all datasets, achieving over 94.4% success rate on a carefully collected real-world dark web dataset. We further conducted a case study on an emergent Dark Net Marketplace (DNM) to demonstrate that DW-GAN eliminated human involvement by automatically solving CAPTCHA challenges with no more than three attempts. Our research enables the CTI community to develop advanced, large-scale dark web monitoring. We make DW-GAN code available to the community as an open-source tool in GitHub.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {mar},
articleno = {21},
numpages = {21},
keywords = {generative adversarial networks, dark web, Automated CAPTCHA breaking}
}

@inproceedings{10.1145/3555776.3578730,
author = {Haines, Thomas and M\"{u}ller, Johannes and Querejeta-Azurmendi, I\~{n}igo},
title = {Scalable Coercion-Resistant E-Voting under Weaker Trust Assumptions},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578730},
doi = {10.1145/3555776.3578730},
abstract = {Electronic voting (e-voting) is regularly used in many countries and organizations for legally binding elections. In order to conduct such elections securely, numerous e-voting systems have been proposed over the last few decades. Notably, some of these systems were designed to provide coercion-resistance. This property protects against potential adversaries trying to swing an election by coercing voters.Despite the multitude of existing coercion-resistant e-voting systems, to date, only few of them can handle large-scale Internet elections efficiently. One of these systems, VoteAgain (USENIX Security 2020), was originally claimed secure under similar trust assumptions to state-of-the-art e-voting systems without coercion-resistance.In this work, we review VoteAgain's security properties. We discover that, unlike originally claimed, VoteAgain is no more secure than a trivial voting system with a completely trusted election authority. In order to mitigate this issue, we propose a variant of VoteAgain which effectively mitigates trust on the election authorities and, at the same time, preserves VoteAgain's usability and efficiency.Altogether, our findings bring the state of science one step closer to the goal of scalable coercion-resistant e-voting being secure under reasonable trust assumptions.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1576–1584},
numpages = {9},
keywords = {attack, coercion-resistance, privacy, verifiability, electronic voting},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3627631.3627652,
author = {R P, Aneesh and Zacharias, Joseph},
title = {TLR-Net :Transfer Learning in Residual U-Net for Enhancing Skin Lesion Segmentation},
year = {2024},
isbn = {9798400716256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627631.3627652},
doi = {10.1145/3627631.3627652},
abstract = {Skin lesion semantic segmentation is a critical task in dermatology, aiding early diagnosis and treatment of skin disorders, including melanoma and other forms of skin cancer. Challenge datasets in skin lesion segmentation play a pivotal role in advancing the field by providing standardised benchmarks, promoting collaboration, and facilitating the development of accurate and clinically relevant segmentation algorithms. This paper presents a novel approach to skin lesion segmentation, focusing on the development of a pretrained model for skin lesion segmentation, leveraging a challenging dataset. Transfer Learning in Residual U-Net (TLR-Net) is proposed in this paper to segment the skin lesions from dermoscopic images. It combines the power of transfer learning and the residual learning framework to achieve highly accurate and efficient skin lesion semantic segmentation. The TLR-Net leverages the U-Net’s encoder-decoder architecture with skip connections for effective feature extraction and upsampling. Additionally, it incorporates residual blocks within the network to enable the learning of residual mappings, enabling deeper and more efficient feature extraction. Crucially, transfer learning is employed to initialise the model with pre-trained weights from a large-scale dataset, enhancing its ability to generalise skin lesion semantic segmentation tasks with limited labelled data. We evaluated the TLR-Net on a diverse and challenging skin lesion dataset, demonstrating its superior performance compared to traditional U-Net and other state-of-the-art segmentation architectures. Our results indicate that the TLR-Net provides more precise delineation of skin lesions, computationally efficient and suitable for real-world applications. This advancement has significant implications in dermatological practice, empowering clinicians with a reliable tool for early diagnosis and better patient outcomes.},
booktitle = {Proceedings of the Fourteenth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {21},
numpages = {8},
keywords = {Deep learning, ResUnet, U-net, dermoscopy, skin lesion segmentation},
location = {<conf-loc>, <city>Rupnagar</city>, <country>India</country>, </conf-loc>},
series = {ICVGIP '23}
}

@inproceedings{10.1145/3372278.3390716,
author = {Fu, Haiyan and Li, Ying and Zhang, Hengheng and Liu, Jinfeng and Yao, Tao},
title = {Rank-embedded Hashing for Large-scale Image Retrieval},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3390716},
doi = {10.1145/3372278.3390716},
abstract = {With the growth of images on the Internet, plenty of hashing methods are developed to handle the large-scale image retrieval task. Hashing methods map data from high dimension to compact codes, so that they can effectively cope with complicated image features. However, the quantization process of hashing results in unescapable information loss. As a consequence, it is a challenge to measure the similarity between images with generated binary codes. The latest works usually focus on learning deep features and hashing functions simultaneously to preserve the similarity between images, while the similarity metric is fixed. In this paper, we propose a Rank-embedded Hashing (ReHash) algorithm where the ranking list is automatically optimized together with the feedback of the supervised hashing. Specifically, ReHash jointly trains the metric learning and the hashing codes in an end-to-end model. In this way, the similarity between images are enhanced by the ranking process. Meanwhile, the ranking results are an additional supervision for the hashing function learning as well. Extensive experiments show that our ReHash outperforms the state-of-the-art hashing methods for large-scale image retrieval.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {563–570},
numpages = {8},
keywords = {large-scale retrieval, image retrieval, image ranking, deep supervised hashing},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@inproceedings{10.1145/3404397.3404468,
author = {Seal, Sudip K. and Lim, Seung-Hwan and Wang, Dali and Hinkle, Jacob and Lunga, Dalton and Tsaris, Aristeidis},
title = {Toward Large-Scale Image Segmentation on Summit},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404468},
doi = {10.1145/3404397.3404468},
abstract = {Semantic segmentation of images is an important computer vision task that emerges in a variety of application domains such as medical imaging, robotic vision and autonomous vehicles to name a few. While these domain-specific image analysis tasks involve relatively small image sizes (∼ 102 \texttimes{} 102), there are many applications that need to train machine learning models on image data with extents that are orders of magnitude larger (∼ 104 \texttimes{} 104). Training deep neural network (DNN) models on large extent images is extremely memory-intensive and often exceeds the memory limitations of a single graphical processing unit, a hardware accelerator of choice for computer vision workloads. Here, an efficient, sample parallel approach to train U-Net models on large extent image data sets is presented. Its advantages and limitations are analyzed and near-linear strong-scaling speedup demonstrated on 256 nodes (1536 GPUs) of the Summit supercomputer. Using a single node of the Summit supercomputer, an early evaluation of a recently released model parallel framework called GPipe is demonstrated to deliver ∼ 2X speedup in executing a U-Net model with an order of magnitude larger number of trainable parameters than reported before. Performance bottlenecks for pipelined training of U-Net models are identified and mitigation strategies to improve the speedups are discussed. Together, these results open up the possibility of combining both approaches into a unified scalable pipelined and data parallel algorithm to efficiently train U-Net models with very large receptive fields on data sets of ultra-large extent images.},
booktitle = {Proceedings of the 49th International Conference on Parallel Processing},
articleno = {27},
numpages = {11},
keywords = {scalable data analytics, pipeline., model parallel, image segmentation, deep neural networks, applied machine learning, U-Net},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@inproceedings{10.1145/3437963.3441653,
author = {Dumais, Susan T.},
title = {Beyond Web Search: How Email Search is Different and Why it Matters},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441653},
doi = {10.1145/3437963.3441653},
abstract = {Web search has transformed how we access all kinds of information and has become a core fabric of everyday life. It is used to find information, buy things, plan travel, understand medical conditions, monitor events, etc. Search in other domains has not received nearly the same attention so our experiences in web search shape our thinking about search more generally even when the scenarios are quite different. This is especially true for email search. Although email was initially designed to facilitate asynchronous communication, it has also become a large repository of personal information. The volume of email continues to grow in both consumer and enterprise settings, and search plays a key role in getting back to needed information. Email search is, however, very different than Web search on many dimensions -- the content being sought is personal and private, metadata such as who sent it or when it was sent is plentiful and important, search intentions are different, people know a lot about what they are looking for, etc. Given these differences, new approaches are required. In this talk I will summarize research we have done using large-scale behavioral logs and complementary qualitative methods to characterize what are people looking for, what they know about what they are looking for, and how this interacts with email management practices. I will then describe several opportunities to help people articulate their information needs and design interfaces and interaction techniques to support this. Finally, I will conclude by pointing to new frontiers in email management and search.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {2},
numpages = {1},
keywords = {interactive information retrieval, email search},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3517745.3561449,
author = {Jain, Akshath and Patra, Deepayan and Xu, Peijing and Sherry, Justine and Gill, Phillipa},
title = {The ukrainian internet under attack: an NDT perspective},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561449},
doi = {10.1145/3517745.3561449},
abstract = {On February 24, 2022, Russia began a large-scale invasion of Ukraine, the first widespread conflict in a country with high levels of network penetration. Because the Internet was designed with resilience under warfare in mind, the war in Ukraine offers the networking community a unique opportunity to evaluate whether and to what extent this design goal has been realized. We provide an early glimpse at Ukrainian network resilience over 54 days of war using data from Measurement Lab's Network Diagnostic Tool (NDT). We find that NDT users' network performance did indeed degrade - e.g. with average packet loss rates increasing by as much as 500% relative to pre-wartime baselines in some regions - and that the intensity of the degradation correlated with the presence of Russian troops in the region. Performance degradation also correlated with changes in traceroute paths; we observed an increase in path diversity and significant changes to routing decisions at Ukrainian border Autonomous Systems (ASes) post-invasion. Overall, the use of diverse and changing paths speaks to the resilience of the Internet's underlying routing algorithms, while the correlated degradation in performance highlights a need for continued efforts to ensure usability and stability during war.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {166–178},
numpages = {13},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.4230/LIPIcs.CCC.2023.33,
author = {Chia, Nai-Hui and Chung, Kai-Min and Hsieh, Yao-Ching and Lin, Han-Hsuan and Lin, Yao-Ting and Shen, Yu-Ching},
title = {On the Impossibility of General Parallel Fast-Forwarding of Hamiltonian Simulation},
year = {2023},
isbn = {9783959772822},
publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
address = {Dagstuhl, DEU},
url = {https://doi.org/10.4230/LIPIcs.CCC.2023.33},
doi = {10.4230/LIPIcs.CCC.2023.33},
abstract = {Hamiltonian simulation is one of the most important problems in the field of quantum computing. There have been extended efforts on designing algorithms for faster simulation, and the evolution time T for the simulation greatly affect algorithm runtime as expected. While there are some specific types of Hamiltonians that can be fast-forwarded, i.e., simulated within time o(T), for some large classes of Hamiltonians (e.g., all local/sparse Hamiltonians), existing simulation algorithms require running time at least linear in the evolution time T. On the other hand, while there exist lower bounds of Ω(T) circuit size for some large classes of Hamiltonian, these lower bounds do not rule out the possibilities of Hamiltonian simulation with large but "low-depth" circuits by running things in parallel. As a result, physical systems with system size scaling with T can potentially do a fast-forwarding simulation. Therefore, it is intriguing whether we can achieve fast Hamiltonian simulation with the power of parallelism.In this work, we give a negative result for the above open problem in various settings. In the oracle model, we prove that there are time-independent sparse Hamiltonians that cannot be simulated via an oracle circuit of depth o(T). In the plain model, relying on the random oracle heuristic, we show that there exist time-independent local Hamiltonians and time-dependent geometrically local Hamiltonians on n qubits that cannot be simulated via an oracle circuit of depth o(T/nc), where the Hamiltonians act on n qubits, and c is a constant. Lastly, we generalize the above results and show that any simulators that are geometrically local Hamiltonians cannot do the simulation much faster than parallel quantum algorithms.},
booktitle = {Proceedings of the Conference on Proceedings of the 38th Computational Complexity Conference},
articleno = {33},
numpages = {45},
keywords = {parallel query lower bound, depth lower bound, hamiltonian simulation},
location = {Warwick, United Kingdom},
series = {CCC '23}
}

@inproceedings{10.1145/3574198.3574240,
author = {Lin, Yuping and Hou, Jie},
title = {A Platform for the Necessities Distribution and Circulation of Residents in the Control Community During COVID-19 Outbreak Period},
year = {2023},
isbn = {9781450397223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574198.3574240},
doi = {10.1145/3574198.3574240},
abstract = {The ravages of the COVID-19 and the continuous mutation of the COVID-19 make this war without gunpowder smoke unstoppable. With the continuous epidemic prevention and control, the resident closure and isolation by community is an effective way to block the large-scale development of the COVID-19. However, the shortage of daily necessities for residents during the lockdown period requires timely arrangements and deployment by local departments to ensure the basic living of the residents under lockdown. A necessary distribution and circulation system in the epidemic prevention and control community was designed and developed in this paper. The proposed necessity distribution and circulation system is mainly to help the government distribute supplies to residents and the circulation of necessities between residents more efficiently. The design process of the system includes the software development process of demand analysis, overall design, detailed design and programming; it was adopted CS three-tier architecture software development mode and the software development technology of .Net + SQLSERVER. The main business modules of the system are including necessity circulation between residents, government supply management, and volunteer necessity delivery business. The system can also be applied to the necessity circulation subsystem of the community healthy life service platform for the daily life of residents.},
booktitle = {Proceedings of the 2022 9th International Conference on Biomedical and Bioinformatics Engineering},
pages = {267–274},
numpages = {8},
keywords = {Necessity Circulation System, Intelligent Community Health Service Platform, COVID-19},
location = {Kyoto, Japan},
series = {ICBBE '22}
}

@inproceedings{10.1145/3505170.3506722,
author = {Gao, Xiang and Jiang, Yi-Min and Shao, Lixin and Raspopovic, Pedja and Verbeek, Menno E. and Sharma, Manish and Rashingkar, Vineet and Jalota, Amit},
title = {Congestion and Timing Aware Macro Placement Using Machine Learning Predictions from Different Data Sources: Cross-design Model Applicability and the Discerning Ensemble},
year = {2022},
isbn = {9781450392105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505170.3506722},
doi = {10.1145/3505170.3506722},
abstract = {Modern very large-scale integration (VLSI) designs typically use a lot of macros (RAM, ROM, IP) that occupy a large portion of the core area. Also, macro placement being an early stage of the physical design flow, followed by standard cell placement, physical synthesis (place-opt), clock tree synthesis and routing, etc., has a big impact on the final quality of result (QoR). There is a need for Electronic Design Automation (EDA) physical design tools to provide predictions for congestion, timing, and power etc., with certainty for different macro placements before running time-consuming flows. However, the diversity of IC designs that commercial EDA tools must support and the limited number of similar designs that can provide training data, make such machine learning (ML) predictions extremely hard. Because of this, ML models usually need to be completely retrained for unseen designs to work properly. However, collecting full flow macro placement ML data is time consuming and impractical. To make things worse, common ML methods, such as regression, support vector machine (SVM), random forest (RF), neural network (NN) in general, lack a good estimation of prediction accuracy or confidence and lack debuggability for cross-design applications. In this paper, we present a novel discerning ensemble technique for cross-design ML prediction for macro placement. We developed our solution based on a large number of designs with different design styles and technology nodes, and tested the solution on 8 leading-edge industry designs and achieved comparable or even better results in a few hours (per design) than manual placement results that take many engineers weeks or even months to achieve. Our method shows great promise for many ML problems in EDA applications, or even in other areas.},
booktitle = {Proceedings of the 2022 International Symposium on Physical Design},
pages = {195–202},
numpages = {8},
keywords = {trusted machine learning, model applicability, macro placement, domain transfer, discerning ensemble},
location = {Virtual Event, Canada},
series = {ISPD '22}
}

@inproceedings{10.1145/3551663.3558685,
author = {Santana, Juan Ram\'{o}n and Sotres, Pablo and P\'{e}rez, Jes\'{u}s and S\'{a}nchez, Luis and Lanza, Jorge and Mu\~{n}oz, Luis},
title = {LoRaWAN-based Smart Parking Service: Deployment and Performance Evaluation},
year = {2022},
isbn = {9781450394833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551663.3558685},
doi = {10.1145/3551663.3558685},
abstract = {The dramatic increase of the urban population has put a lot of pressure in modern urban transportation systems. This not only implies noteworthy air pollution, and waste in time and energy, but also has led to the critical issue of the parking spots scarcity. Finding a parking spot has become one of the most common problems mentioned by drivers due to the day-by-day increase in number of vehicles. To overcome this problem, smart parking services based on sensors to detect parking spot occupancy status and inform drivers about their availability have been proposed. With the successful deployment of this kind of solutions, costs associated with traffic jams or wasted gas can be considerably reduced. However, one of the main challenges to be addressed when planning and deploying this kind of services is the communication technology employed by parking sensors, which are typically buried under the asphalt, and has to be able to cover large areas of the city in the most cost-efficient way. In this paper, we analyze the behavior and performance of a LoRaWAN network employed for supporting a Smart Parking service in the city of Santander. The sensors and network deployment are described in the paper and the thorough experimental assessment and evaluation of the network behavior is presented. The goal of this evaluation is to provide a better understanding of the key factors affecting the communication of outdoor parking spots. Through the continuous monitoring of two parking sensor deployments in the city, we derive some conclusions and lessons learnt that could benefit the planning and deployment of city-scale IoT infrastructures employing LoRaWAN networks as their wireless access technology.},
booktitle = {Proceedings of the 19th ACM International Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, &amp; Ubiquitous Networks},
pages = {107–114},
numpages = {8},
keywords = {smart parking, smart cities, LoRaWAN, LoRa, LPWAN, IoT},
location = {Montreal, Quebec, Canada},
series = {PE-WASUN '22}
}

@inproceedings{10.1145/3510457.3513029,
author = {Liu, Jiangchao and Liu, Jierui and Di, Peng and Liu, Alex X. and Zhong, Zexin},
title = {Record and replay of online traffic for microservices with automatic mocking point identification},
year = {2022},
isbn = {9781450392266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510457.3513029},
doi = {10.1145/3510457.3513029},
abstract = {Using recorded online traffic for the regression testing of web applications has become a common practice in industry. However, this "record and replay" on microservices is challenging because simply recorded online traffic (i.e., values for variables or input/output for function calls) often cannot be successfully replayed because microservices often have various dependencies on the complicated online environment. These dependencies include the states of underlying systems, internal states (e.g., caches), and external states (e.g., interaction with other microservices/middleware). Considering the large size and the complexity of industrial microservices, an automatic, scalable, and precise identification of such dependencies is needed as manual identification is time-consuming. In this paper, we propose an industrial grade solution to identifying all dependencies, and generating mocking points automatically using static program analysis techniques. Our solution has been deployed in a large Internet company (i.e., Ant Group) to handle hundreds of microservices, which consists of hundreds of millions lines of code, with high success rate in replay (99% on average). Moreover, our framework can boost the efficiency of the testing system by refining dependencies that must not affect the behavior of a microservice. Our experimental results show that our approach can filter out 73.1% system state dependency and 71.4% internal state dependency, which have no effect on the behavior of the microservice.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
pages = {221–230},
numpages = {10},
keywords = {static analysis, record and replay, mocking, microservice},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIP '22}
}

@inproceedings{10.1145/3481357.3481527,
author = {Li, Karen and Rashid, Awais and Roudaut, Anne},
title = {Vision: Security-Usability Threat Modeling for Industrial Control Systems},
year = {2021},
isbn = {9781450384230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3481357.3481527},
doi = {10.1145/3481357.3481527},
abstract = {Industrial Control System (ICS) that run large-scale systems such as water, power and manufacturing are increasingly in focus given high profile attacks against such infrastructures. These systems are connected to IT systems and the Internet, the intersections of their users – typically control systems engineers and operators – with security requirements and systems add to the complexity of the threats faced by these environments. The challenges of usable security in IT systems have been studied extensively, including work on security-usability threat modeling (i.e. lack of usability exacerbating security issues). However, no work has examined similar challenges within ICS settings where, in addition to the regular requirements of information confidentiality, information integrity and information availability, requirements such as processsafety, processintegrity and processreliability are paramount for the users. Using the case of a Programmable Logic Controller (PLC), we detail the workflow that the user undertake for a security task. We analyze this workflow using STRIDE, an established threat modeling approach. We then map the threats against an existing security-usability threat model for IT systems whilst also taking into account the specific process-related requirements critical to ICS users. We then derive an initial security-usability threat model for ICS as a first step towards further work in this regard.},
booktitle = {Proceedings of the 2021 European Symposium on Usable Security},
pages = {83–88},
numpages = {6},
keywords = {Web Server Case Study, Security Usability, Programmable Logic Controller, Industrial Control Systems},
location = {Karlsruhe, Germany},
series = {EuroUSEC '21}
}

@inproceedings{10.1145/3568562.3568590,
author = {Nguyen Quoc, Khanh and Bui, Tung and Le, Dong and Tran, Duc and Nguyen, Toan and Nguyen, Huu Trung},
title = {Detecting DGA Botnet based on Malware Behavior Analysis},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568562.3568590},
doi = {10.1145/3568562.3568590},
abstract = {DGA botnet uses the Domain Generation Algorithm to generate domains that are used to establish the connection between malware bots and malicious actors. It has become a serious threat to internet-connected systems. Detection of DGA botnets is a challenging task due to its complexity and performance issues when processing a great amount of data from real-time large-scale networks. In this paper, we propose and develop a DGA botnet detection method using the combination of the Long Short-Term Memory network (LSTM) and network traffic analysis. We also propose a set of rules that can be used for detecting various DGA malware behaviors. Our method recognizes even hard-to-detect dictionary DGAs such as suppobox and matsnu, while providing an F1-score of 0.9888.},
booktitle = {Proceedings of the 11th International Symposium on Information and Communication Technology},
pages = {158–164},
numpages = {7},
keywords = {Traffic Analysis, Datasets, DGA behaviors, DGA Malware, Botnet Detection},
location = {Hanoi, Vietnam},
series = {SoICT '22}
}

@inproceedings{10.1145/3516529.3516599,
author = {Ping, Li},
title = {Research on the evaluation system of fresh food logistics based on AHP-entropy weight method},
year = {2022},
isbn = {9781450384100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3516529.3516599},
doi = {10.1145/3516529.3516599},
abstract = {In recent years, with the rapid popularization and rapid development of the fresh food logistics industry, the e-commerce of online sales of fresh products has gradually emerged, which has attracted strong attention from the society. People can buy fresh vegetables and meat products from the Internet, which greatly saves people's daily food purchasing time and improves purchasing power and efficiency. In addition, the domestic e-commerce industry is currently in a period of rapid development on a large scale, and fresh food logistics is a hot spot in the field of e-commerce. Take the well-known Jingdong Mall as an example. This paper combines the principles and steps of the selection of third-party logistics providers, analyzes the evaluation and indicators of fresh logistics quality accepted by the public, and regulates the development of the fresh product warehousing logistics industry. Corresponding evaluation indexes are put forward, and a more reasonable evaluation index system is further established. Then, by analyzing the existing evaluation methods, a combination of subjective and objective evaluation methods is proposed, and a comprehensive evaluation model based on AHP and entropy weight method is established by using AHP, entropy weight method, etc., for the selection and evaluation of logistics service providers. References are provided. Finally, through the analysis of examples, using the comprehensive evaluation model, the evaluation system of my country's fresh logistics system is proposed, and a complete fresh logistics evaluation system is established. The measures promote the balanced and healthy development of fresh food enterprises in the logistics transportation and distribution service system; on the other hand, it can bring consumers a better shopping experience and enhance the competitiveness of international logistics.},
booktitle = {2021 2nd Artificial Intelligence and Complex Systems Conference},
pages = {347–351},
numpages = {5},
keywords = {Logistics Service, Jingdong Fresh, Evaluation System, Entropy Weight Method, Analytic Hierarchy Process},
location = {Bangkok, Thailand},
series = {AICSconf '21}
}

@proceedings{10.1145/3469266,
title = {DigiBiom '21: Proceedings of the 2021 Workshop on Future of Digital Biomarkers},
year = {2021},
isbn = {9781450386067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the ACM workshop on Future of Digital Biomarkers 2021 (DigiBiom'21). The Workshop on the Future of Digital Biomarkers will offer a unified forum that brings academics, industry researchers and medical practitioners together to explore the role of existing and future mobile technology for modeling, testing, and validating new digital biomarkers. The workshop aims to facilitate a systematic discussion among experts from different knowledge domains including mobile sensing, systems, machine learning, medicine, and health sciences. The workshop aims to (i) identify new digital biomarkers for capturing different physiological and behavioral health conditions and diseases, (ii) identify the key shortcomings of the existing research in terms of scalability, customizability, and sensing affordances, (iii) find realistic solutions by leveraging sensor data from a variety of mobile systems (e.g., smartphones, wearables, and IoT devices), (iv) identify key methodologies for validation and testing of the new biomarker evidence engine.The call for papers attracted highly relevant submissions from around the world. The program committee accepted 5 full papers and 3 short papers out of 10 submissions. In addition to the presentations of the 8 accepted papers, the workshop will feature three keynote speakers and an industry panel.1. Morning Keynote: "Social Media Derived Biomarkers of Mental Health: Opportunities, Pitfalls, and the Next Frontier", Prof. Munmun De Choudhury, School of Interactive Computing, Georgia Institute of Technology.2. Afternoon Keynote: "Digital Epidemiology and the COVID-19 Pandemic", Prof. John Brownstein, Harvard Medical School.3. Industry Panel: “Digital Biomarkers in the Health Technology Startup World”4. Evening Keynote: "The Role of Science Policy in Developing Digital Biomarkers", Dr. Wendy Nilsen, National Science Foundation.},
location = {Virtual Event, Wisconsin}
}

@inproceedings{10.1145/3600211.3604659,
author = {Naggita, Keziah and LaChance, Julienne and Xiang, Alice},
title = {Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604659},
doi = {10.1145/3600211.3604659},
abstract = {Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an “othering” phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to improve the applicability of computer vision models in a global context.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {520–530},
numpages = {11},
keywords = {Machine Learning, Geo-diversity, Datasets, Computer Vision, Africa, AI Ethics},
location = {<conf-loc>, <city>Montr\'{e}al</city>, <state>QC</state>, <country>Canada</country>, </conf-loc>},
series = {AIES '23}
}

@inproceedings{10.1145/3387514.3405871,
author = {Kakarla, Siva Kesava Reddy and Beckett, Ryan and Arzani, Behnaz and Millstein, Todd and Varghese, George},
title = {GRooT: Proactive Verification of DNS Configurations},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405871},
doi = {10.1145/3387514.3405871},
abstract = {The Domain Name System (DNS) plays a vital role in today's Internet but relies on complex distributed management of records. DNS misconfiguration related outages have rendered popular services like GitHub, HBO, LinkedIn, and Azure inaccessible for extended periods. This paper introduces GRoot, the first verifier that performs static analysis of DNS configuration files, enabling proactive and exhaustive checking for common DNS bugs; by contrast, existing solutions are reactive and incomplete. GRoot uses a new, fast verification algorithm based on generating and enumerating DNS query equivalence classes. GRoot symbolically executes the set of queries in each equivalence class to efficiently find (or prove the absence of) any bugs such as rewrite loops. To prove the correctness of our approach, we develop a formal semantic model of DNS resolution. Applied to the configuration files from a campus network with over a hundred thousand records, GRoot revealed 109 bugs within seconds. When applied to internal zone files consisting of over 3.5 million records from a large infrastructure service provider, GRoot revealed around 160k issues of blackholing, initiating a cleanup. Finally, on a synthetic dataset with over 65 million real records, we find GRoot can scale to networks with tens of millions of records.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {310–328},
numpages = {19},
keywords = {Verification, Static Analysis, Formal Methods, DNS},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@article{10.1145/3528223.3530065,
author = {Wang, Sheng-Yu and Bau, David and Zhu, Jun-Yan},
title = {Rewriting geometric rules of a GAN},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3528223.3530065},
doi = {10.1145/3528223.3530065},
abstract = {Deep generative models make visual content creation more accessible to novice users by automating the synthesis of diverse, realistic content based on a collected dataset. However, the current machine learning approaches miss a key element of the creative process - the ability to synthesize things that go far beyond the data distribution and everyday experience. To begin to address this issue, we enable a user to "warp" a given model by editing just a handful of original model outputs with desired geometric changes. Our method applies a low-rank update to a single model layer to reconstruct edited examples. Furthermore, to combat overfitting, we propose a latent space augmentation method based on style-mixing. Our method allows a user to create a model that synthesizes endless objects with defined geometric changes, enabling the creation of a new generative model without the burden of curating a large-scale dataset. We also demonstrate that edited models can be composed to achieve aggregated effects, and we present an interactive interface to enable users to create new models through composition. Empirical measurements on multiple test cases suggest the advantage of our method against recent GAN fine-tuning methods. Finally, we showcase several applications using the edited models, including latent space interpolation and image editing.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {73},
numpages = {16},
keywords = {vision for graphics, user-guided model creation, generative adversarial networks, deep learning}
}

@inproceedings{10.1145/3497777.3498551,
author = {Wung, Wei-Shiang and Ting, Guan-Ting and Hsu, Ruey-Tzer and Hsu, Cheng and Tsai, Yu-Chien and Wang, Caleb and Liu, Yuan-Tai and Chen, Hsi and Huang, Polly},
title = {Twitch’s CDN as an Open Population Ecosystem},
year = {2021},
isbn = {9781450391849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3497777.3498551},
doi = {10.1145/3497777.3498551},
abstract = {The quality and continuity of the video services such as Twitch depend on the scale and well-being of their content distribution networks (CDNs). Each CDN may consist of 1000s of servers, physically feeding the videos to the clients. Opting for a better understanding, researchers have attempted to measure and analyze the CDNs of popular video services&nbsp;[10, 11, 12, 19]. These works are, however, one-time effort. Given the widespread use of Twitch, we find continuous survey of its CDN an important subject of study. The challenge lies in the cost of performing the Internet-scale scans – the probing traffic. The larger the CDNs and the more frequent the scans are, the higher the overhead. Instead of performing full scans repeatedly, we envision a cost-effective alternative that samples and estimates the CDN size (i.e., the number of servers). Only when the size change is significant, does the system trigger a full scan. To this end and inspired by Capture-Mark-Recapture (CMR), a methodology widely used in Ecology to estimate animal population with little human effort, we propose two mechanisms to estimate the CDN size with lightweight traffic. Using a data set collected in Nov 2019, we find a 7.25% average estimation error. Provided an estimation error bound, we can identify as well the best parameter combination to minimize the probing traffic.},
booktitle = {Proceedings of the 16th Asian Internet Engineering Conference},
pages = {56–63},
numpages = {8},
keywords = {Twitch, Server Population Estimation, Content Distribution Network, Capture Mark Recapture},
location = {Virtual Event, Japan},
series = {AINTEC '21}
}

@inproceedings{10.1145/3580305.3599182,
author = {Ganapavarapu, Giridhar and Mukherjee, Sumanta and Martinez Gil, Natalia and Sarpatwar, Kanthi and Rajasekharan, Amaresh and Dhurandhar, Amit and Arya, Vijay and Vaculin, Roman},
title = {AI Explainability 360 Toolkit for Time-Series and Industrial Use Cases},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599182},
doi = {10.1145/3580305.3599182},
abstract = {With the growing adoption of AI, trust and explainability have become critical which has attracted a lot of research attention over the past decade and has led to the development of many popular AI explainability libraries such as AIX360, Alibi, OmniXAI, etc. Despite that, applying explainability techniques in practice often poses challenges such as lack of consistency between explainers, semantically incorrect explanations, or scalability. Furthermore, one of the key modalities that has been less explored, both from the algorithmic and practice point of view, is time-series. Several application domains involve time-series including Industry 4.0, asset monitoring, supply chain or finance to name a few.The AIX360 library (https://github.com/Trusted-AI/AIX360) has been incubated by the Linux Foundation AI &amp; Data open-source projects and it has gained significant popularity: its public GitHub repository has over 1.3K stars and has been broadly adopted in the academic and applied settings. Motivated by industrial applications, large scale client projects and deployments in software products in the areas of IoT, asset management or supply chain, the AIX360 library has been recently expanded significantly to address the above challenges. AIX360 now includes new techniques including support for time-series modality introducing time series based explainers such as TS-LIME, TS Saliency explainer, TS-ICE and TS-SHAP. It also introduces improvements in generating model agnostic, consistent, diverse, and scalable explanations, and new algorithms for tabular data.In this hands-on tutorial, we provide an overview of the library with the focus on the latest additions, time series explainers and use cases such as forecasting, time series anomaly detection or classification, and hands-on demonstrations based on industrial use-cases selected to demonstrate practical challenges and how they are addressed. The audience will be able to evaluate different types of explanations with a focus on practical aspects motivated by real deployments.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5777–5778},
numpages = {2},
keywords = {time-series, industry 4.0, explainability, artificial intelligence},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3574198.3574200,
author = {Zhou, Yuxiang and Kang, Xin and Ren, Fuji},
title = {MDSU-Net: A Multi-attention and Depthwise Separable Convolution Network for Stroke Lesion Segmentation},
year = {2023},
isbn = {9781450397223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574198.3574200},
doi = {10.1145/3574198.3574200},
abstract = {In recent years, deep learning is popular in medical image segmentation tasks. Due to network model construction issues, the contextual dependencies of large-scale medical images cannot sometimes be captured. We propose a novel network MDSU-Net by incorporating a multi-attention mechanism and a depthwise separable convolution within a U-Net framework. The multi-attention consists of a dual attention and four attention gates, which extracts the contextual information and the long-range feature information from large-scale images. MDSU-Net achieves a Dice coefficient of 0.5587, a precision of 0.6424, and a recall of 0.5276 on the Anatomical Tracings of Lesions After Stroke (ATLAS) dataset. Our experiment results suggest that the proposed MDSU-Net outperforms the state-of-the-art methods including U-Net and D-Unet in Dice coefficient, precision, and recall.},
booktitle = {Proceedings of the 2022 9th International Conference on Biomedical and Bioinformatics Engineering},
pages = {11–16},
numpages = {6},
keywords = {Self-attention, MRI, Lesion segmentation, Deep learning},
location = {Kyoto, Japan},
series = {ICBBE '22}
}

@inproceedings{10.1145/3387514.3405881,
author = {Schomp, Kyle and Bhardwaj, Onkar and Kurdoglu, Eymen and Muhaimen, Mashooq and Sitaraman, Ramesh K.},
title = {Akamai DNS: Providing Authoritative Answers to the World's Queries},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405881},
doi = {10.1145/3387514.3405881},
abstract = {We present Akamai DNS, one of the largest authoritative DNS infrastructures in the world, that supports the Akamai content delivery network (CDN) as well as authoritative DNS hosting and DNS-based load balancing services for many enterprises. As the starting point for a significant fraction of the world's Internet interactions, Akamai DNS serves millions of queries each second and must be resilient to avoid disrupting myriad online services, scalable to meet the ever increasing volume of DNS queries, performant to prevent user-perceivable performance degradation, and reconfigurable to react quickly to shifts in network conditions and attacks. We outline the design principles and architecture used to achieve Akamai DNS's goals, relating the design choices to the system workload and quantifying the effectiveness of those designs. Further, we convey insights from operating the production system that are of value to the broader research community.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {465–478},
numpages = {14},
keywords = {Distributed Systems, DNS},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3588444.3591002,
author = {Yu, Ping and Wang, Yi A and Li, Ming and Du, Jianxin and Diaz, Raul},
title = {RMTS: A Real-time Media Transport Stack Based on Commercial Off-the-shelf Hardware},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3591002},
doi = {10.1145/3588444.3591002},
abstract = {The broadcast production industry is undergoing a transformation from Serial Digital Interface (SDI) [18] to Internet Protocol (IP) [1] networks for media transport. Specialized equipment and FPGA implementations for IP based raw media transport are currently dominant due to strict low-latency and reliability requirements. These custom hardware solutions inevitably cause production environment operational complexity and scalability challenges. To enable more flexible and modular media production environments, this paper proposes RMTS, a software stack for real-time media transport based on commercial off-the-shelf (COTS) hardware to improve broadcast efficiency and scalability. RMTS provides end-to-end media transport capability compatible with the Society of Motion Picture and Television Engineers (SMPTE) [22] ST 2110 standard [13]. RMTS offers a time-sensitive scheduling algorithm that implements two timing related models by leveraging rate limiting and time synchronization functionality in Network Interface Cards (NICs): (1) an accurate traffic shaping model, and (2) an on-time delivery model, both compatible with ST 2110-21 "Professional Media Over Managed IP Networks: Traffic Shaping and Delivery Timing for Video" [16]. The high accuracy of the traffic shaping model has been validated through third-party ST 2110 testing tools. As a result, RMTS enables standards-based, realtime media transport on COTS systems for high bandwidth, low-latency media applications.},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {39–45},
numpages = {7},
keywords = {rate limiting, UHD, ultra-high definition video, SMPTE ST 2110},
location = {Denver, CO, USA},
series = {MHV '23}
}

@article{10.1145/3609384,
author = {Wang, Yitu and Li, Shiyu and Zheng, Qilin and Chang, Andrew and Li, Hai and Chen, Yiran},
title = {
EMS-i: An Efficient Memory System Design with Specialized Caching Mechanism for Recommendation Inference},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609384},
doi = {10.1145/3609384},
abstract = {Recommendation systems have been widely embedded into many Internet services. For example, Meta’s deep learning recommendation model (DLRM) shows high prefictive accuracy of click-through rate in processing large-scale embedding tables. The SparseLengthSum (SLS) kernel of the DLRM dominates the inference time of the DLRM due to intensive irregular memory accesses to the embedding vectors. Some prior works directly adopt near data processing (NDP) solutions to obtain higher memory bandwidth to accelerate SLS. However, their inferior memory hierarchy induces low performance-cost ratio and fails to fully exploit the data locality. Although some software-managed cache policies were proposed to improve the cache hit rate, the incurred cache miss penalty is unacceptable considering the high overheads of executing the corresponding programs and the communication between the host and the accelerator. To address the issues aforementioned, we propose EMS-i, an efficient memory system design that integrates Solide State Drive (SSD) into the memory hierarchy using Compute Express Link (CXL) for recommendation system inference. We specialize the caching mechanism according to the characteristics of various DLRM workloads and propose a novel prefetching mechanism to further improve the performance. In addition, we delicately design the inference kernel and develop a customized mapping scheme for SLS operation, considering the multi-level parallelism in SLS and the data locality within a batch of queries. Compared to the state-of-the-art NDP solutions, EMS-i achieves up to 10.9\texttimes{} speedup over RecSSD and the performance comparable to RecNMP with 72% energy savings. EMS-i also saves up to 8.7\texttimes{} and 6.6 \texttimes{} memory cost w.r.t. RecSSD and RecNMP, respectively.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {100},
numpages = {22},
keywords = {compute express link, Recommendation system}
}

@inproceedings{10.1145/3488933.3488937,
author = {Tong, Xiaozhong and Wei, Junyu and Su, Shaojing},
title = {Improved U-Net Network for Infrared Small Target Detection},
year = {2022},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3488937},
doi = {10.1145/3488933.3488937},
abstract = {Infrared small target detection is a key technology in infrared search and tracking systems. Despite the rapid development of computer vision in recent years, infrared small target detection based on deep learning is still few and far between due to the lack of texture information and detailed feature of infrared small target. In this paper, we propose an improved U-Net for infrared small target detection, which adds a multi-scale feature fusion module on top of the U-Net. In order to better capture the key information of infrared small target, the multi-scale feature module fuses the high-level semantic information in the convolutional neural network and the low-level small target detail feature across layers. Experimental results show that our proposed method is able to segment infrared small target well and reduce false alarms caused by complex backgrounds compared to traditional methods. In addition, our proposed method performs better than the original U-Net, demonstrating the effectiveness of our proposed improved U-Net.},
booktitle = {Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {9–13},
numpages = {5},
keywords = {Infrared small target, Feature fusion, Deep learning, Attention mechanism},
location = {Xiamen, China},
series = {AIPR '21}
}

@inproceedings{10.1145/3472716.3472844,
author = {Ma\~{n}as Mart\'{\i}nez, Eduardo Antonio and Cabrera, Elena and Wasielewska, Katarzyna and Kotz, David and Camacho, Jose},
title = {Mining social interactionsin connection traces of a campus wi-fi network},
year = {2021},
isbn = {9781450386296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472716.3472844},
doi = {10.1145/3472716.3472844},
abstract = {Wi-Fi technologies have become one of the most popular means for Internet access. As a result, the use of mobile devices has become ubiquitous and instrumental for society. A device can be identified through its MAC address within an autonomous system. Although some devices attempt to anonymize MAC addresses via randomization, these techniques are not used once the device is associated to the network [7]. As a result, device identification poses a privacy problem in large-scale (e.g., campus-wide) Wi-Fi deployments [5]: if the mobile device can be located, the user who carries that device can also be located. In turn, location information leads to the possibility to extract private knowledge from Wi-Fi users, like social interactions, movement habits, and so forth.In this poster we report preliminary work in which we infer social interactions of individuals from Wi-Fi connection traces in the campus network at Dartmouth College [2]. We make the following contributions: (i) we propose several definitions of a pseudocorrelation matrix from Wi-Fi connection traces, which measure similarity between devices or users according to their temporal association profile to the Access Points (APs); (ii) we evaluate the accuracy of these pseudo-correlation variants in a simulation environment; and (iii) we contrast results with those found on a real trace.},
booktitle = {Proceedings of the SIGCOMM '21 Poster and Demo Sessions},
pages = {6–8},
numpages = {3},
keywords = {wi-fi, privacy, connection logs, Dartmouth},
location = {Virtual Event},
series = {SIGCOMM '21}
}

@inproceedings{10.1145/3508072.3508163,
author = {Djuraxodjaevich Boboev, L Kadrkhuja and Rustamovich Khalikov, Ulugbek and Anvarjonovich Ismailov, Dilshod},
title = {The Automotive Industry's Digital Marketing: A Comparison of Traditional and Digital Marketing Techniques},
year = {2022},
isbn = {9781450387347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508072.3508163},
doi = {10.1145/3508072.3508163},
abstract = {The phrase "digital marketing" refers to the different promotion and advertising activities/techniques utilized to engage with clients via digital channels. Digital marketing aids in the education of end users about products, their features, values, advantages and disadvantages, and large-scale promotions. The auto market in Uzbekistan, which has a significant number of prospective consumers, is thriving, with a wide range of options/varieties accessible and a coupled cutthroat rivalry in which every firm is fighting for survival. In the current circumstances and environment, digital marketing may be quite useful in promoting businesses, ideas, services, and concepts. With the rise of the IT &amp; Communication sector, where almost every Uzbek has access to the internet via computer systems, mobile phones, or tablets, the internet is gradually infiltrating the core of every industry, including the automobile industry, empowering customers to seek relevant information about anything they want. The main purpose of this study is to discuss how digital marketing influences decision-making in Uzbekistan's automobile industry, mainly Lada brand. The research compares traditional marketing methods with digital marketing approaches utilized in the automobile industry. The research is based on consumer input about their pre- and post-purchase experiences with automobiles purchased through Digital Marketing and Traditional Buying Methods.The research is being carried out in Uzbekistan with the help all Lada dealerships by using a structured questionnaire. To compare conventional marketing with digital marketing techniques, many statistical methods are used. It provides marketers insight into how to position their goods in this competitive market.},
booktitle = {The 5th International Conference on Future Networks &amp; Distributed Systems},
pages = {453–457},
numpages = {5},
location = {Dubai, United Arab Emirates},
series = {ICFNDS 2021}
}

@inproceedings{10.1145/3578503.3583603,
author = {Hoseini, Mohamad and Melo, Philipe and Benevenuto, Fabricio and Feldmann, Anja and Zannettou, Savvas},
title = {On the Globalization of the QAnon Conspiracy Theory Through Telegram},
year = {2023},
isbn = {9798400700897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578503.3583603},
doi = {10.1145/3578503.3583603},
abstract = {QAnon is a far-right conspiracy theory that has implications in the real world, with supporters of the theory participating in real-world violent acts like the US capitol attack in 2021. At the same time, the QAnon theory started evolving into a global phenomenon by attracting followers across the globe and, in particular, in Europe, hence it is imperative to understand how QAnon has become a worldwide phenomenon and how this dissemination has been happening in the online space. This paper performs a large-scale data analysis of QAnon through Telegram by collecting 4.4M messages posted in 161 QAnon groups/channels. Using Google’s Perspective API, we analyze the toxicity of QAnon content across languages and over time. Also, using a BERT-based topic modeling approach, we analyze the QAnon discourse across multiple languages. Among other things, we find that the German language is prevalent in our QAnon dataset, even overshadowing English after 2020. Also, we find that content posted in German and Portuguese tends to be more toxic compared to English. Our topic modeling indicates that QAnon supporters discuss various topics of interest within far-right movements, including world politics, conspiracy theories, COVID-19, and the anti-vaccination movement. Taken all together, we perform the first multilingual study on QAnon through Telegram and paint a nuanced overview of the globalization of QAnon.},
booktitle = {Proceedings of the 15th ACM Web Science Conference 2023},
pages = {75–85},
numpages = {11},
keywords = {toxicity analysis, topic modeling, social media, Telegram, QAnon},
location = {Austin, TX, USA},
series = {WebSci '23}
}

@inproceedings{10.1145/3478384.3478425,
author = {Hoy, Rory and Van Nort, Doug},
title = {A Technological and Methodological Ecosystem for Dynamic Virtual Acoustics in Telematic Performance Contexts},
year = {2021},
isbn = {9781450385695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478384.3478425},
doi = {10.1145/3478384.3478425},
abstract = {This paper presents the design and development of a technological ecosystem which facilitates research at the intersection of Virtual Acoustics, Networking, and Telematic Music. Building upon existing robust software packages, we integrate and extend this work in the context of exploring the effect of telematically-shared dynamic virtual acoustic spaces and expressive movement trajectories of player sound source positions upon live play contexts. Through analysis of impulse responses generated by a distributable measurement kit, remote players are able to share these spaces with their collaborators. Supplementary tools developed as part of this research also extend established telematic software solutions, allowing for easier use of network based musical communication systems for both performers and technicians. A network topology of interconnected telematic solutions is described, allowing flexibility between various internet based audio platforms. Applications which challenge and contain these developments are discussed, including a large-scale musical ensemble, web based musical performance, and an ongoing study into collaborative telematic improvisation.},
booktitle = {Proceedings of the 16th International Audio Mostly Conference},
pages = {169–174},
numpages = {6},
keywords = {virtual acoustics, telematic music, spatialization},
location = {virtual/Trento, Italy},
series = {AM '21}
}

@inproceedings{10.1145/3514221.3517888,
author = {Alghamdi, Noura S. and Zhang, Liang and Rundensteiner, Elke A. and Eltabakh, Mohamed Y.},
title = {Scalable Time Series Compound Infrastructure},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517888},
doi = {10.1145/3514221.3517888},
abstract = {Objects ranging from a patient's history of medical tests to an IoT device's series of sensor maintenance records leave digital traces in the form of big time series. These time series objects do not only span exceedingly long time periods (sometimes years), but are also characterized by intermittent yet interrelated time series measurements punctuated by long gaps of silence. This prevalent data type, which we refer to as Time Series Compound objects (or, TSC), has been largely overlooked in the literature. Unique challenges arise when managing, querying and analyzing repositories of these big TSC objects. These include appropriate similarity semantics with time misalignment resiliency, efficient storage of excessively long and complex objects, and TSC-holistic indexing. We demonstrate that state-of-the-art time series systems, although effective at indexing and searching regular time series data, fail to support such big TSC data. In this work, we introduce the first comprehensive solution for managing TSC objects as first class citizen. We introduce new similarity-match semantics as well as a compact misalignment-resilient representation for TSCs. Upon this foundation, we then design a TSC-aware distributed indexing infrastructure Sloth that supports scalable storage, indexing and querying of TB-scale TSC datasets. Our experimental study demonstrates that for TB-scale datasets, the query response time of Sloth is up to one order of magnitude faster than that of existing systems, while the mean average precision (mAP) for approximate kNN similarity match query results by Sloth is 70% more accurate than existing solutions.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1685–1698},
numpages = {14},
keywords = {time series compound, sloth, similarity search, distributed indexing, KNN approximate query},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3615335.3623021,
author = {Lundblade, Kirk Matthew},
title = {Sorting Things Out: Critically Assessing the Impact of Reddit's Post Sorting Algorithms on Qualitative Analysis Methods},
year = {2023},
isbn = {9798400703362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615335.3623021},
doi = {10.1145/3615335.3623021},
abstract = {The content aggregation platform reddit currently reigns as one of the most popular websites on the internet, and is a key resource for scholars from many disciplines. Consideration of extant metanalyses of Reddit-based research reveal a large-scale reliance on data collection methods which are likely to be employed by qualitative researchers and which are also largely entangled with Reddit's own systems for aggregating, sorting, and displaying/providing post data to users. While studies with a more quantitative focus often escape this particular entanglement, research which attempts to critically examine discourse within particular Reddit communities must often contend with sorting methods that allow the selection of a subset of material from the large body of available discourse. This research conducts a thorough analysis of four of Reddit's (as of March 2023) native post sort algorithms (PSAs) available to researchers whose access to the dataset is mediated by feed aggregation; the hot, new, top, and rising sorts (solutioneering, 2021) are critically assessed and materially grounded via in a case study drawn from the author's own research on a particular Reddit community, the r/crusaderkings subreddit (a community centering the digital historical game series Crusader Kings). This case study highlight some of the material effects of each of the native sort algorithms, grounding the discussion in a relevant research context.},
booktitle = {Proceedings of the 41st ACM International Conference on Design of Communication},
pages = {112–118},
numpages = {7},
keywords = {Ethnography, Grounded Theory, Qualitative, Reddit},
location = {<conf-loc>, <city>Orlando</city>, <state>FL</state>, <country>USA</country>, </conf-loc>},
series = {SIGDOC '23}
}

@inproceedings{10.1145/3580305.3599832,
author = {Zhang, Jing and Zhang, Xiaokang and Zhang-Li, Daniel and Yu, Jifan and Yao, Zijun and Ma, Zeyao and Xu, Yiqi and Wang, Haohua and Zhang, Xiaohan and Lin, Nianyi and Lu, Sunrui and Li, Juanzi and Tang, Jie},
title = {GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599832},
doi = {10.1145/3580305.3599832},
abstract = {We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters capable of knowledge-grounded conversation in Chinese using a search engine to access the Internet knowledge. GLM-Dialog offers a series of applicable techniques for exploiting various external knowledge including both helpful and noisy knowledge, enabling the creation of robust knowledge-grounded dialogue LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we also propose a novel evaluation method to allow humans to converse with multiple deployed bots simultaneously and compare their performance implicitly instead of explicitly rating using multidimensional metrics. Comprehensive evaluations from automatic to human perspective demonstrate the advantages of GLM-Dialog comparing with existing open source Chinese dialogue models. We release both the model checkpoint and source code, and also deploy it as a WeChat application to interact with users. We offer our evaluation platform online in an effort to prompt the development of open source models and reliable dialogue evaluation systems. All the source code is available on Github.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5564–5575},
numpages = {12},
keywords = {large language model, dialogue system, dialogue evaluation},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3580305.3599570,
author = {Ghazi, Badih and Kumar, Ravi and Manurangsi, Pasin},
title = {Privacy in Advertising: Analytics and Modeling},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599570},
doi = {10.1145/3580305.3599570},
abstract = {Privacy in general, and differential privacy (DP) in particular, have become important topics in data mining and machine learning. Digital advertising is a critical component of the internet and is powered by large-scale data analytics and machine learning models; privacy concerns around these are on the rise. Despite the central importance of private ad analytics and training privacy-preserving ad prediction models, there has been relatively little exposure of this subject to the broader KDD community. In the past three years, the interest in privacy and the interest in online advertising have been steadily growing in KDD. The aim of this tutorial is to provide KDD researchers with an introduction to the problems that arise in private analytics and modeling in advertising, survey recent results, and describe the main research challenges in the space.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5802},
numpages = {1},
keywords = {private learning, private analytics, online ads, differential privacy},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3373419.3373449,
author = {Guo, Shuai and Tang, Songyuan and Zhu, Jianjun and Fan, Jingfan and Ai, Danni and Song, Hong and Liang, Ping and Yang, Jian},
title = {Improved U-Net for Guidewire Tip Segmentation in X-ray Fluoroscopy Images},
year = {2020},
isbn = {9781450376754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373419.3373449},
doi = {10.1145/3373419.3373449},
abstract = {In percutaneous coronary intervention (PCI), physicians use a guidewire tip to implant stents in vessels with stenosis. Given the small scale and low signal-to-noise ratio of guidewire tips in X-ray fluoroscopy images, physicians experience difficulty in recognizing and locating the tip. The automatic segmentation of the guidewire tip can ease navigation when the physicians implant stents for PCI. In this paper, we propose an end-to-end convolutional neural network-based method for guidewire tip segmentation. The network framework is derived from U-Net, and two specific designs involving reduced dense block and connectivity supervision are embedded in the framework to improve the accuracy and robustness of guidewire tip segmentation. Experiments are performed on clinical data. The proposed method achieves mean sensitivity, F1-score, Jaccard index, Hausdorff distance of 92.95%, 91.35%, 84.14%, and 0.531 mm on testing data, respectively. In addition, the segmentation time is 0.02 s/frame, which can satisfy the requirements for clinical intra-practice.},
booktitle = {Proceedings of the 2019 3rd International Conference on Advances in Image Processing},
pages = {55–59},
numpages = {5},
keywords = {Guidewire tip segmentation, Deep Learning, Connectivity, CNN},
location = {Chengdu, China},
series = {ICAIP '19}
}

@inproceedings{10.1145/3558819.3564591,
author = {Wang, Huihua and Liu, QingXue and Li, Mofei and Zhang, Yuhang and Li, Zhengfang},
title = {Method of Tobacco Plants Number Statistics Based on U-Net and HSV Color Model},
year = {2022},
isbn = {9781450397414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558819.3564591},
doi = {10.1145/3558819.3564591},
abstract = {In order to achieve the high efficiency, accuracy and intelligence of the statistics of the number of tobacco plants based on computer vision, this paper constructs the USE-Net tobacco fields segmentation model to segment the large-scale background in the tobacco fields image, and constructs the tobacco fields mask map based on the HSV color model to remove the background of small-scale objects such as weeds and ridges, so as to obtain the tobacco fields remote sensing image map with only tobacco plants. Through Gaussian filtering, morphological denoising, opening operation denoising on the background removed tobacco fields image, the tobacco plants contour is extracted, and finally the tobacco plants contours is counted to achieve the statistics of the number of tobacco plants. In tobacco fields segmentation, compared with U-Net, USE-Net improved mIoU by 4%, mPA by 7.21%, mPrecision by 4.52%, and mRecall by 7.46%. In the prediction of the number of tobacco plants, the prediction accuracy can reach 91.98%. These results show that our method is effective in tobacco fields segmentation and tobacco plants number statistics.},
booktitle = {Proceedings of the 7th International Conference on Cyber Security and Information Engineering},
pages = {165–170},
numpages = {6},
location = {<conf-loc>, <city>Brisbane</city>, <state>QLD</state>, <country>Australia</country>, </conf-loc>},
series = {ICCSIE '22}
}

@inproceedings{10.1145/3513142.3513194,
author = {Liu, Litao},
title = {Improved Image Classification Accuracy by Convolutional Neural Networks},
year = {2022},
isbn = {9781450386494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3513142.3513194},
doi = {10.1145/3513142.3513194},
abstract = {Along with the development of mobile Internet, images are increasingly used as a more effective information carrier, and deep learning methods have a great efficiency advantage over traditional algorithms in dealing with image classification problems. This paper presents a simple and efficient deep learning method to train a convolutional neural network (CNN) for image classification on a large-scale supervised fashion NBIST image dataset. We first preprocessed the dataset, after which it was fed into the neural network for a minimum of five epochs of training. In the end, the accuracy rate can reach more than 83.6%. This CNN model has a greater efficiency advantage over traditional feature description and detection algorithms and is more suitable for today's image classification tasks.},
booktitle = {Proceedings of the 4th International Conference on Information Technologies and Electrical Engineering},
articleno = {50},
numpages = {5},
keywords = {Machine Learning, Image Classification, Deep Learning, Convolutional Neural Network (CNN)},
location = {Changde, Hunan, China},
series = {ICITEE '21}
}

@inproceedings{10.1145/3503161.3547922,
author = {Qin, Yang and Peng, Dezhong and Peng, Xi and Wang, Xu and Hu, Peng},
title = {Deep Evidential Learning with Noisy Correspondence for Cross-modal Retrieval},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547922},
doi = {10.1145/3503161.3547922},
abstract = {Cross-modal retrieval has been a compelling topic in the multimodal community. Recently, to mitigate the high cost of data collection, the co-occurred pairs (e.g., image and text) could be collected from the Internet as a large-scaled cross-modal dataset, e.g., Conceptual Captions. However, it will unavoidably introduce noise (i.e., mismatched pairs) into training data, dubbed noisy correspondence. Unquestionably, such noise will make supervision information unreliable/uncertain and remarkably degrade the performance. Besides, most existing methods focus training on hard negatives, which will amplify the unreliability of noise. To address the issues, we propose a generalized Deep Evidential Cross-modal Learning framework (DECL), which integrates a novel Cross-modal Evidential Learning paradigm (CEL) and a Robust Dynamic Hinge loss (RDH) with positive and negative learning. CEL could capture and learn the uncertainty brought by noise to improve the robustness and reliability of cross-modal retrieval. Specifically, the bidirectional evidence based on cross-modal similarity is first modeled and parameterized into the Dirichlet distribution, which not only provides accurate uncertainty estimation but also imparts resilience to perturbations against noisy correspondence. To address the amplification problem, RDH smoothly increases the hardness of negatives focused on, thus embracing higher robustness against high noise. Extensive experiments are conducted on three image-text benchmark datasets, i.e., Flickr30K, MS-COCO, and Conceptual Captions, to verify the effectiveness and efficiency of the proposed method. The code is available at urlhttps://github.com/QinYang79/DECL.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4948–4956},
numpages = {9},
keywords = {noisy correspondence, image-text matching, evidential learning, cross-modal retrieval},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3605390.3605406,
author = {Li, Junhao and Kuutila, Miikka and Huusko, Eetu and Kariyakarawana, Nimantha and Savic, Marko and Ahooie, Nazanin Nakhaie and Hosio, Simo and M\"{a}ntyl\"{a}, Mika},
title = {Assessing Credibility Factors of Short-Form Social Media Posts: A Crowdsourced Online Experiment},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605390.3605406},
doi = {10.1145/3605390.3605406},
abstract = {People commonly turn to the Internet and social media for their information needs. Most popular social media platforms focus on short-form content that can be consumed rapidly. Given how fast such content spreads online, its trustworthiness and credibility have become important research areas. We investigate how different factors of social media posts influence their perceived credibility. We generated health-themed short-form social media posts, varied specific aspects of those posts, and deployed the variations on three different online crowdsourcing platforms for credibility assessment. Our quantitative data analysis reveals, for instance, how author professions related to healthcare and science increase the perceived credibility of health-themed posts. Moreover, a higher number of likes and shares increased the credibility in two out of the three platforms. Our qualitative results based on questionnaires highlight personal filtering strategies and critical thinking skills as factors that influence post credibility online. Consequently, our results encourage experts to provide information on social media and to be part of correcting any misinformation as they have higher credibility. Our work strengthens the previous body of work on the credibility of online content in general and acts as a starting point for further studies on social media post content by demonstrating a systematic, crowdsourced, and scalable approach.},
booktitle = {Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {9},
numpages = {14},
keywords = {Social media, Online content, Health claim, Crowdsourcing, Credibility},
location = {<conf-loc>, <city>Torino</city>, <country>Italy</country>, </conf-loc>},
series = {CHItaly '23}
}

@inproceedings{10.1145/3486611.3491139,
author = {Dong, Bing and Markovic, Romana and Carlucci, Salvatore},
title = {The 1st ACM international workshop on big data and machine learning for smart buildings and cities},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3491139},
doi = {10.1145/3486611.3491139},
abstract = {The proliferation of urban sensing, IoT, and big data in buildings, cities, and urban areas provides unprecedented opportunities for a deeper understanding of occupant behavior, transportation, and energy and water usage patterns. However, utilizing the existing data sources and modeling methods in building science to model urban scale occupant behaviors can be pretty challenging. Therefore, technological progress is needed to unlock its full potential. In order to fulfill the latter task, this workshop focuses on the methodologies for big urban and building data collection, analytics, modeling, and real-world technology deployment. The workshop aims to open discussion on the current challenges of big data in smart buildings and cities.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {338–340},
numpages = {3},
keywords = {smart buildings, occupant behavior, modeling and prediction, machine learning, digital cities, big data analysis},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3429395.3429410,
author = {Lu, Yuching and Koki, Totsuka and Chakraborty, Goutam and Matsuhara, Masafumi},
title = {Performance Comparison of Clustering Algorithm Based Collaborative Filtering Recommendation System},
year = {2020},
isbn = {9781450389457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429395.3429410},
doi = {10.1145/3429395.3429410},
abstract = {Importance of recommendation Systems (RS), based on collaborative filtering, is escalating with exponential growth of e-commerce application, e.g., on-line shopping, internet movie and music sites, tourism, news portals, to name a few. The target of recommendation system is to predict user preferences based on their previous activities, and associating users of similar behavior. Of the two main approaches, Content Based (CB) and Collaborative Filtering (CF), CF is increasingly popular because there is no need of domain knowledge and it scales well for large datasets. CF uses archives, like user-item purchase data or user-movie review data, which are large and sparse. Traditional approaches using matrix factorization like singular valued decomposition (SVD), is inefficient, for very a large review data. It is more efficient to cluster item-vectors, where elements of an item-vector are entries from users. As the dimension of an item-vector is large and elements are sparse, conventional clustering algorithms fails. We created item-item adjacency matrix, where the elements are similarity between item-vectors. There are various clustering algorithms that work directly on the adjacency matrix. We used spectral clustering, K-means++, Agglomerative Clustering. Considering item-vectors as nodes and adjacency matrix elements as link weights, we performed graph-clustering using Louvain Algorithm, to discover groups. We used synthetic data, added different levels of noise and run different algorithms to compare their accuracies to restore the original data from the noisy one. Louvain algorithm and Spectral clustering could achieve the highest accuracies.},
booktitle = {Proceedings of the 7th Multidisciplinary in International Social Networks Conference and The 3rd International Conference on Economics, Management and Technology},
articleno = {15},
numpages = {6},
keywords = {Similarity Measure, Recommender System, Cluster-based Collaborative Filtering, Adjacency Matrix},
location = {Kaohsiung, Taiwan},
series = {MISNC2020&amp;IEMT2020}
}

@inproceedings{10.1145/3581783.3611799,
author = {Shi, Gege and Fu, Xueyang and Cao, Chengzhi and Zha, Zheng-Jun},
title = {Alleviating Spatial Misalignment and Motion Interference for UAV-based Video Recognition},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611799},
doi = {10.1145/3581783.3611799},
abstract = {Recognizing activities with Unmanned Aerial Vehicles (UAVs) is essential for many applications, while existing video recognition methods are mainly designed for ground cameras and do not account for UAV changing attitudes and fast motion. This creates spatial misalignment of small objects between frames, leading to inaccurate visual movement in drone videos. Additionally, camera motion relative to objects in the video causes relative movements that visually affect object motion and can result in misunderstandings of video content. To address these issues, we present a novel framework named Attentional Spatial and Adaptive Temporal Relations Modeling. First, to mitigate the spatial misalignment of small objects between frames, we design an Attentional Patch-level Spatial Enrichment (APSE) module that models dependencies among patches and enhances patch-level features. Then, we propose a Multi-scale Temporal and Spatial Mixer (MTSM) module that is capable of adapting to disturbances caused by the UAV flight and modeling various temporal clues. By integrating APSE and MTSM into a single model, our network can effectively and accurately capture spatiotemporal relations for UAV videos. Extensive experiments on several benchmarks demonstrate the superiority of our method over state-of-the-art approaches. For instance, our network achieves a classification accuracy of 68.1% with an absolute gain of 1.3% compared to FuTH-Net on the ERA dataset.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {193–202},
numpages = {10},
keywords = {video recognition, unmanned aerial vehicles (uavs), deep neural network, attention mechanism, action recognition and understanding},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{10.1145/3371158.3371196,
author = {Singh, Davinder and Jain, Naman and Jain, Pranjali and Kayal, Pratik and Kumawat, Sudhakar and Batra, Nipun},
title = {PlantDoc: A Dataset for Visual Plant Disease Detection},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371196},
doi = {10.1145/3371158.3371196},
abstract = {India loses 35% of the annual crop yield due to plant diseases. Early detection of plant diseases remains difficult due to the lack of lab infrastructure and expertise. In this paper, we explore the possibility of computer vision approaches for scalable and early plant disease detection. The lack of availability of sufficiently large-scale non-lab data set remains a major challenge for enabling vision based plant disease detection. Against this background, we present PlantDoc: a dataset for visual plant disease detection. Our dataset contains 2,598 data points in total across 13 plant species and up to 17 classes of diseases, involving approximately 300 human hours of effort in annotating internet scraped images. To show the efficacy of our dataset, we learn 3 models for the task of plant disease classification. Our results show that modelling using our dataset can increase the classification accuracy by up to 31%. We believe that our dataset can help reduce the entry barrier of computer vision techniques in plant disease detection.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {249–253},
numpages = {5},
keywords = {Object Detection, Image Classification, Deep Learning},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/3366423.3382668,
author = {Nigel Shadbolt, Sir},
title = {Architectures for Autonomy: Towards an Equitable Web of Data in the Age of AI},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3382668},
doi = {10.1145/3366423.3382668},
abstract = {Today, the Web connects over half the world's population, many of whom use it to stay connected to a multiplicity of vital digital public and private services, impacting every aspect of their lives. Access to the Web and underlying Internet is seen as essential for all—even a fundamental human right [7]. However, many contend that the power structure on large swaths of the Web has become inverted; they argue that instead of being run for and by users, it has been made to serve the platforms themselves, and the powerful actors that sponsor such platforms to run targeted advertising on their behalf. In such an ad-driven platform ecosystem, users, including their beliefs, data, and attention, have become traded commodities [13].There is concern that the emergence of powerful data analytics and AI techniques threaten to further entrench the power of these same platforms, by putting the control of powerful and valuable new capabilities in their hands rather than the users who produce the data [10]. The fear is that it is giving rise to data and AI monopolies [2,6]. Individuals have no long-term control or agency over their personal data or many of the decisions made using it.This may be one reason we are witnessing a so called Renaissance of Ethics - a plethora of initiatives and activities that call out the range of threats to individual autonomy, self-determination and privacy, the lack of transparency and accountability, a concern around bias and fairness, equity and access in our data driven ecosystem. This keynote will argue as the remaining half of the world's population comes online, we need digital infrastructures that will promote a plurality of methods of data sovereignty and governance instead of imposing a ’single policy fits-all’ platform governance model, which has strained and undermined the ability for governments to protect and support their citizens digital rights.This is an opportunity to re-imagine and re-architect elements of the Web, data, algorithms and institutions so as to ensure a more equitable distribution of these new digital potentialities. Based on our existing research we have been developing methods and tech-nologies pertaining to the following core principles: informational self-determination and autonomy, balanced and equitable access to AI and data, accountability and redress of AI/algorithmic decisions, and new models of ethical participation and contribution.The technology that underpins the modern web has seen exponential rates of change that have continuously improved the capabilities of the processors, memory and communications upon which it depends. This has enabled huge amounts of data to be linked and stored as well as providing for increasing use of AI. A variety of projects will be described where we sought to unlock the potential of this increasingly powerful infrastructure [1, 4, 5, 9]. The lessons learnt through various efforts to develop the Seman-tic Web [8] and the insights gained through the release of open data at scale will be reviewed [11]. We will review our attempts to understand how the blending of humans, algorithms and data at scale results in social machines whose emergent properties results in behaviour and problem solving which any of the individual elements would not have been able to achieve [12]. Understanding these emergent properties of the web was one of the motivating factors behind the establishment of Web Science [3]. We will briefly review the prospects for Web Science.The importance of data as infrastructure to enable wide spread innovation, accountability and trusted reproducible science will be stressed. Recent work will be described that seeks to promote an equitable and balanced Web environment in which privacy can be upheld and better mutualities realised. Developments in technical and institutional architectures that could underpin an Ethical Web of data will be outlined.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3141–3142},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@proceedings{10.1145/3384441,
title = {SIGSIM-PADS '20: Proceedings of the 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
year = {2020},
isbn = {9781450375924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2020 edition of ACM Conference on Principles of Advanced Discrete Simulation (SIGSIM-PADS), to be held in June 15-16, 2020, in sunny Miami, Florida, virtually. The ongoing COVID-19 pandemic has changed many things, both socially and economically, this conference included. This year, we are holding the Conference as a synchronous virtual event with all participants joining via a video conferencing platform. Despite being virtual, the Conference will have a broader participation from the community due to online accessibility and reduced cost.SIGSIM-PADS is the flagship conference for ACM's Special Interest Group on Simulation and Modeling (SIGSIM) with a broad scope in cutting-edge research in the field of computer modeling and simulation. This year's conference will feature two keynote speeches, a panel, and 18 paper presentations. Dr. Adolfy Hoisie from Brookhaven National Laboratory will give the first keynote speech on "System and Application Performance Modeling and Simulation in the AI Era". Dr. Madhav Marathe from University of Virginia will give the second keynote speech on "High Performance Simulations to Support Real-time COVID-19 Response". Immediately after the second keynote, the conference will have a panel discussion on "How will the Coronavirus (SARS-CoV-2) Change Modeling &amp; Simulation?", led by Paul Fishwick from University of Texas at Dallas. The panelists include several experts in the field: Madhav Marathe, David Fishwick, Nigel Gilbert, Andreas Tolk, David Bell, and Navonil Mustafee.A total of 36 manuscripts were submitted to the conference. This 50% increase in submissions from the previous year demonstrates the dynamic nature of simulation research and the strong reputation of SIGSIM-PADS within the field. Submissions were reviewed by 36 program committee members affiliated with institutions (including national laboratories, universities, companies) in several countries (e.g., USA, Germany, Italy, UK). Submissions within scope received at least three high-quality reviews and were then discussed in a general meeting of the Program Committee, following the collegial practices that SIGSIM-PADS has engaged in for many years. Based on this shared decision-making process, one third of the submissions were accepted as full papers. Accepted papers exemplify the breadth of research conducted in the simulation community in domains such as parallel discrete event simulations, large-scale computing, or agent-based models.PADS also continues to pioneer in the Computational Results Replication and Artifact Evaluation initiative supported by ACM. PADS is the first simulation conference to give authors the possibility to have their artifacts evaluated by a dedicated committee, and results of their research independently replicated. This effort increases the trust in modeling and simulation research. In addition, making artifacts accessible and preparing them for reuse promises faster progress in modeling and simulation science and practise. This year, PADS has pushed this initiative a step further: we have included in the proceedings the results of the replication of results. This is an important step. With this, the results of the replication are available to readers, showing how the initiative is being carried out. A total of 17 badges has been requested, of which 16 have been assigned after careful evaluation.},
location = {Miami, FL, Spain}
}

@inproceedings{10.1145/3534678.3542623,
author = {Erickson, Parker and Lee, Victor E. and Shi, Feng and Tang, Jiliang},
title = {Efficient Machine Learning on Large-Scale Graphs},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542623},
doi = {10.1145/3534678.3542623},
abstract = {Machine learning on graph data has become a common area of interest across academia and industry. However, due to the size of real-world industry graphs (hundreds of millions of vertices and billions of edges) and the special architecture of graph neural net- works, it is still a challenge for practitioners and researchers to perform machine learning tasks on large-scale graph data. It typi- cally takes a powerful and expensive GPU machine to train a graph neural network on a million-vertex scale graph, let alone doing deep learning on real enterprise graphs. In this tutorial, we will cover how to develop and run performant graph algorithms and graph neural network models with TigerGraph [3], a massively parallel platform for graph analytics, and its Machine Learning Workbench with PyTorch Geometric [4] and DGL [8] support. Using an NFT transaction dataset [6], we will first investigate transactions using graph algorithms by themselves as methods of graph traversing, clustering, classification, and determining similarities between data. Secondly, we will show how to use those graph-derived features such as PageRank and embeddings to empower traditional machine learning models. Finally, we will demonstrate how to train common graph neural networks with TigerGraph and how to implement novel graph neural network models. Participants will use the Tiger- Graph ML Workbench Cloud to perform graph feature engineering and train their machine learning algorithms during the session.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4788–4789},
numpages = {2},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3460620.3460739,
author = {Faek, Rana and Al-Fawa'reh, Mohammad and Al-Fayoumi, Mustafa},
title = {Exposing Bot Attacks Using Machine Learning and Flow Level Analysis},
year = {2021},
isbn = {9781450388382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460620.3460739},
doi = {10.1145/3460620.3460739},
abstract = {Botnets represent a major threat to Internet security that have continuously developed in scale and complexity. Command-and-control servers (C&amp;C) send commands to bots that execute and perform these commands, thereby implementing attacks such as distributed denial-of-service (DDoS), spam campaigns, or the scanning of compromised hosts. The detection of volumetric attacks in large and complex networks requires an efficient mechanism. Botnet behavior should be analyzed in order to save the network from attack, and preventive measures should be implemented in time. Anomalous botnet tracking strategies are more efficient than signature-based ones, since botnet detection methods rely on anomalies and do not need pre-constructed botnet signatures, therefore they can detect new or unidentified botnets. We use Netflow and machine learning algorithms in this paper to also improve the detection process for intrusion detection algorithms with a novel dataset. We implemented a number of algorithms in our lightweight model to show that Random Forests get the highest accuracy for the algorithms used.},
booktitle = {International Conference on Data Science, E-Learning and Information Systems 2021},
pages = {99–106},
numpages = {8},
keywords = {Net Flow, IDS, Anomaly Detection},
location = {Ma'an, Jordan},
series = {DATA'21}
}

@inproceedings{10.1145/3442381.3449892,
author = {Lelkes, Adam D. and Tran, Vinh Q. and Yu, Cong},
title = {Quiz-Style Question Generation for News Stories},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449892},
doi = {10.1145/3442381.3449892},
abstract = {A large majority of American adults get at least some of their news from the Internet. Even though many online news products have the goal of informing their users about the news, they lack scalable and reliable tools for measuring how well they are achieving this goal, and therefore have to resort to noisy proxy metrics (e.g., click-through rates or reading time) to track their performance. As a first step towards measuring news informedness at a scale, we study the problem of quiz-style multiple-choice question generation, which may be used to survey users about their knowledge of recent news. In particular, we formulate the problem as two sequence-to-sequence tasks: question-answer generation (QAG) and distractor, or incorrect answer, generation (DG). We introduce NewsQuizQA, the first dataset intended for quiz-style question-answer generation, containing 20K human written question-answer pairs from 5K news article summaries. Using this dataset, we propose a series of novel techniques for applying large pre-trained Transformer encoder-decoder models, namely PEGASUS and T5, to the tasks of question-answer generation and distractor generation. We show that our models outperform strong baselines using both automated metrics and human raters. We provide a case study of running weekly quizzes on real-world users via the Google Surveys platform over the course of two months. We found that users generally found the automatically generated questions to be educational and enjoyable. Finally, to serve the research community, we are releasing the NewsQuizQA dataset.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2501–2511},
numpages = {11},
keywords = {question generation, news, natural language generation, distractor generation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3530190.3534844,
author = {Mondal, Joyanta Jyoti and Islam, Md. Farhadul and Zabeen, Sarah and Islam, A. B. M. Alim Al and Noor, Jannatun},
title = {Note: Plant Leaf Disease Network (PLeaD-Net): Identifying Plant Leaf Diseases through Leveraging Limited-Resource Deep Convolutional Neural Network},
year = {2022},
isbn = {9781450393478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530190.3534844},
doi = {10.1145/3530190.3534844},
abstract = {Agriculture is the fundamental source of revenue and Gross Domestic Product (GDP) in many countries where economically developing countries; especially the Global South are no exception. Various types of plant-based diseases are strongly intertwined with the everyday lives of those who are connected with agriculture. Among the diseases, most of them can be diagnosed by leaves. However, due to the variety of illnesses, identifying and classifying any plant leaf disease is difficult and time-consuming. Besides, late identifications of diseases cause losses for the farmers on a large scale, which in turn affects their financial state. Therefore, to overcome this problem, we present a lightweight approach (called PLeaD-Net) to accurately recognize and categorize plant leaf diseases in this paper. Here, leveraging a limited-resource deep convolutional network (Deep CNN) model, we extract information from sick sections of a leaf to accurately identify locations of disease. In comparison to existing deep learning methods and other prior research, our proposed approach achieves a much higher performance using fewer parameters as per our experimental results. In our study and experimentation, we develop and implement an architecture based on Deep CNN. We test our architecture on a publicly available dataset that contains different types of plant leaves images and backgrounds.},
booktitle = {Proceedings of the 5th ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {668–673},
numpages = {6},
keywords = {Transfer Learning, Plant Disease Classification, Machine Learning, Leaf Disease Identification, Deep Learning, Convolutional Neural Network},
location = {Seattle, WA, USA},
series = {COMPASS '22}
}

@inproceedings{10.1145/3397271.3401151,
author = {Yang, Xun and Dong, Jianfeng and Cao, Yixin and Wang, Xun and Wang, Meng and Chua, Tat-Seng},
title = {Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401151},
doi = {10.1145/3397271.3401151},
abstract = {The rapid growth of user-generated videos on the Internet has intensified the need for text-based video retrieval systems. Traditional methods mainly favor the concept-based paradigm on retrieval with simple queries, which are usually ineffective for complex queries that carry far more complex semantics. Recently, embedding-based paradigm has emerged as a popular approach. It aims to map the queries and videos into a shared embedding space where semantically-similar texts and videos are much closer to each other. Despite its simplicity, it forgoes the exploitation of the syntactic structure of text queries, making it suboptimal to model the complex queries.To facilitate video retrieval with complex queries, we propose a Tree-augmented Cross-modal Encoding method by jointly learning the linguistic structure of queries and the temporal representation of videos. Specifically, given a complex user query, we first recursively compose a latent semantic tree to structurally describe the text query. We then design a tree-augmented query encoder to derive structure-aware query representation and a temporal attentive video encoder to model the temporal characteristics of videos. Finally, both the query and videos are mapped into a joint embedding space for matching and ranking. In this approach, we have a better understanding and modeling of the complex queries, thereby achieving a better video retrieval performance. Extensive experiments on large scale video retrieval benchmark datasets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1339–1348},
numpages = {10},
keywords = {video search, natural language understanding, multimedia retrieval, latent tree structure}},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3570748.3570755,
author = {Wang, Caleb and Liu, Yuan-Tai and Huang, Polly},
title = {Jujuby: Design and Deployment of a Crawler for Twitch CDN Mapping},
year = {2022},
isbn = {9781450399814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570748.3570755},
doi = {10.1145/3570748.3570755},
abstract = {Content Delivery Networks (CDNs) deliver much of the world’s content on the Internet today. As demand for content rises, the role of CDNs is as crucial as ever, making it imperative to understand the internal workings of such systems. Focusing on Twitch, one of the most popular live video services, we present in this work (1) the design of a Twitch CDN crawler, (2) two deployment methodologies, (3) a study of the data collected, and (4) a comparison of the deployment methodologies. Towards a Twitch CDN crawler, we sniffed the traffic in a Twitch session. A closer look into the traffic revealed that a load-balancer, named Usher, is in charge of redirecting the clients to their appropriate video servers. This enabled us to design and implement a programmable crawler that can be deployed to discover Twitch’s CDN at a large scale. Previous works have deployed their crawlers on platforms such as PlanetLab and the global Web proxies for broad network coverage. As these platforms become less available, we experimented on two fee-based platforms, i.e., the cloud computing and the virtual private network (VPN) services, for better availability into the future. By running our crawler on the two platforms, we collected data showing that Twitch serves its users in geographically wide but regionally concentrated clusters. These clusters are located primarily in North America, Europe, Asia, and Oceania. We were able to compare as well how the two crawling platforms are suitable for different crawling purposes.},
booktitle = {Proceedings of the 17th Asian Internet Engineering Conference},
pages = {44–52},
numpages = {9},
keywords = {Twitch, Measurement Tool, CDN Mapping},
location = {<conf-loc>, <city>Hiroshima</city>, <country>Japan</country>, </conf-loc>},
series = {AINTEC '22}
}

@inproceedings{10.1145/3485832.3485900,
author = {Zhang, Yumei and Liu, Xinzhi and Sun, Cong and Zeng, Dongrui and Tan, Gang and Kan, Xiao and Ma, Siqi},
title = {ReCFA: Resilient Control-Flow Attestation},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485900},
doi = {10.1145/3485832.3485900},
abstract = {Recent IoT applications gradually adapt more complicated end systems with commodity software. Ensuring the runtime integrity of these software is a challenging task for the remote controller or cloud services. Popular enforcement is the runtime remote attestation which requires the end system (prover) to generate evidence for its runtime behavior and a remote trusted verifier to attest the evidence. Control-flow attestation is a kind of runtime attestation that provides diagnoses towards the remote control-flow hijacking at the prover. Most of these attestation approaches focus on small or embedded software. The recent advance to attesting complicated software depends on the source code and CFG traversing to measure the checkpoint-separated subpaths, which may be unavailable for commodity software and cause possible context missing between consecutive subpaths in the measurements. In this work, we propose a resilient control-flow attestation (ReCFA), which does not need the offline measurement of all legitimate control-flow paths, thus scalable to be used on complicated commodity software. Our main contribution is a multi-phase approach to condensing the runtime control-flow events; as a result, the vast amount of control-flow events are abstracted into a deliverable size. The condensing approach consists of filtering skippable call sites, folding program-structure related control-flow events, and a greedy compression. Our approach is implemented with binary-level static analysis and instrumentation. We employ a shadow stack mechanism at the verifier to enforce context-sensitive control-flow integrity and diagnose the compromised control-flow events violating the security policy. The experimental results on real-world benchmarks show both the efficiency of the control-flow condensing and the effectiveness of security enforcement.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {311–322},
numpages = {12},
keywords = {remote attestation, reference monitor, control-flow integrity, binary rewriting, binary analysis},
location = {<conf-loc>, <city>Virtual Event</city>, <country>USA</country>, </conf-loc>},
series = {ACSAC '21}
}

@inproceedings{10.1145/3583780.3615461,
author = {Silberstein, Natalia and Shoham, Or and Klein, Assaf},
title = {Combating Ad Fatigue via Frequency-Recency Features in Online Advertising Systems},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615461},
doi = {10.1145/3583780.3615461},
abstract = {Online advertising is a driving force of Internet services today. One of the main challenges in advertising systems is finding the right balance between user experience and overall revenue.In this paper we address one of the problems that negatively impacts the user experience, specifically, the repeated display of identical ads to the same user. This problem leads to the phenomenon called "ad fatigue", characterized by diminished interest in the ad, resulting in a decrease in the click-through rate (CTR) as users encounter the same ad repeatedly. Naive solutions such as placing a hard limit on the number of times a specific ad is displayed to a specific user, usually come at the cost of reduced revenue.To address the ad fatigue problem, we introduce a new family of features, called FoRI (Frequency over Recent Intervals). FoRI features integrate information about frequency and recency of previous user-ad interactions within the CTR prediction model. This approach involves allocating these interactions to unevenly distributed time intervals, enabling a higher emphasis on more recent interactions. Furthermore, we introduce new metrics to assess ad fatigue in terms of the repetitiveness and novelty of the displayed ads.We conducted a comprehensive large-scale online evaluation which shows that integrating FoRI features into our CTR prediction model offers two-fold benefits. Firstly, it improves user experience by reducing the occurrence of repeated ads by 15%, and increasing the exposure to unseen ads by 5% (ads not previously displayed to the user), leading to a substantial boost in CTR. Secondly, it significantly increases revenue.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4822–4828},
numpages = {7},
keywords = {recommender systems, recency, online advertising, frequency, ad fatigue, CTR prediction},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@article{10.1145/3604620,
author = {Salamatian, Loqman and Anderson, Scott and Mathews, Joshua and Barford, Paul and Willinger, Walter and Crovella, Mark},
title = {A Manifold View of Connectivity in the Private Backbone Networks of Hyperscalers},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3604620},
doi = {10.1145/3604620},
abstract = {As hyperscalers such as Google, Microsoft, and Amazon play an increasingly important role in today's Internet, they are also capable of manipulating probe packets that traverse their privately owned and operated backbones. As a result, standard traceroute-based measurement techniques are no longer a reliable means for assessing network connectivity in these global-scale cloud provider infrastructures. In response to these developments, we present a new empirical approach for elucidating connectivity in these private backbone networks. Our approach relies on using only "lightweight" (i.e., simple, easily interpretable, and readily available) measurements, but requires applying "heavyweight" mathematical techniques for analyzing these measurements. In particular, we describe a new method that uses network latency measurements and relies on concepts from Riemannian geometry (i.e., Ricci curvature) to assess the characteristics of the connectivity fabric of a given network infrastructure. We complement this method with a visualization tool that generates a novel manifold view of a network's delay space. We demonstrate our approach by utilizing latency measurements from available vantage points and virtual machines running in datacenters of three large cloud providers to study different aspects of connectivity in their private backbones and show how our generated manifold views enable us to expose and visualize critical aspects of this connectivity.},
journal = {Commun. ACM},
month = {jul},
pages = {95–103},
numpages = {9}
}

@inproceedings{10.1145/3548606.3560599,
author = {Si, Wai Man and Backes, Michael and Blackburn, Jeremy and De Cristofaro, Emiliano and Stringhini, Gianluca and Zannettou, Savvas and Zhang, Yang},
title = {Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560599},
doi = {10.1145/3548606.3560599},
abstract = {Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2659–2673},
numpages = {15},
keywords = {trustworthy machine learning, online toxicity, dialogue system},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3447548.3467203,
author = {Luo, Xusheng and Bo, Le and Wu, Jinhang and Li, Lin and Luo, Zhiy and Yang, Yonghua and Yang, Keping},
title = {AliCoCo2: Commonsense Knowledge Extraction, Representation and Application in E-commerce},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467203},
doi = {10.1145/3447548.3467203},
abstract = {Commonsense knowledge used by humans while doing online shopping is valuable but difficult to be captured by existing systems running on e-commerce platforms. While construction of common- sense knowledge graphs in e-commerce is non-trivial, representation learning upon such graphs poses unique challenge compared to well-studied open-domain knowledge graphs (e.g., Freebase). By leveraging the commonsense knowledge and representation techniques, various applications in e-commerce can be benefited. Based on AliCoCo, the large-scale e-commerce concept net assisting a series of core businesses in Alibaba, we further enrich it with more commonsense relations and present AliCoCo2, the first commonsense knowledge graph constructed for e-commerce use. We propose a multi-task encoder-decoder framework to provide effective representations for nodes and edges from AliCoCo2. To explore the possibility of improving e-commerce businesses with commonsense knowledge, we apply newly mined commonsense relations and learned embeddings to e-commerce search engine and recommendation system in different ways. Experimental results demonstrate that our proposed representation learning method achieves state-of-the-art performance on the task of knowledge graph completion (KGC), and applications on search and recommendation indicate great potential value of the construction and use of commonsense knowledge graph in e-commerce. Besides, we propose an e-commerce QA task with a new benchmark during the construction of AliCoCo2, for testing machine common sense in e-commerce, which can benefit research community in exploring commonsense reasoning.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3385–3393},
numpages = {9},
keywords = {knowledge graph embedding, e-commerce, common sense},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1145/3479866,
author = {Huang, Yi-Ching (Janet) and Cheng, Yu-Ting and Liang, Rung-Huei and Hsu, Jane Yung-jen and Chen, Lin-Lin},
title = {Thing Constellation Visualizer: Exploring Emergent Relationships of Everyday Objects},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479866},
doi = {10.1145/3479866},
abstract = {Designing future IoT ecosystems requires new approaches and perspectives to understand everyday practices. While researchers recognize the importance of understanding social aspects of everyday objects, limited studies have explored the possibilities of combining data-driven patterns with human interpretations to investigate emergent relationships among objects. This work presents Thing Constellation Visualizer (thingCV), a novel interactive tool for visualizing the social network of objects based on their co-occurrence as computed from a large collection of photos. ThingCV enables perspective-changing design explorations over the network of objects with scalable links. Two exploratory workshops were conducted to investigate how designers navigate and make sense of a network of objects through thingCV. The results of eight participants showed that designers were actively engaged in identifying interesting objects and their associated clusters of related objects. The designers projected social qualities onto the identified objects and their communities. Furthermore, the designers changed their perspectives to revisit familiar contexts and to generate new insights through the exploration process. This work contributes a novel approach to combining data-driven models with designerly interpretations of thing constellation towards More-Than Human-Centred Design of IoT ecosystems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {479},
numpages = {29},
keywords = {thing constellation, more-than human-centred design, computer-supported creativity, computational thing ethnography, IoT ecosystem design}
}

@article{10.1145/3626189,
author = {Lin, Fuqi and Lu, Xuan and Ai, Wei and Li, Huoran and Ma, Yun and Yang, Yulian and Deng, Hongfei and Wang, Qingxiang and Mei, Qiaozhu and Liu, Xuanzhe},
title = {Adoption of Recurrent Innovations: A Large-Scale Case Study on Mobile App Updates},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3626189},
doi = {10.1145/3626189},
abstract = {Modern technology innovations feature a successive and even recurrent procedure. Intervals between old and new generations of technology are shrinking, and the Internet and Web services have facilitated the fast adoption of an innovation even before the convergence of its predecessor. While the adoption and diffusion of innovations have been studied for decades, most theories and analyses focus on single and one-time innovations. Meanwhile, limited work has investigated successive innovations while lacking user-level analysis, possibly due to the unavailability of fine-grained adoption behavior data. In this study, we present the first large-scale analysis of the adoption of recurrent innovations in the context of mobile app updates, investigating how millions of users consume various versions of thousands of apps on their mobile devices. Our analysis reveals novel patterns of crowd and individual adoption behaviors, which suggest the need for new categories of adopters to be added on top of the Rogers model of innovation diffusion. We show that standard machine learning models are able to pick up various sources of signals to predict whether users in these different categories will adopt a new version of an app and how soon they will adopt it.},
journal = {ACM Trans. Web},
month = {nov},
articleno = {13},
numpages = {26},
keywords = {mobile app, recurrent innovation, diffusion of innovations}
}

@inproceedings{10.1145/3472716.3472851,
author = {Muzayan Haq, Muhammad Yasir and Abhishta, Abhishta and Nieuwenhuis, Lambert J. M.},
title = {The effect of consumer portfolio on the risk profile of cloud provider},
year = {2021},
isbn = {9781450386296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472716.3472851},
doi = {10.1145/3472716.3472851},
abstract = {The economies-of-scale model of Cloud services has brought many financial and technical benefits for organizations. However, recent events like the attack on Dyn and GitHub have shown that successful attacks on big Cloud providers can cause a massive impact on a major portion of the Internet ecosystem. In this project, we will study how the consumer portfolio of a Cloud provider may affect its risk profile. We will use the result to develop a recommender system for choosing a Cloud provider based on consumer security and business requirements. Insights from our research can be used to simulate an alternative more secure market structure for the Cloud ecosystem. We invite fellow researchers to further discuss this idea and possible collaboration.},
booktitle = {Proceedings of the SIGCOMM '21 Poster and Demo Sessions},
pages = {18–20},
numpages = {3},
keywords = {security, oligopoly, decision support system, cyber attacks, cloud},
location = {Virtual Event},
series = {SIGCOMM '21}
}

@inproceedings{10.1145/3566097.3567900,
author = {Liang, Rongjian and Nath, Siddhartha and Rajaram, Anand and Hu, Jiang and Ren, Haoxing},
title = {BufFormer: A Generative ML Framework for Scalable Buffering},
year = {2023},
isbn = {9781450397834},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3566097.3567900},
doi = {10.1145/3566097.3567900},
abstract = {Buffering is a prevalent interconnect optimization technique to help timing closure and is often performed after placement. A common buffering approach is to construct a Steiner tree and then buffers are inserted on the tree based on Ginneken-Lillis style algorithm. Such an approach is difficult to scale with large nets. Our work attempts to solve this problem with a generative machine-learning (ML) approach without Steiner tree construction. Our approach can extract and reuse knowledge from high quality samples and therefore has significantly improved scalability. A generative ML framework, BufFormer, is proposed to construct abstract tree topology while simultaneously determining buffer sizes &amp; locations. A baseline method, FLUTE-based Steiner tree construction followed by Ginneken-Lillis style buffer insertion, is implemented to generate training samples. After training, BufFormer can produce solutions for unseen nets highly comparable to baseline results with a correlation coefficient 0.977 in terms of buffer area and 0.934 for driver-sink delays. On average, BufFormer-generated tree achieves similar delays with slightly larger buffer area. And up to 160X speedup can be achieved for large nets when running on a GPU over the baseline on a single CPU thread.},
booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
pages = {264–270},
numpages = {7},
keywords = {timing optimization, machine learning, interconnect optimization, generative model, buffer insertion},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {ASPDAC '23}
}

@inproceedings{10.1145/3437963.3441804,
author = {Li, Qintong and Li, Piji and Li, Xinyi and Ren, Zhaochun and Chen, Zhumin and de Rijke, Maarten},
title = {Abstractive Opinion Tagging},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441804},
doi = {10.1145/3437963.3441804},
abstract = {In e-commerce, opinion tags refer to a ranked list of tags provided by the e-commerce platform that reflect characteristics of reviews of an item. To assist consumers to quickly grasp a large number of reviews about an item, opinion tags are increasingly being applied by e-commerce platforms. Current mechanisms for generating opinion tags rely on either manual labelling or heuristic methods, which is time-consuming and ineffective. In this paper, we propose the abstractive opinion tagging task, where systems have to automatically generate a ranked list of opinion tags that are based on, but need not occur in, a given set of user-generated reviews. The abstractive opinion tagging task comes with three main challenges: the noisy nature of reviews; the formal nature of opinion tags vs. the colloquial language usage in reviews; and the need to distinguish between different items with very similar aspects. To address these challenges, we propose an abstractive opinion tagging framework, named AOT-Net, to generate a ranked list of opinion tags given a large number of reviews. First, a sentence-level salience estimation component estimates each review's salience score. Next, a review clustering and ranking component ranks reviews in two steps: first, reviews are grouped into clusters and ranked by cluster size; then, reviews within each cluster are ranked by their distance to the cluster center. Finally, given the ranked reviews, a rank-aware opinion tagging component incorporates an alignment feature and alignment loss to generate a ranked list of opinion tags. To facilitate the study of this task, we create and release a large-scale dataset, called eComTag, crawled from real-world e-commerce websites. Extensive experiments conducted on the eComTag dataset verify the effectiveness of the proposed AOT-Net in terms of various evaluation metrics.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {337–345},
numpages = {9},
keywords = {review analysis, e-commerce, abstractive summarization},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3543622.3573188,
author = {Du, Linfeng and Liang, Tingyuan and Sinha, Sharad and Xie, Zhiyao and Zhang, Wei},
title = {FADO: Floorplan-Aware Directive Optimization for High-Level Synthesis Designs on Multi-Die FPGAs},
year = {2023},
isbn = {9781450394178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543622.3573188},
doi = {10.1145/3543622.3573188},
abstract = {Multi-die FPGAs are widely adopted to deploy large-scale hardware accelerators. Two factors impede the performance optimization of high-level synthesis (HLS) designs implemented on multi-die FPGAs. On the one hand, the long net delay due to nets crossing die-boundaries results in an NP-hard problem to properly floorplan and pipeline an application. On the other hand, traditional automated searching flow for HLS directive optimizations targets single-die FPGAs, and hence, it cannot consider the resource constraints on each die and the timing issue incurred by the die-crossings. Further, it leads to an excessively long runtime to legalize the floorplanning of HLS designs generated under each group of configurations during directive optimization due to the large design scale.To co-optimize the directives and floorplan of HLS designs on multi-die FPGAs, we propose the FADO framework, which formulates the directive-floorplan co-search problem based on the multi-choice multi-dimensional bin-packing and solves it using an iterative optimization flow. For each step of directive optimization, a latency-bottleneck-guided greedy algorithm searches for more efficient directive configurations. For floorplanning, instead of repetitively incurring global floorplanning algorithms, we implement a more efficient incremental floorplan legalization algorithm. It mainly applies the worst-fit strategy from the online bin-packing algorithm to balance the floorplan, together with an offline best-fit-decreasing re-packing step to compact the floorplan, followed by pipelining of the long wires crossing die-boundaries.Through experiments on a set of HLS designs mixing dataflow and non-dataflow kernels, FADO not only well-automates the co-optimization and finishes within 693X~4925X shorter runtime, compared with DSE assisted by global floorplanning, but also yields an improvement of 1.16X~8.78X in overall workflow execution time after implementation on the Xilinx Alveo U250 FPGA.},
booktitle = {Proceedings of the 2023 ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {15–25},
numpages = {11},
keywords = {design space exploration, directive optimization, floorplanning, high-level synthesis, multi-die FPGA},
location = {<conf-loc>, <city>Monterey</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {FPGA '23}
}

@article{10.1145/3632178,
author = {Telili, Ahmed and Fezza, Sid Ahmed and Hamidouche, Wassim and Brachemi Meftah, Hanene F. Z.},
title = {2BiVQA: Double Bi-LSTM-based Video Quality Assessment of UGC Videos},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3632178},
doi = {10.1145/3632178},
abstract = {Recently, with the growing popularity of mobile devices as well as video sharing platforms (e.g., YouTube, Facebook, TikTok, and Twitch), User-Generated Content (UGC) videos have become increasingly common and now account for a large portion of multimedia traffic on the internet. Unlike professionally generated videos produced by filmmakers and videographers, typically, UGC videos contain multiple authentic distortions, generally introduced during capture and processing by naive users. Quality prediction of UGC videos is of paramount importance to optimize and monitor their processing in hosting platforms, such as their coding, transcoding, and streaming. However, blind quality prediction of UGC is quite challenging, because the degradations of UGC videos are unknown and very diverse, in addition to the unavailability of pristine reference. Therefore, in this article, we propose an accurate and efficient Blind Video Quality Assessment (BVQA) model for UGC videos, which we name 2BiVQA for double Bi-LSTM Video Quality Assessment. 2BiVQA metric consists of three main blocks, including a pre-trained Convolutional Neural Network to extract discriminative features from image patches, which are then fed into two Recurrent Neural Networks for spatial and temporal pooling. Specifically, we use two Bi-directional Long Short-term Memory networks, the first is used to capture short-range dependencies between image patches, while the second allows capturing long-range dependencies between frames to account for the temporal memory effect. Experimental results on recent large-scale UGC VQA datasets show that 2BiVQA achieves high performance at lower computational cost than most state-of-the-art VQA models. The source code of our 2BiVQA metric is made publicly available at .},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {dec},
articleno = {100},
numpages = {22},
keywords = {temporal pooling, spatial pooling, Bi-LSTM, deep learning, user-generated content, Blind video quality assessment}
}

@article{10.1145/3478124,
author = {Zhao, Minghui and Chang, Tyler and Arun, Aditya and Ayyalasomayajula, Roshan and Zhang, Chi and Bharadia, Dinesh},
title = {ULoc: Low-Power, Scalable and cm-Accurate UWB-Tag Localization and Tracking for Indoor Applications},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478124},
doi = {10.1145/3478124},
abstract = {A myriad of IoT applications, ranging from tracking assets in hospitals, logistics, and construction industries to indoor tracking in large indoor spaces, demand centimeter-accurate localization that is robust to blockages from hands, furniture, or other occlusions in the environment. With this need, in the recent past, Ultra Wide Band (UWB) based localization and tracking has become popular. Its popularity is driven by its proposed high bandwidth and protocol specifically designed for localization of specialized "tags". This high bandwidth of UWB provides a fine resolution of the time-of-travel of the signal that can be translated to the location of the tag with centimeter-grade accuracy in a controlled environment. Unfortunately, we find that high latency and high-power consumption of these time-of-travel methods are the major culprits which prevent such a system from deploying multiple tags in the environment. Thus, we developed ULoc, a scalable, low-power, and cm-accurate UWB localization and tracking system. In ULoc, we custom build a multi-antenna UWB anchor that enables azimuth and polar angle of arrival (henceforth shortened to '3D-AoA') measurements, with just the reception of a single packet from the tag. By combining multiple UWB anchors, ULoc can localize the tag in 3D space. The single-packet location estimation reduces the latency of the entire system by at least 3\texttimes{}, as compared with state of art multi-packet UWB localization protocols, making UWB based localization scalable. ULoc's design also reduces the power consumption per location estimate at the tag by 9\texttimes{}, as compared to state-of-art time-of-travel algorithms. We further develop a novel 3D-AoA based 3D localization that shows a stationary localization accuracy of 3.6 cm which is 1.8\texttimes{} better than the state-of-the-art two-way ranging (TWR) systems. We further developed a temporal tracking system that achieves a tracking accuracy of 10 cm in mobile conditions which is 4.3\texttimes{} better than the state-of-the-art TWR systems.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {140},
numpages = {31},
keywords = {cm-Accurate, Ultra-Wideband, Real-time, Low-power, L-shaped, Hardware, Angle of Arrival, Anchor Board, 3D-Localization,3D-tracking}
}

@inproceedings{10.1145/3573428.3573636,
author = {Miao, Hui and Zhou, Qinglei and Song, Huawei and Wan, Fangjie},
title = {Time Interest Network for Click-Through Rate Prediction},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573636},
doi = {10.1145/3573428.3573636},
abstract = {In the era of the information explosion, it is very important to accurately predict the user's next behavior (such as browsing, collecting and commenting) through the user's characteristics. As a key research issue in the traffic distribution process of the Internet industry, CTR is of great significance to content recommendation and online advertising. However, most existing research ignores the intrinsic structure of user behavior traits: User behavior changes over time, leading to changes in user interest characteristics. Toward that, this paper proposes a new CTR model called click-through rate prediction (TIN) based on the network of temporal interest. The model first maps large-scale sparse input features into low-dimensional embedding vectors, the characteristics of the user's interest in each time window are then extracted by the multi-head self-attention mechanism, Bi-LSTM is then utilized to capture changes of interest between various time periods, and then, it is beneficial for the residual network to achieve gradient multiplication and backpropagation process, effectively avoiding the problem of gradient disappearance, and finally connecting to the multilayer perceptron (MLP) to learn the nonlinear relationship between features. Extensive experiments have conducted on the advertising dataset and the results have demonstrated that the proposed model is competitive with superior CTR estimation ability.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1159–1165},
numpages = {7},
keywords = {User Preferences, Time window, Deep learning, Attention mechanisms},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3637907.3637979,
author = {Tian, Qian and Yi, Wenlu and Wang, Pengfei and Xu, Xiaoshu and Zhang, Yunfeng},
title = {Bibliometric analysis of research on ChatGPT: using VOSviewer},
year = {2024},
isbn = {9798400716676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637907.3637979},
doi = {10.1145/3637907.3637979},
abstract = {ChatGPT, a generative large-scale language model released by OpenAI, is referred to as a technology revolution as significant as the invention of the internet. Considering that ChatGPT has an essential impact on translation, coding, academic writing, etc., this study used bibliometric analysis to provide an overview of research on ChatGPT. A comprehensive review of the Web of Science (WOS) Core Collection related to ChatGPT was carried out. Using VOSviewer (version 1.6.19), the research analyzed co-occurrence and co-citation of keywords and research networks as well (e.g., countries and institutions). Altogether, a total of 206 papers (from 2020-2023) were retrieved from WOS for review. Through the keyword clustering analysis, it was found that Cluster 1 contained seven keywords, among which, "Chatbots" was one of the main keywords in this cluster and had close connections with other clusters. As for the national collaboration networks, most of the collaborative relationships involved the United States, China, and Australia. Regarding institutional cooperation related to ChatGPT, China held three out of the top five positions based on the total link strength, suggesting a high level of importance given to ChatGPT in China. The research provides guidance and inspiration for applications of ChatGPT in the education sector.},
booktitle = {Proceedings of the 2023 6th International Conference on Educational Technology Management},
pages = {235–243},
numpages = {9},
keywords = {Bibliometric analysis, ChatGPT, Chatbot, Literature review, Network analysis, VOS viewer},
location = {<conf-loc>, <city>Guangzhou</city>, <country>China</country>, </conf-loc>},
series = {ICETM '23}
}

@inproceedings{10.1145/3543507.3583548,
author = {Tian, Zhiliang and Xie, Zheng and Lin, Fuqiang and Song, Yiping},
title = {A Multi-view Meta-learning Approach for Multi-modal Response Generation},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583548},
doi = {10.1145/3543507.3583548},
abstract = {As massive conversation examples are easily accessible on the Internet, we are now able to organize large-scale conversation corpora to build chatbots in a data-driven manner. Multi-modal social chatbots produce conversational utterances according to both textual utterances and vision signals. Due to the difficulty of bridging different modalities, the dialogue generation model of chatbots falls into local minima that only capture the mapping between textual input and textual output, as a result, it almost ignores the non-textual signals. Further, similar to the dialogue model with plain text as input and output, the generated responses from multi-modal dialogue also lack diversity and informativeness. In this paper, to address the above issues, we propose a Multi-View Meta-Learning (MultiVML) algorithm that groups samples in multiple views and customizes generation models to different groups. We employ a multi-view clustering to group the training samples so as to attend more to the unique information in non-textual modality. Tailoring different sets of model parameters for each group boosts the genereation diversity via meta-learning. We evaluate MultiVML on two variants of the OpenViDial benchmark datasets. The experiments show that our model not only better explore the information from multiple modalities, but also excels baselines in both quality and diversity.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1938–1947},
numpages = {10},
keywords = {Response generation, Multi-modal, Meta-learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.14778/3407790.3407816,
author = {Nakandala, Supun and Zhang, Yuhao and Kumar, Arun},
title = {Cerebro: a data system for optimized deep learning model selection},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407816},
doi = {10.14778/3407790.3407816},
abstract = {Deep neural networks (deep nets) are revolutionizing many machine learning (ML) applications. But there is a major bottleneck to wider adoption: the pain and resource intensiveness of model selection. This empirical process involves exploring deep net architectures and hyper-parameters, often requiring hundreds of trials. Alas, most ML systems focus on training one model at a time, reducing throughput and raising overall resource costs; some also sacrifice reproducibility. We present Cerebro, a new data system to raise deep net model selection throughput at scale without raising resource costs and without sacrificing reproducibility or accuracy. Cerebro uses a new parallel SGD execution strategy we call model hopper parallelism that hybridizes task- and data-parallelism to mitigate the cons of these prior paradigms and offer the best of both worlds. Experiments on large ML benchmark datasets show that Cerebro offers 3x to 10x runtime savings relative to data-parallel systems like Horovod and Parameter Server and up to 8x memory/storage savings or up to 100x network savings relative to task-parallel systems. Cerebro also supports heterogeneous resources and fault tolerance.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2159–2173},
numpages = {15}
}

@inproceedings{10.1145/3479239.3485699,
author = {Xia, Zhongda and Zhang, Yu and Liang, Teng and Zhang, Xinggong and Fang, Binxing},
title = {Adapting Named Data Networking (NDN) for Better Consumer Mobility Support in LEO Satellite Networks},
year = {2021},
isbn = {9781450390774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479239.3485699},
doi = {10.1145/3479239.3485699},
abstract = {Large low Earth orbit (LEO) satellite constellations provide low-latency and high-bandwidth Internet connectivity at the global scale. One major challenge is to handle frequent satellite handovers. Named Data Networking (NDN) adopts a pull-based communication model, which allows users to retrieve data that fail to come back because of satellite handovers by retransmitting the corresponding requests, hence simplifying mobility management when retrieving data. However, we find that relying on such retransmissions alone can be highly inefficient in typical LEO satellite constellations. Specifically, typical inter-satellite topologies and satellite handover strategies may produce bad cases for retransmissions, generating a significant amount of additional traffic. Motivated by this observation, this paper attempts to consolidate NDN's advantage in mobility management with the Data Recovery Link Service (DRLS), a shim layer service operating between the network and link layer in the NDN protocol stack. DRLS hides recurring satellite handovers from forwarding by recovering data from the previously connected satellite via alternative paths, thus ensuring the bidirectional request-response exchange of NDN without retransmitting requests. A prototype of DRLS is implemented in the reference NDN software forwarder and evaluated through simulations. Results prove the efficacy of the proposed mechanism at reducing the overall traffic volume.},
booktitle = {Proceedings of the 24th International ACM Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {207–216},
numpages = {10},
keywords = {ndn link service, ndn consumer mobility, named-data networking (ndn), large leo satellite constellations, information-centric networking (icn)},
location = {Alicante, Spain},
series = {MSWiM '21}
}

@inproceedings{10.1145/3571600.3571642,
author = {Sindhura, Chitimireddy and Vengama, Harsha Vardhan Reddy and Gorthi, Subrahmanyam},
title = {Efficient-PSP-Net: A Lightweight Model for the Segmentation of Liver and Tumor in CT Scans},
year = {2023},
isbn = {9781450398220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571600.3571642},
doi = {10.1145/3571600.3571642},
abstract = {The segmentation of the liver and tumors present in the liver is an essential task for the diagnosis of liver cancer and surgical treatment planning. While most of the works were developed with the goal of improving segmentation accuracy, there is a high demand for the development of lightweight architectures to deploy in clinical settings with limited computational resources. Developing such resource-efficient models while maintaining the performance of the state-of-the-art architectures is challenging due to the high class imbalance and small observable changes between tumorous and non-tumorous regions, which would generally require extensive and complex deep learning architectures. The contributions of this paper are twofold; First, this paper proposes to encode information extracted at different scales using a “Pyramid Pooling Module” (PPM) architecture. While different variants of pyramidal architectures are widely used in the context of scene parsing in computer vision, this paper explores the use of the PPM architecture for the segmentation of liver and tumor. Second, the paper proposes using the popular lightweight “Efficient-Net” as the backbone network of PSP-Net for feature extraction, thereby reducing the overall model size while achieving relatively similar performance to state-of-the-art architectures. The performance of the proposed model is evaluated on the MICCAI 2017 LiTS dataset. The model achieved a DSC of 95.06% and 79.08% for liver and tumor, respectively, with only 1.76M parameters.},
booktitle = {Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {41},
numpages = {6},
keywords = {PSP-Net, Liver and Tumor, Lightweight, LiTS, Efficient-Net, CT.},
location = {<conf-loc>, <city>Gandhinagar</city>, <country>India</country>, </conf-loc>},
series = {ICVGIP '22}
}

@inproceedings{10.1145/3477314.3507135,
author = {Sudharsan, Bharath and Breslin, John G. and Ali, Muhammad Intizar and Corcoran, Peter and Ranjan, Rajiv},
title = {ElastiQuant: elastic quantization strategy for communication efficient distributed machine learning in IoT},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507135},
doi = {10.1145/3477314.3507135},
abstract = {In training distributed machine learning, communicating model updates among workers has always been a bottleneck. The magnitude of impact on the quality of resultant models is higher when distributed training on low hardware specification devices and in uncertain real-world IoT networks where congestion, latency, bandwidth issues are common. In this scenario, gradient quantization plus encoding is an effective way to reduce cost when communicating model updates. Other approaches can be to limit the client-server communication frequency, adaptive compression by varying the spacing between quantization levels, reusing outdated gradients, deep compression to reduce transmission packet size, and adaptive tuning of the number of bits transmitted per round. The optimization levels provided by such and other non-comprehensive approaches do not suffice for high-dimensional NN models with large size model updates.This paper presents ElastiQuant, an elastic quantization strategy that aims to reduce the impact caused by limitations in distributed IoT training scenarios. The distinguishable highlights of this comprehensive work are: (i) theoretical assurances and bounds on variance and number of communication bits are provided, (ii) worst-case variance analysis is performed, and (iii) momentum is considered in convergence assurance. ElastiQuant experimental evaluation and comparison with top schemes by distributed training 5 ResNets on 18 edge GPUs over ImageNet and CIFAR datasets show: improved solution quality in terms of ≈ 2--11 % training loss reduction, ≈ 1--4 % accuracy boost, and ≈ 4--22 % variance drop; positive scalability due to higher communication compression resulting in saving bandwidth and ≈ 4--30 min per epoch training speedups.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {246–254},
numpages = {9},
keywords = {scalable model training, quantization, gradient compression, distributed machine learning, IoT devices},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3544109.3544404,
author = {Luo, Jinman and Li, Xiaoxia and Feng, Youjun and Wang, Lina},
title = {Research on Web Big Data Migration Algorithm based on Association Rule Mining Model},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544404},
doi = {10.1145/3544109.3544404},
abstract = {With the popularization of the Internet and the rapid development of information technology, big data on the Web has spread across all fields of life and has become an important part of people's lives in the cloud computing era. However, the scale of Web big data is so huge that users cannot directly obtain the information they need from it. Data migration is a key part of data system integration to ensure smooth system upgrades and updates, and it also plays an important role in cloud computing. The quality of data migration is not only an important prerequisite for the new system to be put into use, but also a strong guarantee for stable operation in the future. When cloud computing lays out data sets, because it mainly considers the load in the data, it will generally give priority to distributing the data sets in the data with lower load. Such a layout method makes it possible to perform tasks of these two types of applications. When the number of network visits increases, problems such as increasing the number of network visits will be prevented. For these two special applications, this paper designs an efficient and dynamic data migration algorithm based on the association rule mining model to optimize the migration performance.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {1030–1034},
numpages = {5},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/3474717.3483986,
author = {Song, Yatong and Li, Jiawei and Chen, Liying and Chen, Shuiping and He, Renqing and Sun, Zhizhao},
title = {A Semantic Segmentation based POI Coordinates Generating Framework for On-demand Food Delivery Service},
year = {2021},
isbn = {9781450386647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474717.3483986},
doi = {10.1145/3474717.3483986},
abstract = {Nowadays, on-demand food delivery service has become fashionable in China. The efficiency of food delivery relies heavily on accurate coordinates of destination Points of Interest (POI). However, the coordinates of the destination POIs from the existing geospatial data warehouses still have many problems that perplex couriers severely. The major problems can be concluded in two categories: 1) the deviation of POI coordinates; 2) the lack of POI coordinates. To address these problems, we propose a POI-coordinate-generating framework based on couriers' and users' behavioral data of historical waybills. In particular, we start with a combinatorial strategy to assign waybills to Areas of Interest (AOI). Second, we generate a destination POI name by processing the user address for each waybill, and all waybills are grouped by the corresponding POI name. Then, a data density image of the behavioral data is generated for each group, with the ground-truth location of the POI labeled. Finally, a U-Net is trained by using the images generated in the previous step to infer locations of the POIs. We evaluated this framework by launching experiments and case studies on large-scale datasets, and the result shows our framework can predict coordinates of POIs accurately. These predicted coordinates can be used to calibrate deviated coordinates of many POIs and complement the geospatial data warehouse.},
booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
pages = {379–388},
numpages = {10},
keywords = {U-Net, Text Similarity, POI Generation, Geospatia Data, Address Parsing},
location = {Beijing, China},
series = {SIGSPATIAL '21}
}

@article{10.1145/3517221,
author = {Chen, Xiuying and Li, Mingzhe and Gao, Shen and Chan, Zhangming and Zhao, Dongyan and Gao, Xin and Zhang, Xiangliang and Yan, Rui},
title = {Follow the Timeline! Generating an Abstractive and Extractive Timeline Summary in Chronological Order},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3517221},
doi = {10.1145/3517221},
abstract = {Today, timestamped web documents related to a general news query flood the Internet, and timeline summarization targets this concisely by summarizing the evolution trajectory of events along the timeline. Unlike traditional document summarization, timeline summarization needs to model the time series information of the input events and summarize important events in chronological order. To tackle this challenge, in this article we propose our Unified Timeline Summarizer, which can generate abstractive and extractive timeline summaries in time order. Concretely, in the encoder part, we propose a graph-based event encoder that relates multiple events according to their content dependency and learns a global representation of each event. In the decoder part, to ensure the chronological order of the abstractive summary, we propose to extract the feature of event-level attention in its generation process with sequential information retained and use it to simulate the evolutionary attention of the ground truth summary. The event-level attention can also be used to assist in extracting a summary, where the extracted summary also comes in time sequence. We augment the previous Chinese large-scale timeline summarization dataset and collect a new English timeline dataset. Extensive experiments conducted on these datasets and on the out-of-domain Timeline 17 dataset show that our Unified Timeline Summarizer achieves state-of-the-art performance in terms of both automatic and human evaluations.1},
journal = {ACM Trans. Inf. Syst.},
month = {jan},
articleno = {9},
numpages = {30},
keywords = {abstractive summarization, extractive summarization, Timeline summarization}
}

@inproceedings{10.1145/3599589.3599591,
author = {Liu, Jing and Zhang, Feng Guo and Zhao, Hao Ping and Lu, Qi De and Feng, Bin and Feng, Lichang},
title = {Recognition and Detection of UAV Based on Transfer Learning},
year = {2023},
isbn = {9781450399586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599589.3599591},
doi = {10.1145/3599589.3599591},
abstract = {With the increasing application scenarios of UAVs in industry, agriculture, military and other fields, the potential threats to national security and public security cannot be ignored. In addition, effective UAV detection and/or tracking is becoming an increasingly important security service. This paper integrates deep learning and image processing technology to conduct research in this context. In this paper, a transfer learning based UAV detection model (YOLOV5-UAV) is proposed. In order to reduce the influence of the amount of supervised data and the imbalance of target distribution on the performance of the model, the dataset is constructed based on self-shot videos and Internet downloaded videos in different natural scenes, combined with Mosaic data enhancement and adaptive scaling techniques. Therefore, the problem of data security is also effectively solved. Furthermore, real-time tests were carried out in two different time periods, namely day and night, from multiple scales, multiple perspectives and multiple natural scenes, for purpose of verifying the validity of the model. The applicability of different detection models is compared and analyzed for small target, moving background and weak contrast between UAV and background. The results show that YOLOV5-UAV model has a good performance in both detection accuracy and detection speed.},
booktitle = {Proceedings of the 2023 8th International Conference on Multimedia and Image Processing},
pages = {6–12},
numpages = {7},
keywords = {Small Target,UAV Detection,Transfer Learning},
location = {<conf-loc>, <city>Tianjin</city>, <country>China</country>, </conf-loc>},
series = {ICMIP '23}
}

@inproceedings{10.1145/3458817.3476196,
author = {Wang, Yiduo and Li, Cheng and Shao, Xinyang and Chen, Youxu and Yan, Feng and Xu, Yinlong},
title = {Lunule: an agile and judicious metadata load balancer for CephFS},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476196},
doi = {10.1145/3458817.3476196},
abstract = {For a decade, the Ceph distributed file system (CephFS) has been widely used to serve the ever-growing big data in many key fields ranging from Internet services to AI computing. To scale out the massive metadata access, CephFS adopts a dynamic subtree partitioning method, splitting the hierarchical namespace and distributing subtrees across multiple metadata servers. However, this method suffers from a severe imbalance problem that may result in poor performance due to its inaccurate imbalance prediction, ignorance of workload characteristics, and unnecessary/invalid migration activities. To eliminate these inefficiencies, we propose Lunule, a novel CephFS metadata load balancer, which employs an imbalance factor model for accurately determining when to trigger re-balance and tolerate benign imbalanced situations. Lunule further adopts a workload-aware migration planner to appropriately select subtree migration candidates. Compared to baselines, Lunule achieves better load balance, increases the metadata throughput by up to 315.8%, and shortens the tail job completion time by up to 64.6% for five real-world workloads and their mixture, respectively. Besides, Lunule is capable of handling the metadata cluster expansion and the client workload growth, and scales linearly on a cluster of 16 MDSs.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {47},
numpages = {16},
location = {<conf-loc>, <city>St. Louis</city>, <state>Missouri</state>, </conf-loc>},
series = {SC '21}
}

@article{10.1109/TNET.2021.3119769,
author = {Li, Zhuo and Liu, Jindian and Yan, Liu and Zhang, Beichuan and Luo, Peng and Liu, Kaihua},
title = {Smart Name Lookup for NDN Forwarding Plane via Neural Networks},
year = {2021},
issue_date = {April 2022},
publisher = {IEEE Press},
volume = {30},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3119769},
doi = {10.1109/TNET.2021.3119769},
abstract = {Name lookup is a key technology for the forwarding plane of content router in Named Data Networking (NDN). To realize the efficient name lookup, what counts is deploying a high-performance index in content routers. So far, the proposed indexes have shown good performance, most of which are optimized for or evaluated with URLs collected from the current Internet, as the large-scale NDN names are not available yet. Unfortunately, the performance of these indexes is always impacted in terms of lookup speed, memory consumption and false positive probability, as the distributions of URLs retrieved in memory may differ from those of real NDN names independently generated by content-centric applications online. Focusing on this gap, a smart mapping model named Pyramid-NN via neural networks is proposed to build an index called LNI for NDN forwarding plane. Through learning the distributions of the names retrieved in the static memory, LNI that will be trained by real NDN names offline and preset in content routers in the future can not only reduce the memory consumption and the probability of false positive, but also ensure the performance of real NDN name lookup. Experimental results show that LNI-based FIB can reduce the memory consumption to 58.258 MB. Moreover, as it can be deployed on SRAMs, the throughput is about 177 MSPS, which well meets the current network requirement for fast packet processing.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {529–541},
numpages = {13}
}

@article{10.1145/3570726,
author = {Trevisan, Martino and Soro, Francesca and Mellia, Marco and Drago, Idilio and Morla, Ricardo},
title = {Attacking DoH and ECH: Does Server Name Encryption Protect Users’ Privacy?},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3570726},
doi = {10.1145/3570726},
abstract = {Privacy on the Internet has become a priority, and several efforts have been devoted to limit the leakage of personal information. Domain names, both in the TLS Client Hello and DNS traffic, are among the last pieces of information still visible to an observer in the network. The Encrypted Client Hello extension for TLS, DNS over HTTPS or over QUIC protocols aim to further increase network confidentiality by encrypting the domain names of the visited servers.In this article, we check whether an attacker able to passively observe the traffic of users could still recover the domain name of websites they visit even if names are encrypted. By relying on large-scale network traces, we show that simplistic features and off-the-shelf machine learning models are sufficient to achieve surprisingly high precision and recall when recovering encrypted domain names. We consider three attack scenarios, i.e., recovering the per-flow name, rebuilding the set of visited websites by a user, and checking which users visit a given target website. We next evaluate the efficacy of padding-based mitigation, finding that all three attacks are still effective, despite resources wasted with padding. We conclude that current proposals for domain encryption may produce a false sense of privacy, and more robust techniques should be envisioned to offer protection to end users.},
journal = {ACM Trans. Internet Technol.},
month = {feb},
articleno = {19},
numpages = {22},
keywords = {eavesdropping, network traffic, encryption, Privacy}
}

@proceedings{10.1145/3502181,
title = {HPDC '22: Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing},
year = {2022},
isbn = {9781450391993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the International Symposium on High-Performance Parallel and Distributed Computing (ACM HPDC 2022), the premier conference at the intersection of high performance and distributed computing, now in our 30th year. ACM HPDC has a focus on high-performance parallel and distributed computing topics over the years including platforms spanning clouds, clusters, grids, edge, big data, massively multicore, and extreme-scale computing systems. One of the unique features of HPDC is that it welcomes a blend of ideas ranging from applied research in the form of experience papers on operational deployments and applications, and more fundamental research in parallel and distributed techniques and systems. The conference has always appreciated the heroic work taken to deploy real systems and applications and the insights gained by live measurement and experimentation. The HPDC 2022 program is no exception with topics ranging from clouds, edge/IoT, big data ecosystems, to novel post-Moore computing technologies, to name a few. In addition to regular research papers, this year also saw the introduction of a new paper category on open-source tools and data papers, to encourage the inclusion of new methods and tools in the program.In addition to a strong technical program, the conference has three exciting keynote addresses to be delivered by Dr. Franck Cappello (Argonne National Labs), Dr. Sudhanva Gurumurthi (AMD) and Prof. Manish Parashar (University of Utah). HPDC 2022 achievement award was given to Dr. Franck Cappello for pioneering contributions in methods, tools, and testbeds for resilient high performance parallel and distributed computing. This year the conference will include seven workshops on cutting edge topics including edge and serverless computing, AI/HPC, and performance modeling and telemetry. The program will also include a poster session highlighting emerging HPDC research.},
location = {Minneapolis, MN, USA}
}

@inproceedings{10.1145/3478431.3499420,
author = {Vykopal, Jan and \v{S}v\'{a}bensk\'{y}, Valdemar and Seda, Pavel and \v{C}eleda, Pavel},
title = {Preventing Cheating in Hands-on Lab Assignments},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499420},
doi = {10.1145/3478431.3499420},
abstract = {Networking, operating systems, and cybersecurity skills are exercised best in an authentic environment. Students work with real systems and tools in a lab environment and complete assigned tasks. Since all students typically receive the same assignment, they can consult their approach and progress with an instructor, a tutoring system, or their peers. They may also search for information on the Internet. Having the same assignment for all students in class is standard practice efficient for learning and developing skills. However, it is prone to cheating when used in a summative assessment such as graded homework, a mid-term test, or a final exam. Students can easily share and submit correct answers without completing the assignment. In this paper, we discuss methods for automatic problem generation for hands-on tasks completed in a computer lab environment. Using this approach, each student receives personalized tasks. We developed software for generating and submitting these personalized tasks and conducted a case study. The software was used for creating and grading a homework assignment in an introductory security course enrolled by 207 students. The software revealed seven cases of suspicious submissions, which may constitute cheating. In addition, students and instructors welcomed the personalized assignments. Instructors commented that this approach scales well for large classes. Students rarely encountered issues while running their personalized lab environment. Finally, we have released the open-source software to enable other educators to use it in their courses and learning environments.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {78–84},
numpages = {7},
keywords = {summative assessment, operating systems, networking, homework, exercise, cybersecurity, case study, automatic problem generation},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{10.5555/3539845.3540069,
author = {Kaiser, M. and Griessl, R. and Kucza, N. and Haumann, C. and Tigges, L. and Mika, K. and Hagemeyer, J. and Porrmann, F. and R\"{u}ckert, U. and Berge, M. vor dem and Krupop, S. and Porrmann, M. and Tassemeier, M. and Trancoso, P. and Qararyah, F. and Zouzoula, S. and Casimiro, A. and Bessani, A. and Cecilio, J. and Andersson, S. and Brunnegard, O. and Eriksson, O. and Weiss, R. and Meierh\"{o}fer, F and Salomonsson, H. and Malekzadeh, E. and \"{O}dman, D. and Khurshid, A. and Felber, P. and Pasin, M. and Schiavoni, V. and M\'{e}n\'{e}trey, J. and Gugala, K. and Zierhoffer, P. and Knauss, E. and Heyn, H.},
title = {VEDLIoT: very efficient deep learning in IoT},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {The VEDLIoT project targets the development of energy-efficient Deep Learning for distributed AIoT applications. A holistic approach is used to optimize algorithms while also dealing with safety and security challenges. The approach is based on a modular and scalable cognitive IoT hardware platform. Using modular microserver technology enables the user to configure the hardware to satisfy a wide range of applications. VEDLIoT offers a complete design flow for Next-Generation IoT devices required for collaboratively solving complex Deep Learning applications across distributed systems. The methods are tested on various use-cases ranging from Smart Home to Automotive and Industrial IoT appliances. VEDLIoT is an H2020 EU project which started in November 2020. It is currently in an intermediate stage with the first results available.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {963–968},
numpages = {6},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3551349.3560436,
author = {Ling, Yuxi and Wang, Kailong and Bai, Guangdong and Wang, Haoyu and Dong, Jin Song},
title = {Are they Toeing the Line? Diagnosing Privacy Compliance Violations among Browser Extensions},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560436},
doi = {10.1145/3551349.3560436},
abstract = {Browser extensions have emerged as integrated characteristics in modern browsers, with the aim to boost the online browsing experience. Their advantageous position between a user and the Internet endows them with easy access to the user’s sensitive data, which has raised mounting privacy concerns from both legislators and extension users. In this work, we propose an end-to-end approach to automatically diagnosing the privacy compliance violations among extensions. It analyzes the compliance of privacy policy versus regulation requirements and their actual privacy-related practices during runtime. This approach can serve the extension users, developers and store operators as an efficient and practical detection mechanism for privacy compliance violations. Our approach utilizes the state-of-the-art language processing model BERT for annotating the policy texts, and a hybrid technique to analyze an extension’s source code and runtime behavior. To facilitate the model training, we construct a corpus named PrivAud-100 which contains 100 manually annotated privacy policies. Our large-scale diagnostic evaluation reveals that the vast majority of existing extensions suffer from privacy non-compliance issues. Around 92% of them have at least one violation of either their privacy policies or data collection practices. Based on our findings, we further propose an index to facilitate the filtering and identification of privacy-incompliant extensions with high accuracy&nbsp;(over 90%). Our work should raise the awareness of extension users, service providers, and platform operators, and encourage them to implement solutions toward better privacy compliance. To facilitate future research in this area, we have released our dataset, corpus and analyzer.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {10},
numpages = {12},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1145/3437075.3437084,
author = {Yaqiong, Chi and Zijie, Yang and Feng, Liu and Jiayin, Qi},
title = {Data Privacy Maturity Assessment Practice of Digital Transformation Enterprises under the COVID-19: Taking an Industrial Company in Xiamen as an Example},
year = {2021},
isbn = {9781450375061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437075.3437084},
doi = {10.1145/3437075.3437084},
abstract = {The Internet under the COVID-19 has brought more revolutionary changes to global trade and data flows. It tears apart national boundaries and further promotes exchanges and trade on a global scale. In this context, data privacy issues have been paid more attention, especially in the context of the epidemic situation, trade data privacy relying on digital transformation enterprises needs to be regulated and controlled. Taking an industrial company in Xiamen as an example, this article conducts online and offline research on companies through literature research and expert interviews, and refers to the Intel privacy maturity model, and constructs 12 indicators in three areas: platform risks, corporate behaviors, and external threats, so as to apply the data privacy maturity of an industrial enterprise in Xiamen. Empirical evidence shows that the maturity of data privacy of this enterprise is in the intermediate stage of institutionalization, and the overall maturity needs to be further developed in the field of adjusting enterprise behavior and preventing external threats. This evaluation system has good applicability to the data privacy issues of digital transformation companies under the epidemic, and can provide reference for the development of corporate data privacy maturity in the domestic manufacturing industry.},
booktitle = {Proceedings of the 2020 International Conference on Big Data in Management},
pages = {33–40},
numpages = {8},
keywords = {privacy threats, privacy maturity, digital transformation, data privacy},
location = {Manchester, United Kingdom},
series = {ICBDM 2020}
}

@inproceedings{10.1145/3396851.3402123,
author = {Hashmi, Md Umar and Bu\v{s}i\'{c}, Ana and Deka, Deepjyoti and Pereira, Lucas},
title = {Energy Storage Optimization for Grid Reliability},
year = {2020},
isbn = {9781450380096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396851.3402123},
doi = {10.1145/3396851.3402123},
abstract = {Large scale renewable energy source (RES) integration planned for multiple power grids around the world will require additional resources/reserves to achieve secure and stable grid operations to mitigate the inherent intermittency of RES. In this paper, we present formulations to understand the effect of fast storage reserves in improving grid reliability under different cost functions. Our formulations not only aim to minimize imbalance but also maintain state-of-charge (SoC) of storage. The proposed approaches rely on a macroscopic supply-demand model of the grid with total power output of energy storage as the control variable. We show that accounting for system response due to inertia and local governor response enables a more realistic quantification of storage requirements for damping net load fluctuations. Simulation case studies are embedded in the paper by using datasets from the Elia TSO in Belgium and BPA in the USA. The numerical results benchmark the marginal effect on reliability due to increasing storage size under different system responses and associated cost functions. Further we observe myopic control of batteries proposed approximates deterministic control of batteries for faster time scale reserve operation.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Future Energy Systems},
pages = {516–522},
numpages = {7},
keywords = {real-time operation, myopic algorithm, grid reliability, frequency response, Storage optimization, SAIDI, McCormick relaxation},
location = {Virtual Event, Australia},
series = {e-Energy '20}
}

@inproceedings{10.1145/3474085.3481025,
author = {Taraghi, Babak},
title = {End-to-end Quality of Experience Evaluation for HTTP Adaptive Streaming},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481025},
doi = {10.1145/3474085.3481025},
abstract = {Exponential growth in multimedia streaming traffic over the Internet motivates the research and further investigation of the user's perceived quality of such services. Enhancement of experienced quality by the users becomes more substantial when service providers compete on establishing superiority by gaining more subscribers or customers. Quality of Experience (QoE) enhancement would not be possible without an authentic and accurate assessment of the streaming sessions. HTTP Adaptive Streaming (HAS) is today's prevailing technique to deliver the highest possible audio and video content quality to the users. An end-to-end evaluation of QoE in HAS covers the precise measurement of the metrics that affect the perceived quality, eg. startup delay, stall events, and delivered media quality. Mentioned metrics improvements could limit the service's scalability, which is an important factor in real-world scenarios. In this study, we will investigate the stated metrics, best practices and evaluations methods, and available techniques with an aim to (i) design and develop practical and scalable measurement tools and prototypes, (ii) provide a better understanding of current technologies and techniques (eg. Adaptive Bitrate algorithms), (iii) conduct in-depth research on the significant metrics in a way that improvements of QoE with scalability in mind would be feasible, and finally (iv) provide a comprehensive QoE model which outperforms state-of-the-art models.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2936–2939},
numpages = {4},
keywords = {subjective evaluation, quality of experience, qoe model, objective evaluation, http adaptive streaming, adaptive bitrate},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3386762.3386773,
author = {Li, Yang and Qin, Zhaoming and Zhang, Fan and Qin, Yuchao and Hua, Haochen and Cao, Junwei},
title = {Distributed Power Dispatching Solution for A Future Economic and Environment-friendly Energy Internet},
year = {2020},
isbn = {9781450376891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386762.3386773},
doi = {10.1145/3386762.3386773},
abstract = {In recent years, the research of power dispatch has been popular in the field of energy Internet (EI) system operation and management. In this paper, a distributed algorithm considering multiple objectives is applied to solve power dispatching problem within the scenario of EI. The power dispatching problem is formulated as a convex optimization problem, and the distributed subgradient-push strategy is adopted in each agent. In this manner, the decentralized scheme can be more scalable for the power dispatching tasks in large EI systems. Case study is conducted to evaluate the feasibility of the proposed method.},
booktitle = {Proceedings of the 2020 The 9th International Conference on Informatics, Environment, Energy and Applications},
pages = {72–77},
numpages = {6},
keywords = {power dispatching, energy Internet, distributed algorithm, Convex optimization},
location = {Amsterdam, Netherlands},
series = {IEEA '20}
}

@inproceedings{10.1145/3503161.3547875,
author = {Hu, Mengshun and Jiang, Kui and Liao, Liang and Nie, Zhixiang and Xiao, Jing and Wang, Zheng},
title = {Progressive Spatial-temporal Collaborative Network for Video Frame Interpolation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547875},
doi = {10.1145/3503161.3547875},
abstract = {Most video frame interpolation (VFI) algorithms infer the intermediate frame with the help of adjacent frames through the cascaded motion estimation and content refinement.However, the intrinsic correlations between motion and content are barely investigated, commonly producing interpolated results with inconsistency and blurry contents.Specifically, we first discover a simple yet essential domain knowledge that contents and motions characteristics should be homogeneous to a certain degree from the same objects, and formulate the consistency into the loss function for model optimization. Based on this, we propose to learn the collaborative representation between motions and contents, and construct a novel progressive spatial-temporal Collaborative network (Prost-Net) for video frame interpolation.Specifically, we develop a content-guided motion module (CGMM) and a motion-guided content module (MGCM) for individual content and motion representation. In particular, the predicted motion in CGMM is used to guide the fusion and distillation of contents for intermediate frame interpolation, and vice versa. Furthermore, by considering collaborative strategy in a multi-scale framework, our Prost-Net progressively optimizes motions and contents in a coarse-to-fine manner, making it robust to various challenging scenarios (occlusion and large motions) in VFI. Extensive experiments on the benchmark datasets demonstrate that our method significantly outperforms state-of-the-art methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {2145–2153},
numpages = {9},
keywords = {video frame interpolation, multi-scale, motion-guided content, content-guided motion, collaborative network},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3397271.3401062,
author = {He, Guoxiu and Kang, Yangyang and Jiang, Zhuoren and Liu, Jiawei and Sun, Changlong and Liu, Xiaozhong and Lu, Wei},
title = {Creating a Children-Friendly Reading Environment via Joint Learning of Content and Human Attention},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401062},
doi = {10.1145/3397271.3401062},
abstract = {Technological advancements have led to increasing availability of erotic literature and pornography novels online, which can be alluring to adolescence and children. Unfortunately, because of the inherent complexity of these indecent contents and training data sparseness, it is a challenging task to detect these readings in the Cyberspace while children can easily access them. In this study, we propose a novel framework, Joint Learning of Content and Human Attention (GoodMan), to identify indecent readings by augmenting natural language understanding models with large scale human reading behaviors (dwell time per page) on portable devices. From the text modeling viewpoint, the innovative joint attention trained by joint learning is employed to orchestrate the content attention and human behavior attention via the BiGRU. From the data augmentation perspective, various users' reading behaviors on the same text can generate considerable training instances with joint attention, which can be effective to address the cold start problem. We conduct an extensive set of experiments on an online ebook dataset (with human reading behaviors on portable devices). The experimental results show insights into the task and demonstrate the superiority of the proposed model against alternative solutions.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {279–288},
numpages = {10},
keywords = {user modeling, reading behavior, neural networks, nature language understanding, long text},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3447548.3467146,
author = {Liu, Hao and Gao, Qian and Li, Jiang and Liao, Xiaochao and Xiong, Hao and Chen, Guangxing and Wang, Wenlin and Yang, Guobao and Zha, Zhiwei and Dong, Daxiang and Dou, Dejing and Xiong, Haoyi},
title = {JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467146},
doi = {10.1145/3447548.3467146},
abstract = {In modern internet industries, deep learning based recommender systems have became an indispensable building block for a wide spectrum of applications, such as search engine, news feed, and short video clips. However, it remains challenging to carry the well-trained deep models for online real-time inference serving, with respect to the time-varying web-scale traffics from billions of users, in a cost-effective manner. In this work, we present JIZHI - a Model-as-a-Service system - that per second handles hundreds of millions of online inference requests to huge deep models with more than trillions of sparse parameters, for over twenty real-time recommendation services at Baidu, Inc. In JIZHI, the inference workflow of every recommendation request is transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the pipeline refers to a staged computation or I/O intensive task processor. With traffics of real-time inference requests arrived, each modularized processor can be run in a fully asynchronized way and managed separately. Besides, JIZHI introduces the heterogeneous and hierarchical storage to further accelerate the online inference process by reducing unnecessary computations and potential data access latency induced by ultra-sparse model parameters. Moreover, an intelligent resource manager has been deployed to maximize the throughput of JIZHI over the shared infrastructure by searching the optimal resource allocation plan from historical logs and fine-tuning the load shedding policies over intermediate system feedback. Extensive experiments have been done to demonstrate the advantages of JIZHI from the perspectives of end-to-end service latency, system-wide throughput, and resource consumption. Since launched in July 2019, JIZHI has helped Baidu saved more than ten million US dollars in hardware and utility costs per year while handling 200% more traffics without sacrificing the inference efficiency.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3289–3298},
numpages = {10},
keywords = {recommendation system, online inference, intelligent resource management, MLOPS},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3460537.3460552,
author = {Du, Liang and Tao, Yuan and Chen, Tianmei and Wang, Qing and Lv, Hanyu},
title = {An Advanced PBFT-based Consensus Algorithm for a Bidding Consortium Blockchain},
year = {2021},
isbn = {9781450389624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460537.3460552},
doi = {10.1145/3460537.3460552},
abstract = {The rapid development of blockchain technology has given rise to many applications like digital currency. Building a consortium blockchain for a bidding system is a promising way to benefit the bidding businesses, by reducing cost and sharing data among tenderers and bidders. Generally, Practical Byzantine Fault Tolerance (PBFT) algorithm is the wide-ly-used consensus algorithm in a consortium blockchain. However, this algorithm meets its bottleneck, when there are a large number of nodes in the consortium blockchain net-work. In order to enhance the scalability of the consensus algorithm, this paper puts for-ward an advanced PBFT-based consensus algorithm, named ANPBFT, by taking ad-vantages of the message aggregation technology and tree topology technology. Specially, the consensus nodes in our proposed algorithm are divided into active nodes and passive nodes. Then, the consensus of block information is equivalent to the confirmation of iden-tity aggregation signature and aggregation promise of information key. The experimental results show that our proposed algorithm has better performance than the original PBFT algorithm. It also shows good scalability and can be used in a large-scale license-chain system.},
booktitle = {2021 The 3rd International Conference on Blockchain Technology},
pages = {176–182},
numpages = {7},
keywords = {Tree Topology, Practical Byzantine Fault Tolerance, Message Aggregation, Consortium Blockchain, Consensus algorithm},
location = {Shanghai, China},
series = {ICBCT '21}
}

@article{10.1145/3506716,
author = {Wei, Lili and Lang, Congyan and Liang, Liqian and Feng, Songhe and Wang, Tao and Chen, Shidi},
title = {Weakly Supervised Video Object Segmentation via Dual-attention Cross-branch Fusion},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3506716},
doi = {10.1145/3506716},
abstract = {Recently, concerning the challenge of collecting large-scale explicitly annotated videos, weakly supervised video object segmentation (WSVOS) using video tags has attracted much attention. Existing WSVOS approaches follow a general pipeline including two phases, i.e., a pseudo masks generation phase and a refinement phase. To explore the intrinsic property and correlation buried in the video frames, most of them focus on the later phase by introducing optical flow as temporal information to provide more supervision. However, these optical flow-based studies are greatly affected by illumination and distortion and lack consideration of the discriminative capacity of multi-level deep features. In this article, with the goal of capturing more effective temporal information and investigating a temporal information fusion strategy accordingly, we propose a unified WSVOS model by adopting a two-branch architecture with a multi-level cross-branch fusion strategy, named as dual-attention cross-branch fusion network (DACF-Net). Concretely, the two branches of DACF-Net, i.e., a temporal prediction subnetwork (TPN) and a spatial segmentation subnetwork (SSN), are used for extracting temporal information and generating predicted segmentation masks, respectively. To perform the cross-branch fusion between TPN and SSN, we propose a dual-attention fusion module that can be plugged into the SSN flexibly. We also pose a cross-frame coherence loss (CFCL) to achieve smooth segmentation results by exploiting the coherence of masks produced by TPN and SSN. Extensive experiments demonstrate the effectiveness of proposed approach compared with the state-of-the-arts on two challenging datasets, i.e., Davis-2016 and YouTube-Objects.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {mar},
articleno = {46},
numpages = {20},
keywords = {attention, temporal information, weakly supervised, video object segmentation}
}

@inproceedings{10.1145/3469096.3469867,
author = {Tabone, Andr\'{e} and Camilleri, Kenneth and Bonnici, Alexandra and Cristina, Stefania and Farrugia, Reuben and Borg, Mark},
title = {Pornographic content classification using deep-learning},
year = {2021},
isbn = {9781450385961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469096.3469867},
doi = {10.1145/3469096.3469867},
abstract = {Controlling the distribution of sensitive content such as pornography has become paramount with the ever-growing accessibility to the internet. Manual filtering of such large volumes of data is practically impossible, thus, the automatic detection of said material is sought after by Law Enforcement Agencies (LEAs) and has been tackled in various manners. However, the sorting of flagged pornographic documents is still done manually using scales that describe hierarchical degrees of content severity. In this paper, we address pornography detection by creating a model capable of locating and labelling sexual organs in images and extend this model to perform image classification to provide the user with one of 19 semantically meaningful descriptors of the content. Generating these descriptors serves as a proof of concept before approaching LEAs to work with illegal CSA material and scales such as COPINE. After creating our own custom sexual organ object detection dataset for the task at hand, we achieved an object detection mean average precision score of 63.63% and a top-3 classification accuracy of 87.78%.},
booktitle = {Proceedings of the 21st ACM Symposium on Document Engineering},
articleno = {15},
numpages = {10},
keywords = {pornography classification, object detection, deep learning},
location = {Limerick, Ireland},
series = {DocEng '21}
}

@inproceedings{10.1145/3474085.3481534,
author = {Liu, Hao and Li, Xin and Liu, Bing and Jiang, Deqiang and Liu, Yinsong and Ren, Bo and Ji, Rongrong},
title = {Show, Read and Reason: Table Structure Recognition with Flexible Context Aggregator},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3481534},
doi = {10.1145/3474085.3481534},
abstract = {We investigate the challenging problem of table structure recognition in this work. Many recent methods adopt graph-based context aggregator with strong inductive bias to reason sparse contextual relationships of table elements. However, the strong constraints may be too restrictive to represent the complicated table relationships. In order to learn more appropriate inductive bias from data, we try to introduce Transformer as context aggregator in this work. Nevertheless, Transformer taking dense context as input requires larger scale data and may suffer from unstable training procedure due to the weakening of inductive bias. To overcome the above limitations, we in this paper design a FLAG (FLexible context AGgregator), which marries Transformer with graph-based context aggregator in an adaptive way. Based on FLAG, an end-to-end framework requiring no extra meta-data or OCR information, termed FLAG-Net, is proposed to flexibly modulate the aggregation of dense context and sparse one for the relational reasoning of table elements. We investigate the modulation pattern in FLAG and show what contextual information is focused, which is vital for recognizing table structure. Extensive experimental results on benchmarks demonstrate the performance of our proposed FLAG-Net surpasses other compared methods by a large margin.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {1084–1092},
numpages = {9},
keywords = {table structure recognition, relational reasoning, neural network},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3458817.3476152,
author = {Pauloski, J. Gregory and Huang, Qi and Huang, Lei and Venkataraman, Shivaram and Chard, Kyle and Foster, Ian and Zhang, Zhao},
title = {KAISA: an adaptive second-order optimizer framework for deep neural networks},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476152},
doi = {10.1145/3458817.3476152},
abstract = {Kronecker-factored Approximate Curvature (K-FAC) has recently been shown to converge faster in deep neural network (DNN) training than stochastic gradient descent (SGD); however, K-FAC's larger memory footprint hinders its applicability to large models. We present KAISA, a K-FAC-enabled, Adaptable, Improved, and ScAlable second-order optimizer framework that adapts the memory footprint, communication, and computation given specific models and hardware to improve performance and increase scalability. We quantify the tradeoffs between memory and communication cost and evaluate KAISA on large models, including ResNet-50, Mask R-CNN, U-Net, and BERT, on up to 128 NVIDIA A100 GPUs. Compared to the original optimizers, KAISA converges 18.1--36.3% faster across applications with the same global batch size. Under a fixed memory budget, KAISA converges 32.5% and 41.6% faster in ResNet-50 and BERT-Large, respectively. KAISA can balance memory and communication to achieve scaling efficiency equal to or better than the baseline optimizers.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {13},
numpages = {14},
keywords = {second-order optimization, machine learning, distributed computing, data-parallel algorithms, K-FAC},
location = {<conf-loc>, <city>St. Louis</city>, <state>Missouri</state>, </conf-loc>},
series = {SC '21}
}

@inproceedings{10.1145/3425329.3425391,
author = {Jing-cong, Zhu and Xiao-guang, Zhu and Lei, Guan},
title = {Quantitative Analysis and Research on Emergency Linkage System Performance Based on Stochastic Petri Net},
year = {2020},
isbn = {9781450387873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425329.3425391},
doi = {10.1145/3425329.3425391},
abstract = {With the large-scale and centralized of chemical enterprises, more and more attention has been paid to the emergency linkage system of emergencies. Perfect emergency linkage can curb or delay the development of the situation. In order to quantitatively study the performance of emergency linkage system in chemical enterprises, a stochastic Petri net structural model was built based on emergency linkage process and elements. Then, the accessible set is analyzed, isomorphic transformation is carried out in combination with Markov chain and the performance of each link in the emergency linkage action has been analyzed and calculated. Finally, the average working time of the emergency linkage system in completing all scenarios has been obtained. The analysis of the calculation results can be used to optimize the emergency linkage system.},
booktitle = {Proceedings of the 2nd World Symposium on Software Engineering},
pages = {308–313},
numpages = {6},
keywords = {quantitative analysis, emergency linkage, SPN, Markov chain},
location = {Chengdu, China},
series = {WSSE '20}
}

@inproceedings{10.5555/3400397.3400519,
author = {Gehlot, Vijay},
title = {From Petri Nets to colored Petri Nets: a tutorial introduction to nets based formalism for modeling and simulation},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Petri Net, a widely studied mathematical formalism, is a graphical notation for modeling systems. Petri Nets provide the foundation for modeling concurrency, communication, synchronization, and resource sharing constraints that are inherent to many systems. However, Petri Nets do not scale well when it comes to modeling and simulating large systems. Colored Petri Nets (CPNs) extend Petri Nets with a high level programming language, making them more suitable for modeling large systems. The CPN language allows the creation of models as a set of modules in a hierarchical manner and permits both timed and untimed models. Untimed models are used to validate the logical correctness of a system, whereas timed models are used to evaluate performance. This tutorial introduces the reader to the vocabulary and constructs of both Petri Nets and CPNs and illustrates the use of CPN Tools in creating and simulating models by means of familiar simple examples.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1519–1533},
numpages = {15},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3569966.3570061,
author = {Ruan, Qinxiang and Jiang, Fuchun and Xiao, Min and Lin, Weiming and Weng, Weijie},
title = {A Object Detection Network for Remote Sensing Images Based on Global Multiscale Attention and Local Context Dynamic Interaction},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569966.3570061},
doi = {10.1145/3569966.3570061},
abstract = {Abstract: Remote sensing image (RSI) object detection has been applied in the fields of marine information mining, building change detection, and ship monitoring. However, the application of advanced general-purpose object detection of RSI usually has significant degradation of performance. This is mainly due to the special natures of RSI such as complex background interference, arbitrarily oriented objects, small objects, and multi-scale. For these issues, we propose a novel object detection network (GLC-Net) for RSI based on global multiscale attention and local context dynamic interaction. Specifically, a global multiscale attention module (GMA) is designed to mitigate background interference. By incorporating the attention mechanism in the multiscale feature map, the model can focus on remote sensing object features and improve the efficiency and accuracy of detection tasks. To deal with the problem of the arbitrary orientation of objects, the rotated region proposal network is utilized with reference to the centroid offset representation. For the lack of information on small objects and the cross-scale problem, Contextual Dynamic Interaction Detection Head (CDI Head) is designed for feature filtering and enhancement by extracting local contextual information and ROI features in a one-to-one dynamic interaction. Extensive experiments had been conducted on two public benchmark datasets. The experiments demonstrate that the proposed GLC-Net has superior performance in RSI object detection.},
booktitle = {Proceedings of the 5th International Conference on Computer Science and Software Engineering},
pages = {325–331},
numpages = {7},
keywords = {object detection, attention mechanism, Remote sensing image, Contextual Dynamic Interaction},
location = {<conf-loc>, <city>Guilin</city>, <country>China</country>, </conf-loc>},
series = {CSSE '22}
}

@inproceedings{10.1145/3490322.3490335,
author = {Xie, Jiagui and Li, Zhiping and Jin, Jian and Zhang, Bo and Hua, Yuqing},
title = {Research on Blockchain Storage Extension Based on DHT},
year = {2022},
isbn = {9781450385091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490322.3490335},
doi = {10.1145/3490322.3490335},
abstract = {With the rapid growth of blockchain data, the storage cost of maintaining the full node of a blockchain is getting higher and higher. This has led to the gradual weakening of the decentralization of the blockchain, increasing the risk of 51% attacks on the blockchain system. Based on DHT, we propose a blockchain storage expansion mechanism, using the Kademlia protocol to assign unique ID for every node in the blockchain network, and divide the entire node into several node clusters according to certain rules, each node cluster stores a complete blockchain data, and the nodes in the cluster only need to store a small part of the blockchain data. At the same time, in order to reduce the storage pressure of the nodes in the cluster, a dynamic reorganization mechanism is proposed, and the cluster scale will be dynamically adjusted according to the storage capacity of the nodes in the cluster. Finally, we established an experimental simulation model based on Colored Petri Nets, and verified the superiority of our proposed scheme by comparing it with the other two storage schemes.},
booktitle = {Proceedings of the 4th International Conference on Big Data Technologies},
pages = {79–85},
numpages = {7},
keywords = {Storage expansion, Simulation, Kademlia, DHT, Blockchain},
location = {Zibo, China},
series = {ICBDT '21}
}

@article{10.1145/3531012,
author = {Han, Haiyang and Alexoudi, Theoni and Vagionas, Chris and Pleros, Nikos and Hardavellas, Nikos},
title = {A Practical Shared Optical Cache With Hybrid MWSR/R-SWMR NoC for Multicore Processors},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3531012},
doi = {10.1145/3531012},
abstract = {Conventional electronic memory hierarchies are intrinsically limited in their ability to overcome the memory wall due to scaling constraints. Optical caches and interconnects can mitigate these constraints, and enable processors to reach performance and energy efficiency unattainable by purely electronic means. However, the promised benefits cannot be realized through a simple replacement process; to reach its full potential, the architecture needs to be holistically redesigned. This article proposes Pho$, an opto-electronic memory hierarchy architecture for multicores. Pho$ replaces conventional core-private electronic caches with a large shared optical L1 built with optical SRAMs. The shared optical cache is supported by Pho$Net, a novel hybrid MWSR/R-SWMR optical NoC that provides low-latency and high-bandwidth communication between the electronic cores and the shared optical L1 at low optical loss. Pho$Net’s unique network arbitration protocol seamlessly co-arbitrates the request and reply sub-networks and facilitates cache requests and replies that optimize for the common case of cache hits. Through Pho$ we solve the problems that render previous designs impractical. Our results show that Pho$ achieves on average 1.41\texttimes{} performance speedup (3.89\texttimes{} max) and 31% lower energy-delay product (90% max) against conventional designs. Moreover, the Pho$Net optical NoC for core-cache communication consumes 70% less power compared to directly applying previously proposed optical NoC architectures.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {oct},
articleno = {76},
numpages = {28},
keywords = {optical caches, energy efficiency, network-on-chip, cache hierarchy, Nanophotonic interconnects}
}

@inproceedings{10.1145/3474085.3475512,
author = {Li, Jizhizi and Ma, Sihan and Zhang, Jing and Tao, Dacheng},
title = {Privacy-Preserving Portrait Matting},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475512},
doi = {10.1145/3474085.3475512},
abstract = {Recently, there has been an increasing concern about the privacy issue raised by using personally identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable portrait images. To fill the gap, we present P3M-10k in this paper, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting. P3M-10k consists of 10,000 high-resolution face-blurred portrait images along with high-quality alpha mattes. We systematically evaluate both trimap-free and trimap-based matting methods on P3M-10k and find that existing matting methods show different generalization capabilities when following the Privacy-Preserving Training (PPT) setting, i.e., training on face-blurred images and testing on arbitrary images. To devise a better trimap-free portrait matting model, we propose P3M-Net, which leverages the power of a unified framework for both semantic perception and detail matting, and specifically emphasizes the interaction between them and the encoder to facilitate the matting process. Extensive experiments on P3M-10k demonstrate that P3M-Net outperforms the state-of-the-art methods in terms of both objective metrics and subjective visual quality. Besides, it shows good generalization capacity under the PPT setting, confirming the value of P3M-10k for facilitating future research and enabling potential real-world applications. The source code and dataset are available at https://github.com/JizhiziLi/P3M.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3501–3509},
numpages = {9},
keywords = {trimap, semantic segmentation, privacy-preserving, portrait matting, deep learning, benchmark},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3609437.3609452,
author = {Zhang, Yuqi and Ouyang, Lingzhi and Huang, Yu and Ma, Xiaoxing},
title = {Conflict-free Replicated Priority Queue: Design, Verification and Evaluation},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609452},
doi = {10.1145/3609437.3609452},
abstract = {Internet-scale distributed systems often rely on replication to achieve fault-tolerance and load distribution. To provide low latency and high availability, the systems are often required to accept updates on one replica immediately and then propagate the updates among replicas asynchronously. Conflict-free Replicated Data Type (CRDT) is a principled approach to addressing the challenge for these systems to resolve conflicts among concurrent updates. Although many CRDTs have been studied, little research has been done on Conflict-free Replicated Priority Queue (CRPQ), which is a collection of elements that focuses on maintaining element orderings based on their priority values, and can be used in many applications scenarios such as task scheduling and network routing. In this work, we discuss the design rationales of CRPQs and introduce two CRPQ designs: Add-Win CRPQ and Remove-Win CRPQ. The correctness of the designs is formally verified using TLA+. We also demonstrate the effectiveness of our designs by implementing them over Redis. Our evaluation shows that both CRPQs perform well in terms of data consistency and memory overhead.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {302–312},
numpages = {11},
keywords = {priority queue, model checking, TLA+, CRDT},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@article{10.1145/3645110,
author = {Banik, Debajyoty and Paul, Rahul and Rathore, Rajkumar Singh and Jhaveri, Rutvij H.},
title = {Improved Regression Analysis with Ensemble Pipeline Approach for Applications Across Multiple Domains},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3645110},
doi = {10.1145/3645110},
abstract = {In this research, we introduce two new machine learning regression methods: the Ensemble Average and the Pipelined Model. These methods aim to enhance traditional regression analysis for predictive tasks and have undergone thorough evaluation across three datasets: Kaggle House Price, Boston House Price, and California Housing, using various performance metrics. The results consistently show that our models outperform existing methods in terms of accuracy and reliability across all three datasets. The Pipelined Model, in particular, is notable for its ability to combine predictions from multiple models, leading to higher accuracy and impressive scalability. This scalability allows for their application in diverse fields like technology, finance, and healthcare. Furthermore, these models can be adapted for real-time and streaming data analysis, making them valuable for applications such as fraud detection, stock market prediction, and IoT sensor data analysis. Enhancements to the models also make them suitable for big data applications, ensuring their relevance for large datasets and distributed computing environments. It’s important to acknowledge some limitations of our models, including potential data biases, specific assumptions, increased complexity, and challenges related to interpretability when using them in practical scenarios. Nevertheless, these innovations advance predictive modeling, and our comprehensive evaluation underscores their potential to provide increased accuracy and reliability across a wide range of applications. The results indicate that the proposed models outperform existing models in terms of accuracy and robustness for all three datasets. The source code can be found at https://huggingface.co/DebajyotyBanik/Ensemble-Pipelined-Regression/tree/main.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {feb},
keywords = {Ensemble Average, Pipeline Scheme, Random Forest, Light Gradient Boost, XgBoost.}
}

@inproceedings{10.1145/3548606.3560582,
author = {Wyss, Marc and Giuliari, Giacomo and Mohler, Jonas and Perrig, Adrian},
title = {Protecting Critical Inter-Domain Communication through Flyover Reservations},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560582},
doi = {10.1145/3548606.3560582},
abstract = {To protect against naturally occurring or adversely induced congestion in the Internet, we propose the concept of flyover reservations, a fundamentally new approach for addressing the availability demands of critical low-volume applications. In contrast to path-based reservation systems, flyovers are fine-grained "hop-based" bandwidth reservations on the level of individual autonomous systems. We demonstrate the scalability of this approach experimentally through simulations on large graphs. Moreover, we bring the flyovers' potential to full fruition by introducing Helia, a protocol for secure flyover reservation setup and data transmission. We evaluate Helia's performance based on an implementation in DPDK, demonstrating authentication and forwarding of reservation traffic at 160 Gbps. Our security analysis shows that Helia can resist a large variety of powerful attacks against reservation admission and traffic forwarding. Despite its simplicity, Helia outperforms current state-of-the-art reservation systems in many key metrics.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2961–2974},
numpages = {14},
keywords = {denial-of-service defense, bandwidth reservations},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3376044.3376064,
author = {Huang, Ching-Wei and Chen, Yaw-Chung},
title = {ZeroCalo: A Lightweight Blockchain Based on DHT Network},
year = {2020},
isbn = {9781450377430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3376044.3376064},
doi = {10.1145/3376044.3376064},
abstract = {The blockchain technology introduced in Satoshi Nakamoto's paper "Bitcoin: A Peer-to-Peer Electronic Cash System" bring a realistic distributed way of transaction to this world. The alternative blockchain Ethereum improved some demerits of Bitcoin. However, the major problem still exists - hash-based proof-of-work requires heavy computing power and hurts the scalability. In this paper, we propose a lightweight blockchain called ZeroCalo. It constructs thousands of miner nodes into DHT (Distributed Hash Table) structure. Efficient message broadcast and key lookup are developed. With them, a consensus-based proof-of-work algorithm for distributed ledger is fulfilled. The evaluation shows that a transaction requires O(logN) of time and O(N) of bandwidth consumption. ZeroCalo consumes very little energy while providing good TPS. This property makes it possible to be deployed on IOT device network or low-end hosts in cloud environment.},
booktitle = {Proceedings of the 2019 2nd International Conference on Blockchain Technology and Applications},
pages = {38–42},
numpages = {5},
keywords = {Proof-of-work, Lightweight, IOT, Energy, Distributed ledger, DHT, Consensus, Cloud, Blockchain},
location = {Xi'an, China},
series = {ICBTA '19}
}

@inproceedings{10.1145/3386723.3387824,
author = {Ghani, Adnane and El haj, El Hassan Ibn and Hammouch, Ahmed and Chaoub, Abdelaali},
title = {Adaptation of quality for video streaming in a Bittorrent P2P IMS network},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387824},
doi = {10.1145/3386723.3387824},
abstract = {The internet multimedia subsystem (IMS) has been upgraded to support Peer-to-Peer (P2P) content distribution services. Bittorent can be used with IMS, it uses an scalable process of content distribution. BitTorrent is a P2P file-sharing application designed to distribute large files to a large user population efficiently. This is done by making use of the upload bandwidth of all nodes (called peers) downloading the file. The peer heterogeneity constraint imposes a challenge that is the guarantee of a certain level of Quality of Service (QoS) of the video for the different peers, and the transfer poses a challenge for maintaining quality of service (QoS) in IMS. In parallel of P2P development, recently, video coding standards have appeared, which are enabling the deployment of video content delivery over Internet in real-time with high QoS. In this article we have linked the Scalable Video Coding (SVC) quality adaptation model with the BitTorrent protocol and the IMS network, knowing that the file to be downloaded by the Bittorrent protocol is divided into several chunks, we exploited the chunks selection technique used by Bittorrent to link the SVC coding quality adaptation model with the bittorrent protocol and the P2P IMS network. We performed several simulations with the bittorent protocol to extract the following parameters(Start Time, first Chunk Time, last Chunk Time, Stop Time, dwonload Duration), we made a comparison between the P2P Bittorent protocol and the P2P Zeta protocol.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {5},
numpages = {8},
keywords = {peer to peer, Scalable video coding, Quality adaptation, NS2, NGN, Bittorent},
location = {Marrakech, Morocco},
series = {NISS '20}
}

@inproceedings{10.5555/3433701.3433782,
author = {Boixaderas, Isaac and Zivanovic, Darko and Mor\'{e}, Sergi and Bartolome, Javier and Vicente, David and Casas, Marc and Carpenter, Paul M. and Radojkovi\'{c}, Petar and Ayguad\'{e}, Eduard},
title = {Cost-aware prediction of uncorrected DRAM errors in the field},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {This paper presents and evaluates a method to predict DRAM uncorrected errors, a leading cause of hardware failures in large-scale HPC clusters. The method uses a random forest classifier, which was trained and evaluated using error logs from two years of production of the MareNostrum 3 supercomputer. By enabling the system to take measures to mitigate node failures, our method reduces lost compute time by up to 57%, a net saving of 21,000 node-hours per year. We release all source code as open source.We also discuss and clarify aspects of methodology that are essential for a DRAM prediction method to be useful in practice. We explain why standard evaluation metrics, such as precision and recall, are insufficient, and base the evaluation on a cost-benefit analysis. This methodology can help ensure that any DRAM error predictor is clear from training bias and has a clear cost-benefit calculation.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {61},
numpages = {15},
keywords = {random forest, memory system, machine learning, error prediction, cost-benefit analysis},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{10.1145/3447548.3467155,
author = {Doha, Rashed and Al Hasan, Mohammad and Anwar, Sohel and Rajendran, Veera},
title = {Deep Learning based Crop Row Detection with Online Domain Adaptation},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467155},
doi = {10.1145/3447548.3467155},
abstract = {Detecting crop rows from video frames in real time is a fundamental challenge in the field of precision agriculture. Deep learning based semantic segmentation method, namely U-net, although successful in many tasks related to precision agriculture, performs poorly for solving this task. The reasons include paucity of large scale labeled datasets in this domain, diversity in crops, and the diversity of appearance of the same crops at various stages of their growth. In this work, we discuss the development of a practical real-life crop row detection system in collaboration with an agricultural sprayer company. Our proposed method takes the output of semantic segmentation using U-net, and then apply a clustering based probabilistic temporal calibration which can adapt to different fields and crops without the need for retraining the network. Experimental results validate that our method can be used for both refining the results of the U-net to reduce errors and also for frame interpolation of the input video stream.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2773–2781},
numpages = {9},
keywords = {semantic segmentation, crop row detection},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3428757.3429119,
author = {Kosugi, Naoko and Sato, Shunsuke and Yoshiyama, Kenji and Noguchi, Dai and Yamanaka, Katsuo and Kazui, Hiroaki},
title = {Ninchisho Chienowa-net: A web system that calculates and publishes the probability of success of coping methods for behavioral and psychological symptoms of dementia},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429119},
doi = {10.1145/3428757.3429119},
abstract = {This paper introduces the newest features and analysis of access logs of Ninchisho Chienowa-net, a web system that publishes the probability that coping methods for various symptoms occurring in patients with dementia will "go well" (probability of success). With an aim of reducing the burden on dementia caregivers, Ninchisho Chienowa-net not only publishes the probability of success of coping methods and information on dementia care, but is also equipped with features to collect experiences of coping methods for specific symptoms and to support the discovery of various coping methods. Since its launch in 2015, the site has seen a steady increase in the number of registered users and amount of access. Analysis of the access logs of Ninchisho Chienowa-net carried out in this study revealed that the access rate for the list of individual care topics is high and that it is necessary to prepare for large-scale access in the future.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {383–389},
numpages = {7},
keywords = {Web system, Text data analysis, Probability of success, Dementia care, Coping method},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3456887.3457104,
author = {Hu, Fang},
title = {Bidirectional Promotion of Ideological and Political Education in Curriculum and Online Teaching—Taking Online Teaching of Mathematics as an Example},
year = {2021},
isbn = {9781450389969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456887.3457104},
doi = {10.1145/3456887.3457104},
abstract = {In consideration of the influence brought by COVID-19, online teaching has been implemented among all students at all grades throughout the whole China for the very first time, and consequently, many teachers focusing on teaching career and scientific research became “network anchors”. After all, each education fellow needs to face and solve the problem of how to integrate online teaching with ideological and political education in curriculum in depth in face of the new situation so that they could be promoted mutually. This paper elaborates the mutual promotion of ideological and political education in curriculum and online teaching as well as the relevant explorations on their further promotion in the post-epidemic period.As explicitly specified in the Guiding Opinions on the Organization and Management of Online Education in Universities and Colleges during the Epidemic Prevention and Control Period promulgated by the Ministry of Education of the People's Republic of China during the outbreak of COVID-19, universities and colleges should…carry out online teaching activities such as online teaching and online learning actively during the epidemic prevention and control period in order to achieve the purpose of “teaching and learning out of class” and guarantee both teaching progress and quality[1]. With online teaching implemented among all students at all grades throughout the whole China for the very first time, many teachers focusing on teaching career and scientific research became “anchors”. After all, each education fellow needs to face and solve the problem of how to integrate online teaching with ideological and political education in curriculum in depth in face of the new situation so that they could be promoted mutually.The ideological and political education in curriculum (hereinafter referred to as “the Education”) essentially means taking the guidance of ideal and belief, value idea and moral concept as the very important teaching contents and implementing the above in the teaching process of professional courses, besides imparting professional knowledge, so as to form the pattern of super ideological and political and big moral education together with ideological and political courses and finally character and civic virtue subtly[2]. According to the report on Make Overall Plans, Cope with Changes and Create a New Pattern—Win the Battle of Revitalizing Undergraduate Education Comprehensively made by Wu Yan, the director of the Department of Higher Education of the Ministry of Education in Xiamen University in August 2020, the essence of winning the battle lies in the Education. He affirmed the contributions that the Education had made to online teaching totally. By responding to the relevant deployment and arrangement of the Ministry of Education on “teaching and learning out of class” actively, our university was active in exploring online teaching based on platforms such as MOOC, Superstar Learning and Zhihuishu on February 17, 2020 and opened 11 mathematics courses, including the self-developed MOOC Advanced Mathematics IIA, Advanced Mathematics IIB, Advanced Mathematics IIC, Mathematical Statistics, Management Statistics, etc.Education essentially means enabling students to study and is aimed to improve their comprehensive quality. The ideological and political factors in course teaching are explored on the basis of realizing the teaching objective so as to educate people subtly. By taking the course Mathematical Statistics as an example, the course team introduced “collecting and sorting of statistical data” in virtue of the chart of changing trend of COVID-19 infection growth (Figure 1) so that students could fully understand the severity of the current epidemic, learn about the epidemic's development tread with fluctuating data in a scientific and reasonable manner and motivate their interests data analyzing, disposing and exploring. Students were also encouraged to adjust their study and life at home, eliminate their anxiety and panic, and enhance their confidence in winning the fight against the epidemic.The traditional “teaching + learning” mode can be changed into “learning + teaching” mode by integrating the Education into the mathematics course teaching and fully exerting the leading role of “ideological and political elements” to achieve students-based teaching and focus on three dimensions of students’ development and learning and learning effect [3] in order to center on students, motivate their endogenous power of learning, change their learning attitude from passively to initiatively, and liberate teachers from the operation and control work so that they could have more time focusing on teaching design and realization of teaching goal. Practice shows that in the “learning + teaching” mode, students become more active in class teaching and their satisfactory degree has increased to 92% from 85% at the very beginning. Therefore, students could obtain a higher sense of achievements.The ideological and political factors are all inclusive. The course team developed the Education by choosing the contents in such aspects as patriotism, cultural history, professional quality and value orientation based on teaching contents of mathematics courses and explored the class examination reform in depth. For example, Advanced Mathematics IIA is assessed in the form of Xuexitong process assessment (40%) + online final examination score (40%) + course summary assessment (20%). Students should be guided from the course learning objective so as to make transformation from learning mathematics to using it, from mastering knowledge point to highlighting the quality of mathematics learning, from doing mathematics questions to considering with it in order to foster students’ professional quality of concerning learning, in-depth thinking and lifelong learning.Lin Yifu, a famous economist and the professor at Peking University once said: “Such a positive and open measure not only helps China to shoulder the responsibilities of world's higher education but also means an initiative action for sharing the Chinese wisdom and enhancing China's speaking right and influence!” Director Wu Yan pointed out in the report that the core for winning the fight against the epidemic was online teaching. In the spring term of 2020, there were 1.0845 million online teachers from the universities nationwide. The online teaching reached 3.537 billion person-times and online courses were 17.1968 million course-times. Online teaching had the all-dimensional coverage of all regions. With unprecedented scale, range and degree, the large-scale online education practice exerts profound influence on the learning revolution and paradigmatic reform significance of China's and even the world's higher education. The international organizations such as UNESCO and the experts of different countries highly affirmed China's online teaching achievements during the epidemic period. Robert Paru, the Education Division Head of the Chinese Representative Office of the UNESCO said: “we really appreciate China's sharing with the whole world on its online teaching of higher education and wish we could have further cooperation.” Nicolas Dirks, the former president of the University of California, Berkeley said that “China's colleges and universities lead the education research field and play a leading role in providing high-quality online education for the students all over the world”. This is actually a typical and vivid case about the Education. Each student in each class could feel the strengths, wisdoms and plans that China offered in terms of the fight against the epidemic, education and teaching in the era of Internet + education. All these unprecedented actions not only become a significant event along the history but also mean a surprise to the whole world. With the action above, we have enhanced our national self-confidence and national pride. Every teacher and every student involving online teaching are proud of being a Chinese.Mathematics is a kind of very amazing subject. The basic ideas that the mathematics courses follow for solving problems are to establish a mathematical model against the actual problem on the basis of analyzing their quantity relations. Some special expressive symbols, which are written in streamline form, smooth and charming, are used to quantize the specific relations. For many actual issues such as the area of trapezoid with curve side, continuous compounding, volume of column with curved edge, irregular column, etc., the course team reflected the rules of graphics generation in the online teaching live streaming room directly and specifically in the form of animation by using the mathematics software. Students cannot help claiming the charm of mathematics! It is worth mentioning that the esthetics in mathematics courses is not confined to the charm of form, for the charm can also be reflected in the logic. Internal logic of the teaching contents is stressed in a majority of mathematics courses. For instance, in the course Advanced Mathematics, when introducing the contents of Chapter 1 Functions and Limits, the course team guided students to expand the studies on the limit issues in the elementary mathematics in the advanced mathematics to discuss the infinity so as to guide them study from the limit to “infinity”, “infinitely close” and “trend”. In addition, when introducing the concept of limit, the general function limit was expanded from the simplest and most direct sequence limit, which is just the general rules for cognitive development (from special to general). Similarly, after the limit of the simple basic function is introduced, four arithmetic operations and composite function—the most commonly used mathematic algorithm was used to expand the simple limit to the limit of a complicated function. Practice proves that the integration of such kind of ideological and political elements by the course team teachers in online teaching could fully exert students’ passion, guide them to think deeply and realize in-depth learning so as to experience the internal charm of mathematics further.For instance, when developing teaching design for the concept and nature of limit in the course Advanced Mathematics, the course team quoted the words of Zhuang Zi, a famous ancient Chinese philosopher by introducing one sentence contained in Zhuangzi – The World: “for a stick that is one foot (a Chinese measuring unit) long, if we use half of it every day, then the stick can be used permanently”. Through the above, students could know about the understandings and comprehension of Chinese ancient ideologists on limit idea and the wisdom of the ancient Chinese people and enhance sense of national pride. For instance, when developing teaching design for the data collection and sorting in the course Management Statistics, the course team introduced the establishment of China's meteorological observing networks that consists of over 40 meteorological stations and more than 100 rainfall measurement stations by Zhu Kezhen, the originator and founder of historical climatology in China. Zhu observed and recorded the climate and weather conditions every day, collected historical climatology data extensively and worked together with Yuan Minwei to compile the Phenology, the rich historical phonological data and the research results of which can be hardly found in the phonological works of other countries. By learning Zhu Kezhen's contributions in modern meteorological sciences and natural science history, students could be guided to feel the dedication, persistence and cautious and earnest science spirit of the statistics workers so as to build the correct world outlook, lookout on life and value.For instance, in the courses Mathematical Statistics and Management Statistics, the course team made data horizontal analysis and speed analysis by using the latest data about the fight against the epidemic. As for the horizontal analysis, it obtained the daily accumulative amount of COVID-19 suspected cases, confirmed cases, cured cases, death cases and its gradual (daily) increment; as for the speed analysis, it obtained the link relative ratio of suspended cases, confirmed cases, cured cases, death cases and the death rate. By analyzing the data on the fight against the epidemic, each teacher and student involving online teaching could experience the epidemic in person, and the determination and respectable actions of the medical workers from all part of the country and scientific researchers in aiding Hubei and Wuhan in face of the severe epidemic conditions. By combining these political and ideological elements and the cutting-edge knowledge of the basic mathematical principle organically, students could be guided to ponder study, growth, individual development and national development in depth.As the professional required course of students majoring in science, industry and management, mathematics courses provide them with the basic professional quality. By learning these courses, students could understand and grasp the basic concepts, theories, ideas and methods of calculus, ordinary differential equation (ODE), probability theory, linear algebra, mathematical statistics and management statistics so as to obtain solid mathematics foundation and favorable mathematic accomplishments. The combination of mathematic theory and major could lay a solid foundation on the professional knowledge learning, cultivate students’ capacity of analyzing and solving problems by mathematics ideas and methods and offer students solid mathematics foundation for their study in postgraduate period and further learning of relevant courses. Practice proves that by exploring the ideological and political elements in the mathematics courses fully and integrating them into online teaching organically, the course team could enhance the class timeliness effectively and students’ rate of head raising and class participation so as to make the course connect the ideological and political elements and the basic professional courses seamlessly.Shanghai took the initiative in exploring the reform of ideological and political education courses in colleges and universities and created the special course General Plan of a Great Country, which provides a model that can be referenced for boosting the reform of ideological and political education [4]. In the upcoming post-epidemic era, the Education will still be a systematic project for educating people in colleges and universities [5] and a required path for intensifying the political theoretical level and moral education of students in modern times. The course teaching integrated with the Education enables students to grasp the professional knowledge and skills but also cultivates their patriotism, professional quality and scientific exploration.},
booktitle = {2021 2nd International Conference on Computers, Information Processing and Advanced Education},
pages = {916–918},
numpages = {3},
location = {Ottawa, ON, Canada},
series = {CIPAE 2021}
}

@article{10.5555/3455716.3455789,
author = {Johndrow, James and Orenstein, Paulo and Bhattacharya, Anirban},
title = {Scalable approximate MCMC algorithms for the horseshoe prior},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have significantly improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove guarantees for the accuracy of the approximate algorithm, and show that gradually decreasing the approximation error as the chain extends results in an exact algorithm. The scalability of the algorithm is illustrated in simulations with problem size as large as N = 5,000 observations and p = 50,000 predictors, and an application to a genome-wide association study with N = 2,267 and p = 98,385. The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {73},
numpages = {61},
keywords = {shrinkage prior, perturbation theory, MCMC approximation, high dimensional, Bayesian}
}

@article{10.1145/3551647,
author = {Joshi, Amogh Manoj and Nayak, Deepak Ranjan and Das, Dibyasundar and Zhang, Yudong},
title = {LiMS-Net: A Lightweight Multi-Scale CNN for COVID-19 Detection from Chest CT Scans},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3551647},
doi = {10.1145/3551647},
abstract = {Recent years have witnessed a rise in employing deep learning methods, especially convolutional neural networks (CNNs) for detection of COVID-19 cases using chest CT scans. Most of the state-of-the-art models demand a huge amount of parameters which often suffer from overfitting in the presence of limited training samples such as chest CT data and thereby, reducing the detection performance. To handle these issues, in this paper, a lightweight multi-scale CNN called LiMS-Net is proposed. The LiMS-Net contains two feature learning blocks where, in each block, filters of different sizes are applied in parallel to derive multi-scale features from the suspicious regions and an additional filter is subsequently employed to capture discriminant features. The model has only 2.53M parameters and therefore, requires low computational cost and memory space when compared to pretrained CNN architectures. Comprehensive experiments are carried out using a publicly available COVID-19 CT dataset and the results demonstrate that the proposed model achieves higher performance than many pretrained CNN models and state-of-the-art methods even in the presence of limited CT data. Our model achieves an accuracy of 92.11% and an F1-score of 92.59% for detection of COVID-19 from CT scans. Further, the results on a relatively larger CT dataset indicate the effectiveness of the proposed model.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jan},
articleno = {5},
numpages = {17},
keywords = {LiMS-Net, chest CT scan, COVID-19, lightweight CNN, Deep learning}
}

@inproceedings{10.1145/3528535.3531517,
author = {Pfandzelter, Tobias and Bermbach, David},
title = {Celestial: Virtual Software System Testbeds for the LEO Edge},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528535.3531517},
doi = {10.1145/3528535.3531517},
abstract = {As private space companies such as SpaceX and Telesat are building large LEO satellite constellations to provide global broadband Internet access, researchers have proposed to embed compute services within satellite constellations to provide computing services on the *LEO edge*. While the LEO edge is merely theoretical at the moment, providers are expected to rapidly develop their satellite technologies to keep the upper hand in the new space race.In this paper, we answer the question of how researchers can explore the possibilities of LEO edge computing and evaluate arbitrary software systems in an accurate runtime environment and with cost-efficient scalability. To that end, we present Celestial, a virtual testbed for the LEO edge based on microVMs. Celestial can efficiently emulate individual satellites and their movement as well as ground station servers with realistic network conditions and in an application-agnostic manner, which we show empirically. Additionally, we explore opportunities and implications of deploying a real-time remote sensing application on LEO edge infrastructure in a case study on Celestial.},
booktitle = {Proceedings of the 23rd ACM/IFIP International Middleware Conference},
pages = {69–81},
numpages = {13},
keywords = {software testbeds, satellite networks, LEO edge},
location = {Quebec, QC, Canada},
series = {Middleware '22}
}

@inproceedings{10.1145/3394486.3403198,
author = {Wang, Rui and Kashinath, Karthik and Mustafa, Mustafa and Albert, Adrian and Yu, Rose},
title = {Towards Physics-informed Deep Learning for Turbulent Flow Prediction},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403198},
doi = {10.1145/3394486.3403198},
abstract = {While deep learning has shown tremendous success in a wide range of domains, it remains a grand challenge to incorporate physical principles in a systematic manner to the design, training, and inference of such models. In this paper, we aim to predict turbulent flow by learning its highly nonlinear dynamics from spatiotemporal velocity fields of large-scale fluid flow simulations of relevance to turbulence modeling and climate modeling. We adopt a hybrid approach by marrying two well-established turbulent flow simulation techniques with deep learning. Specifically, we introduce trainable spectral filters in a coupled model of Reynolds-averaged Navier-Stokes (RANS) and Large Eddy Simulation (LES), followed by a specialized U-net for prediction. Our approach, which we call Turbulent-Flow Net, is grounded in a principled physics model, yet offers the flexibility of learned representations. We compare our model with state-of-the-art baselines and observe significant reductions in error for predictions 60 frames ahead. Most importantly, our method predicts physical fields that obey desirable physical characteristics, such as conservation of mass, whilst faithfully emulating the turbulent kinetic energy field and spectrum, which are critical for accurate prediction of turbulent flows.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1457–1466},
numpages = {10},
keywords = {video forward prediction, turbulent flows, spatiotemporal forecasting, physics-informed machine learning, deep learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3574131.3574433,
author = {Ji, Daoming and You, Weikang and Chen, Yisong and Wang, Guoping and Li, Sheng},
title = {Semantic-assisted Unified Network for Feature Point Extraction and Matching},
year = {2023},
isbn = {9798400700316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574131.3574433},
doi = {10.1145/3574131.3574433},
abstract = {Feature point matching between two images is an essential part of 3D reconstruction, augmented reality, panorama stitching, etc. The quality of the initial feature point matching stage greatly affects the overall performance of a system. We present a unified feature point extraction-matching method, making use of semantic segmentation results to constrain feature point matching. To integrate high-level semantic information into feature points efficiently, we propose a unified feature point extraction and matching network, called SP-Net, which can detect feature points and generate feature descriptors simultaneously and perform feature point matching with accurate outcomes. Compared with previous works, our method can extract multi-scale context of the image, including shallow information and high-level semantic information of the local area, which is more stable when handling complex conditions such as changing illumination or large viewpoint. In evaluating the feature-matching benchmark, our method shows superior performance over the state-of-art method. As further validation, we propose SP-Net++ as an extension for 3D reconstruction. The experimental results show that our neural network can obtain accurate feature point positioning and robust feature matching to recover more cameras and get a well-shaped point cloud. Our semantic-assisted method can improve the stability of feature points as well as specific applicability for complex scenes.},
booktitle = {Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {12},
numpages = {9},
keywords = {semantic information, matching, extraction, deep learning, Feature point},
location = {Guangzhou, China},
series = {VRCAI '22}
}

@inproceedings{10.5555/3507788.3507843,
author = {Nash, Henry and Hickey, Martin},
title = {Building operators in the wild - going beyond just install and upgrade},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Building scalable, distributed cloud-native solutions results in many moving parts. While cloud-native development leads to more performant (and upgradeable-by-component) solutions, there are unique challenges related to deploying and maintaining such applications.For many years, the Kubernetes community focused on solving how to deploy a cloud-native application and upgrade it. A number of solutions exist, the most popular being Helm. At their core, solutions like Helm provide a common method for describing how to configure Kubernetes components for an application. Helm simplifies the install, upgrade, or delete operations of an applications. It is however not designed for managing the application instances running in the cluster. Operations outside of the basic scope might be application-specific, requiring specific application knowledge in order to be carried out reliably and safely. For example, how do you ensure caches are written through before maintenance operations happen? How might you circulate or synchronize security tokens when they change?Typically, a human operator needs to control application-specific operations (often called Day 2 operations), implementing a series of steps to perform an overall task. Enter the Kubernetes operator. A Kubernetes operator provides a standardized structure which incorporates the human operator's knowledge in an automated way that minimizes the need for real-time human support. It's often said that operators "automate activities for a human operator." In other words, minimize the "3 o'clock in the morning call" for the site reliability engineer!However, there are other potential ways of providing the same functionality. The first solution you might try is to script a series of steps that you can externally apply to your cluster. You could, indeed, a achieve some if not all of the tasks needed with such an approach. The downside to this is that many of the cloud-native patterns and approaches would end up being re-used and have to be re-coded in those external solutions. Plus, externally applied actions are harder to monitor.Operators offer a better option, baking in human knowledge that runs inside the cluster with access to all the internal cloud-native concepts and support that Kubernetes provides. Operators are application-specific extensions to Kubernetes, giving those applications access to concepts and support that is usually added by human operators.This operator concept was first launched by CoreOS for stateful applications like databases, caches, and monitoring systems. Kubernetes deals with the deployment of both stateful and stateless distributed applications. However, handling full-fledged scaling, upgrading, and reconfiguration of operations without losing data or availability is a bigger challenge. This requires application domain knowledge that is driven by the human operator.As with many things in life, there is more than one way to do something. This also applies to the automation and management of Kubernetes applications. The best way to understand how useful operators are is to see an example in action, and examine the pros and cons of the various ways the Day 2 functionality can be achieved.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {287–288},
numpages = {2},
keywords = {operators, operations, openshift, distributed computing, decentralized computing, day-2, cloud-native, Kubernetes, DevOps},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/3575813.3597355,
author = {Menon, Vishnu and Bichpuriya, Yogesh and Sarangan, Venkatesh and Rajagopal, Narayanan},
title = {A Best-effort Energy Storage as a Service Model for Supporting Renewable Generators in Day-ahead Electricity Markets},
year = {2023},
isbn = {9798400700323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575813.3597355},
doi = {10.1145/3575813.3597355},
abstract = {Net zero targets are encouraging higher adoption of Renewable Energy Generators (REGens). The volatile nature of these sources introduces challenges such as reliability of supply and grid stability. Energy storage systems (ESS) are viewed as a solution to address these challenges at both grid-scale renewable generation and smaller distributed generation. In this paper, we propose a model for an ESS to offer its storage to multiple, independently-managed, third-party REGens participating in the day-ahead electricity markets. In anticipation of the forecast errors from these disparate REGens, the ESS operator takes suitable counter-measures (charging/ discharging of the storage system through market transactions). This is done in a way to reduce the imbalance in the market commitments made by the individual REGens without reserving any storage volume for each REGen. For this service, the ESS gets paid from each of the REGens. We call this set-up as a “best-effort energy storage as a service (ESaaS)”. To the best of our knowledge, ours is one of the very few papers to discuss this set-up. We present strategies for pricing and operating such an ESaaS system. Empirical results using real world data indicate that the proposed set-up is beneficial for both REGens and ESS operators. It also reduces the total imbalance (of REGens and ESS) thus aiding the system operator as well.},
booktitle = {Proceedings of the 14th ACM International Conference on Future Energy Systems},
pages = {485–496},
numpages = {12},
keywords = {Renewables, Optimization, Imbalance, Energy storage, Electricity markets},
location = {Orlando, FL, USA},
series = {e-Energy '23}
}

@inproceedings{10.1145/3519935.3519967,
author = {Dadush, Daniel and Jiang, Haotian and Reis, Victor},
title = {A new framework for matrix discrepancy: partial coloring bounds via mirror descent},
year = {2022},
isbn = {9781450392648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519935.3519967},
doi = {10.1145/3519935.3519967},
abstract = {Motivated by the Matrix Spencer conjecture, we study the problem of finding signed sums of matrices with a small matrix norm. A well-known strategy to obtain these signs is to prove, given matrices A1, …, An ∈ ℝm \texttimes{} m, a Gaussian measure lower bound of 2−O(n) for a scaling of the discrepancy body {x ∈ ℝn: || ∑i=1n xi Ai|| ≤ 1}. We show this is equivalent to covering its polar with 2O(n) translates of the cube 1/n B∞n, and construct such a cover via mirror descent. As applications of our framework, we show:  Matrix Spencer for Low-Rank Matrices. If the matrices satisfy ||Ai||≤ 1 and (Ai) ≤ r, we can efficiently find a coloring x ∈ {± 1}n with discrepancy ||∑i=1n xi Ai ||≲ √n log(min(rm/n, r)). This improves upon the naive O(√n logr) bound for random coloring and proves the matrix Spencer conjecture when r m ≤ n.  Matrix Spencer for Block Diagonal Matrices. For block diagonal matrices with ||Ai||≤ 1 and block size h, we can efficiently find a coloring x ∈ {± 1}n with ||∑i=1n xi Ai ||≲ √n log(hm/n). This bound was previously shown in [Levy, Ramadas and Rothvoss, IPCO 2017] under the assumption h ≤ √n, which we remove. Using our proof, we reduce the matrix Spencer conjecture to the existence of a O(log(m/n)) quantum relative entropy net on the spectraplex.  Matrix Discrepancy for Schatten Norms. We generalize our discrepancy bound for matrix Spencer to Schatten norms 2 ≤ p ≤ q. Given ||Ai||Sp ≤ 1 and (Ai) ≤ r, we can efficiently find a partial coloring x ∈ [−1,1]n with |{i : |xi| = 1}| ≥ n/2 and ||∑i=1n xi Ai||Sq ≲ √n min(p, log(rk)) · k1/p−1/q, where k := min(1,m/n).  Our partial coloring bound is tight when m = Θ(√n). We also provide tight lower bounds of Ω(√n) for rank-1 matrix Spencer when m = n, and Ω(√min(m,n)) for S2 → S∞ discrepancy, precluding a matrix version of the Koml\'{o}s conjecture.},
booktitle = {Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing},
pages = {649–658},
numpages = {10},
keywords = {operator norm, mirror descent, matrix, discrepancy, covering number, Gaussian measure},
location = {Rome, Italy},
series = {STOC 2022}
}

@inproceedings{10.1145/3384441.3395988,
author = {Wu, Xiaoliang and Zhang, Bo and Jin, Dong},
title = {Parallel Simulation of Quantum Key Distribution Networks},
year = {2020},
isbn = {9781450375924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384441.3395988},
doi = {10.1145/3384441.3395988},
abstract = {With the significantly growing investment in quantum communi-cations, quantum key distribution (QKD), as a key application toshare a security key between two remote parties, has been deployedin urban areas and even at a continental scale. To meet the designrequirements of QKD on a quantum communication network, todayresearchers extensively conduct simulation-based evaluations in ad-dition to physical experiments for cost efficiency. A practical QKDsystem must be implemented on a large scale via a network, notjust between a few pairs of users. Existing discrete-event simulatorsoffer models for QKD hardware and protocols based on sequentialexecution. In this work, we investigate the parallel simulation ofQKD networks for scalability enhancement. Our contributions layin the exploration of QKD network characteristics to be leveragedfor parallel simulation. We also develop a parallel simulator forQKD networks with an optimized scheme for network partition.Experimental results show that to simulate a 64-node QKD net-work, our parallel simulator can complete the experiment 9 timesfaster than a sequential simulator running on the same machine.Our linear-regression-based network partition scheme can furtheraccelerate the simulation experiments up to two times than using arandomized network partition scheme.},
booktitle = {Proceedings of the 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {187–196},
numpages = {10},
keywords = {quantum key distribution, parallel discrete-event simulation, bb84},
location = {Miami, FL, Spain},
series = {SIGSIM-PADS '20}
}

@inproceedings{10.5555/3571885.3571947,
author = {Mu, Baorun and Soori, Saeed and Can, Bugra and G\"{u}rb\"{u}zbalaban, Mert and Dehnavi, Maryam Mehri},
title = {HyLo: a hybrid low-rank natural gradient descent method},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {This work presents a Hybrid Low-Rank Natural Gradient Descent method, called HyLo, that accelerates the training time of deep neural networks. Natural gradient descent (NGD) requires computing the inverse of the Fisher information matrix (FIM), which is typically expensive at large-scale. Kronecker factorization methods such as KFAC attempt to improve NGD's running time by approximating the FIM with Kronecker factors. However, the size of Kronecker factors increases quadratically as the model size grows. Instead, in HyLo, we use the Sherman-Morrison-Woodbury variant of NGD (SNGD) and propose a reformulation of SNGD to resolve its scalability issues. HyLo uses a computationally-efficient low-rank factorization to achieve superior timing for Fisher inverses. We evaluate HyLo on large models including ResNet-50, U-Net, and ResNet-32 on up to 64 GPUs. HyLo converges 1.4\texttimes{}-2.1\texttimes{} faster than the state-of-the-art distributed implementation of KFAC and reduces the computation and communication time up to 350\texttimes{} and 10.7\texttimes{} on ResNet-50.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {47},
numpages = {16},
keywords = {optimization, natural gradient descent, deep neural networks},
location = {Dallas, Texas},
series = {SC '22}
}

@inproceedings{10.1145/3459637.3481941,
author = {Sheng, Xiang-Rong and Zhao, Liqin and Zhou, Guorui and Ding, Xinyao and Dai, Binding and Luo, Qiang and Yang, Siran and Lv, Jingshan and Zhang, Chi and Deng, Hongbo and Zhu, Xiaoqiang},
title = {One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481941},
doi = {10.1145/3459637.3481941},
abstract = {Traditional industry recommendation systems usually use data in a single domain to train models and then serve the domain. However, a large-scale commercial platform often contains multiple domains, and its recommendation system often needs to make click-through rate (CTR) predictions for multiple domains. Generally, different domains may share some common user groups and items, and each domain may have its own unique user groups and items. Moreover, even the same user may have different behaviors in different domains. In order to leverage all the data from different domains, a single model can be trained to serve all domains. However, it is difficult for a single model to capture the characteristics of various domains and serve all domains well. On the other hand, training an individual model for each domain separately does not fully use the data from all domains. In this paper, we propose the Star Topology Adaptive Recommender (STAR) model to train a single model to serve all domains by leveraging data from all domains simultaneously, capturing the characteristics of each domain, and modeling the commonalities between different domains. Essentially, the net- work of each domain consists of two factorized networks: one centered network shared by all domains and the domain-specific network tailored for each domain. For each domain, we combine these two factorized networks and generate a unified network by element-wise multiplying the weights of the shared network and those of the domain-specific network, although these two factorized networks can be combined using other functions, which is open for further research. Most importantly, STAR can learn the shared network from all the data and adapt domain-specific parameters according to the characteristics of each domain. The experimental results from production data validate the superiority of the proposed STAR model. Since late 2020, STAR has been deployed in the display advertising system of Alibaba, obtaining 8.0% improvement on CTR and 6.0% increase on RPM (Revenue Per Mille).},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4104–4113},
numpages = {10},
keywords = {recommender system, multi-domain learning, display advertising},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3609437.3609464,
author = {Zhang, Yuting and Zhu, Jiahao and Yang, Yixin and Wen, Ming and Jin, Hai},
title = {Comparing the Performance of Different Code Representations for Learning-based Vulnerability Detection},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609464},
doi = {10.1145/3609437.3609464},
abstract = {Software vulnerabilities can cause severe security threats to cyberspace, and it is of significant importance to conduct automated vulnerability detection research. Considering that the source code contains rich syntax and semantic information, plenty of learning-based techniques and code representation methods have been proposed to detect vulnerabilities automatically. The most popular code representation methods include static code metrics, code token sequences and code graph structures. Although promising results have been reported by recent studies, there is an emerging urgent demand to understand which aspects contributed and affected most to the performance of learning-based techniques. To address this gap, the paper empirically evaluated and compared various learning-based vulnerability detection approaches, including five different code representations and thirteen different learning models. More importantly, the paper extended a large-scale dataset collected from open-source software systems, and the extensive evaluations have revealed novel and interesting findings that shed light on future research on learning-based vulnerability detection.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {174–184},
numpages = {11},
keywords = {deep learning, code representation, Vulnerability detection},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1145/3517208.3523756,
author = {Papadopoulos, Panagiotis and Pachilakis, Michalis and Chariton, Antonios A. and Markatos, Evangelos P.},
title = {OUTOPIA: private user discovery on the internet},
year = {2022},
isbn = {9781450392556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517208.3523756},
doi = {10.1145/3517208.3523756},
abstract = {Before being able to communicate with one another over the Internet, users in messaging applications need to discover each other and learn their IP addresses. Today, this User Discovery process is closely coupled with the communication provider. As a result, these providers are able to find (i) who is talking to whom, (ii) who is friends with whom and (iii) where is everybody located in the Internet address space at any time, even when there was no communication channel ever established, positioning this way themselves as powerful "Big Brothers".In this paper, we show that it is easy for friends to discover each other without the need of a centralised service provider that monitors each and every move they make. We propose OUTOPIA: a system to provide privacy-preserving User Discovery on the Internet. With OUTOPIA, users are able to discover each other, without revealing their social connections. We implemented a prototype of our approach and showed that it is inherently scalable, able to handle tens of thousands of users per server. Our preliminary performance results suggest that users are able to discover each other in no more than a few milliseconds, while generating negligible traffic overall.},
booktitle = {Proceedings of the 15th European Workshop on Systems Security},
pages = {8–14},
numpages = {7},
location = {Rennes, France},
series = {EuroSec '22}
}

@inproceedings{10.1145/3368555.3384447,
author = {Zhang, Ruru and He, Jiawen and Shi, Shenda and E, Haihong and Ou, Zhonghong and Song, Meina},
title = {BMM-Net: automatic segmentation of edema in optical coherence tomography based on boundary detection and multi-scale network},
year = {2020},
isbn = {9781450370462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368555.3384447},
doi = {10.1145/3368555.3384447},
abstract = {Retinal effusions and cysts caused by the leakage of damaged macular vessels and choroid neovascularization are symptoms of many ophthalmic diseases. Optical coherence tomography (OCT), which provides clear 10-layer cross-sectional images of the retina, is widely used to screen various ophthalmic diseases. A large number of researchers have carried out relevant studies on deep learning technology to realize the semantic segmentation of lesion areas, such as effusion on OCT images, and achieved good results. However, in this field, problems of the low contrast of the lesion area and unevenness of lesion size limit the accuracy of the deep learning semantic segmentation model. In this paper, we propose a boundary multi-scale multi-task OCT segmentation network (BMM-Net) for these two challenges to segment the retinal edema area, subretinal fluid, and pigment epithelial detachment in OCT images. We propose a boundary extraction module, a multi-scale information perception module, and a classification module to capture accurate position and semantic information and collaboratively extract meaningful features. We train and verify on the AI Challenger competition dataset. The average Dice coefficient of the three lesion areas is 3.058% higher than the most commonly used model in the field of medical image segmentation and reaches 0.8222.},
booktitle = {Proceedings of the ACM Conference on Health, Inference, and Learning},
pages = {51–59},
numpages = {9},
keywords = {semantic segmentation, optical coherence tomography, medical imaging analysis},
location = {Toronto, Ontario, Canada},
series = {CHIL '20}
}

@inproceedings{10.1145/3529372.3530916,
author = {Holzmann, Helge and Ruest, Nick and Bailey, Jefferson and Dempsey, Alex and Fritz, Samantha and Lee, Peggy and Milligan, Ian},
title = {ABCDEF: the 6 key features behind scalable, multi-tenant web archive processing with ARCH: archive, big data, concurrent, distributed, efficient, flexible},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3530916},
doi = {10.1145/3529372.3530916},
abstract = {Over the past quarter-century, web archive collection has emerged as a user-friendly process thanks to cloud-hosted solutions such as the Internet Archive's Archive-It subscription service. Despite advancements in collecting web archive content, no equivalent has been found by way of a user-friendly cloud-hosted analysis system. Web archive processing and research require significant hardware resources and cumbersome tools that interdisciplinary researchers find difficult to work with. In this paper, we identify six principles - the ABCDEFs (Archive, Big data, Concurrent, Distributed, Efficient, and Flexible) - used to guide the development and design of a system. These make the transformation of, and working with, web archive data as enjoyable as the collection process. We make these objectives - largely common sense - explicit and transparent in this paper. They can be employed by every computing platform in the area of digital libraries and archives and adapted by teams seeking to implement similar infrastructures. Furthermore, we present ARCH (Archives Research Compute Hub)1, the first cloud-based system designed from scratch to meet all of these six key principles. ARCH is an interactive interface, closely connected with Archive-It, engineered to provide analytical actions, specifically generating datasets and in-browser visualizations. It efficiently streamlines research workflows while eliminating the burden of computing requirements. Building off past work by both the Internet Archive (Archive-It Research Services) and the Archives Unleashed Project (the Archives Unleashed Cloud), this merged platform achieves a scalable processing pipeline for web archive research. It is open-source and can be considered a reference implementation of the ABCDEF, which we have evaluated and discussed in terms of feasibility and compliance as a benchmark for similar platforms.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {13},
numpages = {11},
keywords = {web archives, distributed computing, data processing, big data},
location = {Cologne, Germany},
series = {JCDL '22}
}

@inproceedings{10.1145/3469213.3471362,
author = {Zhou, Chen and Liu, Rao and Ba, Yu and Wang, Haixia and Ju, Rongbin and Song, Minggang and Zou, Nan and Li, Weidong},
title = {Study on the optimization of the day-ahead addition space for large-scale energy storage participation in auxiliary services},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3471362},
doi = {10.1145/3469213.3471362},
abstract = {Energy storage signs bilateral trading plans with new energy power plants according to the monthly forecast of new energy output, and reports adjustable space to the dispatch before the day. As for the large error in the monthly forecast of new energy output, energy storage can make a day-ahead forecast and report additional space according to the difference between it and the monthly forecast, i.e. report additional space, to improve the economic efficiency of energy storage power plants. We obtain the new energy next day output prediction results based on the scenario generation method. In order to maximize the net revenue of daily operation of energy storage, an optimization model of the storage day-ahead add-on space is established based on the comprehensive consideration of auxiliary service revenue, battery aging cost and penalty risk. The simulation of the proposed strategy based on the model shows that the optimization results obtained in this paper can make fuller use of large-scale energy storage resources and improve the economic efficiency of energy storage plants.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {331},
numpages = {6},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3573428.3573708,
author = {Wang, Yu and Liu, Mingsheng and Yi, Hu and Xie, Yunchi and Tan, Zhengyu and Ji, Cunyu},
title = {High-density Image Object Counting Network Based on Spatial Context and Channel Attention: Take crowds and vehicles in a statistically dense scene as an example},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573708},
doi = {10.1145/3573428.3573708},
abstract = {Accurate object counting is a challenging task in image analysis, low-density image object counting can usually be achieved by object detection algorithms, and high-density object counting still has limited counting accuracy. We propose a high-density image object counting network based on spatial context and channel attention, abbreviated as HIOC-Net, which divides low-level features into multiple blocks of different scales through a spatial context-aware module to extract rich contextual features, and then uses the channel attention-aware module to process the interdependence of feature information in the channel dimension, so that the model can focus on useful features, suppressing the irrelevant background. This paper conducts extensive experiments on large-scale crowd and vehicle counting datasets, including ShanghaiTech, UCF_CC_50, WorldExpo'10, TRANCOS, and HBR_YD datasets. The results show that our method not only surpasses many state-of-the-art methods in counting accuracy but also achieves competitive localization accuracy, resulting in high-quality object density maps.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1592–1596},
numpages = {5},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3494885.3494947,
author = {Li, Linrun and Qin, Zhangjian and Zhang, Qin},
title = {Landslide Recognition Based on the Improved U-net},
year = {2021},
isbn = {9781450390675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494885.3494947},
doi = {10.1145/3494885.3494947},
abstract = {Landslide is one of the most frequent geological disasters in the world, which causes a huge loss every year and seriously threatens the safety of people's life and property. Rapid and accurate landslide recognition can reduce losses and improve the efficiency of disaster prevention and mitigation. The traditional landslide recognition methods require a large amount of human and financial resources, which is extremely inefficient. In order to improve the recognition efficiency, this paper works out an improved U-net structure for automatic recognition of landslides. Parts of the feature extraction network of the original U-net structure are optimized into Resnet50 in the improved U-net structure, and an attention mechanism is added to enhance its ability to extract features, which significantly improves the accuracy of landslide recognition. The method in this paper is tested and verified on a large-scale landslide event in western Sichuan, China. The experimental results prove that the improved U-net structure increases the mIoU indicator to 80.66% and the mPA to 92.14%. Compared with the traditional method, the improved U-net model is able to recognize the landslide areas quickly and accurately, which proves its effectiveness and feasibility.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Software Engineering},
pages = {338–345},
numpages = {8},
keywords = {residual network, remote sensing, landslide recognition, deep learning, attention mechanism, U-net},
location = {Singapore, Singapore},
series = {CSSE '21}
}

@inproceedings{10.1145/3490354.3494378,
author = {Altman, Erik},
title = {Synthesizing credit card transactions},
year = {2022},
isbn = {9781450391481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490354.3494378},
doi = {10.1145/3490354.3494378},
abstract = {As noted by Turing Laureates Geoffrey Hinton and Yan LeCun [16], two elements have been essential to AI's recent boom: (1) deep neural nets and the theory and practice behind them; and (2) cloud computing with its abundant labeled data and large computing resources.Abundant labeled data is available for key domains such as images, speech, natural language processing, and recommendation engines. However, in many other domains such data is not available, or access is highly restricted for privacy reasons, as with health and financial data. Even when abundant data is available, it is often not labeled. Doing such labeling is labor-intensive and non-scalable.To get around these data problems there have been many proposals to generate synthetic data [20, 24, 29, 30, 35, 39]. However, to the best of our knowledge, key domains still lack labeled data or have at most toy data; or the synthetic data must have access to real data from which it can mimic new data. Looking to some of the challenges outlined in [3] at ICAIF'2020, this paper outlines work to generate realistic synthetic data without those restrictions and for an important domain: credit card transactions - including both normal and fraudulent transactions.At first glance it may appear simple to generate such transactions - just formalize a few items of the nature, "Sally sold slacks to Sue on Sunday." However, there are many patterns and correlations in real purchases. And there are millions of merchants and innumerable locations. And those merchants offer a wide variety of goods. Determining who shops where and when becomes daunting. Challenging also is the question of how much people pay. Inserting fraudulent transactions in the mix and doing all of these things with no real seed data provide final challenges.Generating good data to overcome these obstacles benefits from a mixture of technical approaches and domain knowledge. Those domains of knowledge include mechanics of credit card processing as well as a broad set of consumer domains, from electronics to clothing to hair styling to home improvement and many more. We also find that creation of a virtual world depicting people's commercial lives facilitates generation of high-quality, realistic data. This paper outlines some of our key techniques and provides evidence that the data generated is indeed realistic via comparisons to Federal Reserve data, recent data from a major card issuer, and more. At the end of the paper we also provide a link to a public sample of our data [2].Although beyond the scope of this paper, our synthetic credit-card data also facilitates development and training of models to predict fraud. Those models coupled with the synthetic dataset also provide foundations for designing acceleration hardware, just as GPUs, TPUs [10, 19] and other devices have been used for domains such as image classification, object detection, natural language processing, etc.},
booktitle = {Proceedings of the Second ACM International Conference on AI in Finance},
articleno = {13},
numpages = {9},
keywords = {virtual world, synthetic data, simulation, credit cards, agent-based},
location = {Virtual Event},
series = {ICAIF '21}
}

@inproceedings{10.1145/3375998.3376040,
author = {Deng, Deyu and Mai, Jiaxing Jason and Leung, Carson K. and Cuzzocrea, Alfredo},
title = {Cognitive-Based Hybrid Collaborative Filtering with Rating Scaling on Entropy to Defend Shilling Influence},
year = {2020},
isbn = {9781450377027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375998.3376040},
doi = {10.1145/3375998.3376040},
abstract = {In the current era of big data, huge volumes a wide variety of valuable data are generated and collected at a high velocity. Hence, data science solutions are in demand to data mine these big data for valuable information and useful knowledge embedded in these big data in order to transform this information and knowledge into recommendations and actions. In particular, recommendation systems (RecSys or RS)---which are tools that can provide suggestions to users based on various metrics---have been playing an important role in society since the booming of the Internet. Making more accurate predictions can both potentially increase company revenue and enhance user experience. So, it has been a hot topic. More specifically, collaborative filtering (CF) has been a popular technique applied in RS. The key ideas behind most of the CF algorithms are to filter items based on other users' opinions. Since the recommendation process is based on user interactions, one of the challenges is how to prevent shilling attacks (or shilling attack ratings). In this paper, we propose methods to integrate users' rating entropy into collaborative filtering so as to defend shilling attacks and reduce noisy ratings, and thus achieve higher prediction accuracy. Evaluation results show the effectiveness of our cognitive-based hybrid collaborative filtering methods in rating scaling on entropy for defending shilling influence.},
booktitle = {Proceedings of the 2019 8th International Conference on Networks, Communication and Computing},
pages = {176–185},
numpages = {10},
keywords = {shilling, recommendation systems, information retrieval, information noise reduction, databases, data science, data mining, data analytics, collaborative filtering},
location = {Luoyang, China},
series = {ICNCC '19}
}

@article{10.1109/TCBB.2021.3100893,
author = {Ko, Young-Joon and Kim, Sangsoo and Pan, Cheol-Ho and Park, Keunwan},
title = {Identification of Functional Microbial Modules Through Network-Based Analysis of Meta-Microbial Features Using Matrix Factorization},
year = {2021},
issue_date = {Sept.-Oct. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3100893},
doi = {10.1109/TCBB.2021.3100893},
abstract = {As the microbiome is composed of a variety of microbial interactions, it is imperative in microbiome research to identify a microbial sub-community that collectively conducts a specific function. However, current methodologies have been highly limited to analyzing conditional abundance changes of individual microorganisms without considering group-wise collective microbial features. To overcome this limitation, we developed a network-based method using nonnegative matrix factorization (NMF) to identify functional meta-microbial features (MMFs) that, as a group, better discriminate specific environmental conditions of samples using microbiome data. As proof of concept, large-scale human microbiome data collected from different body sites were used to identify body site-specific MMFs by applying NMF. The statistical test for MMFs led us to identify highly discriminative MMFs on sample classes, called synergistic MMFs (SYMMFs). Finally, we constructed a SYMMF-based microbial interaction network (SYMMF-net) by integrating all of the SYMMF information. Network analysis revealed core microbial modules closely related to critical sample properties. Similar results were also found when the method was applied to various disease-associated microbiome data. The developed method interprets high-dimensional microbiome data by identifying functional microbial modules on sample properties and intuitively representing their systematic relationships via a microbial network.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {2851–2862},
numpages = {12}
}

@inproceedings{10.1145/3503161.3547841,
author = {Tan, Yi and Hao, Yanbin and Zhang, Hao and Wang, Shuo and He, Xiangnan},
title = {Hierarchical Hourglass Convolutional Network for Efficient Video Classification},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547841},
doi = {10.1145/3503161.3547841},
abstract = {Videos naturally contain dynamic variation over the temporal axis, which will result in the same visual clues (e.g., semantics, objects) changing their scale, position, and perspective patterns between adjacent frames. A primary trend in video CNN is adopting spatial-2D convolution for spatial semantics and temporal-1D convolution for temporal dynamics. Though the direction achieves a favorable balance between efficiency and efficacy, it suffers from misalignment of visual clues with large displacements. Particularly, rigid temporal convolution would fail to capture correct motions when a specific target moves out of the reception field of temporal convolution between adjacent frames.To tackle large visual displacements between temporal neighbors, we propose a new temporal convolution namedHourglass Convolution (HgC). The temporal reception field of HgC has an hourglass shape, where the spatial reception field is enlarged in prior &amp; post temporal frames, enabling an ability to capture large displacement. Moreover, since videos contain long, short-term movements viewed from multiple temporal interval levels, we hierarchically organize the HgC net to both capture temporal dynamics from frame (short-term) and clip (long-term) levels. Besides, we also adopt strategies, such as low-resolution for short-term modeling and channel reduction for long-term modeling, from efficiency concerns. With HgC, our H$^2$CN equips off-the-shelf CNNs with a strong ability in capturing spatio-temporal dynamics at a neglectable computation overhead. We validate the efficiency and efficacy of HgC on standard action recognition benchmarks, including Something-Something V1&amp;V2, Diving48, and EGTEA Gaze+. We also analyse the complementarity of frame-level motion and clip-level motion with visualizations. The code and models will be available at https://github.com/ty-97/H2CN.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5880–5891},
numpages = {12},
keywords = {video classification, neural network, convolution, attention},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@inproceedings{10.1145/3523286.3524510,
author = {Zhou, Kun and Wei, Lianghao},
title = {Grading Prediction of Kidney Renal Clear Cell Carcinoma by Deep Learning},
year = {2022},
isbn = {9781450395755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523286.3524510},
doi = {10.1145/3523286.3524510},
abstract = {The grade of cancer is a way to classify cancer based on certain characteristics of cancer tissue. It is an important issue for the precise diagnosis, treatment, and mechanistic research of cancer. With the rapid development of genome sequencing technology, it has become possible to obtain large amounts of gene expression data, and large-scale genomic data to predict the grade of cancer is a challenging problem. In this study, we used gene expression data to propose a pathway-related deep neural network (K-Net) for predicting the grade of Kidney renal clear cell carcinoma (KIRC) tissues. K-Net provides the capability of model interpretability that most conventional fully-connected neural networks lack, describing which pathways play an important role in the process of predicting grade. The predictive performance of K-Net was evaluated with multiple cross-validation experiments. The K-Net prediction accuracy of 74%. More meaningfully, in contrast to using genes as features, this new classification model using enriched pathways as features can well explain which pathways play an important role in KIRC tissues from highly differentiated to poorly differentiated. Cancer development is a process of degradation of certain functions and enhancement of certain functions of tumor tissue, and understanding which pathways play an important role in cancer development can help explore research directions in cancer treatment.},
booktitle = {2022 2nd International Conference on Bioinformatics and Intelligent Computing},
pages = {37–42},
numpages = {6},
keywords = {transcriptomic data, pathway, deep Learning, cancer grading},
location = {<conf-loc>, <city>Harbin</city>, <country>China</country>, </conf-loc>},
series = {BIC 2022}
}

@inproceedings{10.1145/3640912.3640945,
author = {Li, Binyong and Fang, Jie and Deng, Xianhui and Sheng, Yuhan},
title = {Petri net-based illegal information flow detection method for node-chain access control model},
year = {2024},
isbn = {9798400716683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640912.3640945},
doi = {10.1145/3640912.3640945},
abstract = {To address the problem of illegal information flow in the node chain access control model constructed by the existing attribute-based access control (ABAC) model when applied to a large-scale node chain scenario with more than 100 nodes, an illegal information flow detection method is proposed for the node chain access control model based on the classical Petri net. The method introduces the reachable identity and reachable identity graph of the classical single-Token Petri net, which can analyze the illegal information flow in the ABAC-based node-chain access control model conveniently and visually. Meanwhile, it can effectively use the reachable identity state in the Petri net to detect the illegal information flow in the model. The case analysis shows that the method can effectively detect the illegal information flow appearing in the model.},
booktitle = {Proceedings of the 2023 International Conference on Communication Network and Machine Learning},
pages = {165–169},
numpages = {5},
location = {<conf-loc>, <city>Zhengzhou</city>, <country>China</country>, </conf-loc>},
series = {CNML '23}
}

@inproceedings{10.1145/3511808.3557151,
author = {Xia, Deguo and Liu, Xiyan and Zhang, Wei and Zhao, Hui and Li, Chengzhou and Zhang, Weiming and Huang, Jizhou and Wang, Haifeng},
title = {DuTraffic: Live Traffic Condition Prediction with Trajectory Data and Street Views at Baidu Maps},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557151},
doi = {10.1145/3511808.3557151},
abstract = {The task of live traffic condition prediction, which aims at predicting live traffic conditions (i.e., fast, slow, and congested) based on traffic information on roads, plays a vital role in intelligent transportation systems, such as navigation, route planning, and ride-hailing services. Existing solutions have adopted aggregated trajectory data to generate traffic estimates, which inevitably suffer from GPS drift caused by cluttered urban road scenarios. In addition, the trajectory information alone is insufficient to provide evidence for sudden traffic situations and perception of street-wise elements. To alleviate these problems, in this paper, we present DuTraffic, which is a robust and production-ready solution for live traffic condition prediction by taking both trajectory data and street views into account. Specifically, the vision-based detection and segmentation modules are developed to forecast traffic flow by using street views. Then, we propose a spatial-temporal-based module, TRST-Net, to learn the latent trajectory representation. Finally, a bilinear model is introduced to mix these two representations and then predicts live traffic conditions with trajectory data and street views in a mutually complementary manner. The task is recast as a multi-task learning problem, which could benefit from the strong representation of latent space manifold modeling. Extensive experiments conducted on large-scale, real-world datasets from Baidu Maps demonstrate the superiority and effectiveness of DuTraffic. In addition, DuTraffic has already been deployed in production at Baidu Maps since December 2020, handling tens of millions of requests every day. This demonstrates that DuTraffic is a practical and robust industrial solution for live traffic condition prediction.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {3575–3583},
numpages = {9},
keywords = {transportation, traffic vision, traffic prediction, spatial-temporal, real-time traffic, live traffic condition prediction, baidu maps},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3425577.3425582,
author = {Hai, Li and Guo, HaoZhou},
title = {Face Detection with Improved Face R-CNN Training Method},
year = {2021},
isbn = {9781450388023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425577.3425582},
doi = {10.1145/3425577.3425582},
abstract = {Faster R-CNN is a popular method in object detection applications. Based on Faster R-CNN, this paper proposes a facial feature enhancement method similar to the attention mechanism, and gets an improved model Face R-CNN. Face R-CNN combines with the syntax-guided network (SG-NET) to merge the generated image with the original convolutional features, which enhances the focus on the face area in the feature map and can effectively achieve face detection in the case of large-scale occlusion .Face R-CNN is trained and tested on the Wider Face dataset. Through the analysis of the experimental results, it can be concluded that Face R-CNN has a more obvious detection effect on occluding faces, and its average accuracy rate is 3.5% higher than Faster R-CNN, which can effectively improve the detection accuracy of occlusion faces.},
booktitle = {Proceedings of the 3rd International Conference on Control and Computer Vision},
pages = {22–25},
numpages = {4},
keywords = {SG-NET, Faster R-CNN, Face R-CNN, Face Detection},
location = {Macau, China},
series = {ICCCV '20}
}

@inproceedings{10.1145/3489517.3530566,
author = {Wang, Huimin and Tong, Xingyu and Ma, Chenyue and Shi, Runming and Chen, Jianli and Wang, Kun and Yu, Jun and Chang, Yao-Wen},
title = {CNN-inspired analytical global placement for large-scale heterogeneous FPGAs},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530566},
doi = {10.1145/3489517.3530566},
abstract = {The fast-growing capacity and complexity are challenging for FPGA global placement. Besides, while many recent studies have focused on the eDensity-based placement as its great efficiency and quality, they suffer from redundant frequency translation. This paper presents a CNN-inspired analytical placement algorithm to effectively handle the redundant frequency translation problem for large-scale FPGAs. Specifically, we compute the density penalty by a fully-connected propagation and gradient to a discrete differential convolution backward. With the FPGA heterogeneity, vectorization plays a vital role in self-adjusting the density penalty factor and the learning rate. In addition, a pseudo net model is used to further optimize the site constraints by establishing connections between blocks and their nearest available regions. Finally, we formulate a refined objective function and a degree-specific gradient preconditioning to achieve a robust, high-quality solution. Experimental results show that our algorithm achieves an 8% reduction on HPWL and 15% less global placement runtime on average over leading commercial tools.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {637–642},
numpages = {6},
location = {San Francisco, California},
series = {DAC '22}
}

@inproceedings{10.1145/3412815.3416891,
author = {Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
title = {Toward Communication Efficient Adaptive Gradient Method},
year = {2020},
isbn = {9781450381031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412815.3416891},
doi = {10.1145/3412815.3416891},
abstract = {In recent years, distributed optimization is proven to be an effective approach to accelerate training of large scale machine learning models such as deep neural networks. With the increasing computation power of GPUs, the bottleneck of training speed in distributed training is gradually shifting from computation to communication. Meanwhile, in the hope of training machine learning models on mobile devices, a new distributed training paradigm called "federated learning'' has become popular. The communication time in federated learning is especially important due to the low bandwidth of mobile devices. While various approaches to improve the communication efficiency have been proposed for federated learning, most of them are designed with SGD as the prototype training algorithm. While adaptive gradient methods have been proven effective for training neural nets, the study of adaptive gradient methods in federated learning is scarce. In this paper, we propose an adaptive gradient method that can guarantee both the convergence and the communication efficiency for federated learning.},
booktitle = {Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
pages = {119–128},
numpages = {10},
keywords = {federated learning, convergence analysis, adaptive method},
location = {Virtual Event, USA},
series = {FODS '20}
}

@inproceedings{10.1145/3604078.3604099,
author = {Wang, Jiaxin and Yu, Jianqiao and Ma, Xiaohong and Sun, Yi and Liu, Jia},
title = {GAUNet: Gated Attention U-Net for Medical Image Segmentation},
year = {2023},
isbn = {9798400708237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604078.3604099},
doi = {10.1145/3604078.3604099},
abstract = {Despite of their robust power on modeling global dependency, Transformer-based methods for medical segmentation usually depend on complex computation and large-scale pre-training. We aim to design a simple approach without complex computation and pre-training. Gated attention units (GAU) with a single head self-attention performs well for modeling global context features, thus we apply GAU to extract semantic information but it still lacks of localization due to insufficient local details. In this paper, we propose a novel GAU-based hybrid cascaded U-shape GAUNet for medical segmentation method. To compensate the shortcoming of GAU, we design a GAU-Conv module for encoder-decoder to extract global context dependency and reinforce localization. To further improve the performance, we redesign U-Net skip connection with a ReLU to strengthen the detailed localization when decodering. GAUNet achieves the state of the art performance on Synapse multi-organ and cardiac datasets without any pre-trained model.},
booktitle = {Proceedings of the 15th International Conference on Digital Image Processing},
articleno = {21},
numpages = {6},
keywords = {UNet, Self-attention, Medical image segmentation, Gated attention units (GAU)},
location = {<conf-loc>, <city>Nanjing</city>, <country>China</country>, </conf-loc>},
series = {ICDIP '23}
}

@inproceedings{10.1145/3632314.3632315,
author = {Che, Liming and Zhang, Jun},
title = {Parametric Design of Flexible Belt Conveyor Components Based on Solidworks},
year = {2023},
isbn = {9798400709401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632314.3632315},
doi = {10.1145/3632314.3632315},
abstract = {The coal resource is an important energy source in China, because there is still a certain gap between the large-scale fully mechanized mining equipment in China and foreign countries, and so is the belt conveyor. Under the leadership of the national double carbon goals, in order to improve the modeling efficiency of the belt conveyor and effectively and quickly model the components of various models and parameters, the design of the hydraulic tension belt conveyor in the middle is based on VB, with the design manual of DT Ⅱ (A) belt conveyor as a reference Net language and Solidworks carry out secondary development to realize parametric design. The middle hydraulic tension belt conveyor is designed, and the connection with Solidworks is established through Microsoft Visual Studio programming platform. The operation code is obtained through macro recording operation, and the Solidworks Api help query code and parameter meaning are re developed. Finally, a secondary development application that can automatically model and design through input parameters will be formed, which will ultimately improve the design efficiency and facilitate the subsequent design of different parameters.},
booktitle = {Proceedings of the 2023 International Conference on Intelligent Sensing and Industrial Automation},
articleno = {1},
numpages = {6},
keywords = {Secondary Development, Parametric Design, Belt Conveyor},
location = {<conf-loc>, <city>Virtual Event</city>, <country>China</country>, </conf-loc>},
series = {ISIA '23}
}

@inproceedings{10.5555/3437539.3437719,
author = {Wen, Hsiang-Ting and Cai, Yu-Jie and Hsu, Yang and Chang, Yao-Wen},
title = {Via-based redistribution layer routing for InFO packages with irregular pad structures},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {The integrated fan-out (InFO) wafer-level chip-scale package is introduced for modern system-in-package designs with larger I/O counts and higher interconnection density. A redistribution layer (RDL) in an InFO package is an extra metal layer for inter-chip connections. To achieve flexible and compact inter-chip connections, the RDL routing problem for InFO packages has become a crucial problem for modern electronic designs. In advanced high-density InFO packages, multiple RDLs with flexible vias are often adopted. On the other hand, to integrate chips of different technology nodes into one package, irregular pad structures need to be considered. To our best knowledge, however, there is no published work for RDL routing considering flexible vias or irregular pad structures. In this paper, we present the first work to handle the routing problem with pre-assigned pad pairs (i.e., the hardest pre-assignment routing problem) on the via-based multi-chip multi-layer InFO package with irregular pad structures. We first propose a layer assignment method based on a weighted maximum planar subset of chords algorithm to concurrently route as many inter-chip nets as possible. We then propose an octagonal tile model with a layout partitioning method to tackle increasingly popular irregular structures. Finally, we develop an efficient linear-programming-based layout optimization algorithm to find solutions with high-quality wirelength and via arrangements. Experimental results demonstrate the effectiveness and robustness of our algorithm.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {180},
numpages = {6},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.1109/TNET.2022.3155110,
author = {Xue, Kaiping and He, Peixuan and Yang, Jiayu and Xia, Qiudong and Wei, David S. L.},
title = {SCD2: Secure Content Delivery and Deduplication With Multiple Content Providers in Information Centric Networking},
year = {2022},
issue_date = {Aug. 2022},
publisher = {IEEE Press},
volume = {30},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3155110},
doi = {10.1109/TNET.2022.3155110},
abstract = {As one of the promising next generation network architectures, information centric networking (ICN) is highly anticipated to improve the bandwidth usage of the Internet and reduce duplicate traffic. Since contents in ICN are disseminated in the whole network, ICN is much more vulnerable and the issue of how to deliver contents securely has been intensively discussed. However, the scalability of the existing schemes is limited. A scalable scheme is expected to be able to achieve fine-grained access control and at the same time also support multiple content providers scenario with efficient key management at user side. Besides, different content providers may publish some identical contents and these contents may be cached in the same intermediate routers, which causes high data redundancy and in turn exerts an adverse impact on the performance of ICN. In this paper, we propose a Secure Content Delivery and Deduplication scheme, called SCD2, to achieve secure and efficient fine-grained access control in ICN with multiple content providers. We first propose a scalable key-policy attribute-based encryption (SKP-ABE) to provide fine-grained access control and allow different attribute authorities to share some public attributes to simplify the key management. Furthermore, based on SKP-ABE, we design a simple but effective mechanism to conduct content deduplication. Finally, we implement a prototype of SCD2 to test its performance and compare it with some existing schemes. The results show that SCD2 has lower storage overhead, a higher degree of deduplication, and better retrieval efficiency.},
journal = {IEEE/ACM Trans. Netw.},
month = {mar},
pages = {1849–1864},
numpages = {16}
}

@inproceedings{10.1145/3394171.3413502,
author = {Fu, Yuqian and Zhang, Li and Wang, Junke and Fu, Yanwei and Jiang, Yu-Gang},
title = {Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413502},
doi = {10.1145/3394171.3413502},
abstract = {Humans can easily recognize actions with only a few examples given, while the existing video recognition models still heavily rely on the large-scale labeled data inputs. This observation has motivated an increasing interest in few-shot video action recognition, which aims at learning new actions with only very few labeled samples. In this paper, we propose a depth guided Adaptive Meta-Fusion Network for few-shot video recognition which is termed as AMeFu-Net. Concretely, we tackle the few-shot recognition problem from three aspects: firstly, we alleviate this extremely data-scarce problem by introducing depth information as a carrier of the scene, which will bring extra visual information to our model; secondly, we fuse the representation of original RGB clips with multiple non-strictly corresponding depth clips sampled by our temporal asynchronization augmentation mechanism, which synthesizes new instances at feature-level; thirdly, a novel Depth Guided Adaptive Instance Normalization (DGAdaIN) fusion module is proposed to fuse the two-stream modalities efficiently. Additionally, to better mimic the few-shot recognition process, our model is trained in the meta-learning way. Extensive experiments on several action recognition benchmarks demonstrate the effectiveness of our model.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1142–1151},
numpages = {10},
keywords = {video recognition, multi-modality fusion, meta-learning, few-shot learning, data augmentation, adaptive instance normalization},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3613330.3613342,
author = {Ma, Tingjun and Zhou, Changyin},
title = {Selection of regularization model for linear regression under high-dimensional data},
year = {2023},
isbn = {9798400707520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613330.3613342},
doi = {10.1145/3613330.3613342},
abstract = {The data collected in current practical applications in various fields is gradually developing towards the direction of ultra-high-dimensional and large-scale, and a considerable portion of traditional analysis methods significantly reduce the processing efficiency of high-dimensional data. Therefore, it is essential to establish methods that focus on processing high dimensional data. In this paper, the Elastic-net model is selected as the basic regularization model for processing high-dimensional sparse data, and a penalty factor is added to enhance its ability to retain key features. To reduce the computational burden brought by high-dimensional data, we propose applying the "two-step" procedure of SSR+PCD screening rule and fitting method to the model containing penalty factors. In terms of the selection of tuning parameters, the traditional Cross-validation is replaced by Information Criterion, and the application of Information Criterion is extended to the regularization model with screening rules, so as to broaden the application range of Information Criterion. Through data simulation studies, we confirm the rationality of penalty factor addition and the ability of the selected Information Criterion to choose tuning parameters under this model, and an example is given to illustrate its application in processing high-dimensional gene expression data.},
booktitle = {Proceedings of the 2023 7th International Conference on Deep Learning Technologies},
pages = {91–97},
numpages = {7},
keywords = {Regularization, Information Criterion, High-dimensional data, Feature Selection, Elastic-net model},
location = {<conf-loc>, <city>Dalian</city>, <country>China</country>, </conf-loc>},
series = {ICDLT '23}
}

@article{10.1145/3580872,
author = {Deng, Kaikai and Zhao, Dong and Han, Qiaoyue and Zhang, Zihan and Wang, Shuyue and Zhou, Anfu and Ma, Huadong},
title = {Midas: Generating mmWave Radar Data from Videos for Training Pervasive and Privacy-preserving Human Sensing Tasks},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3580872},
doi = {10.1145/3580872},
abstract = {Millimeter wave radar is a promising sensing modality for enabling pervasive and privacy-preserving human sensing. However, the lack of large-scale radar datasets limits the potential of training deep learning models to achieve generalization and robustness. To close this gap, we resort to designing a software pipeline that leverages wealthy video repositories to generate synthetic radar data, but it confronts key challenges including i) multipath reflection and attenuation of radar signals among multiple humans, ii) unconvertible generated data leading to poor generality for various applications, and iii) the class-imbalance issue of videos leading to low model stability. To this end, we design Midas to generate realistic, convertible radar data from videos via two components: (i) a data generation network (DG-Net) combines several key modules, depth prediction, human mesh fitting and multi-human reflection model, to simulate the multipath reflection and attenuation of radar signals to output convertible coarse radar data, followed by a Transformer model to generate realistic radar data; (ii) a variant Siamese network (VS-Net) selects key video clips to eliminate data redundancy for addressing the class-imbalance issue. We implement and evaluate Midas with video data from various external data sources and real-world radar data, demonstrating its great advantages over the state-of-the-art approach for both activity recognition and object detection tasks.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {9},
numpages = {26},
keywords = {radar sensing, human activity recognition, data generation, cross domain translation}
}

@inproceedings{10.1145/3581807.3581888,
author = {Li, Weijie and Wei, Pingsun and Sun, Jun and Xiao, Xiaoting and Mu, Xiaomin and Hu, Zhenghui},
title = {LEPD-Net: A Lightweight Efficient Network with Pyramid Dilated Convolution for Seed Sorting},
year = {2023},
isbn = {9781450397056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581807.3581888},
doi = {10.1145/3581807.3581888},
abstract = {To achieve long-term economic growth, competitiveness, and sustainability, speed and accuracy are the key requirements when it comes to seed purity sorting. However, current seed sorting methods suffer from large number of model parameters and computational complexity, make it a great challenge to deploy them in real-time applications, especially on devices with limited resources. To issue above problems, in this paper, a lightweight efficient network with pyramid dilated convolution, namely LEPD-Net, is proposed for seed sorting. First, a residual spatial pyramid module (RSPM) is elaborately designed, which uses dilated convolution with different dilation rates to enlarge the structural characteristics of the receptive field and effectively extracts multi-scale features. Then the depth-wise separable convolution to reduce the amount of model parameters and the computational complexity. In addition, to further improve the performance, a novel lightweight coordinate attention module is introduced, which uses the local cross-channel interaction to obtain the attention value of each channel and strengthen the network's ability to learn seed key features. Finally, the seed sorting task is completed through the learned features. Experimental results show that our proposed method achieves an accuracy of 96.00% and 97.25% on the Maize dataset and Red Kidney Bean dataset, respectively. The number of parameters is only 0.26M, which is far less than state-of-the-art networks (e.g., MobileNetv2, Shufflenetv2, and PPLC-Net).},
booktitle = {Proceedings of the 2022 11th International Conference on Computing and Pattern Recognition},
pages = {551–558},
numpages = {8},
location = {Beijing, China},
series = {ICCPR '22}
}

@inproceedings{10.1145/3394171.3413776,
author = {Zhao, Sicheng and Li, Yaxian and Yao, Xingxu and Nie, Weizhi and Xu, Pengfei and Yang, Jufeng and Keutzer, Kurt},
title = {Emotion-Based End-to-End Matching Between Image and Music in Valence-Arousal Space},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413776},
doi = {10.1145/3394171.3413776},
abstract = {Both images and music can convey rich semantics and are widely used to induce specific emotions. Matching images and music with similar emotions might help to make emotion perceptions more vivid and stronger. Existing emotion-based image and music matching methods either employ limited categorical emotion states which cannot well reflect the complexity and subtlety of emotions, or train the matching model using an impractical multi-stage pipeline. In this paper, we study end-to-end matching between image and music based on emotions in the continuous valence-arousal (VA) space. First, we construct a large-scale dataset, termed Image-Music-Emotion-Matching-Net (IMEMNet), with over 140K image-music pairs. Second, we propose cross-modal deep continuous metric learning (CDCML) to learn a shared latent embedding space which preserves the cross-modal similarity relationship in the continuous matching space. Finally, we refine the embedding space by further preserving the single-modal emotion relationship in the VA spaces of both images and music. The metric learning in the embedding space and task regression in the label space are jointly optimized for both cross-modal matching and single-modal VA prediction. The extensive experiments conducted on IMEMNet demonstrate the superiority of CDCML for emotion-based image and music matching as compared to the state-of-the-art approaches.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2945–2954},
numpages = {10},
keywords = {valence-arousal space, emotion matching, deep metric learning, affective computing},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1109/ISCA52012.2021.00078,
author = {Olgun, Ataberk and Patel, Minesh and Ya\u{g}lik\c{c}i, A. Giray and Luo, Haocong and Kim, Jeremie S. and Bostanci, F. Nisa and Vijaykumar, Nandita and Ergin, O\u{g}uz and Mutlu, Onur},
title = {QUAC-TRNG: high-throughput true random number generation using quadruple row activation in commodity DRAM chips},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00078},
doi = {10.1109/ISCA52012.2021.00078},
abstract = {True random number generators (TRNG) sample random physical processes to create large amounts of random numbers for various use cases, including security-critical cryptographic primitives, scientific simulations, machine learning applications, and even recreational entertainment. Unfortunately, not every computing system is equipped with dedicated TRNG hardware, limiting the application space and security guarantees for such systems. To open the application space and enable security guarantees for the overwhelming majority of computing systems that do not necessarily have dedicated TRNG hardware (e.g., processing-in-memory systems), we develop QUAC-TRNG, a new high-throughput TRNG that can be fully implemented in commodity DRAM chips, which are key components in most modern systems.QUAC-TRNG exploits the new observation that a carefully-engineered sequence of DRAM commands activates four consecutive DRAM rows in rapid succession. This QUadruple ACtivation (QUAC) causes the bitline sense amplifiers to non-deterministically converge to random values when we activate four rows that store conflicting data because the net deviation in bitline voltage fails to meet reliable sensing margins.We experimentally demonstrate that QUAC reliably generates random values across 136 commodity DDR4 DRAM chips from one major DRAM manufacturer. We describe how to develop an effective TRNG (QUAC-TRNG) based on QUAC. We evaluate the quality of our TRNG using the commonly-used NIST statistical test suite for randomness and find that QUAC-TRNG successfully passes each test. Our experimental evaluations show that QUAC-TRNG reliably generates true random numbers with a throughput of 3.44 Gb/s (per DRAM channel), outperforming the state-of-the-art DRAM-based TRNG by 15.08X and 1.41X for basic and throughput-optimized versions, respectively. We show that QUAC-TRNG utilizes DRAM bandwidth better than the state-of-the-art, achieving up to 2.03X the throughput of a throughput-optimized baseline when scaling bus frequencies to 12 GT/s.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {944–957},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/DS-RT52167.2021.9576140,
author = {Cicirelli, Franco and Nigro, Libero},
title = {Parallel simulation of stochastic reward nets using theatre},
year = {2022},
isbn = {9781665433266},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DS-RT52167.2021.9576140},
doi = {10.1109/DS-RT52167.2021.9576140},
abstract = {This work aims at the development of tools for supporting modelling and analysis of timed systems by Stochastic Reward Nets (SRN). In a first approach it was proposed and experimented a formal reduction of SRN over Timed Automata (TA) in the context of the Uppaal popular toolbox. The reduction has the merit to allow both exhaustive model checking of an SRN model, useful for the assessment of qualitative properties (e.g., absence of deadlocks, occurrence of particular event sequences etc.), and quantitative analysis through the statistical model checker, which is based on simulations. However, although Uppaal enabled formal reasoning on the semantics of SRN, its practical usage suffers of scalability problems, that is it can introduce severe limitations in time and space when studying complex models. To cope with this problem, this paper describes a Java implementation of the SRN operational core engine, using the lock-free and efficient Theatre actor system which permits the parallel simulation of large models. The realization can be used for functional property checking on an untimed version of a source SRN model, and quantitative estimation of measurables through simulations. The paper discusses the design and implementation of the core engine of SRN on top of Theatre, together with supported intuitive configuration process of an SRN model, and reports some experimental results using a scalable grid computing model. The experiments confirm Theatre/SRN are capable of exploiting the potential of modern multi-core machines and can deliver good execution performances on large models.},
booktitle = {Proceedings of the 2021 IEEE/ACM 25th International Symposium on Distributed Simulation and Real Time Applications},
articleno = {13},
numpages = {8},
keywords = {theatre, stochastic reward nets, performability analysis, high-performance computing, actors, Java},
location = {Valencia, Spain},
series = {DS-RT '21}
}

@inproceedings{10.1145/3603607.3613479,
author = {Hegland, Frode Alexander},
title = {Where are we with this?},
year = {2023},
isbn = {9798400702396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603607.3613479},
doi = {10.1145/3603607.3613479},
abstract = {With the introduction of powerful VR in the form of the Vision Pro next year, 40 years after the Macintosh, we will see the introduction of potentially ground-breaking means through which we can interact with our information and each other, to truly augment our mental abilities. However, I am concerned that we are wasting this historic opportunity. We think we ‘know’ what digital text is, we think digital text is what we use in word processing applications, email, messages, spreadsheets, the web and that's pretty much it. We ‘know’ what digital text is, so we stop exploring what it can be. Doug Engelbart remarked that dreaming is hard work. He was so right, and I believe that we need to be shaken out of our paradigms to think anew. And I think that is–potentially–the most valuable thing being at the cusp of a headset world gives us; the opportunity to think anew, to inspire new and more open thinking about what interactivity is, what augmentation can do, how we can view our information, how we can connect, how we can become better, more connected, humans. There are some very basic issues we need to address before we can truly say we are opening up the vistas available to us. If we as a community do not look into this deeply, I think we are headed for VR experiences as boxed-in as the CDs and DVDs of yesteryear. If we do not, as a community, really consider these issues and delegate them to the large tech companies who makes the equipment, we will not own the future, we will not own the potential, they will, and their motivations are not the same as ours. A scenario. I open a book while wearing a headset–it doesn't matter if I'm in VR or AR mode–and it floats in front of me, at a comfortable reading distance and angle. So far so nice and simple. I then make a gesture and all the images; the photographs, charts, graphs and tables flow out of the book and onto the wall at the back of the room. There is also a beautiful 3D model encoded in one of the pages. In the traditional, flat, version of the book, this model appears as a still image and the metadata is recorded at the back of the book for situations like this. I take this model out and put it right in front of me so that I can look at it properly and interact with it. I also decide to take the Reference section out of the back of the book an place it on the side, over here, and from that I can easily summon the sources and see relationships between them. This sounds like a very interesting environment for Spatial Hypertext right? Maybe we should call it something new, since it is fully dimensional, not flattened. Apple has introduced the name Spatial Computing with the Vision Pro, so how about Spatial Computing Hypertext? That is only intended as a provocation, not as a serious suggestion. We just need to think about what this space is. This space is not cyberspace, it is a visual environment on a human scale. Headset computing is built for humans, it is not built for machine-machine interaction or offloading thinking, which, in contrast, much of AI is. It therefore needs to fit us, like no technology has needed to before, if it is to extend us.},
booktitle = {Proceedings of the 6th Workshop on Human Factors in Hypertext},
articleno = {2},
numpages = {2},
keywords = {headset, VR, Spatial Computing, Metaverse, HMD, AI},
location = {Rome, Italy},
series = {HUMAN '23}
}

@inproceedings{10.1145/3574198.3574205,
author = {Raina, Deepak and Verma, Kashish and Chandrashekhara, Sheragaru Hanumanthappa and Saha, Subir Kumar},
title = {Slim U-Net: Efficient Anatomical Feature Preserving U-net Architecture for Ultrasound Image Segmentation},
year = {2023},
isbn = {9781450397223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574198.3574205},
doi = {10.1145/3574198.3574205},
abstract = {We investigate the applicability of U-Net based models for segmenting Urinary Bladder (UB) in male pelvic view UltraSound (US) images. The segmentation of UB in the US image aids radiologists in diagnosing the UB. However, UB in US images has arbitrary shapes, indistinct boundaries and considerably large inter- and intra-subject variability, making segmentation a quite challenging task. Our study of the state-of-the-art (SOTA) segmentation network, U-Net, for the problem reveals that it often fails to capture the salient characteristics of UB due to the varying shape and scales of anatomy in the noisy US image. Also, U-net has an excessive number of trainable parameters, reporting poor computational efficiency during training. We propose a Slim U-Net to address the challenges of UB segmentation. Slim U-Net proposes to efficiently preserve the salient features of UB by reshaping the structure of U-Net using a less number of 2D convolution layers in the contracting path, in order to preserve and impose them on expanding path. To effectively distinguish the blurred boundaries, we propose a novel annotation methodology, which includes the background area of the image at the boundary of a marked region of interest (RoI), thereby steering the model’s attention towards boundaries. In addition, we suggested a combination of loss functions for network training in the complex segmentation of UB. The experimental results demonstrate that Slim U-net is statistically superior to U-net for UB segmentation. The Slim U-net further decreases the number of trainable parameters and training time by and , respectively, compared to the standard U-Net, without compromising the segmentation accuracy. The project page with source code is available at https://sites.google.com/view/slimunet.},
booktitle = {Proceedings of the 2022 9th International Conference on Biomedical and Bioinformatics Engineering},
pages = {41–48},
numpages = {8},
keywords = {Urinary bladder, Ultrasound image segmentation, U-Net},
location = {Kyoto, Japan},
series = {ICBBE '22}
}

@inproceedings{10.1145/3372297.3417261,
author = {Abraham, Ittai and Pinkas, Benny and Yanai, Avishay},
title = {Blinder -- Scalable, Robust Anonymous Committed Broadcast},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417261},
doi = {10.1145/3372297.3417261},
abstract = {Anonymous Committed Broadcast is a functionality that extends DC-nets and allows a set of clients to privately commit messages to set of servers, which can then simultaneously open all committed messages in a random ordering. Anonymity holds since no one can learn the ordering or the content of the client's committed message. We present Blinder, the first system that provides a scalable and fully robust solution for anonymous committed broadcast. Blinder maintains both properties of security (anonymity) and robustness (aka. 'guaranteed output delivery' or 'availability') in the face of a global active (malicious) adversary. Moreover, Blinder is censorship resistant, that is, an honest client cannot be blocked from participating. Blinder obtains its security and scalability by carefully combining classical and state-of-the-art techniques from the fields of anonymous communication and secure multiparty computation (MPC). Relying on MPC for such a system is beneficial since it naturally allows the parties (servers) to enforce some properties on accepted messages prior their publication. A GPU based implementation of Blinder with 5 servers, which accepts 1 million clients, incurs a latency of less than 8 minutes; faster by a factor of $&gt;100$ than the 3-servers Riposte protocol (S&amp;P '15), which is not robust and not censorship resistant; we get an even larger factor when comparing to AsynchroMix and PowerMix (CCS '19), which are the only ones that guarantee fairness (or robustness in the online phase).},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1233–1252},
numpages = {20},
keywords = {secure multiparty computation, robustness, anonymous broadcast},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/3459637.3481948,
author = {Shen, Qijie and Tao, Wanjie and Zhang, Jing and Wen, Hong and Chen, Zulong and Lu, Quan},
title = {SAR-Net: A Scenario-Aware Ranking Network for Personalized Fair Recommendation in Hundreds of Travel Scenarios},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481948},
doi = {10.1145/3459637.3481948},
abstract = {The travel marketing platform of Alibaba serves an indispensable role for hundreds of different travel scenarios from Fliggy, Taobao, Alipay apps, etc. To provide personalized recommendation service for users visiting different scenarios, there are two critical issues to be carefully addressed. First, since the traffic characteristics of different scenarios, e.g., individual data scale or representative topic, are significantly different, it is very challenging to train a unified model to serve all. Second, during the promotion period, the exposure of some specific items will be re-weighted due to manual intervention, resulting in biased logs, which will degrade the ranking model trained using these biased data. In this paper, we propose a novel Scenario-Aware Ranking Network (SAR-Net) to address these issues. SAR-Net harvests the abundant data from different scenarios by learning users' cross-scenario interests via two specific attention modules, which leverage the scenario features and item features to modulate the user behavior features, respectively. Then, taking the encoded features of previous module as input, a scenario-specific linear transformation layer is adopted to further extract scenario-specific features, followed by two groups of debias expert networks, i.e., scenario-specific experts and scenario-shared experts. They output intermediate results independently, which are further fused into the final result by a multi-scenario gating module. In addition, to mitigate the data fairness issue caused by manual intervention, we propose the concept of Fairness Coefficient (FC) to measures the importance of individual sample and use it to reweigh the prediction in the debias expert networks. Experiments on an offline dataset covering over 80 million users and 1.55 million travel items and an online A/B test demonstrate the effectiveness of our SAR-Net and its superiority over state-of-the-art methods. SAR-Net has also been deployed in the online travel marketing platform of Alibaba and is serving hundreds of travel scenarios.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4094–4103},
numpages = {10},
keywords = {scenario-aware, recommender system, multi-scenario learning, fairness coefficient, click-through rate prediction},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3372297.3423358,
author = {Mohassel, Payman and Rindal, Peter and Rosulek, Mike},
title = {Fast Database Joins and PSI for Secret Shared Data},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3423358},
doi = {10.1145/3372297.3423358},
abstract = {We present a scalable protocol for database joins on secret shared data in the honest-majority three-party setting. The key features of our protocol are a rich set of SQL-like join/select queries and the ability to compose join operations together due to the inputs and outputs being generically secret shared between the parties. Provided that all joins operate on unique primary keys, no information is revealed to any party during the protocol. In particular, not even the sizes of intermediate joins are revealed. All of our protocols are constant-round and achieve O(n) communication and computation overhead for joining two tables of n rows.These properties make our protocol ideal for outsourced secure computation. In this setting several non-colluding servers are setup and the input data is shared among them. These servers then perform the relevant secret shared computation and output the result. This model has recently been gaining traction in industry, e.g. Facebook's Crypten, Cape Privacy's TFEncrypted, Mozilla Telemetry.We additionally implement two applications on top of our framework. The first application detects voter registration errors within and between agencies of 50 US states, in a privacy-preserving manner. The second application allows several organizations to compare network security logs to more accurately identify common security threats, e.g. the IP addresses of a bot net. In both cases, the practicality of these applications depends on efficiently performing joins on millions of secret shared records. For example, our three party protocol can perform a join on two sets of 1 million records in 4.9 seconds or, alternatively, compute the cardinality of this join in just 3.1 seconds.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1271–1287},
numpages = {17},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/3507473.3507475,
author = {Chen, Cheng and Li, Shuo and Ding, Zhijun},
title = {Automatic Modeling Method for PThread Programs Based on Program Dependence Net},
year = {2022},
isbn = {9781450385213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507473.3507475},
doi = {10.1145/3507473.3507475},
abstract = {Program Dependence Net (PDNet) based on Coloured Petri Net (CPN) is a kind of multi-threaded program model describing and distinguishing program dependences. This paper presents an automatic modeling method to construct a PDNet which is consistent with the behavior of multi-threaded C programs (PThread programs) from source code. A big challenge for modeling multi-threaded programs is shared function in different threads. To handle it, we set independent places to represent variables, and simulate address space and thread stack through different fields in colour set. Another challenge is to describe complicated data structures including array, pointer and struct when programs operate on them. Native arc expressions for PDNet can't describe pointer reference or array operator. We design a recursive function to translate program expressions into native arc expressions automatically, which solve the problem of calculating complicated program expressions. On the basis of our method, we also implement our tool and prove the usefulness, high efficiency and scalability of our method through experiments in our benchmarks including a subset benchmarks of SV-COMP [17] which is a famous competition on software verification.},
booktitle = {Proceedings of the 2021 3rd International Conference on Software Engineering and Development},
pages = {9–18},
numpages = {10},
keywords = {Program Verification, Program Dependence Net, Petri Net, Model checking},
location = {Xiamen, China},
series = {ICSED '21}
}

