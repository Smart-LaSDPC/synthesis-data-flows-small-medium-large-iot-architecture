@inproceedings{10.1145/2088960.2088971,
author = {Dar, Kashif and Taherkordi, Amirhosein and Vitenberg, Roman and Rouvoy, Romain and Eliassen, Frank},
title = {Adaptable service composition for very-large-scale Internet of Things systems},
year = {2011},
isbn = {9781450310734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088960.2088971},
doi = {10.1145/2088960.2088971},
abstract = {The (future) Internet of Things (IoT), with service composition point of view, raises additional challenges especially with respect to handling the scale, dynamicity and heterogeneity of the target networking environment. Therefore, the services offered by IoT resources can not be composed by simply extending existing Service Oriented Architecture (SOA) approaches, since it requires the integration of a huge number of real world services that demands for the user-centric and a situation-aware composition process. In this paper, we present an architectural approach that enables efficient and adaptive composition of services by locally orchestrating the distributed web-enabled services in Very Large Scale (VLS) IoT systems and globally choreographing the different enterprise level Web services.},
booktitle = {Proceedings of the Workshop on Posters and Demos Track},
articleno = {11},
numpages = {2},
keywords = {reconfiguration, orchestration, choreography, adaptable service composition, IoT resources},
location = {Lisbon, Portugal},
series = {PDT '11}
}

@inproceedings{10.1145/2093190.2093192,
author = {Dar, Kashif and Taherkordi, Amirhosein and Rouvoy, Romain and Eliassen, Frank},
title = {Adaptable service composition for very-large-scale internet of things systems},
year = {2011},
isbn = {9781450310727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2093190.2093192},
doi = {10.1145/2093190.2093192},
abstract = {Internet of Things (IoT) is considered as a future paradigm whose main challenge is to give an IP based transparent access to the huge number of services available as IoT resources. Due to the large number of resource-constrained devices and the dynamic nature of IoT environments, this integration problem becomes more intricate. The current state-of-the-art is mostly focused on the integration of IP enabled smart objects on the basis of Service Oriented Architecture (SOA) and the Representational State Transfer (REST) architectural style. However, beyond these approaches, we intend to address the flexible and adaptive composition of services in Very Large Scale (VLS) IoT systems by exploiting the concepts of service orchestration and choreography. In particular, we present an architectural model that enables efficient integration of services by locally orchestrating distributed web-enabled services in VLS IoT systems and globally choreographing Web-based applications.},
booktitle = {Proceedings of the 8th Middleware Doctoral Symposium},
articleno = {2},
numpages = {6},
keywords = {service orchestration and choreography, reconfiguration, internet of things, adaptable services composition, RESTful IoT resources},
location = {Lisbon, Portugal},
series = {MDS '11}
}

@inproceedings{10.1145/2659651.2659696,
author = {Stepanova, T. and Zegzhda, D.},
title = {Applying Large-scale Adaptive Graphs to Modeling Internet of Things Security},
year = {2014},
isbn = {9781450330336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659651.2659696},
doi = {10.1145/2659651.2659696},
abstract = {Lots of upcoming IT trends are based on the concept of heterogeneous networks: Internet of Things is amongst them. Modern heterogeneous networks are characterized by hardly predictable behavior, hundreds of parameters of network nodes and connections and lack of single basis for development of control methods and algorithms. To overcome listed problems one need to implement topological modeling of dynamically changing structures. In this paper authors propose basic theoretical framework that will allow estimation of controllability, resiliency, scalability and other determinant parameters of complex heterogeneous networks.},
booktitle = {Proceedings of the 7th International Conference on Security of Information and Networks},
pages = {479–482},
numpages = {4},
keywords = {sustainability, security modeling, large-scale adaptive graph, internet of things},
location = {Glasgow, Scotland, UK},
series = {SIN '14}
}

@inproceedings{10.1145/2491266.2491270,
author = {Hong, Kirak and Lillethun, David and Ramachandran, Umakishore and Ottenw\"{a}lder, Beate and Koldehofe, Boris},
title = {Mobile fog: a programming model for large-scale applications on the internet of things},
year = {2013},
isbn = {9781450321808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491266.2491270},
doi = {10.1145/2491266.2491270},
abstract = {The ubiquitous deployment of mobile and sensor devices is creating a new environment, namely the Internet of Things(IoT), that enables a wide range of future Internet applications. In this work, we present Mobile Fog, a high level programming model for the future Internet applications that are geospatially distributed, large-scale, and latency-sensitive. We analyze use cases for the programming model with camera network and connected vehicle applications to show the efficacy of Mobile Fog. We also evaluate application performance through simulation.},
booktitle = {Proceedings of the Second ACM SIGCOMM Workshop on Mobile Cloud Computing},
pages = {15–20},
numpages = {6},
keywords = {situation awareness applications, programming model, internet of things, future internet applications, fog computing, cloud computing},
location = {Hong Kong, China},
series = {MCC '13}
}

@inproceedings{10.1145/2598784.2598789,
author = {Jenkins, Tom},
title = {Prototyping speculative objects for the internet of things},
year = {2014},
isbn = {9781450329033},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598784.2598789},
doi = {10.1145/2598784.2598789},
abstract = {Digital media technologies allow the systems to be created that are rhetorical and create alternate values and experiences. Building objects that question these assumptions can help to reframe technological artifacts. Building inexpensive prototyping platforms that augment everyday objects in minimalist ways is a proposal for an alternative to existing, human-centered Internet of Things (IoT) devices. These platforms begin to move towards interactions among and between things as a bottom-up design study into ubiquitous small-scale computing and its aesthetic applications.},
booktitle = {Proceedings of the 2014 Companion Publication on Designing Interactive Systems},
pages = {163–166},
numpages = {4},
keywords = {prototyping, materiality, making, iot, internet of things, design},
location = {Vancouver, BC, Canada},
series = {DIS Companion '14}
}

@inproceedings{10.1145/2642803.2642828,
author = {Maia, Pedro and Cavalcante, Everton and Gomes, Porf\'{\i}rio and Batista, Thais and Delicato, Flavia C. and Pires, Paulo F.},
title = {On the Development of Systems-of-Systems based on the Internet of Things: A Systematic Mapping},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642828},
doi = {10.1145/2642803.2642828},
abstract = {The Internet of Things (IoT) has emerged as a paradigm in which smart things actively collaborate among them and with other physical and virtual objects available in the Web in order to perform high level tasks. These things can be engaged in complex relationships including the composition and collaboration with other independent and heterogeneous systems in order to provide new functionalities, thus leading to the so-called systems-of-systems (SoS). In the context of integrating IoT-based systems in order to compose complex, large-scale SoS, this paper presents a systematic mapping aimed to discuss current scenarios and approaches in the development of IoT-based SoS, as well as some challenges and research opportunities in this context.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {23},
numpages = {8},
keywords = {Systems-of-Systems, Systematic Mapping, SoS, IoT, Internet of Things},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/2602458.2611455,
author = {Dustdar, Schahram},
title = {Principles and methods for elastic computing},
year = {2014},
isbn = {9781450325776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602458.2611455},
doi = {10.1145/2602458.2611455},
abstract = {In this talk I will address one of the most relevant challenges for a decade to come: How to design, model, and execute distributed systems utilizing the Internet of Things, software services, as well as human based services, considering modern Cloud Computing and Elasticity principles. Elasticity is seen as one of the main characteristics of Cloud Computing today. Is elasticity simply scalability on steroids? In this talk I will discuss the main principles of elasticity, present a fresh look at this problem, and examine how to integrate people, software services, and things into one composite system, which can be modeled, programmed, and deployed on a large scale in an elastic way.},
booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {cloud computing},
location = {Marcq-en-Bareul, France},
series = {CBSE '14}
}

@inproceedings{10.1145/2559206.2578879,
author = {Jenkins, Tom and Bogost, Ian},
title = {Designing for the internet of things: prototyping material interactions},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2578879},
doi = {10.1145/2559206.2578879},
abstract = {The Internet of Things (IoT) offers fertile ground to consider the nature of electronic prototyping, especially in building systems from the lowest level. While constructing artifacts to interact directly with everyday materials and contexts, we've found it important to approach the IoT from the very lowest levels of hardware to avoid both abstracting away from real knowledge of the platform itself as well as to reduce implementation cost for massive deployment.Building new, inexpensive platforms that augment everyday objects in minimal ways is our proposal for an alternative to top-down control of IoT devices. We intend to move towards interactions among and between things as a bottom-up design study into ubiquitous small-scale computing and its potential aesthetic applications.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {731–740},
numpages = {10},
keywords = {prototyping, materiality, making, iot, internet of things, design},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@inproceedings{10.1145/2396761.2398587,
author = {Ma, Youzhong and Rao, Jia and Hu, Weisong and Meng, Xiaofeng and Han, Xu and Zhang, Yu and Chai, Yunpeng and Liu, Chunqiu},
title = {An efficient index for massive IOT data in cloud environment},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398587},
doi = {10.1145/2396761.2398587},
abstract = {The Internet of Things (IOT) has been widely applied in many fields, while the IOT data are always large volume, update frequently and inherently multi-dimensional, these characteristics bring big challenges to the traditional DBMSs. The traditional DBMSs have rich functionality and can deal with multi-attributes access efficiently, they can not scale good enough to deal with large volume data and can not support high insert throughput. The cloud-based database systems have good scalability, but they don't support multi-dimensional access natively.In order to deal with the large volume of IOT data, we propose an update and query efficient index framework (UQE-Index) based on key-value store that can support high insert throughput and provide efficient multi-dimensional query simultaneously. We implemented a prototype based on HBase and did comprehensive experiments to test our solution's scalability and efficiency.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2129–2133},
numpages = {5},
keywords = {internet of things, index, cloud},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1145/1993966.1993975,
author = {Pintus, Antonio and Carboni, Davide and Piras, Andrea},
title = {The anatomy of a large scale social web for internet enabled objects},
year = {2011},
isbn = {9781450306249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993966.1993975},
doi = {10.1145/1993966.1993975},
abstract = {The ongoing evolution of the Internet of Things toward the Web of Things, where Web-enabled smart objects connect and communicate using the protocols of the Web, has raised several research issues from protocols adoption and communication models to architectural styles. In this paper we present our vision about the anatomy of a scalable architecture for a large scale social Web of Things for smart objects and the solutions adopted. Main faced issues include a reasoned exploration of design choices in conjunction with the related state-of-art analysis, technologies, concepts and social aspects behind our proposed solution. Among them, a prototype is proposed and two experimented scenarios are described. Finally, this paper reports the conclusion, challenges and future works toward the evolution of our social Web of Things architecture and tool.},
booktitle = {Proceedings of the Second International Workshop on Web of Things},
articleno = {6},
numpages = {6},
keywords = {social networks, information systems, computer systems organization, Web of Things, REST},
location = {San Francisco, California, USA},
series = {WoT '11}
}

@inproceedings{10.1145/2093190.2093193,
author = {Hachem, Sara and Teixeira, Thiago and Issarny, Val\'{e}rie},
title = {Ontologies for the internet of things},
year = {2011},
isbn = {9781450310727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2093190.2093193},
doi = {10.1145/2093190.2093193},
abstract = {Challenges the Internet of Things (IoT) is facing are directly inherited from today's Internet. However, they are amplified by the anticipated large scale deployments of devices and services, information flow and direct user involvment in the IoT. Challenges are many and we focus on addressing those related to scalability, heterogeneity of IoT components, and the highly dynamic and unknown nature of the network topology. In this paper, we give an overview of a service-oriented middleware solution that addresses those challenges using semantic technologies to provide interoperability and flexibility. We especially focus on modeling a set of ontologies that describe devices and their functionalities and thoroughly model the domain of physics. The physics domain is indeed at the core of the IoT, as it allows the approximation and estimation of functionalities usually provided by things. Those functionalities will be deployed as services on appropriate devices through our middleware.},
booktitle = {Proceedings of the 8th Middleware Doctoral Symposium},
articleno = {3},
numpages = {6},
keywords = {service-oriented middleware, sensors and actuators, semantic web, ontology, internet of things},
location = {Lisbon, Portugal},
series = {MDS '11}
}

@inproceedings{10.1145/2633661.2633674,
author = {Sigg, Stephan and Fu, Xiaoming},
title = {Social opportunistic sensing and social centric networking: enabling technology for smart cities},
year = {2014},
isbn = {9781450330367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2633661.2633674},
doi = {10.1145/2633661.2633674},
abstract = {In recent years, with tremendous advances in areas like mobile devices, algorithms for distributed systems, communication technology or protocols, all basic technological pieces to realise a Smart City are at hand. Missing, however, is a mechanism that bridges these pieces to ease the creation of Smart Cities at a larger scale. In this visionary paper, we discuss challenges of Smart Cities and propose enabling technologies to bridge the above mentioned pieces for their actual realisation. In particular, we introduce the concepts of Social Opportunistic Sensing (SOS) and Social Centric Networking (SCN). While the former is an enabling technology to interconnect all parties in a Smart City, the latter has the potential to enhance offline social networks in Internet of Things (IoT) enhanced Smart Cities by connecting individuals based on their automatically updated profile via context-based routing.},
booktitle = {Proceedings of the 2014 ACM International Workshop on Wireless and Mobile Technologies for Smart Cities},
pages = {83–90},
numpages = {8},
keywords = {smart cities, participatory sensing, environmental sensing, crowdsourcing, context centric networking},
location = {Philadelphia, Pennsylvania, USA},
series = {WiMobCity '14}
}

@inproceedings{10.1145/2479787.2479821,
author = {Anantharam, Pramod and Barnaghi, Payam and Sheth, Amit},
title = {Data processing and semantics for advanced internet of things (IoT) applications: modeling, annotation, integration, and perception},
year = {2013},
isbn = {9781450318501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479787.2479821},
doi = {10.1145/2479787.2479821},
abstract = {This tutorial presents tools and techniques for effectively utilizing the Internet of Things (IoT) for building advanced applications, including the Physical-Cyber-Social (PCS) systems. The issues and challenges related to IoT, semantic data modelling, annotation, knowledge representation (e.g. modelling for constrained environments, complexity issues and time/location dependency of data), integration, analysis, and reasoning will be discussed. The tutorial will describe recent developments on creating annotation models and semantic description frameworks for IoT data (e.g. such as W3C Semantic Sensor Network ontology). A review of enabling technologies and common scenarios for IoT applications from the data and knowledge engineering point of view will be discussed. Information processing, reasoning, and knowledge extraction, along with existing solutions related to these topics will be presented. The tutorial summarizes state-of-the-art research and developments on PCS systems, IoT related ontology development, linked data, domain knowledge integration and management, querying large-scale IoT data, and AI applications for automated knowledge extraction from real world data.},
booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
articleno = {5},
numpages = {5},
keywords = {traffic analytics, semantic sensor web, reasoning, ontology, modelling, knowledge engineering, internet of things (IoT), integration, inference, healthcare, cyber-physical-social systems, annotation},
location = {Madrid, Spain},
series = {WIMS '13}
}

@inproceedings{10.1145/2676743.2676746,
author = {Marie, Pierrick and Lim, L\'{e}on and Manzoor, Atif and Chabridon, Sophie and Conan, Denis and Desprats, Thierry},
title = {QoC-aware context data distribution in the internet of things},
year = {2014},
isbn = {9781450332347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676743.2676746},
doi = {10.1145/2676743.2676746},
abstract = {The Internet of Things (IoT) is a very dynamic and heterogeneous environment that generates plethora of sensor data, accessible to develop new smart pervasive applications. However, the substantial amount of effort required to collect and disseminate context data with sufficient quality prevents the context consumers to take advantage of the IoT to its full potential. Consequently, novel research efforts are required to design middleware solutions able to deliver relevant context data to consumer applications while hiding the complexity of data distribution in heterogeneous and large-scale environments. This paper presents the INCOME framework that enables context producers to express the level of Quality of Context (QoC) they are able to provide and context consumers to set thresholds on the QoC they expect in order to determine how to distribute context data. Our experiments show that context data can be annotated with QoC metadata and distributed from producers to consumers with a reasonable additional cost even on resource-constrained devices such as Raspberry Pi.},
booktitle = {Proceedings of the 1st ACM Workshop on Middleware for Context-Aware Applications in the IoT},
pages = {13–18},
numpages = {6},
keywords = {quality of context, middleware, distributed event-based systems, IoT},
location = {Bordeaux, France},
series = {M4IOT '14}
}

@inproceedings{10.1145/2393347.2396421,
author = {Singh, Vivek K. and Gao, Mingyan and Jain, Ramesh},
title = {Situation recognition: an evolving problem for heterogeneous dynamic big multimedia data},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396421},
doi = {10.1145/2393347.2396421},
abstract = {With the growth in social media, internet of things, and planetary-scale sensing there is an unprecedented need to assimilate spatio-temporally distributed multimedia streams into actionable information. Consequently the concepts like objects, scenes, and events, need to be extended to recognize situations (e.g. epidemics, traffic jams, seasons, flash mobs). This paper motivates and computationally grounds the problem of situation recognition. It describes a systematic approach for combining multimodal real-time big data into actionable situations. Specifically it presents a generic approach for modeling and recognizing situations. A set of generic building blocks and guidelines help the domain experts model their situations of interest. The created models can be tested, refined, and deployed into practice using a developed system (EventShop). Results of applying this approach to create multiple situation-aware applications by combining heterogeneous streams (e.g. Twitter, Google Insights, Satellite imagery, Census) are presented.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1209–1218},
numpages = {10},
keywords = {social networks, situation detection, situation awareness, situation, sensor networks, modeling, events},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2676733.2676736,
author = {Lim, L\'{e}on and Conan, Denis},
title = {Distributed event-based system with multiscoping for multiscalability},
year = {2014},
isbn = {9781450332224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676733.2676736},
doi = {10.1145/2676733.2676736},
abstract = {Distributed Event-Based System (DEBS) provides a versatile solution for asynchronously exchanging data in a distributed system, loosely-coupled in space and time. The software architecture of a DEBS is composed of an over-lay network of brokers that are responsible for routing data from producers to consumers. An important issue is the cost (in terms of exchanged messages) of the installation of advertisement or subscription filters on the brokers and the cost of routing notifications. The problem is exacerbated in large and heterogeneous systems involving clouds, cloudlets, desktops, laptops, mobile phones, and smart objects of the Internet of Things (IoT). In this paper, we associate the system concept of scale (of multiscale distributed systems) with the concept of scope (of DEBS) and we introduce DEBS with multiscoping. We also extend the requirements of distributed routing to deal with multiscoping. In the context of the IoT, we show in an illustrative example that the solution allows application designers and system administrators to tag advertisements and subscriptions for semantically delimiting scopes that are superposed.},
booktitle = {Proceedings of the 9th Workshop on Middleware for Next Generation Internet Computing},
articleno = {3},
numpages = {6},
keywords = {scoping, multiscalability, middleware, distributed event-based systems, IoT},
location = {Bordeaux, France},
series = {MW4NG '14}
}

@inproceedings{10.1145/2676743.2676744,
author = {Grace, Paul and Barbosa, Justan and Pickering, Brian and Surridge, Mike},
title = {Taming the interoperability challenges of complex IoT systems},
year = {2014},
isbn = {9781450332347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676743.2676744},
doi = {10.1145/2676743.2676744},
abstract = {The Internet of Things is characterised by extreme heterogeneity of communication protocols and data formats; hence ensuring diverse devices can interoperate with one another remains a significant challenge. Model-driven development and testing solutions have been proposed as methods to aid software developers achieve interoperability compliance in the face of this increasing complexity. However, current approaches often involve complicated and domain specific models (e.g. web services described by WSDL). In this paper, we explore a lightweight, middleware independent, model-driven development framework to help developers tame the challenges of composing IoT services that interoperate with one another. The framework is based upon two key contributions: i) patterns of interoperability behaviour, and ii) a software framework to monitor and reason about interoperability success or failure. We show using a case-study from the FI-WARE Future Internet Service domain that this interoperability framework can support non-expert developers address interoperability challenges. We also deployed tools built atop the framework and made them available in the XIFI large-scale FI-PPP test environment.},
booktitle = {Proceedings of the 1st ACM Workshop on Middleware for Context-Aware Applications in the IoT},
pages = {1–6},
numpages = {6},
keywords = {software testing, model-driven software engineering, interoperability, internet of things, architectural patterns},
location = {Bordeaux, France},
series = {M4IOT '14}
}

@inproceedings{10.1145/2663165.2663335,
author = {Hasan, Souleiman and Curry, Edward},
title = {Thematic event processing},
year = {2014},
isbn = {9781450327855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663165.2663335},
doi = {10.1145/2663165.2663335},
abstract = {Event-based systems follow a decoupled mode of interaction between event producers and consumers in space, time, and synchronization to enable scalability within distributed systems. We recognize a fourth dimension of coupling due to the need for mutual agreements on terms that describe event types, attributes, and values. Semantic coupling is challenging in large-scale, open, and heterogeneous environments such as the Internet of Things (IoT). It requires event producers and consumers to agree on event semantics and can limit scalability due to the difficulties in establishing such agreements. In this paper we propose a new thematic event processing approach based on enhancing events and subscriptions with terms representing their themes to clarify their domains and meanings in addition to their pay-load. Experiments conducted using large heterogeneous sets of smart-city and energy management events suggest up to 85% of matching accuracy at a rate of 500 events/sec of throughput. This represents around 15% improvement in accuracy and 150% in throughput over non-thematic approaches. This suggests the viability of thematic event processing to scale to environments such as the IoT.},
booktitle = {Proceedings of the 15th International Middleware Conference},
pages = {109–120},
numpages = {12},
keywords = {uncertainty, theme tags, semantic matching, internet of things, event processing, distributional semantics, approximate matching},
location = {Bordeaux, France},
series = {Middleware '14}
}

@inproceedings{10.1145/2488222.2488348,
author = {Stojanovic, Nenad D. and Stojanovic, Ljiljana and Stuehmer, Roland},
title = {Tutorial: personal big data management in the cyber-physical systems - the role of event processing},
year = {2013},
isbn = {9781450317580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488222.2488348},
doi = {10.1145/2488222.2488348},
abstract = {Processing can have for the management of personal information in cyberphysical systems. In particular, we present the challenges for this process, mainly related to the Internet of Things and Big Data processing, 2) elaborate on current efforts in developing an event-driven platform that supports the above mentioned requirements and c) present examples from one real-life scenario (remote patient monitoring). In the central part we present the platform for realizing such an approach. The platform consists of three main components: a distributed publish/subscribe enabled service bus responsible for collecting events from heterogeneous, distributed sources, Event Cloud, a peer-to-peer semantic-based repository responsible for the storage of events and distributed Complex Event Processing responsible for the complex combination of events in real-time. Two main advantages of the platforms are its scalability (cloud-based nature) and the expressivity of the requests that can be defined (combination of real-time and historical queries) based on Web technologies such as RDF and SPARQL.},
booktitle = {Proceedings of the 7th ACM International Conference on Distributed Event-Based Systems},
pages = {281–288},
numpages = {8},
keywords = {mobile event processing, cyberphysical systems},
location = {Arlington, Texas, USA},
series = {DEBS '13}
}

@inproceedings{10.4108/icst.urb-iot.2014.257268,
author = {Brambilla, Giacomo and Picone, Marco and Cirani, Simone and Amoretti, Michele and Zanichelli, Francesco},
title = {A simulation platform for large-scale internet of things scenarios in urban environments},
year = {2014},
isbn = {9781631900372},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.urb-iot.2014.257268},
doi = {10.4108/icst.urb-iot.2014.257268},
abstract = {The Internet of Things (IoT) refers to the interconnection of billions of IP-enabled devices, denoted as "smart objects", with limited capabilities, in terms of computational power and memory capacity, which typically operate in constrained environments, in an Internet-like structure. Large-scale systems and applications that rely on such a high number of devices, due to their complexity, need careful analysis and test, before being deployed to target environments. Traditional IoT simulators do not focus on the simulation of large scale deployments, as they are intended to evaluate and analyze low-level networking aspects, with groups of smart objects arranged in specific topologies. In this paper, we illustrate an efficient simulation methodology, which is particularly suitable to test IoT systems with a large number of interconnected devices in Urban environments from an application-layer perspective. The main advantages of such an approach are: i) the capability to simulate large-scale systems with thousands of geographically distributed devices; ii) the maximization of code reuse; and iii) the high generality of simulated nodes, which can be characterized by multiple network interfaces and protocols, as well as different mobility, network, and energy consumption models.},
booktitle = {Proceedings of the First International Conference on IoT in Urban Space},
pages = {50–55},
numpages = {6},
keywords = {urban environments, smart cities, internet of things, discrete event simulation},
location = {<conf-loc>, <city>Rome</city>, <country>Italy</country>, </conf-loc>},
series = {URB-IOT '14}
}

@inproceedings{10.1109/CCGrid.2013.100,
author = {Tracey, David and Sreenan, Cormac},
title = {A holistic architecture for the internet of things, sensing services and big data},
year = {2013},
isbn = {9780768549965},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2013.100},
doi = {10.1109/CCGrid.2013.100},
abstract = {Wireless Sensor Networks (WSNs) increasingly enable applications and services to interact with the physical world. Such services may be located across the Internet from the sensing network. Cloud services and big data approaches may be used to store and analyse this data to improve scalability and availability, which will be required for the billions of devices envisaged in the Internet of Things (IoT). The potential of WSNs is limited by the relatively low number deployed and the difficulties imposed by their heterogeneous nature and limited (or proprietary) development environments and interfaces. This paper proposes a set of requirements for achieving a pervasive, integrated information system of WSNs and associated services. It also presents an architecture which is termed holistic as it considers the flow of the data from sensors through to services. The architecture provides a set of abstractions for the different types of sensors and services. It has been designed for implementation on a resource constrained node and to be extensible to server environments. This paper presents a 'C' implementation of the core architecture, including services on Linux and Contiki (using the Constrained Application Protocol (CoAP)) and a Linux service to integrate with the Hadoop HBase datastore.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {546–553},
numpages = {8},
keywords = {wireless sensor networks, tuple space, protocols, information model, cloud computing, big data},
location = {Delft, Netherlands},
series = {CCGRID '13}
}

@inproceedings{10.1145/2676743.2676747,
author = {Hu, Wenyan and Hu, Xiping and Deng, Jun-qi and Zhu, Chunsheng and Fotopoulos, Georgios and Ngai, Edith C.-H. and Leung, Victor C. M.},
title = {Mood-fatigue analyzer: towards context-aware mobile sensing applications for safe driving},
year = {2014},
isbn = {9781450332347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676743.2676747},
doi = {10.1145/2676743.2676747},
abstract = {Nowadays more and more organizations focus on reducing traffic accidents and defensive measures for safe driving. The vigilance level (e.g., negative emotion and fatigue) also accounts for the road injuries. Till now, there is no systematic solution for different mobile devices that can effectively infer the mood and fatigue of drivers in real-time or conveniently be used by drivers, nor incentive scheme for drivers in large scale to stimulate their positive and secure driving collaboratively with friends in a social context. In this paper, we propose the Mood-Fatigue Analyzer (MFA), a systematic solution that can be used in different middlewares on mobile devices, which can transform the data from sensors to context-aware mobile sensing applications for safe driving. The MFA employs multidimensional methods to get the drivers' real-time mood and fatigue information by sensors using the Internet of Things (IoT) deployed in and out of cars. Besides promoting safe driving with integrated sensors, the MFA could be built on a multi-tier vehicular social network (VSN) platform, which enables communication among drivers in a social context via cloud platform. Architecture implementation and experimental results of the MFA have demonstrated its desired functionalities and efficiency in drivers' daily lives and real-world deployment.},
booktitle = {Proceedings of the 1st ACM Workshop on Middleware for Context-Aware Applications in the IoT},
pages = {19–24},
numpages = {6},
keywords = {vehicular sensor application, context-aware, cloud},
location = {Bordeaux, France},
series = {M4IOT '14}
}

@inproceedings{10.1145/2578726.2578755,
author = {Dao, Minh-Son and Pongpaichet, Siripen and Jalali, Laleh and Kim, Kyoungsook and Jain, Ramesh and Zettsu, Koiji},
title = {A Real-time Complex Event Discovery Platform for Cyber-Physical-Social Systems},
year = {2014},
isbn = {9781450327824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578726.2578755},
doi = {10.1145/2578726.2578755},
abstract = {We are living in the Internet of Things (IoT) era where all the (smart) objects around us are connected and communicated with each other to serve our life better without the need of explicit instruction. Soon we have to cope with trillions of heterogeneous data streams coming from IoT. Since data is not information, methods for discovering useful and correlative information from data and utilising them for the better life, in real-time mode, are the utmost requirements.In order to tackle this problem, we introduce an Event Information Management platform (EvIM) that can be used to develop applications run as cyber-physical-social systems. EvIM includes two components (1) EventWarehouse: is built for harvesting, storing, and analysing data coming from large scale heterogeneous sensors, and (2) EventShop: plays as a real-time complex spatio-temporal event processing system.Differ from conventional systems that use data-driven or pre-defined event-based approaches, the proposed platform can alleviate the burden of human intervention meanwhile increase the scalability, robustness, feasibility, and applicability by offering series of services that not only automatically discover correlations among sensors' data to extract useful information but also can help users design and monitor situations visually, efficiently and effectively, under on-fly mode.},
booktitle = {Proceedings of International Conference on Multimedia Retrieval},
pages = {201–208},
numpages = {8},
keywords = {Sensors Networks, Internet of Things, Data Stream Management System, Cyber-Physical Cloud Computing, Complex Event Processing, Anomaly Analysis},
location = {Glasgow, United Kingdom},
series = {ICMR '14}
}

@inproceedings{10.1145/2611286.2611300,
author = {An, Kyoungho and Gokhale, Aniruddha and Schmidt, Douglas and Tambe, Sumant and Pazandak, Paul and Pardo-Castellote, Gerardo},
title = {Content-based filtering discovery protocol (CFDP): scalable and efficient OMG DDS discovery protocol},
year = {2014},
isbn = {9781450327374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611286.2611300},
doi = {10.1145/2611286.2611300},
abstract = {The OMG Data Distribution Service (DDS) has been deployed in many mission-critical systems and increasingly in Internet of Things (IoT) applications since it supports a loosely-coupled, data-centric publish/subscribe paradigm with a rich set of quality-of-service (QoS) policies. Effective data communication between publishers and subscribers requires dynamic and reliable discovery of publisher/-subscriber endpoints in the system, which DDS currently supports via a standardized approach called the Simple Discovery Protocol (SDP). For large-scale systems, however, SDP scales poorly since the discovery completion time grows as the number of applications and endpoints increases. To scale to much larger systems, a more efficient discovery protocol is required.This paper makes three contributions to overcoming the current limitations with DDS SDP. First, it describes the Content-based Filtering Discovery Protocol (CFDP), which is our new endpoint discovery mechanism that employs content-based filtering to conserve computing, memory and network resources used in the DDS discovery process. Second, it describes the design of a CFDP prototype implemented in a popular DDS implementation. Third, it analyzes the results of empirical studies conducted in a testbed we developed to evaluate the performance and resource usage of our CFDP approach compared with SDP.},
booktitle = {Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
pages = {130–141},
numpages = {12},
keywords = {discovery, data distribution service, Pub/Sub, P2P},
location = {Mumbai, India},
series = {DEBS '14}
}

@inproceedings{10.1145/2631675.2631679,
author = {Goldschmidt, Thomas},
title = {A View-based Approach Towards an Engineering Platform for Industrial Automation in the Cloud},
year = {2014},
isbn = {9781450329002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631675.2631679},
doi = {10.1145/2631675.2631679},
abstract = {Recently, cloud computing gained more and more traction, not only in fast moving domains such as private and enterprise software, but also in more traditional domains such as industrial automation. To some extent this is also driven by the advent of the Internet of Things (IoT) which encompasses aspects from both automation as well as cloud computing. However, for rolling out automation software as a service solutions to low-end, long-tail markets with thousands of small customers important aspects for cloud scalability such as easy self service for the customer are still missing. There exists a large gap between the engineering efforts required to configure an automation system and the effort automation companies and their customers can afford. At the same time, tools for implementing Domain-Specific Languages (DSLs) have recently become more and more efficient and easy to use. Tailored DSLs that make use of abstractions for the particular (sub-)domains and omitting other complexities would allow customers to handle their applications in a SaaS-oriented, self-service manner. In this paper we present a view-based approach for engineering languages for a multi-domain automation cloud platform that facilitates modern DSL frameworks. This will allow automation SaaS providers to rapidly design sub-domain specific engineering tools based on a common platform. End-customers can then use these tailored languages to engineer their specific applications in an efficient manner.},
booktitle = {Proceedings of the 2nd Workshop on View-Based, Aspect-Oriented and Orthographic Software Modelling},
pages = {39–42},
numpages = {4},
keywords = {Language Workbenches, Industrial Automation, Domain-Specific Languages, Cloud Computing},
location = {York, United Kingdom},
series = {VAO '14}
}

@inproceedings{10.1145/2002259.2002290,
author = {Pu, Calton},
title = {A world of opportunities: CPS, IOT, and beyond},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002290},
doi = {10.1145/2002259.2002290},
abstract = {The continuous evolution of computing and networking technologies (e.g., Moore's Law) is creating a new world populated by many sensors on physical and social environments. This emerging new world goes much further than the original visions of ubiquitous computing and World Wide Web. Aspects of this new world have received various names such as Cyber Physical Systems (CPS) and Internet of Things (IOT). CPS links many physical sensor data to detailed simulation models running on large data centers. IOT brings together many appliances, making much more environmental data available and supporting control of these appliances. CPS/IOT applications are many, including personalized healthcare, intelligent transportation, smart grid, sustainable environment, and disaster recovery as representative examples. These CPS/IOT applications are motivated and strongly pushed by significant new social, economic, and human benefits. At the same time, these applications are also mission-critical with serious quality of service requirements such as real-time performance, continuous availability, high security and privacy.We will argue that the traditional process-oriented programming languages and software architectures should be augmented by distributed event-based facilities and abstractions for the construction of large scale distributed CPS/IOT applications. In addition to the focus on performance, we anticipate that other quality of service dimensions such as availability, reliability, security, and privacy will become important concerns in the research on distributed event-based systems. We will discuss research opportunities and challenges that bring the distributed event-based systems technology to CPS/IOT applications.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {229–230},
numpages = {2},
keywords = {internet of things, distributed events, cyber physical systems},
location = {New York, New York, USA},
series = {DEBS '11}
}

@article{10.1145/2633684,
author = {Hasan, Souleiman and Curry, Edward},
title = {Approximate Semantic Matching of Events for the Internet of Things},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/2633684},
doi = {10.1145/2633684},
abstract = {Event processing follows a decoupled model of interaction in space, time, and synchronization. However, another dimension of semantic coupling also exists and poses a challenge to the scalability of event processing systems in highly semantically heterogeneous and dynamic environments such as the Internet of Things (IoT). Current state-of-the-art approaches of content-based and concept-based event systems require a significant agreement between event producers and consumers on event schema or an external conceptual model of event semantics. Thus, they do not address the semantic coupling issue. This article proposes an approach where participants only agree on a distributional statistical model of semantics represented in a corpus of text to derive semantic similarity and relatedness. It also proposes an approximate model for relaxing the semantic coupling dimension via an approximation-enabled rule language and an approximate event matcher. The model is formalized as an ensemble of semantic and top-k matchers along with a probability model for uncertainty management. The model has been empirically validated on large sets of events and subscriptions synthesized from real-world smart city and energy management systems. Experiments show that the proposed model achieves more than 95% F1Score of effectiveness and thousands of events/sec of throughput for medium degrees of approximation while not requiring users to have complete prior knowledge of event semantics. In semantically loosely-coupled environments, one approximate subscription can compensate for hundreds of exact subscriptions to cover all possibilities in environments which require complete prior knowledge of event semantics. Results indicate that approximate semantic event processing could play a promising role in the IoT middleware layer.},
journal = {ACM Trans. Internet Technol.},
month = {aug},
articleno = {2},
numpages = {23},
keywords = {uncertainty, semantic matching, event processing, distributional semantics, Internet of Things, Approximate matching}
}

@proceedings{10.1145/2491159,
title = {HotPlanet '13: Proceedings of the 5th ACM workshop on HotPlanet},
year = {2013},
isbn = {9781450321778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We welcome you to The 5th ACM HotPlanet Workshop in conjunction with ACM SIGCOMM 2013.The last decade has seen a rapid, planet-scale growth in deployment and usage of smart mobile devices, ambient sensors, smartphone applications and advanced communication technologies. nThis era has prompted for large-scale, planet-wide data collection, storage, processing and dissemination technologies, advancing our knowledge about human behaviour and interactions at a planetary scale. Evolution of such technologies and methodologies, in addition to the high investments in Internet of things (IoT) deployments in Asia, has inherently led to a number of security, privacy and ethical issues as well as new systems, networking, and application challenges.In the 5th ACM HotPlanet workshop, we bring together networking, wireless, mobile computing and systems research to understand the challenges ahead and advance the dialogue on topics related to large-scale measurements and big data analytics centred around individuals. We are delighted to present an exciting interdisciplinary program this year, including a number of cuttingedge measurement, large scale analysis, mobility, and sensing papers. Our keynote speaker, Nic Lane from Microsoft Research Asia, is delivering a vibrant keynote speech on the state-of-the-art of smartphone sensing and large-scale population guided sensing research. A 3-minute madness session for presenting novel and thought-provoking ideas is also included. To conclude, leading experts of the research community are entertaining a panel discussion on the latest and future developments in the areas of planet-scale sensing, measurement and data mining systems.We hope you will enjoy the discussions, the exciting activities, and interesting papers in the program. We wish to thank the authors for submitting excellent papers, and the Technical Program Committee for the hard job of reviewing several high quality submissions.},
location = {Hong Kong, China}
}

@proceedings{10.1145/2541608,
title = {MW4NextGen '13: Proceedings of the 8th Workshop on Middleware for Next Generation Internet Computing},
year = {2013},
isbn = {9781450325516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {While dependability and security become cornerstones of the information society, they are impaired by change, imprecision, and emerging behavior due to scale, dynamism, and heterogeneity. To address these challenges for next generation Internet computing, key extrafunctional properties should not be an "add on" or an "end to end task" anymore, but rather built in by means of Middleware.Service oriented computing, cloud computing, socio-technical systems, and Web 2.0-style applications are important steps for next generation Internet computing, but still fall short when non functional (a.k.a. extra-functional) quality properties (e.g., dependability, security, performance, and scalability) need to be addressed. The emerging Internet communication architecture (e.g., from projects on the Internet of Things, the Future Internet, etc.) also requires middleware support for delivering computing applications and services. We can see many Internet Computing systems following proprietary end-to-end solutions and being weaved with application-specific approaches. This clearly hinders re-use, which can only be successfully leveraged by Middleware-based solutions. This in turn requires new flexibility for Middleware (adaptivity, elasticity, resilience) and new ways of collaboration between Middleware and applications/services.Therefore, extra-functional quality properties need to be addressed not only by interfacing and communication standards, but also in terms of actual mechanisms, protocols, and algorithms. Some of the challenges are the administrative heterogeneity, the loose coupling between coarsegrained operations and long-running interactions, high dynamicity, and the required flexibility during run-time. Recently, massive-scale (e.g., big data, millions of participating parties in different roles) and mobility were added to the crucial challenges for Internet computing middleware. The proposed workshop consequently welcomes contributions on how specifically middleware can address the above challenges of next generation Internet computing.},
location = {Beijing, China}
}

@proceedings{10.1145/2676733,
title = {MW4NG '14: Proceedings of the 9th Workshop on Middleware for Next Generation Internet Computing},
year = {2014},
isbn = {9781450332224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {While dependability and security become cornerstones of the information society, they are impaired by change, imprecision, and emerging behavior due to scale, dynamism, and heterogeneity. To address these challenges for next generation Internet computing, key extrafunctional properties should not be an "add on" or an "end to end task" anymore, but rather built in by means of Middleware.Service oriented computing, cloud computing, socio-technical systems, and Web 2.0-style applications are important steps for next generation Internet computing, but still fall short when non functional (a.k.a. extra-functional) quality properties (e.g., dependability, security, performance, and scalability) need to be addressed. The emerging Internet communication architecture (e.g., from projects on the Internet of Things, the Future Internet, etc.) also requires middleware support for delivering computing applications and services. We can see many Internet Computing systems following proprietary end-to-end solutions and being weaved with application-specific approaches. This clearly hinders re-use, which can only be successfully leveraged by Middleware-based solutions. This in turn requires new flexibility for Middleware (adaptivity, elasticity, resilience) and new ways of collaboration between Middleware and applications/services.Therefore, extra-functional quality properties need to be addressed not only by interfacing and communication standards, but also in terms of actual mechanisms, protocols, and algorithms. Some of the challenges are the administrative heterogeneity, the loose coupling between coarsegrained operations and long-running interactions, high dynamism, and the required flexibility during run-time. Recently, massive-scale (e.g., big data, millions of participating parties in different roles) and mobility were added to the crucial challenges for Internet computing middleware. The workshop consequently comprises contributions on how specifically middleware can address the above challenges of next generation Internet computing.},
location = {Bordeaux, France}
}

@proceedings{10.1145/2491224,
title = {ICN '13: Proceedings of the 3rd ACM SIGCOMM workshop on Information-centric networking},
year = {2013},
isbn = {9781450321792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The 3rd ACM SIGCOMM Workshop on Information-Centric Networking (ICN 2013). The fundamental concept in Information-Centric Networking (ICN) is to evolve the Internet from today's host based packet delivery towards directly retrieving information objects by names in a secure, reliable, scalable, and efficient way. These architectural design efforts aim to directly address the challenges that arise from the increasing demands for highly scalable content distribution, from accelerated growths of mobile devices, from wide deployment of the Internet-of-Things (IoT), and from the need to secure the global Internet.Rapid progress has been made over the last few years, initial designs are sketched, new research challenges exposed, and prototype implementations are deployed on testbeds of various scales. The research efforts have reached a new stage that allows one to experiment with proposed architectures and to apply a proposed architectural design to address real world problems. It also becomes important to compare different design approaches and develop methodologies for architecture evaluations. Some research areas, such as routing and caching, have drawn considerable attention; some other areas, such as trust management, effective and efficient application of cryptography, experience from prototyping, and lessons from experimentations, to name a few, have yet to be fully explored.This workshop presents original contributions on Information-Centric Networking architecture topics, specific algorithms and protocols, as well as results from implementations and bexperimentation, with an emphasis on applying the new approach to address real world problems and on experimental investigations. New for this year is that the workshop includes a poster/demo session.We received a large number of submissions and as the workshop is limited in time we were only able to accept 20% of them as full papers. To promote sharing of latest results among workshop attendees, we also accepted 17% of the submissions as posters or demos.},
location = {Hong Kong, China}
}

@proceedings{10.1145/2501221,
title = {BigMine '13: Proceedings of the 2nd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
year = {2013},
isbn = {9781450323246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The aim of this workshop is to bring together people from both academia and industry to present their most recent work related to big-data issues, and exchange ideas and thoughts in order to advance this big-data challenge, which has been considered as one of the most exciting opportunities in the past 10 years.Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.},
location = {Chicago, Illinois}
}

@inproceedings{10.1145/1851322.1851324,
author = {Paul, Sanjoy},
title = {Role of mobile handhelds in redefining how we work, live and experience the world around us: some challenges and opportunities},
year = {2010},
isbn = {9781450301978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851322.1851324},
doi = {10.1145/1851322.1851324},
abstract = {Mobile handheld devices have been changing form and functionality over the last decade. It has changed from a limited-function device, with the primary function being making phone calls, to a multi-function device that morphs into a media device for watching movies, or into a remote control for home appliances, or into a wallet for making payments. In my presentation, I will argue that the range and diversity of mobile devices will continue to increase, and in addition, mobile handhelds will become extensions of other devices, such as, TV and PC, thereby enabling users to have an immersive experience while consuming services. As a result, techniques that enable cross-channel/device continuity of services will be essential to provide the right user experience leading to the importance of models, techniques and middleware infrastructures that abstract this complexity from application development and delivery perspectives. Developing intelligent mobile applications, such as, augmented reality, that optimize use of context information, such as, location, time, network connectivity, device capability, user behavior, etc. will also become important. I'll also argue that smart experience and not processing power will be the key to success for any mobile device/application. This will require new perspective on how to approach designing mobile solutions. Another viewpoint that will be covered in the presentation is that of using mobile devices as the "gateway" between the physical world and the inter-networked world. This is starting to happen with the explosion of sensors embedded in the physical world, advances in ultra low power short range RF technology and the ubiquity of mobile devices bridging the physical world with the Internet. In that context, I'll discuss how mobile devices, in conjunction with sensors, are now providing context based services to the user, managing their health, providing advisory services, and enabling the "Internet of Things". Finally, I'll talk about three categories of applications: (1) consumer, (2) enterprise and (3) industry-specific. Differences between the requirements of each category of mobile applications will be discussed and examples will be given with real working demos. Challenges in scaling and deploying mobile applications in real-world environment will be highlighted. In addition, privacy and security issues with integrating both personal (consumer) and enterprise applications on the same mobile handheld will be discussed as real business challenges. In that context, the value of end-to-end testing of mobile applications will be highlighted and potential approaches to testing will be discussed. Finally, the talk will end with a discussion around the potential of using a network of mobile handhelds as a platform for conducting geographically distributed complex applications and in future, for deploying such applications in a scalable manner.},
booktitle = {Proceedings of the Second ACM SIGCOMM Workshop on Networking, Systems, and Applications on Mobile Handhelds},
pages = {1–2},
numpages = {2},
keywords = {mobile applications},
location = {New Delhi, India},
series = {MobiHeld '10}
}

@inproceedings{10.1145/2387238.2387241,
author = {Loureiro, Antonio A.F.},
title = {Sensing, tracking and contextualizing entities in ubiquitous computing},
year = {2012},
isbn = {9781450316286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2387238.2387241},
doi = {10.1145/2387238.2387241},
abstract = {Information and Communications Technology is increasingly becoming a part of our daily activities. Nowadays, we are able to sense a broad range of entities in the world: from physical entities that comprise the Internet of Things (IoT) to social entities that include people in social networks. We can expect to have a ubiquitous sensing infrastructure in different environments, with varying needs and complexities. While these sensors/entities are mostly static, smart phones are taking center stage as the most widely adopted and ubiquitous computing device. Smart phones have enormous potential for the study of human social networks and human behavior "in vivo," in a natural context outside laboratories. Besides their computing power, smart phones are currently available with an increasing rich set of embedded sensors, such as GPS, accelerometer, microphone, camera, gyroscope and digital compass. Sensing vast areas becomes more feasible when people carrying their portable devices collect data and collaborate among themselves. Systems that enable sensed data in this way are named participatory sensing systems (PSSs). In those networks, the shared data is not limited to sensor readings passively generated by the device, but also includes proactive user observations. A challenge is how to obtain meaningful information from this large amount of data at different scales along the time. Some of the traditional techniques that have been used to process those distinct data sources are information fusion, data mining and machine learning. Notice that the consumers of such information can be, in one extreme, a large set of cooperating or non-cooperating entities, and, in the other, an individual or a "thing", for instance.Two important attributes of the sensed data are the location and time where they are collected. Localization and tracking play a key role in ubiquitous computing due to the fact that a wide variety of envisioned applications that collect sensed data rely on their ability to detect, localize and track different entities. This adds a new dimension to this problem when obtaining meaningful information from data associated to different entities. An important problem here is target tracking, which is the capacity of detecting and continuously tracking the state of a target (entity), or a set of targets. This is another fundamental problem in ubiquitous computing.Finally, the last dimension considered in this talk is context, which can be defined as any information used to characterize an entity. Context and context-awareness provide a ubiquitous computing environment with the capacity of adapting available services by autonomically deriving the entities' needs from the context they are.In this talk, we will discuss each one of these basic building blocks, and discuss how the combination of sensing, tracking and contextualization give us information to make sound decisions in ubiquitous computing that can definitely lead to useful services in areas as diverse as economics, social network, ecology, security, environment and health.},
booktitle = {Proceedings of the 15th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {3–4},
numpages = {2},
keywords = {ubiquitous computing, tracking, sensing, context},
location = {Paphos, Cyprus},
series = {MSWiM '12}
}

@inproceedings{10.1145/2068816.2068837,
author = {Li, Yuheng and Zhang, Yiping and Yuan, Ruixi},
title = {Measurement and analysis of a large scale commercial mobile internet TV system},
year = {2011},
isbn = {9781450310130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2068816.2068837},
doi = {10.1145/2068816.2068837},
abstract = {Large scale, Internet based mobile TV deployment presents both tremendous opportunities and challenges for mobile operators and technology providers. This paper presents a measurement based study on a large scale mobile TV service offering in China. Within the one month measurement period, our dataset captured over 1 million unique mobile devices and more than 49 million video sessions. Analysis showed that mobile viewing patterns are different from that of landline based IPTV and VoD systems. In particular, the average viewing time is significantly shorter, and the channel popularity distribution is more skewed towards top ranked channels than that of landline based systems. For the channel sojourn time, the distribution follows a piecewise model, which combines lognormal and pareto distribution. The lognormal part, which fits the majority of video sessions, more closely resembles the mobile phone call holding time, rather than the power law distribution in the landline IPTV case. In comparing the 3G and WiFi access methods, we found that users exhibit different behaviors when accessing from different networks. In 3G networks, where users are subject to data charge, users tend to have shorter channel sojourn time and prefer lower bit-rate channels. The parameters of the distributions are also different. Understanding these user behaviors and their implications on network traffic are critical for the success of future mobile TV industry.},
booktitle = {Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference},
pages = {209–224},
numpages = {16},
keywords = {sojourn time, mobile video, human behavior, distribution},
location = {Berlin, Germany},
series = {IMC '11}
}

@inproceedings{10.1145/2517351.2517391,
author = {Mattiacci, Daniele and Kosta, Sokol and Mei, Alessandro and Stefa, Julinda},
title = {Supporting interoperability of things in IoT systems},
year = {2013},
isbn = {9781450320276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517351.2517391},
doi = {10.1145/2517351.2517391},
abstract = {The Internet of the future will be of things: Large scale IoT systems integrating various technologies (tracking, wired and wireless sensor and actuator networks, enhanced communication protocols, distributed intelligence for smart objects) will change the way we live and interact with the environment. Unfortunately, a standardization for IoT systems that allows for integration of sensors, data, services and applications in a smooth way and for interoperability of different technologies was still missing.In this work we aim at demonstrating the benefits that an Architecture Reference Model for IoT systems brings: Integration of applications, services, and various sensors in a way that is transparent to the technology. Our demo features a health-monitoring application that runs on top of the reference model. Through the functionality of the application we show how the reference model makes it easy to readily build working systems and how it supports interoperability of system components independently on the underneath hardware technology.},
booktitle = {Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems},
articleno = {71},
numpages = {2},
location = {Roma, Italy},
series = {SenSys '13}
}

@inproceedings{10.5555/2499604.2499610,
author = {Hoang, Tu and Yang, Lan},
title = {Scalable and transparent approach to media archive using digital object architecture},
year = {2013},
isbn = {9781627480307},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The blooming age of the Internet and technology advancement in hardware and software infrastructure has introduced a major shift from text-based to media-based information, from static web pages or small text files to interactive media-rich representation. The exploding number of media files yield a challenge in storage, management and ownership; the rapid technology advancements impose potential integration and scalability problems. This paper proposes an architecture for media archival that supports scalability and transparency by adopting the components of Digital Object Architecture, a powerful open-source architecture for information management developed by Corporation for National Research Initiative. We design and deploy a small-scaled system on Windows Azure Cloud Service and fork a client application to demonstrate the interaction between clients and the system. We also mount several tests against the system to verify its functionality and measure its performance under increasing load. Experimental results demonstrate the ability of our architecture to support transparent media storage, access and management over a very long time frame, and also the ability to scale to handle large load. However, our results also indicate that the storage component yields low utilization rate due to non-optimal load-balancing mechanism.},
booktitle = {Proceedings of the 46th Annual Simulation Symposium},
articleno = {6},
numpages = {8},
keywords = {storage abstraction, scalable storage system, persistent resource naming, media archive system, digital object architecture},
location = {San Diego, California},
series = {ANSS 13}
}

@inproceedings{10.1145/2674061.2675035,
author = {Palmer, Christopher and Lazik, Patrick and Buevich, Maxim and Gao, Jingkun and Berges, Mario and Rowe, Anthony and Pereira, Ricardo Lopes and Martin, Christopher},
title = {Mortar.io: a concrete building automation system: demo abstract},
year = {2014},
isbn = {9781450331449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674061.2675035},
doi = {10.1145/2674061.2675035},
abstract = {The commoditization of wireless sensing systems makes it feasible to include BAS functionality in small and medium-sized buildings. The configuration complexity and cost of installation is now the dominant barrier to adoption. In this demo we introduce a platform called Mortar.io, which focuses on ease-of-installation, secure configuration, and management of BAS sub-systems in a manner that can scale from small to large installations. Unlike cloud-reliant systems, Mortar.io distributes storage and control functionality across end devices making it robust to network and internet outages. The system, once initialized, can run autonomously on a low-cost controller within a building or connect to the cloud for remote monitoring and configuration. We will also show our efficient multi-resolution data store that buffers data locally and replicates aggregate data across devices for reliability. A publish-subscribe model built on top of XMPP is used for messaging with per-device access control and a transducer schema. Finally, a web portal provides an interface to monitor and schedule lighting, plug-loads, environmental sensors and HVAC from a single uniform interface.},
booktitle = {Proceedings of the 1st ACM Conference on Embedded Systems for Energy-Efficient Buildings},
pages = {204–205},
numpages = {2},
location = {Memphis, Tennessee},
series = {BuildSys '14}
}

@proceedings{10.1145/2377836,
title = {WAS4FI-Mashups '12: Proceedings of the 2nd International Workshop on Adaptive Services for the Future Internet and 6th International Workshop on Web APIs and Service Mashups},
year = {2012},
isbn = {9781450315661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Future Internet and Service Mashups provide novel infrastructures linked to objects, services and things of the real world to meet the changing global needs of business and society. Thanks to them, integration of data and services worldwide distributed are more easily accessible, reachable and usable. The synergy existing among Adaptive Services for Future Internet and Web APIs and Service Mashups have provided us with the chance of celebrating these two workshops together and publishing these joined proceedings for those researches and developers interested on these supplementary fields.Future Internet has emerged as a new initiative to pave a novel infrastructure linked to objects and things of the real world to meet the changing global needs of business and society. Technologies such as those based on XML and RDF or OWL to describe data and semantic information, SOAP or REST to define protocols and BPEL or BPMN to orchestrate business processes are being increasingly used to specify Future Internet services in a standardized manner, which will allow software and data no longer to be stored and distributed on individual computers. Instead, multitenancy will enable their remote access as Software as a Service (SaaS), even by performing the integration into larger networks of communicating software (e.g., a mashup or a plug-in to a Cloud platform). Future Internet applications will have to support the interoperability between many diverse stakeholders by governing the convergence and life-cycle of Internet of Contents (IoC), Services (IoS), Things (IoT), and Networks (IoN). In this sense, there is a need for both researchers and practitioners to develop platforms made up of adaptive Future Internet applications. Hence, the emergence and consolidation of Service-Oriented Architectures (SOA), Cloud Computing and Wireless Sensor Networks (WSN) rise benefits, such as flexibility, scalability, security, interoperability, and adaptability, for building these applications.},
location = {Bertinoro, Italy}
}

@inproceedings{10.1145/2647868.2654889,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2654889},
doi = {10.1145/2647868.2654889},
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {675–678},
numpages = {4},
keywords = {parallel computation, open source, neural networks, machine learning, computer vision},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.1145/2160601.2160610,
author = {Mahla, Arvind and Martin, Deepak and Ahuja, Ishani and Niyaz, Quamar and Seth, Aaditeshwar},
title = {Motivation and design of a content distribution architecture for rural areas},
year = {2012},
isbn = {9781450312622},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2160601.2160610},
doi = {10.1145/2160601.2160610},
abstract = {Decreasing prices of digital cameras, phones and video cameras have made these devices accessible to low-income communities and to development organizations working in these communities. As a result, the social media revolution around user generated content on YouTube and Facebook has found equivalents in rural areas; here, community radio, community video, and interactive voice based systems are used for social media instead of Internet websites to host user generated content. We observe that there is much benefit to be gained from sharing this content across different rural locations, but it is hard due to unavailable or flaky Internet access in these areas. In this paper, we analyze a 1000+ video content production and consumption dataset from a nonprofit organization that specializes in participatory video production about agricultural best practices, and observe that solutions to connectivity in rural areas could greatly benefit from caching of content since much production and consumption tends to be local. Based on this insight, we propose a delay tolerant network architecture for content distribution, that recognizes content objects as first class entities cachable at different nodes in the network, and uses an always-on control channel on GPRS/EDGE connections to assist in the routing of data. Finally, we simulate the dataset in accordance with our architectural design to study the performance of different routing and caching algorithms in terms of delivery latency and other metrics for 6 different districts. We find that our proposed architecture is suitable to provide content distribution services with minimal investments in IT infrastructure, and we plan to do a small-scale field deployment shortly.},
booktitle = {Proceedings of the 2nd ACM Symposium on Computing for Development},
articleno = {6},
numpages = {10},
location = {Atlanta, Georgia},
series = {ACM DEV '12}
}

@inproceedings{10.1145/2661714.2661726,
author = {Stohr, Denny and Wilk, Stefan and Effelsberg, Wolfgang},
title = {Monitoring of User Generated Video Broadcasting Services},
year = {2014},
isbn = {9781450331579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661714.2661726},
doi = {10.1145/2661714.2661726},
abstract = {Mobile video broadcasting services offer users the opportunity to instantly share content from their mobile handhelds to a large audience over the Internet. However, existing data caps in cellular network contracts and limitations in their upload capabilities restrict the adoption of mobile video broadcasting services. Additionally, the quality of those video streams is often reduced by the lack of skills of recording users and the technical limitations of the video capturing devices. Our research focuses on large-scale events that attract dozens of users to record video in parallel. In many cases, available network infrastructure is not capable to upload all video streams in parallel. To make decisions on how to appropriately transmit those video streams, a suitable monitoring of the video generation process is required. For this scenario, a measurement framework is proposed that allows Internet-scale mobile broadcasting services to deliver samples in an optimized way. Our framework architecture analyzes three zones for effectively monitoring user-generated video. Besides classical Quality of Service metrics on the network state, video quality indicators and additional auxiliary sensor information is gathered. Aim of this framework is an efficient coordination of devices and their uploads based on the currently observed system state.},
booktitle = {Proceedings of the First International Workshop on Internet-Scale Multimedia Management},
pages = {39–42},
numpages = {4},
keywords = {video composition, video broadcast, network monitoring, mobile, mix, measurement, cellular networks},
location = {Orlando, Florida, USA},
series = {WISMM '14}
}

@inproceedings{10.1145/2508075.2508464,
author = {Sousa, Tiago Boldt},
title = {Sensors, actuators and services: a distributed approach},
year = {2013},
isbn = {9781450319959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508075.2508464},
doi = {10.1145/2508075.2508464},
abstract = {Proliferation of the Internet is enabling the use of sensors and actuators to capture data and control devices remotely in a multitude of domains. Still, there is a general lack of best practices while designing such large scale real-time systems. This paper describes a generic architecture used on the implementation of a framework for deploying such systems in the cloud, enabling run-time evolution of the system with new sensors, actuators or services possibly developed by third-parties being integrated dynamically. Such architecture orchestrates the flow of information in the ecosystem and scales transparently to external components when needed, requiring no change in them. Adoption in the Portuguese nation-wide AAL project AAL4ALL is then described.},
booktitle = {Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, &amp; Applications: Software for Humanity},
pages = {161–166},
numpages = {6},
keywords = {software engineering, software architecture, interoperability, distributed systems},
location = {Indianapolis, Indiana, USA},
series = {SPLASH '13}
}

@inproceedings{10.1145/1995896.1995926,
author = {Polfliet, Stijn and Ryckbosch, Frederick and Eeckhout, Lieven},
title = {Optimizing the datacenter for data-centric workloads},
year = {2011},
isbn = {9781450301022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995896.1995926},
doi = {10.1145/1995896.1995926},
abstract = {The amount of data produced on the internet is growing rapidly. Along with data explosion comes the trend towards more and more diverse data, including rich media such as audio and video. Data explosion and diversity leads to the emergence of data-centric workloads to manipulate, manage and analyze the vast amounts of data. These data-centric workloads are likely to run in the background and include application domains such as data mining, indexing, compression, encryption, audio/video manipulation, data warehousing, etc.Given that datacenters are very much cost sensitive, reducing the cost of a single component by a small fraction immediately translates into huge cost savings because of the large scale. Hence, when designing a datacenter, it is important to understand data-centric workloads and optimize the ensemble for these workloads so that the best possible performance per dollar is achieved.This paper studies how the emerging class of data-centric workloads affects design decisions in the datacenter. Through the architectural simulation of minutes of run time on a validated full-system x86 simulator, we derive the insight that for some data-centric workloads, a high-end server optimizes performance per total cost of ownership (TCO), whereas for other workloads, a low-end server is the winner. This observation suggests heterogeneity in the datacenter, in which a job is run on the most cost-efficient server. Our experimental results report that a heterogeneous datacenter achieves an up to 88%, 24% and 17% improvement in cost-efficiency over a homogeneous high-end, commodity and low-end server datacenter, respectively.},
booktitle = {Proceedings of the International Conference on Supercomputing},
pages = {182–191},
numpages = {10},
keywords = {workload characterization, heterogeneity, datacenter, data-centric workloads},
location = {Tucson, Arizona, USA},
series = {ICS '11}
}

@article{10.1145/1629175.1629209,
author = {Witman, Paul D. and Ryan, Terry},
title = {Think big for reuse},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1629175.1629209},
doi = {10.1145/1629175.1629209},
abstract = {IntroductionMany organizations are successful with software reuse at fine to medium granularities -- ranging from objects, subroutines, and components through software product lines. However, relatively little has been published on very large-grained reuse. One example of this type of large-grained reuse might be that of an entire Internet banking system (applications and infrastructure) reused in business units all over the world. In contrast, "large scale" software reuse in current research generally refers to systems that reuse a large number of smaller components, or that perhaps reuse subsystems. In this article, we explore a case of an organization with an internal development group that has been very successful with large-grained software reuse.BigFinancial, and the BigFinancial Technology Center (BTC) in particular, have created a number of software systems that have been reused in multiple businesses and in multiple countries. BigFinancial and BTC thus provided a rich source of data for case studies to look at the characteristics of those projects and why they have been successful, as well as to look at projects that have been less successful and to understand what has caused those results and what might be done differently to prevent issues in the future. The research is focused on technology, process, and organizational elements of the development process, rather than on specific product features and functions.Supporting reuse at a large-grained level may help to alleviate some of the issues that occur in more traditional reuse programs, which tend to be finer-grained. In particular, because BigFinancial was trying to gain commonality in business processes and operating models, reuse of large-grained components was more closely aligned with its business goals. This same effect may well not have happened with finer-grained reuse, due to the continued ability of business units to more readily pick and choose components for reuse.BTC is a technology development unit of BigFinancial, with operations in both the eastern and western US. Approximately 500 people are employed by BTC, reporting ultimately through a single line manager responsible to the Global Retail Business unit head of BigFinancial. BTC is organized to deliver both products and infrastructure components to BigFinancial, and its product line has through the years included consumer Internet banking services, teller systems, ATM software, and network management tools. BigFinancial has its U.S. operations headquartered in the eastern U.S., and employs more than 8,000 technologists worldwide.In cooperation with BTC, we selected three cases for further study from a pool of about 25. These cases were the Java Banking Toolkit (JBT) and its related application systems, the Worldwide Single Signon (WSSO) subsystem, and the BigFinancial Message Switch (BMS).},
journal = {Commun. ACM},
month = {jan},
pages = {142–147},
numpages = {6}
}

@inproceedings{10.1145/1930286.1930300,
author = {Limsaiprom, Prajit and Tantatsanawong, Panjai},
title = {Visualization of information diffusion model in future internet},
year = {2010},
isbn = {9781450304016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1930286.1930300},
doi = {10.1145/1930286.1930300},
abstract = {The Internet is radically changing the way from web 1.0, web 2.0 to web 3.0 in recent years and a new type of communications is on the rise, its call social network such as Facebook, Myspace, Skype and so on. There are many threats to an open Internet today. While the technical development of the Internet today has concentrated the awareness of several critical shortcomings in terms of performance, reliability, scalability, security and many other categories including societal, economical and business aspects. The rise of the Future Internet accelerates the creation of various large-scale social networks and considerable attention has been brought to social networks as an important medium for the information diffusion model, which could be used to describe the relationships and activities between human beings. The visualization of information diffusion model for Social Network in Future Internet is focused in this research. This research uses social network analysis (SNA) to analyze the key factors influencing information diffusion model about density, centrality and the cohesive subgroup, reveals useful insights which are the relationships and activities between human. The preprocessing with partitioning large-scale social networks is a clustering of the vertices in the network such that each vertex is assigned to exactly one cluster. The partitions store discrete characteristics of vertices. This research also presents each partitioning social network with visualization to show the information diffusion patterns of social network. They are applied as a guide to further investigation of social network behaviors to improve the security model and monitoring the risk for social networking in Future Internet.},
booktitle = {Proceedings of the 6th Asian Internet Engineering Conference},
pages = {103–110},
numpages = {8},
keywords = {visualization, social network analysis, social network, security model, information diffusion model, cluster},
location = {Bangkok, Thailand},
series = {AINTEC '10}
}

@article{10.1145/2543698.2543704,
author = {Gracanin, Denis and Zhang, Xiaoyu},
title = {Caffe Neve: A Podcasting-Capable Framework for Crearing Flexible and Extensible 3D Applications},
year = {2014},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
url = {https://doi.org/10.1145/2543698.2543704},
doi = {10.1145/2543698.2543704},
abstract = {The popularization of podcasting provides Internet users more opportunities to access interesting and entertaining multimedia content. The podcasting content hosting sites became the shared platforms for all Internet users to access third-party contributions. More and more Internet users are encouraged to become active content providers that share their resources and multimedia productions. Building a large scale multi-user 3D application can benefit from the same content distribution pattern as the development cost can be distributed if the application implementation is outsourced. The application is constructed by integrating 3D content from various third-party service providers. In addition, the implementation outsourcing can inspire contributors to provide creative application content. Unfortunately, sharing and distributing 3D resources are more challenging and the podcasting framework cannot resolve the problems effectively.We introduce a standardized framework that supports the seamless integration of distributed 3D resources as coarse-grained components of constructed Distributed Virtual Environments (DVEs). Component Application Framework For Extensible NEtworked Virtual Environments (Caffe Neve) uses service-oriented architecture (SOA) and incorporates streaming technology for dynamic content delivery. Using Caffe Neve, third-party developers can create distributed services to stream 3D content. The framework provides an application integrator that constructs a shared virtual environment and delivers the application content to the end users.In this paper we describe the framework details. The application integration infrastructure of the framework is designed based on a distributed Model-View-Controller model. The framework uses an ontology to support the semantic level content integration and service coordination. In addition, the framework incorporates streaming within the architecture for the real-time data delivery and improved performance. By deploying services within the framework, 3D resource owners can conveniently podcast or broadcast their 3D content to the end users. In the end, we demonstrate how to use the framework to construct 3D podcast and broadcast applications. Regardless of the type of 3D applications and delay tolerance, performance is important and has to be carefully studied. We evaluate the framework performance by examining the sample application and present the analysis results. We also present the experiment results on the simulated framework application in large scale by using simulation tools.},
journal = {Comput. Entertain.},
month = {apr},
articleno = {5},
numpages = {30},
keywords = {distributed virtual environments, X3D, 3D podcasting}
}

@inproceedings{10.1145/2391229.2391240,
author = {Palasamudram, Darshan S. and Sitaraman, Ramesh K. and Urgaonkar, Bhuvan and Urgaonkar, Rahul},
title = {Using batteries to reduce the power costs of internet-scale distributed networks},
year = {2012},
isbn = {9781450317610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2391229.2391240},
doi = {10.1145/2391229.2391240},
abstract = {Modern Internet-scale distributed networks have hundreds of thousands of servers deployed in hundreds of locations and networks around the world. Canonical examples of such networks are content delivery networks (called CDNs) that we study in this paper. The operating expenses of large distributed networks are increasingly driven by the cost of supplying power to their servers. Typically, CDNs procure power through long-term contracts from co-location providers and pay on the basis of the power (KWs) provisioned for them, rather than on the basis of the energy (KWHs) actually consumed. We propose the use of batteries to reduce both the required power supply and the incurred power cost of a CDN. We provide a theoretical model and an algorithmic framework for provisioning batteries to minimize the total power supply and the total power costs of a CDN. We evaluate our battery provisioning algorithms using extensive load traces derived from Akamai's CDN to empirically study the achievable benefits. We show that batteries can provide up to 14% power savings, that would increase to 22% for more power-proportional next-generation servers, and would increase even more to 35.3% for perfectly power-proportional servers. Likewise, the cost savings, inclusive of the additional battery costs, range from 13.26% to 33.8% as servers become more power-proportional. Further, much of these savings can be achieved with a small cycle rate of one full discharge/charge cycle every three days that is conducive to satisfactory battery lifetimes. In summary, we show that a CDN can utilize batteries to significantly reduce both the total supplied power and the total power costs, thereby establishing batteries as a key element in future distributed network architecture. While we use the canonical example of a CDN, our results also apply to other similar Internet-scale distributed networks.},
booktitle = {Proceedings of the Third ACM Symposium on Cloud Computing},
articleno = {11},
numpages = {14},
keywords = {network architecture, internet content delivery, energy storage, energy efficiency, cloud computing},
location = {San Jose, California},
series = {SoCC '12}
}

@inproceedings{10.1145/2513228.2513287,
author = {Paul, Anand},
title = {Graph based M2M optimization in an IoT environment},
year = {2013},
isbn = {9781450323482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513228.2513287},
doi = {10.1145/2513228.2513287},
abstract = {In this paper, a graph based M2M optimization in an IoT environment is presented. A parallel reconfigurable M2M architecture with multiple dynamic reconfigurable unit in an IoT scenario is modeled using a directed acyclic graph (DAG) to represent the whole environment. Parallel M2M establish communication within the network and are partitioned and reconfigured dynamically for large scale network such as IoT. Simulation were performed for multiple M2M array for different state, timing and power consumption along with the scheduling scheme are considered.},
booktitle = {Proceedings of the 2013 Research in Adaptive and Convergent Systems},
pages = {45–46},
numpages = {2},
keywords = {network, graphs model, M2M, IoT, DAG (directed acyclic graph)},
location = {Montreal, Quebec, Canada},
series = {RACS '13}
}

@inproceedings{10.1145/2072298.2071959,
author = {Sengamedu, Srinivasan H. and Sanyal, Subhajit and Satish, Sriram},
title = {Detection of pornographic content in internet images},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071959},
doi = {10.1145/2072298.2071959},
abstract = {Pornographic image detection is an important and challenging problem. Detection of pornography on the Internet is even more challenging because of the scale (billions of images) and diversity (small to very large images, graphic, grey scale images, etc.) of image content. The performance requirements (precision, recall, and speed) are also very stringent. Because of this, no single technique provides the required performance. In this paper, we describe a framework for detecting images with pornographic content. The framework combines various techniques based on object-level and pixel-level analysis of image content. To enable high-precision, we detect body parts (including faces) in images. For high-recall, low-level techniques like color and texture features are used. For adaptation to new datasets, we also support learning of appropriate color models from weakly-labeled datasets. In addition to image-based analysis, both text-based and site-level analysis are performed. Unlike many adult detection techniques, we explicitly leverage techniques like texture analysis and face detection for non-adult content identification. The multiple cues are combined in a systematic manner using ROC analysis and boosting. Evaluations on real world web data indicate that the system has the best performance among the systems compared.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1141–1144},
numpages = {4},
keywords = {body part detection, adult image detection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2502081.2502127,
author = {Huang, Junshi and Liu, Hairong and Shen, Jialie and Yan, Shuicheng},
title = {Towards efficient sparse coding for scalable image annotation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502127},
doi = {10.1145/2502081.2502127},
abstract = {Nowadays, content-based retrieval methods are still the development trend of the traditional retrieval systems. Image labels, as one of the most popular approaches for the semantic representation of images, can fully capture the representative information of images. To achieve the high performance of retrieval systems, the precise annotation for images becomes inevitable. However, as the massive number of images in the Internet, one cannot annotate all the images without a scalable and flexible (i.e., training-free) annotation method. In this paper, we particularly investigate the problem of accelerating sparse coding based scalable image annotation, whose off-the-shelf solvers are generally inefficient on large-scale dataset. By leveraging the prior that most reconstruction coefficients should be zero, we develop a general and efficient framework to derive an accurate solution to the large-scale sparse coding problem through solving a series of much smaller-scale subproblems. In this framework, an active variable set, which expands and shrinks iteratively, is maintained, with each snapshot of the active variable set corresponding to a subproblem. Meanwhile, the convergence of our proposed framework to global optimum is theoretically provable. To further accelerate the proposed framework, a sub-linear time complexity hashing strategy, e.g. Locality-Sensitive Hashing, is seamlessly integrated into our framework. Extensive empirical experiments on NUS-WIDE and IMAGENET datasets demonstrate that the orders-of-magnitude acceleration is achieved by the proposed framework for large-scale image annotation, along with zero/negligible accuracy loss for the cases without/with hashing speed-up, compared to the expensive off-the-shelf solvers.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {947–956},
numpages = {10},
keywords = {sparse coding, large-scale image annotation, hash-accelerated sparsity induced scalable optimization},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1007/978-3-642-25821-3_2,
author = {Xu, Tianyin and Chen, Yang and Jiao, Lei and Zhao, Ben Y. and Hui, Pan and Fu, Xiaoming},
title = {Scaling microblogging services with divergent traffic demands},
year = {2011},
isbn = {9783642258206},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25821-3_2},
doi = {10.1007/978-3-642-25821-3_2},
abstract = {Today's microblogging services such as Twitter have long outgrown their initial designs as SMS-based social networks. Instead, a massive and steadily-growing user population of more than 100 million is using Twitter for everything from capturing the mood of the country to detecting earthquakes and Internet service failures. It is unsurprising that the traditional centralized client-server architecture has not scaled with user demands, leading to server overload and significant impairment of availability. In this paper, we argue that the divergence in usage models of microblogging services can be best addressed using complementary mechanisms, one that provides reliable messages between friends, and another that delivers events from popular celebrities and media outlets to their thousands or even millions of followers. We present &lt;em&gt;Cuckoo&lt;/em&gt; , a new microblogging system that offloads processing and bandwidth costs away from a small centralized server base while ensuring reliable message delivery. We use a 20-day Twitter availability measurement to guide our design, and trace-driven emulation of 30,000 Twitter users to evaluate our Cuckoo prototype. Compared to the centralized approach, Cuckoo achieves 30-50% server bandwidth savings and 50-60% CPU load reduction, while guaranteeing reliable message delivery.},
booktitle = {Proceedings of the 12th ACM/IFIP/USENIX International Conference on Middleware},
pages = {20–40},
numpages = {21},
location = {Lisbon, Portugal},
series = {Middleware'11}
}

@article{10.1145/2693193.2693195,
author = {Neville-Neil, George},
title = {Too Big to Fail: Visibility leads to debuggability.},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {11},
issn = {1542-7730},
url = {https://doi.org/10.1145/2693193.2693195},
doi = {10.1145/2693193.2693195},
abstract = {Our project has been rolling out a well-known, distributed key/value store onto our infrastructure, and we’ve been surprised - more than once - when a simple increase in the number of clients has not only slowed things, but brought them to a complete halt. This then results in rollback while several of us scour the online forums to figure out if anyone else has seen the same problem. The entire reason for using this project’s software is to increase the scale of a large system, so I have been surprised at how many times a small increase in load has led to a complete failure. Is there something about scaling systems that’s so difficult that these systems become fragile, even at a modest scale?},
journal = {Queue},
month = {nov},
pages = {10–12},
numpages = {3}
}

@inproceedings{10.1145/2020408.2020528,
author = {Jin, Xin and Wang, Chi and Luo, Jiebo and Yu, Xiao and Han, Jiawei},
title = {LikeMiner: a system for mining the power of 'like' in social media networks},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020528},
doi = {10.1145/2020408.2020528},
abstract = {Social media is becoming increasingly ubiquitous and popular on the Internet. Due to the huge popularity of social media websites, such as Facebook, Twitter, YouTube and Flickr, many companies or public figures are now active in maintaining pages on those websites to interact with online users, attracting a large number of fans/followers by posting interesting objects, e.g., (product) photos/videos and text messages. 'Like' has now become a very popular social function by allowing users to express their like of certain objects. It provides an accurate way of estimating user interests and an effective way of sharing/promoting information in social media. In this demo, we propose a system called LikeMiner to mine the power of 'like' in social media networks. We introduce a heterogeneous network model for social media with 'likes', and propose 'like' mining algorithms to estimate representativeness and influence of objects. The implemented prototype system demonstrates the effectiveness of the proposed approach using the large scale Facebook data.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {753–756},
numpages = {4},
keywords = {social media, recommendation, ranking, like, information network, influence analysis, data mining},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{10.1145/2107736.2107738,
author = {Narayanan, Vijay and Bhandarkar, Milind},
title = {Modeling with Hadoop},
year = {2011},
isbn = {9781450312011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2107736.2107738},
doi = {10.1145/2107736.2107738},
abstract = {Apache Hadoop has become the platform of choice for developing large-scale data-intensive applications. In this tutorial, we will discuss the design philosophy and architecture of Hadoop, describe how to design and develop Hadoop applications and higher-level application frameworks to crunch several terabytes of data, and describe some uses of Hadoop for practical data mining and modeling applications. We will describe how to run some common data mining algorithms on Hadoop, provide examples of large scale model training and scoring systems in the internet domain, and a demonstration of a simple modeling task end to end.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference Tutorials},
articleno = {2},
numpages = {1},
location = {San Diego, California},
series = {KDD '11 Tutorials}
}

@inproceedings{10.1145/2393347.2393437,
author = {Roy, Suman Deb and Mei, Tao and Zeng, Wenjun and Li, Shipeng},
title = {SocialTransfer: cross-domain transfer learning from social streams for media applications},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393437},
doi = {10.1145/2393347.2393437},
abstract = {The usage and applications of social media have become pervasive. This has enabled an innovative paradigm to solve multimedia problems (e.g., recommendation and popularity prediction), which are otherwise hard to address purely by traditional approaches. In this paper, we investigate how to build a mutual connection among the disparate social media on the Internet, using which cross-domain media recommendation can be realized. We accomplish this goal through SocialTransfer---a novel cross-domain real-time transfer learning framework. While existing transfer learning methods do not address how to utilize the real time social streams, our proposed SocialTransfer is able to effectively learn from social streams to help multimedia applications, assuming an intermediate topic space can be built across domains. It is characterized by two key components: 1) a topic space learned in real time from social streams via Online Streaming Latent Dirichlet Allocation (OSLDA), and 2) a real-time cross-domain graph spectra analysis based transfer learning method that seamlessly incorporates learned topic models from social streams into the transfer learning framework. We present as use cases of emph{SocialTransfer} two video recommendation applications that otherwise can hardly be achieved by conventional media analysis techniques: 1) socialized query suggestion for video search, and 2) socialized video recommendation that features socially trending topical videos. We conduct experiments on a real-world large-scale dataset, including 10.2 million tweets and 5.7 million YouTube videos and show that emph{SocialTransfer} outperforms traditional learners significantly, and plays a natural and interoperable connection across video and social domains, leading to a wide variety of cross-domain applications.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {649–658},
numpages = {10},
keywords = {transfer learning, social media, recommendation, cross-domain media retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2187980.2188059,
author = {Pintus, Antonio and Carboni, Davide and Piras, Andrea},
title = {Paraimpu: a platform for a social web of things},
year = {2012},
isbn = {9781450312301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187980.2188059},
doi = {10.1145/2187980.2188059},
abstract = {The Web of Things is a scenario where potentially billions of connected smart objects communicate using the Web protocols, HTTP in primis. A Web of Things envisioning and design has raised several research issues, from protocols adoption and communication models to architectural styles and social aspects facing. In this demo we present the prototype of a scalable architecture for a large scale social Web of Things for smart objects and services, named Paraimpu. It is a Web-based platform which allows to add, use, share and inter-connect real HTTP-enabled smart objects and "virtual" things like services on the Web and social networks. Paraimpu defines and uses few strong abstractions, in order to allow mash-ups of heterogeneous things introducing powerful rules for data adaptation. Adding and inter-connecting objects is supported through user friendly models and features.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {401–404},
numpages = {4},
keywords = {web of things, social networks, rest},
location = {Lyon, France},
series = {WWW '12 Companion}
}

@inproceedings{10.1145/2480362.2480484,
author = {Nikolaevskiy, Ilya and Lukyanenko, Andrey and Polishchuk, Tatiana and Polishchuk, Valentin and Gurtov, Andrei},
title = {isBF: scalable in-packet bloom filter based multicast},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480484},
doi = {10.1145/2480362.2480484},
abstract = {Bloom filter based forwarding was proposed recently in several protocol alternatives to IP multicast. Even though some of these protocols avoid the state in intermediate routers, they still have scalability limitations and require explicit network management as well as non-trivial functionality from the network components.In this work we propose an architecture based on in-packet Bloom filter forwarding, in which the burden of scalability management is left to the multicast source and end-hosts. We present several algorithms to improve the scalability of multicast transmission and evaluate them in a real Internet topology. Our evaluation confirms the ability of the proposed stateless design to save up to 70% of traffic volume in the large scale topology for big groups of subscribers, and up to 30% for small groups.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {646–648},
numpages = {3},
keywords = {multicast, internet, in-packet bloom filters, architecture},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.1145/2396761.2398532,
author = {Lu, Yao and Zhang, Wei and Zhang, Ke and Xue, Xiangyang},
title = {Semantic context learning with large-scale weakly-labeled image set},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398532},
doi = {10.1145/2396761.2398532},
abstract = {There are a large number of images available on the web; meanwhile, only a subset of web images can be labeled by professionals because manual annotation is time-consuming and labor-intensive. Although we can now use the collaborative image tagging system, e.g., Flickr, to get a lot of tagged images provided by Internet users, these labels may be incorrect or incomplete. Furthermore, semantics richness requires more than one label to describe one image in real applications, and multiple labels usually interact with each other in semantic space. It is of significance to learn semantic context with large-scale weakly-labeled image set in the task of multi-label annotation. In this paper, we develop a novel method to learn semantic context and predict the labels of web images in a semi-supervised framework. To address the scalability issue, a small number of exemplar images are first obtained to cover the whole data cloud; then the label vector of each image is estimated as a local combination of the exemplar label vectors. Visual context, semantic context, and neighborhood consistency in both visual and semantic spaces are sufficiently leveraged in the proposed framework. Finally, the semantic context and the label confidence vectors for exemplar images are both learned in an iterative way. Experimental results on the real-world image dataset demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {1859–1863},
numpages = {5},
keywords = {weakly labeled, semantic context, large scale, image annotation},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1145/2430475.2430488,
author = {Zhu, Jiaxin and Lin, Hongwu and Zhou, Minghui and Mei, Hong},
title = {Review code evolution history in OSS universe},
year = {2012},
isbn = {9781450318884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430475.2430488},
doi = {10.1145/2430475.2430488},
abstract = {Software evolves all the time because of the changing requirements, in particular, in the diverse Internet environment. Evolution history recorded in software repositories, e.g., Version Control Systems, reflects people's software development practice. Exploring this history could help practitioners to reuse the best practices therefore improve productivity and software quality. Because of the difficulty of collecting and standardizing data, most existing work could only utilize small project set. In this study, we target the open source software universe to build a universal code evolution model for large-scale data. We consider code evolution from two aspects: code version changing history in a single project and code reuse history in the whole universe. In the model, files/modules are built as nodes, and relations (version change or reuse) between files/modules are built as connections. Based on the model, we design and implement a code evolution review framework, i.e., Code Evolution Reviewer (CER), which provides a series of data interfaces to review code evolution history, in particular, code version changing in single project and code reuse among projects. Further, CER could be utilized to explore best practices across large-scale project set.},
booktitle = {Proceedings of the Fourth Asia-Pacific Symposium on Internetware},
articleno = {13},
numpages = {4},
keywords = {code reuse, code evolution, best practice, OSS universe},
location = {Qingdao, China},
series = {Internetware '12}
}

@inproceedings{10.1145/1807128.1807147,
author = {Kiciman, Emre and Livshits, Benjamin and Musuvathi, Madanlal and Webb, Kevin C.},
title = {Fluxo: a system for internet service programming by non-expert developers},
year = {2010},
isbn = {9781450300360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807128.1807147},
doi = {10.1145/1807128.1807147},
abstract = {Over the last 10-15 years, our industry has developed and deployed many large-scale Internet services, from e-commerce to social networking sites, all facing common challenges in latency, reliability, and scalability. Over time, a relatively small number of architectural patterns have emerged to address these challenges, such as tiering, caching, partitioning, and pre- or post-processing compute intensive tasks. Unfortunately, following these patterns requires developers to have a deep understanding of the trade-offs involved in these patterns as well as an end-to-end understanding of their own system and its expected workloads. The result is that non-expert developers have a hard time applying these patterns in their code, leading to low-performing, highly suboptimal applications.In this paper, we propose FLUXO, a system that separates an Internet service's logical functionality from the architectural decisions made to support performance, scalability, and reliability. FLUXO achieves this separation through the use of a restricted programming language designed 1) to limit a developer's ability to write programs that are incompatible with widely used Internet service architectural patterns; and 2) to simplify the analysis needed to identify how architectural patterns should be applied to programs. Because architectural patterns are often highly dependent on application performance, workloads and data distributions, our platform captures such data as a runtime profile of the application and makes it available for use when determining how to apply architectural patterns. This separation makes service development accessible to non-experts by allowing them to focus on application features and leaving complicated architectural optimizations to experts writing application-agnostic, profile-guided optimization tools.To evaluate FLUXO, we show how a variety of architectural patterns can be expressed as transformations applied to FLUXO programs. Even simple heuristics for automatically applying these optimizations can show reductions in latency ranging from 20-90% without requiring special effort from the application developer. We also demonstrate how a simple shared-nothing tiering and replication pattern is able to scale our test suite, a web-based IM, email, and addressbook application.},
booktitle = {Proceedings of the 1st ACM Symposium on Cloud Computing},
pages = {107–118},
numpages = {12},
keywords = {languages for datacenter programming, compiler optimizations},
location = {Indianapolis, Indiana, USA},
series = {SoCC '10}
}

@article{10.1145/2523001.2523005,
author = {Wang, Zhi and Zhu, Wenwu and Chen, Xiangwen and Sun, Lifeng and Liu, Jiangchuan and Chen, Minghua and Cui, Peng and Yang, Shiqiang},
title = {Propagation-based social-aware multimedia content distribution},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2523001.2523005},
doi = {10.1145/2523001.2523005},
abstract = {Online social networks have reshaped how multimedia contents are generated, distributed, and consumed on today's Internet. Given the massive number of user-generated contents shared in online social networks, users are moving to directly access these contents in their preferred social network services. It is intriguing to study the service provision of social contents for global users with satisfactory quality of experience. In this article, we conduct large-scale measurement of a real-world online social network system to study the social content propagation. We have observed important propagation patterns, including social locality, geographical locality, and temporal locality. Motivated by the measurement insights, we propose a propagation-based social-aware delivery framework using a hybrid edge-cloud and peer-assisted architecture. We also design replication strategies for the architecture based on three propagation predictors designed by jointly considering user, content, and context information. In particular, we design a propagation region predictor and a global audience predictor to guide how the edge-cloud servers backup the contents, and a local audience predictor to guide how peers cache the contents for their friends. Our trace-driven experiments further demonstrate the effectiveness and superiority of our design.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {52},
numpages = {20},
keywords = {video service, Social network}
}

@article{10.1145/1880153.1880155,
author = {Gyarmati, L\'{a}szl\'{o} and Trinh, Tuan Anh},
title = {Scafida: a scale-free network inspired data center architecture},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/1880153.1880155},
doi = {10.1145/1880153.1880155},
abstract = {Data centers have a crucial role in current Internet architecture supporting content-centric networking. State-of-the-art data centers have different architectures like fat-tree, DCell, or BCube. However, their architectures share a common property: symmetry. Due to their symmetric nature, a tricky point with these architectures is that they are hard to be extended in small quantities. Contrary to state-of-the-art data center architectures, we propose an asymmetric data center topology generation method called Scafida inspired by scale-free networks; these data centers have not only small diameters and high fault tolerance, inherited by scale-free networks, but can also be scaled in smaller and less homogenous increments. We extend the original scale-free network generation algorithm of Barabasi and Albert to meet the physical constraints of switches and routers. Despite the fact that our method artificially limits the node degrees in the network, our data center architectures keep the preferable properties of scale-free networks. Based on extensive simulations we present preliminary results that are promising regarding the error tolerance, scalability, and flexibility of the architecture.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {oct},
pages = {4–12},
numpages = {9},
keywords = {scale-free network, data center}
}

@inproceedings{10.5555/2043527.2043531,
author = {Pehkonen, Vesa and Reittu, Hannu},
title = {Szemer\'{e}di-type clustering of peer-to-peer streaming system},
year = {2011},
isbn = {9780983628316},
publisher = {International Teletraffic Congress},
abstract = {In this work we made a preliminary clustering analysis of an experimental peer-to-peer system, tested in a small scale experiments within PlanetLab. The application was an Internet-TV like streaming system based on Chord architecture. Our clustering is inspired by the Szemer\'{e}di's Regularity Lemma (SzRL). Such approach was already demonstrated in biology and appeared to be a powerful tool. Szemer\'{e}di's result suggests that the nodes of a large enough graph can be partitioned in few clusters in such a way that link distribution between most of the pairs look like random. Our main goal is to study what can this type of clustering tell us about p2p systems using our experimental system as source of data. We searched clusterings of Szemer\'{e}di-type by using max likelihood as guidance. Our graph is directed and weighted. The link direction indicates a client-server relation and the value is the proportion of all chunks obtained from such a link during the whole experiment. We think that the preliminary results are interesting. Most of the cluster pairs have very distinguished patterns of link distribution, indicating that such a novel approach has potential in classifying peers effectively. The values of weights between clusters and their distribution show some apparent patterns. We end up with 9 cluster pairs.Contributions: practical implementations of streaming system by V. P. and analysis by H. R.},
booktitle = {Proceedings of the 2011 International Workshop on Modeling, Analysis, and Control of Complex Networks},
pages = {23–30},
numpages = {8},
location = {San Francisco, California},
series = {Cnet '11}
}

@inproceedings{10.1145/2499788.2499832,
author = {Zhang, Lumin and Pei, Shaojie and Deng, Lei and Han, Yi and Zhao, Jinhui and Hong, Feng},
title = {Microblog sentiment analysis based on emoticon networks model},
year = {2013},
isbn = {9781450322522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499788.2499832},
doi = {10.1145/2499788.2499832},
abstract = {With the repaid development of Internet and communication technologies, microblog has become a valuable social media for public sentiment analysis. Emoticons, strongly associated with subjectivity and sentiments, are also increasing popular for users to directly express their feelings, emotions and moods in microblog platforms. In this paper, we address the problem of public sentiment analysis by leveraging emoticons, and develop emoticon networks approaches. Based on large-scale corpus, we use FP-growth algorithm combining with retrieve distance to aggregate similar emoticons, and build emoticon networks model based on Mutual Information. Then, we propose a microblog orientation analysis framework for both emoticon messages and non-emoticon messages. Experimental evaluations show that our approach could perform effectively for microblog sentiment analysis. Although we worked with Chinese in our research, the technique can be used with any other language.},
booktitle = {Proceedings of the Fifth International Conference on Internet Multimedia Computing and Service},
pages = {134–138},
numpages = {5},
keywords = {sentiment analysis, opinion mining, microblog, large-scale multimedia, emoticon networks},
location = {Huangshan, China},
series = {ICIMCS '13}
}

@inproceedings{10.1145/1877911.1877920,
author = {Wang, Qing and Qi, Xiaozhen and Xu, Jiong},
title = {e-Silkroad: a sample of combining social media with cultural tourism},
year = {2010},
isbn = {9781450301725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1877911.1877920},
doi = {10.1145/1877911.1877920},
abstract = {With the development of Web2.0, very large scale resources of multimedia have emerged in the internet. In this paper, we present a novel framework of building a tour guide based on the online knowledge resources, e.g., e-Silkroad, a photographic guide of traditional Silkroad. The tour guide is jointly established by text information from Wikipedia and images from flickr website. Our method starts from a keyword "silkroad" in Wiki and typical cities are extracted and regarded as the key threads of the guide. Then a great number of images and their description tags are downloaded from Flickr website. To highlight the most interesting place and more active tourist, the framework computes the hot spots and photographers in the dataset. To introduce each place along the silkroad, all the images are classified into four categories by its content, including person, food, man-made, and sights. Finally, the images are registered into Google Maps according to the geog-tag descriptions along silk routes to generate e-Silkroad. In our evaluation experiment, 20676 images were downloaded from 35 key cities along silkroad. Experimental results show that it is effective from social media to cultural tourism under the connected environment.},
booktitle = {Proceedings of the 1st ACM International Workshop on Connected Multimedia},
pages = {27–32},
numpages = {6},
keywords = {tour guide, sparse representation, social media, silkroad, image classification},
location = {Firenze, Italy},
series = {CMM '10}
}

@article{10.14778/3402755.3402795,
author = {Jin, Xin and Lin, Cindy Xide and Luo, Jiebo and Han, Jiawei},
title = {SocialSpamGuard: a data mining-based spam detection system for social media networks},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402755.3402795},
doi = {10.14778/3402755.3402795},
abstract = {We have entered the era of social media networks represented by Facebook, Twitter, YouTube and Flickr. Internet users now spend more time on social networks than search engines. Business entities or public figures set up social networking pages to enhance direct interactions with online users. Social media systems heavily depend on users for content contribution and sharing. Information is spread across social networks quickly and effectively. However, at the same time social media networks become susceptible to different types of unwanted and malicious spammer or hacker actions. There is a crucial need in the society and industry for security solution in social media. In this demo, we propose SocialSpamGuard, a scalable and online social media spam detection system based on data mining for social network security. We employ our GAD clustering algorithm for large scale clustering and integrate it with the designed active learning algorithm to deal with the scalability and real-time detection challenges.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1458–1461},
numpages = {4}
}

@inproceedings{10.1145/2405136.2405137,
author = {Costa, Fernando and Veiga, Lu\'{\i}s and Ferreira, Paulo},
title = {VMR: volunteer MapReduce over the large scale internet},
year = {2012},
isbn = {9781450316088},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2405136.2405137},
doi = {10.1145/2405136.2405137},
abstract = {Volunteer Computing systems (VC) harness computing resources of machines from around the world to perform distributed independent tasks. Existing infrastructures follow a master/worker model, with a centralized architecture, which limits the scalability of the solution given its dependence on the server. We intend to create a distributed model, in order to improve performance and reduce the burden on the server.In this paper we present VMR, a VC system able to run MapReduce applications on top of volunteer resources, over the large scale Internet. We describe VMR's architecture and evaluate its performance by executing several MapReduce applications on a wide area testbed.Our results show that VMR successfully runs MapReduce tasks over the Internet. When compared to an unmodified VC system, VMR obtains a performance increase of over 60% in application turnaround time, while reducing the bandwidth use by an order of magnitude.},
booktitle = {Proceedings of the 10th International Workshop on Middleware for Grids, Clouds and e-Science},
articleno = {1},
numpages = {6},
keywords = {volunteer computing, large scale distributed systems, MapReduce},
location = {Montreal, Quebec, Canada},
series = {MGC '12}
}

@inproceedings{10.1145/1806799.1806809,
author = {Zheng, Zibin and Lyu, Michael R.},
title = {Collaborative reliability prediction of service-oriented systems},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806809},
doi = {10.1145/1806799.1806809},
abstract = {Service-oriented architecture (SOA) is becoming a major software framework for building complex distributed systems. Reliability of the service-oriented systems heavily depends on the remote Web services as well as the unpredictable Internet. Designing effective and accurate reliability prediction approaches for the service-oriented systems has become an important research issue. In this paper, we propose a collaborative reliability prediction approach, which employs the past failure data of other similar users to predict the Web service reliability for the current user, without requiring real-world Web service invocations. We also present a user-collaborative failure data sharing mechanism and a reliability composition model for the service-oriented systems. Large-scale real-world experiments are conducted and the experimental results show that our collaborative reliability prediction approach obtains better reliability prediction accuracy than other approaches.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {35–44},
numpages = {10},
keywords = {web service, user-collaboration, reliability prediction, reliability composition},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/2660129.2660139,
author = {Wang, Zhehao and Qu, Zening and Burke, Jeff},
title = {Demo overview-Matryoshka: design of NDN multiplayer online game},
year = {2014},
isbn = {9781450332064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660129.2660139},
doi = {10.1145/2660129.2660139},
abstract = {Massive multiplayer online games (MOG) have become increasingly popular over the past decade. Peer-to-peer structures were explored for commercial online games. However, maintaining security and availability while scaling users has driven most multiplayer online games towards a client-server or client-superpeer architecture.Client-server multiplayer games face certain problems: a small number of points of failure and traffic centralizing at several servers. Users of popular games complain about the decrease in quality of service, largely caused by these two factors. In order to tackle the problems of traditional clientserver online games, this demo presents Matryoshka, a pure peer-to-peer multiplayer online game using the named data networking[1] (NDN) future internet architecture.NDN has several major strengths over IP; among them are natural multicast support, content-based security and mobility support. By utilizing the strength of multicast and content caching, we believe that a pure peer-to-peer MOG design in NDN can avoid challenges and limitations found in IP.Synchronization in a serverless distributed environment is a key problem for pure peer-to-peer structure. Namespace synchronization in just such a situation has been studied for other serverless NDN applications like chat and file sharing. The ChronoSync[2] model is proposed for both use cases. Other cases like vehicular network also study synchronization in a physical environment.The challenges faced by an online game are different, which we explore in this project. In this case, the environment is a virtual world. Each player has an area of interest, and it only needs to know things in this virtual area instead of everything happening in the game, and this we define as 'locality in the game world'. In addition, players whose areas of interest intersect with each other should reach consistent conclusions about things in the intersected area, which introduces the synchronization problem.NDN's content caching and natural multicast support feature may facilitate the distribution of game synchronization data. We utilize these features by statically and recursively partitioning the whole virtual environment into octants, thus providing a shared namespace for every peer running the game. Then, all the peers that care about the same region can share the data brought by synchronization interests towards the same nodes in the octree. Figure 1 presents the octree partition of the game world.},
booktitle = {Proceedings of the 1st ACM Conference on Information-Centric Networking},
pages = {209–210},
numpages = {2},
keywords = {synchronization, named data networking, massive multiplayer online game},
location = {Paris, France},
series = {ACM-ICN '14}
}

@inproceedings{10.1145/1941530.1941532,
author = {Lampropoulos, Konstantinos and Diaz-Sanchez, Daniel and Almenares, Florina and Weik, Peter and Denazis, Spyros},
title = {Introducing a cross federation identity solution for converged network environments},
year = {2010},
isbn = {9781450306317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1941530.1941532},
doi = {10.1145/1941530.1941532},
abstract = {The Future Internet architecture, based on the integration of existing networks and services, and the addition of many new devices like sensors, face a series of important technical challenges, one of them being the management of diverse user identities. The diversity and plethora of the services and procedures affected by the unassociated existing user identities stress the necessity for a holistic solution to deal with the different aspects of the identity management problem. Existing efforts propose limited identity solutions that can only be applied within well defined boundaries and cannot extend their functionality to support converged network environments and service operations across different administrative domains. This paper presents a Dynamic Identity Mapping Association N' Discovery System (DIMANDS) as a holistic identity solution for large scale heterogeneous network environments. This solution offers cross federation identity services and is based on a universal discovery mechanism which spans across different networks, layers and federations. It is also empowered with a unified trust framework which can collect and process diverse trust information to provide trust decisions on a widely accepted format.},
booktitle = {Principles, Systems and Applications of IP Telecommunications},
pages = {1–11},
numpages = {11},
keywords = {trust management, privacy, identity management, discovery},
location = {Munich, Germany},
series = {IPTComm '10}
}

@article{10.5555/2460156.2460182,
author = {Malan, David J.},
title = {Implementing a massive open online course (MOOC)},
year = {2013},
issue_date = {June 2013},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {28},
number = {6},
issn = {1937-4771},
abstract = {Distance education is by no means new. Indeed, long before the Internet came along were universities making educational content available to students off campus via VHS, CD-ROM, and other media.But what is new is the scale on which universities and, in some cases, individual faculty are now operating. Massive open online courses (otherwise known as MOOCs) from non-profits like edX and for-profits like Coursera and Udacity have repeatedly drawn upwards of 100,000 registrants from all over the world. How to teach so many students effectively, though, is non-obvious. How to disseminate content to so many students, particularly large videos, is technically challenging (if not expensive). And how to collect, evaluate, and return work to so many students is a feat unto itself.Even so, we set out in Fall 2012 to tackle each of those challenges and more. We present in this tutorial how to implement (and how not to implement) a MOOC, based on lessons learned while designing and implementing CS50x: Harvard University's Introduction to Computer Science I, edX's largest fall course with 120,000 registered students. We present what we did, how we did it, why we did it, and what we would and wouldn't do again so that others might build upon our own experience.In particular,• we present how to capture and encode content (e.g., faculty on video) using commodity hardware and open-source tools like FFmpeg;• we present how to choose among YouTube, iTunes U, Amazon Web Services, and the like for delivery of content;• we present CS50 Check, an open-source autograding framework that we developed for CS50x (with other courses in mind) that supports behavioral testing of programs written in any interpreted or compiled language;• we present CS50 Run, an open-source, web-based code editor that we developed for CS50x (with other courses in mind) that enables students to write, within a browser, code in any language, execution of which happens server-side;• and we present the pedagogy behind CS50x itself and the accompanying challenges of scale, among them logistics discussions among students, issues of academic dishonesty, and hidden costs in time.In addition, we explore in this tutorial the research potential that MOOCs' scale offers, including opportunities for A/B testing and more.Ultimately, this tutorial's attendees will exit with an understanding of the time, costs, opportunities, and challenges involved in offering a course on scale.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {136–137},
numpages = {2}
}

@inproceedings{10.5555/2043468.2043472,
author = {Dong, Wei and Ge, Zihui and Lee, Seungjoon},
title = {3G meets the internet: understanding the performance of hierarchical routing in 3G networks},
year = {2011},
isbn = {9780983628309},
publisher = {International Teletraffic Congress},
abstract = {The volume of Internet traffic over 3G wireless networks is sharply rising. In contrast to many Internet services utilizing replicated resources, such as Content Distribution Networks (CDN), the current 3G standard architecture employs hierarchical routing, where all user data traffic goes through a small number of aggregation points using logical tunnels. In this paper, we study the performance implications of the interplay when 3G users access Internet services.We first identify a number of scenarios in which 3G users' service performance can be affected under hierarchical routing in comparison to an idealized flat routing. We then quantify this service impact by analyzing trace data obtained from a large-scale 3G network and a CDN provider. We find that the performance difference between hierarchical routing and flat routing increases when a 3G user accesses highly replicated service, and can further aggravates when the DNS caching is not properly managed under vertical handoff. For example, in our data analysis, the detour under hierarchical routing can cause a packet to travel extra distance by up to 1627km on the average case, which can lead to around 45.4% increase in round-trip latency. We also perform a measurement study to demonstrate that user mobility and web applications can lead to unexpected performance-impacting interactions, which can degrade the download throughput by up to an order of magnitude (0.9Mbps vs. 10.8Mbps).},
booktitle = {Proceedings of the 23rd International Teletraffic Congress},
pages = {15–22},
numpages = {8},
location = {San Francisco, California},
series = {ITC '11}
}

@article{10.1145/2019591.2019596,
author = {Jelasity, M\'{a}rk and Bilicki, Vilmos},
title = {Scalable Stealth Mode P2P Overlays of Very Small Constant Degree},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/2019591.2019596},
doi = {10.1145/2019591.2019596},
abstract = {P2P technology has recently been adopted by Internet-based malware as a fault tolerant and scalable communication medium. Due to its decentralized and self-organizing nature, P2P malware is harder to detect and block, especially if it utilizes specialized techniques for hiding. We analyze a number of hiding strategies through extensive and realistic simulations over a model of the AS-level Internet topology. We show that the most effective strategy to avoid detection is to drastically reduce the maximal number of peers a node communicates with. While overlay networks of a small constant maximal degree are generally considered to be unscalable, we argue that it is possible to design them to be scalable, efficient, and robust. An important implication is that stealth mode P2P malware that is very difficult to discover with state-of-the-art methods is a plausible threat. We discuss algorithms and theoretical results that support the scalability of stealth mode overlays, and we present realistic event-based simulations of a proof-of-concept system. Besides the context of P2P malware, some of our results are of general interest in the area of constant degree overlays in connection with the problem of how to maintain reasonable performance and reliability with the smallest degree possible.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {oct},
articleno = {27},
numpages = {20},
keywords = {gossip-based protocols, botnets, Distributed hash tables}
}

@inproceedings{10.1145/2678508.2678527,
author = {Bruneau-Queyreix, Joachim and N\'{e}gru, Daniel and Batalla, Jordi Mongay and Borcoci, Eugen},
title = {Home-Boxes: context-aware distributed middleware assisting content delivery solutions},
year = {2014},
isbn = {9781450332200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678508.2678527},
doi = {10.1145/2678508.2678527},
abstract = {Within the Future Media Internet, residential gateways, involving overlay network creation, are foreseen to become an efficient alternative solution for content distribution. This poster proposes a new residential gateway middleware approach, called the Home-Box (HB), exploiting an innovative popularity-based and collaborative caching strategy, able to achieve scalable and cost-effective Video-on-Demand (VoD) services. The middleware offers to Content Providers an efficient way to get media content closer to End-Users. Besides, the caching mechanism is coupled with context-aware features in order to select the best serving HBs, leveraging the HB middleware overlay with an overall load-balancing system. The solution has been implemented and deployed in a large scale test bed; results outlines the Home-Box, in collaboration with its neighbors, as a suitable and rightful asset alternative and/or complementarity to existing VoD service infrastructures such as Content Delivery Networks.},
booktitle = {Proceedings of the Posters and Demos Session of the 15th International Middleware Conference},
pages = {37–38},
numpages = {2},
keywords = {replication, overlay network, media-oriented middleware, media delivery, home-gateways, future media internet, context-awareness},
location = {Bordeaux, France},
series = {Middleware Posters and Demos '14}
}

@inproceedings{10.1145/2539150.2539267,
author = {Adeyeye, Michael and Van Gelder, Antoine and Ojo, Sunday},
title = {Routing Cost and Latency in the VillageTelco Wireless Mesh Network},
year = {2013},
isbn = {9781450321136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2539150.2539267},
doi = {10.1145/2539150.2539267},
abstract = {Afrimesh is the Network Management System (NMS) used in the VillageTelco (VT) project. The NMS uses both Simple Network Management Protocol (SNMP) and Internet Control Message Protocol (ICMP) as its network management protocols. It provides a thorough report on network health, such as interference, noise level and latency. In addition, it displays the current network topology and can run at a node thereby making it efficient to identify compromised or isolated node(s). This article presents the performance of a typical mesh network. The performance metrics include signalling overhead and network latency of a small scale mesh network with few nodes. Experiments showed that an additional hop in a network increases the network latency by averagely 35ms. In addition, the signalling overhead of a VT network increases with time at every node. However, the footprint is small and would not impede the performance of a network in a large-scale deployment.},
booktitle = {Proceedings of International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {710–713},
numpages = {4},
keywords = {Wireless Mesh Networks, Network Management, Afrimesh},
location = {Vienna, Austria},
series = {IIWAS '13}
}

@inproceedings{10.1145/2645791.2645824,
author = {Varlamis, Iraklis and Tsirakis, Nikos and Poulopoulos, Vasilis and Tsantilas, Panagiotis},
title = {An automatic wrapper generation process for large scale crawling of news websites},
year = {2014},
isbn = {9781450328975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645791.2645824},
doi = {10.1145/2645791.2645824},
abstract = {The creation and maintenance of a large-scale news content aggregator is a tedious task, which requires more than a simple RSS aggregator. Many news sites appear every day on the Internet, providing new content in different refresh rates; well established news sites restrict access to their content only to subscribers or online readers, without offering RSS feeds, whereas other sites update their CMS or website tem-plate and lead crawlers to fetch errors. The main problem that arises from this continuous generation and alteration of pages on the Internet is the automated discovery of the appropriate and useful content and the dynamic rules that crawlers need to apply in order not to become outdated. In this paper we present an innovative mechanism for extracting useful content (title, body and media) from news articles web pages, based on automatic extraction of patterns that form each domain. The system is able to achieve high performance by combining information gathered while discovering the structure of a news site, together with "knowledge" that acquires at each crawling step, in order to improve the quality of the next steps of its own procedure. Additionally, the system can recognize changes in patterns in order to rebuild the domain rules whenever the domain changes structure. This system has been successfully implemented in palo.rs, the first news search engine in Serbia.},
booktitle = {Proceedings of the 18th Panhellenic Conference on Informatics},
pages = {1–6},
numpages = {6},
keywords = {Web crawling, Web Mining},
location = {Athens, Greece},
series = {PCI '14}
}

@article{10.1145/2037676.2037686,
author = {Lin, Yu-Ru and Candan, K. Sel\c{c}cuk and Sundaram, Hari and Xie, Lexing},
title = {SCENT: Scalable compressed monitoring of evolving multirelational social networks},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7S},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/2037676.2037686},
doi = {10.1145/2037676.2037686},
abstract = {We propose SCENT, an innovative, scalable spectral analysis framework for internet scale monitoring of multirelational social media data, encoded in the form of tensor streams. In particular, a significant challenge is to detect key changes in the social media data, which could reflect important events in the real world, sufficiently quickly. Social media data have three challenging characteristics. First, data sizes are enormous; recent technological advances allow hundreds of millions of users to create and share content within online social networks. Second, social data are often multifaceted (i.e., have many dimensions of potential interest, from the textual content to user metadata). Finally, the data is dynamic; structural changes can occur at multiple time scales and be localized to a subset of users. Consequently, a framework for extracting useful information from social media data needs to scale with data volume, and also with the number and diversity of the facets of the data. In SCENT, we focus on the computational cost of structural change detection in tensor streams. We extend compressed sensing (CS) to tensor data. We show that, through the use of randomized tensor ensembles, SCENT is able to encode the observed tensor streams in the form of compact descriptors. We show that the descriptors allow very fast detection of significant spectral changes in the tensor stream, which also reduce data collection, storage, and processing costs. Experiments over synthetic and real data show that SCENT is faster (17.7x--159x for change detection) and more accurate (above 0.9 F-score) than baseline methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {nov},
articleno = {29},
numpages = {22},
keywords = {tensor analysis, stream mining, social network analysis, multirelational learning, Social media}
}

@inproceedings{10.5555/2735522.2735581,
author = {Rabl, Tilmann and Jacobsen, Hans-Arno},
title = {Materialized views in Cassandra},
year = {2014},
publisher = {IBM Corp.},
address = {USA},
abstract = {Many web companies deal with enormous data sizes and request rates beyond the capabilities of traditional database systems. This has led to the development of modern Big Data Platforms (BDPs). BDPs handle large amounts of data and activity through massively distributed infrastructures. To achieve performance and availability at Internet scale, BDPs restrict querying capability, and provide weaker consistency guarantees than traditional ACID transactions. The reduced functionality as found in key-value stores is sufficient for many web applications.An important requirement of many big data systems is an online view of the current status of the data and activity. Typical big data systems such as key-value stores only allow a key-based access. In order to enable more complex querying mechanisms, while satisfying necessary latencies materialized views are employed. The efficiency of the maintenance of these views is a key factor of the usability of the system. Expensive operations such as full table scans are impractical for small, frequent modifications on Internet-scale data sets. In this paper, we present an efficient implementation of materialized views in key-value stores that enables complex query processing and is tailored for efficient maintenance.},
booktitle = {Proceedings of 24th Annual International Conference on Computer Science and Software Engineering},
pages = {351–354},
numpages = {4},
location = {Markham, Ontario, Canada},
series = {CASCON '14}
}

@inproceedings{10.1145/1815961.1816002,
author = {Janapa Reddi, Vijay and Lee, Benjamin C. and Chilimbi, Trishul and Vaid, Kushagra},
title = {Web search using mobile cores: quantifying and mitigating the price of efficiency},
year = {2010},
isbn = {9781450300537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1815961.1816002},
doi = {10.1145/1815961.1816002},
abstract = {The commoditization of hardware, data center economies of scale, and Internet-scale workload growth all demand greater power efficiency to sustain scalability. Traditional enterprise workloads, which are typically memory and I/O bound, have been well served by chip multiprocessors com- prising of small, power-efficient cores. Recent advances in mobile computing have led to modern small cores capable of delivering even better power efficiency. While these cores can deliver performance-per-Watt efficiency for data center workloads, small cores impact application quality-of-service robustness, and flexibility, as these workloads increasingly invoke computationally intensive kernels. These challenges constitute the price of efficiency. We quantify efficiency for an industry-strength online web search engine in production at both the microarchitecture- and system-level, evaluating search on server and mobile-class architectures using Xeon and Atom processors.},
booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
pages = {314–325},
numpages = {12},
keywords = {web search, mobile cores, energy efficiency, bing},
location = {Saint-Malo, France},
series = {ISCA '10}
}

@inproceedings{10.5555/2147671.2147733,
author = {Hwang, Haesung and Ata, Shingo and Murata, Masayuki},
title = {Realization of name lookup table in routers towards content-centric networks},
year = {2011},
isbn = {9783901882449},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {The future Internet is expected to be a highly intelligent that can route packets not only with an explicit destination address but also with the content. This paradigm shift in the network architecture from host- to content-centric communication naturally leads to the contemplation of the shift in the network layer devices, i.e. routers. In other words, the hardware architecture of routers should also be able to support content-centric communication. In this paper, we propose a new router architecture to manage a large information of contents and large-scale number of users. In order to complete the packet forwarding within the network layer, routers acting as the brokers of a publish/subscribe system should maintain the information of content names and the subscribers. We propose three memory structures for name lookup tables in routers, each with a different combination of memory types depending on the usage purpose. The proposed memory architecture is evaluated with parameters such as memory cost, latency, and utilization using real-life and synthetic databases that have a Zipf distribution. We show the memory architecture which has the lowest manufacturing cost and the lowest latency for storing the given database within a fixed budget.},
booktitle = {Proceedings of the 7th International Conference on Network and Services Management},
pages = {353–357},
numpages = {5},
location = {Paris, France},
series = {CNSM '11}
}

@inproceedings{10.1145/1878537.1878654,
author = {Tajali, Maryam Bashardoust and Abhari, Abdolreza},
title = {Peer-to-peer delivery system for short video sharing},
year = {2010},
isbn = {9781450300698},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.1145/1878537.1878654},
doi = {10.1145/1878537.1878654},
abstract = {Peer-to-peer streaming applications are recently emerged successes that are able to serve users adequately on large scales. Recently, multimedia streaming of the user generated media content has been highly demanded for instance on YouTube. P2P systems have the capability of providing a reliable and scalable platform for Internet users to view any multimedia content. Hence, serving multimedia files over Internet can be improved by adopting P2P networks. In this research, we have designed and implemented a P2P network simulator to serve multimedia content on the Internet efficiently. The data was simulated from the current popular serving website YouTube and then was fed into the P2P simulator. The simulated user traffic from our proposed P2P model is then compared with the traditional ClientServer system in terms of performance analysis.},
booktitle = {Proceedings of the 2010 Spring Simulation Multiconference},
articleno = {112},
numpages = {6},
keywords = {video-on-demand, simulator, peer-to-peer, multimedia, client-server, YouTube},
location = {Orlando, Florida},
series = {SpringSim '10}
}

@inproceedings{10.1145/2030718.2030720,
author = {Seskar, Ivan},
title = {Future wireless experimentation: a testbed perspective},
year = {2011},
isbn = {9781450308670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030718.2030720},
doi = {10.1145/2030718.2030720},
abstract = {Wireless network testbeds have become increasingly important for realistic, at-scale experimental evaluation of new network architectures, protocols and radio technologies. Wireless testbeds such as ORBIT and GENI campus networks are being used for experimental research on a wide variety of research topics including dynamic spectrum access, cognitive radio networks, DTN, vehicular networks, future Internet architecture and so on. Increasingly, these testbeds are also being federated to cater for experimenter needs, which cannot be fulfilled by a single testbed, and to provide a wider variety of environmental settings at different scales. We will start this talk with a brief overview of existing and planned large-scale public domain testbed initiatives. This will be followed by discussion of impact of: a) increasing complexity of the wireless devices, b) scale and heterogeneity of the testbeds, c) attempts to harmonize and standardize management frameworks and d) complexity of planed experiments. We will conclude the talk by pointing out some of the key remaining challenges for the successful deployment of next generation of wireless network testbeds.},
booktitle = {Proceedings of the 6th ACM International Workshop on Wireless Network Testbeds, Experimental Evaluation and Characterization},
pages = {1–2},
numpages = {2},
keywords = {wireless testbeds, vehicular networks, dynamic spectrum access, delay tolerant networks, cognitive radio, at-scale experimentation},
location = {Las Vegas, Nevada, USA},
series = {WiNTECH '11}
}

@inproceedings{10.1145/2505906.2505910,
author = {Chen, Yang and Liu, Bingyang and Chen, Yu and Li, Ang and Yang, Xiaowei and Bi, Jun},
title = {PacketCloud: an open platform for elastic in-network services},
year = {2013},
isbn = {9781450323666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505906.2505910},
doi = {10.1145/2505906.2505910},
abstract = {The Internet was designed with the end-to-end principle where the network layer provided merely the best-effort forwarding service. This design makes it challenging to add new services to the network layer. However, as the Internet connectivity becomes a commodity, users and applications increasingly demand new in-network services. This paper proposes PacketCloud, a cloudlet-based open platform to host elastic in-network services. PacketCloud can help both Internet Service Providers (ISPs) and emerging application/content providers deploy their services to strategic network locations. We have implemented a proof-of-concept prototype of PacketCloud based on the MobilityFirst architecture. PacketCloud introduces a small additional delay, and can scale well to handle high-throughput data traffic.},
booktitle = {Proceedings of the Eighth ACM International Workshop on Mobility in the Evolving Internet Architecture},
pages = {17–22},
numpages = {6},
keywords = {open platform, mobilityfirst, in-network services, future internet architecture, elasticity, cloud computing},
location = {Miami, Florida, USA},
series = {MobiArch '13}
}

@inproceedings{10.1145/1980822.1980835,
author = {Mvelase, Promise and Dlodlo, Nomusa and Williams, Quentin and Adigun, Matthew},
title = {Virtual enterprise model for enabling cloud computing for SMMEs},
year = {2011},
isbn = {9781450304740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980822.1980835},
doi = {10.1145/1980822.1980835},
abstract = {Although technology is a significant aspect for most businesses, it has also proven to increase productivity, reduce other costs and eventually improve a company's bottom line. Small, Medium and Micro enterprises (SMMEs) do not usually have adequate funds to acquire ICT infrastructure. They do not require the infrastructure on a daily basis. Therefore it is not economically viable for them to purchase their own infrastructure. It is for this reason that they have to take advantage of Cloud computing capabilities. Cloud computing is a style of computing in which dynamically scalable resources are provided as a virtualised service. The Cloud is an emerging concept derived from a service-centric view. All capabilities and resources of a Cloud are provided to users as a service to be accessed through the Internet without any knowledge, expertise, or control over the underlying technology infrastructure that supports them. Users are billed on a pay-per-use basis or through subscription. To provide on demand service, the Cloud adopts virtualisation, a service-oriented architecture, infrastructure scalability, the web and utility computing as the underlying concepts.This paper focuses on the implementation of virtual enterprises (VE), to enable SMMEs to respond quickly to customers' demands and market opportunities. The virtual enterprise model is based on the ability to create temporary co-operations and to realise the value of a short term business opportunity that the partners cannot (or can, but only to a lesser extent) capture on their own. The duration of the alliances vary depending on the actual goal. There are alliances made for a single business opportunity and which are dissolved at the end of such process, and long-term alliances that last for an indefinite number of business processes or for a specified time span.The model of virtual enterprise is made possible by means of virtualisation technology which is one of the building blocks of Cloud computing. To achieve a common goal, enterprises integrate resources, organisational models, and process models. Through a virtual business operating environment offered by Cloud computing, the SMMEs will be able to increase productivity and gain competitive advantage due to the cost benefit incurred.We propose a virtual enterprise enabled cloud enterprise architecture based on the concept of virtual enterprise at both business and technology levels. The business level comprises of organisational models, process models, skills, and competences while technology level comprises of IT resources.},
booktitle = {Proceedings of the 2011 International Conference on Intelligent Semantic Web-Services and Applications},
articleno = {13},
numpages = {6},
keywords = {virtual enterprise, small medium and micro enterprises, cloud computing},
location = {Amman, Jordan},
series = {ISWSA '11}
}

@inproceedings{10.1145/2557642.2557656,
author = {Ma, He and Seo, Beomjoo and Zimmermann, Roger},
title = {Dynamic scheduling on video transcoding for MPEG DASH in the cloud environment},
year = {2014},
isbn = {9781450327053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2557642.2557656},
doi = {10.1145/2557642.2557656},
abstract = {The Dynamic Adaptive Streaming over HTTP (referred as MPEG DASH) standard is designed to provide high quality of media content over the Internet delivered from conventional HTTP web servers. The visual content, divided into a sequence of segments, is made available at a number of different bitrates so that an MPEG DASH client can automatically select the next segment to download and play back based on current network conditions. The task of transcoding media content to different qualities and bitrates is computationally expensive, especially in the context of large-scale video hosting systems. Therefore, it is preferably executed in a powerful cloud environment, rather than on the source computer (which may be a mobile device with limited memory, CPU speed and battery life). In order to support the live distribution of media events and to provide a satisfactory user experience, the overall processing delay of videos should be kept to a minimum. In this paper, we propose a novel dynamic scheduling methodology on video transcoding for MPEG DASH in a cloud environment, which can be adapted to different applications. The designed scheduler monitors the workload on each processor in the cloud environment and selects the fastest processors to run high-priority jobs. It also adjusts the video transcoding mode (VTM) according to the system load. Experimental results show that the proposed scheduler performs well in terms of the video completion time, system load balance, and video playback smoothness.},
booktitle = {Proceedings of the 5th ACM Multimedia Systems Conference},
pages = {283–294},
numpages = {12},
keywords = {video transcoding, scheduling, cloud computing, DASH},
location = {Singapore, Singapore},
series = {MMSys '14}
}

@inproceedings{10.1145/1958746.1958768,
author = {Karnouskos, Stamatis and da Silva, Per Goncalves and Ilic, Dejan},
title = {Assessment of high-performance smart metering for the web service enabled smart grid era},
year = {2011},
isbn = {9781450305198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1958746.1958768},
doi = {10.1145/1958746.1958768},
abstract = {The electricity network is undergoing a significant change towards a more adaptive, intelligent, self-managing, collaborative and information-driven grid. According to the smart grid vision, any electronic device connected to it will be able to communicate its consumed or produced energy almost in real time. Based on the analysis of this newly acquired information, a new generation of services and decision support systems can be realized, enabling more intelligent decisions, and ultimately a more efficient energy system. Therefore, high-performance acquisition of smart metering information from large scale distributed infrastructures is of key importance for the upcoming Internet-based enterprise services and mash-up applications. We have used open source software to build a web service-based advanced metering infrastructure of simulated smart meters, concentrators, and a smart metering platform, all interconnected via web services. We measure in a methodological fashion the performance of the various components of the architecture and evaluate their limitations. Finally we identify key performance indicators that need to be considered when deploying large-scale smart metering systems, and discuss on challenges and directions that arise.},
booktitle = {Proceedings of the 2nd ACM/SPEC International Conference on Performance Engineering},
pages = {133–144},
numpages = {12},
location = {Karlsruhe, Germany},
series = {ICPE '11}
}

@inproceedings{10.1145/2393347.2393359,
author = {Wang, Zhi and Sun, Lifeng and Chen, Xiangwen and Zhu, Wenwu and Liu, Jiangchuan and Chen, Minghua and Yang, Shiqiang},
title = {Propagation-based social-aware replication for social video contents},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393359},
doi = {10.1145/2393347.2393359},
abstract = {Online social network has reshaped the way how video contents are generated, distributed and consumed on today's Internet. Given the massive number of videos generated and shared in online social networks, it has been popular for users to directly access video contents in their preferred social network services. It is intriguing to study the service provision of social video contents for global users with satisfactory quality-of-experience. In this paper, we conduct large-scale measurement of a real-world online social network system to study the propagation of the social video contents. We have summarized important characteristics from the video propagation patterns, including social locality, geographical locality and temporal locality. Motivated by the measurement insights, we propose a propagation-based social-aware replication framework using a hybrid edge-cloud and peer-assisted architecture, namely PSAR, to serve the social video contents. Our replication strategies in PSAR are based on the design of three propagation-based replication indices, including a geographic influence index and a content propagation index to guide how the edge-cloud servers backup the videos, and a social influence index to guide how peers cache the videos for their friends. By incorporating these replication indices into our system design, PSAR has significantly improved the replication performance and the video service quality. Our trace-driven experiments further demonstrate the effectiveness and superiority of PSAR, which improves the local download ratio in the edge-cloud replication by 30%, and the local cache hit ratio in the peer-assisted replication by 40%, against traditional approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {29–38},
numpages = {10},
keywords = {video replication, social network, hybrid edge-cloud and P2P},
location = {Nara, Japan},
series = {MM '12}
}

@article{10.1145/2160803.2160818,
author = {Karnouskos, Stamatis and da Silva, Per Goncalves and Ilic, Dejan},
title = {Assessment of high-performance smart metering for the web service enabled smart grid era (abstracts only)},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/2160803.2160818},
doi = {10.1145/2160803.2160818},
abstract = {The electricity network is undergoing a significant change towards a more adaptive, intelligent, self-managing, collaborative and information-driven grid. According to the smart grid vision, any electronic device connected to it will be able to communicate its consumed or produced energy almost in real time. Based on the analysis of this newly acquired information, a new generation of services and decision support systems can be realized, enabling more intelligent decisions, and ultimately a more efficient energy system. Therefore, high-performance acquisition of smart metering information from large scale distributed infrastructures is of key importance for the upcoming Internet-based enterprise services and mash-up applications. We have used open source software to build a web service-based advanced metering infrastructure of simulated smart meters, concentrators, and a smart metering platform, all interconnected via web services. We measure in a methodological fashion the performance of the various components of the architecture and evaluate their limitations. Finally we identify key performance indicators that need to be considered when deploying large-scale smart metering systems, and discuss on challenges and directions that arise.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {dec},
pages = {13},
numpages = {1}
}

@article{10.1145/2071396.2071402,
author = {Mei, Tao and Li, Lusong and Hua, Xian-Sheng and Li, Shipeng},
title = {ImageSense: Towards contextual image advertising},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/2071396.2071402},
doi = {10.1145/2071396.2071402},
abstract = {The daunting volumes of community-contributed media contents on the Internet have become one of the primary sources for online advertising. However, conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the Web page, without considering the inherent characteristics of visual contents. This article presents a contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a Web page and seamlessly inserts the ads in the nonintrusive areas within each individual image. The proposed system, called ImageSense, supports scalable advertising of, from root to node, Web sites, pages, and images. In ImageSense, the ads are selected based on not only textual relevance but also visual similarity, so that the ads yield contextual relevance to both the text in the Web page and the image content. The ad insertion positions are detected based on image salience, as well as face and text detection, to minimize intrusiveness to the user. We evaluate ImageSense on a large-scale real-world images and Web pages, and demonstrate the effectiveness of ImageSense for online image advertising.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {6},
numpages = {18}
}

@inproceedings{10.1145/1873951.1873967,
author = {Zhang, Tianzhu and Xu, Changsheng and Zhu, Guangyu and Liu, Si and Lu, Hanqing},
title = {A generic framework for event detection in various video domains},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1873967},
doi = {10.1145/1873951.1873967},
abstract = {Event detection is essential for the extensively studied video analysis and understanding area. Although various approaches have been proposed for event detection, there is a lack of a generic event detection framework that can be applied to various video domains (e.g. sports, news, movies, surveillance). In this paper, we present a generic event detection approach based on semi-supervised learning and Internet vision. Concretely, a Graph-based Semi-Supervised Multiple Instance Learning (GSSMIL) algorithm is proposed to jointly explore small-scale expert labeled videos and large-scale unlabeled videos to train the event models to detect video event boundaries. The expert labeled videos are obtained from the analysis and alignment of well-structured video related text (e.g. movie scripts, web-casting text, close caption). The unlabeled data are obtained by querying related events from the video search engine (e.g. YouTube) in order to give more distributive information for event modeling. A critical issue of GSSMIL in constructing a graph is the weight assignment, where the weight of an edge specifies the similarity between two data points. To tackle this problem, we propose a novel Multiple Instance Learning Induced Similarity (MILIS) measure by learning instance sensitive classifiers. We perform the thorough experiments in three popular video domains: movies, sports and news. The results compared with the state-of-the-arts are promising and demonstrate our proposed approach is performance-effective.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {103–112},
numpages = {10},
keywords = {web-casting text, semi-supervised learning, multiple instance learning, internet, event detection, broadcast video},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{10.1145/2487575.2487647,
author = {Zhao, Peilin and Hoi, Steven C.H.},
title = {Cost-sensitive online active learning with application to malicious URL detection},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487647},
doi = {10.1145/2487575.2487647},
abstract = {Malicious Uniform Resource Locator (URL) detection is an important problem in web search and mining, which plays a critical role in internet security. In literature, many existing studies have attempted to formulate the problem as a regular supervised binary classification task, which typically aims to optimize the prediction accuracy. However, in a real-world malicious URL detection task, the ratio between the number of malicious URLs and legitimate URLs is highly imbalanced, making it very inappropriate for simply optimizing the prediction accuracy. Besides, another key limitation of the existing work is to assume a large amount of training data is available, which is impractical as the human labeling cost could be potentially quite expensive. To solve these issues, in this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL), which only queries a small fraction of training data for labeling and directly optimizes two cost-sensitive measures to address the class-imbalance issue. In particular, we propose two CSOAL algorithms and analyze their theoretical performance in terms of cost-sensitive bounds. We conduct an extensive set of experiments to examine the empirical performance of the proposed algorithms for a large-scale challenging malicious URL detection task, in which the encouraging results showed that the proposed technique by querying an extremely small-sized labeled data (about 0.5% out of 1-million instances) can achieve better or highly comparable classification performance in comparison to the state-of-the-art cost-insensitive and cost-sensitive online classification algorithms using a huge amount of labeled data.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {919–927},
numpages = {9},
keywords = {online learning, malicious url detection, cost-sensitive learning, active learning},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2488388.2488522,
author = {Zhao, Yuchen and Sundaresan, Neel and Shen, Zeqian and Yu, Philip S.},
title = {Anatomy of a web-scale resale market: a data mining approach},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488522},
doi = {10.1145/2488388.2488522},
abstract = {Reuse and remarketing of content and products is an integral part of the internet. As E-commerce has grown, online resale and secondary markets form a significant part of the commerce space. The intentions and methods for reselling are diverse. In this paper, we study an instance of such markets that affords interesting data at large scale for mining purposes to understand the properties and patterns of this online market.As part of knowledge discovery of such a market, we first formally propose criteria to reveal unseen resale behaviors by elastic matching identification (EMI) based on the account transfer and item similarity properties of transactions. Then, we present a large-scale system that leverages MapReduce paradigm to mine millions of online resale activities from petabyte scale heterogeneous e-commerce data. With the collected data, we show that the number of resale activities leads to a power law distribution with a 'long tail', where a significant share of users only resell in very low numbers and a large portion of resales come from a small number of highly active resellers. We further conduct a comprehensive empirical study from different aspects of resales, including the temporal, spatial patterns, user demographics, reputation and the content of sale postings. Based on these observations, we explore the features related to "successful" resale transactions and evaluate if they can be predictable. We also discuss uses of this information mining for business insights and user experience on a real-world online marketplace.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1533–1544},
numpages = {12},
keywords = {reseller, resale market, prediction, mapreduce, ebay, e-commerce, big data, behavioral analysis},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2556325.2567861,
author = {Buffardi, Kevin and Edwards, Stephen H.},
title = {Adaptive and social mechanisms for automated improvement of eLearning materials},
year = {2014},
isbn = {9781450326698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556325.2567861},
doi = {10.1145/2556325.2567861},
abstract = {Online environments introduce unprecedented scale for formal and informal learning communities. In these environments, user-contributed content enables social constructivist approaches to education. In particular, students can help each other by providing hints and suggestions on how to approach problems, by rating each other's suggestions, and by engaging in discussions about the questions. In addition, students can also learn through composing their own questions. Furthermore, with grounding in Item Response Theory, data mining and statistical student models can assess questions and hints for their quality and effectiveness. As a result, internet-scale learning environments allow us to move from simple, canned quizzing systems to a new model where automated, data-driven analysis continuously assesses and refines the quality of teaching material. Our poster describes a framework and prototype of an online drill-and-practice system that leverages user-contributed content and large-scale data to organically improve itself.},
booktitle = {Proceedings of the First ACM Conference on Learning @ Scale Conference},
pages = {165–166},
numpages = {2},
keywords = {social constructivism, item response theory, internet-scale data, computer-supported cooperative learning (cscl), automated assessment, adaptive feedback, active learning},
location = {Atlanta, Georgia, USA},
series = {L@S '14}
}

@article{10.1109/TNET.2010.2040289,
author = {Lin, Bill and Keslassy, Isaac},
title = {The concurrent matching switch architecture},
year = {2010},
issue_date = {August 2010},
publisher = {IEEE Press},
volume = {18},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2010.2040289},
doi = {10.1109/TNET.2010.2040289},
abstract = {Network operators need high-capacity router architectures that can offer scalability, provide throughput guarantees, and maintain packet ordering. However, current centralized crossbar-based architectures cannot scale to fast line rates and high port counts. On the other hand, while load-balanced switch architectures that rely on two identical stages of fixed configuration meshes appear to be an effective way to scale Internet routers to very high capacities, they incur a large worst-case packet reordering that is at best quadratic to the switch size. In this paper, we introduce the concurrent matching switch (CMS) architecture, which also uses two identical stages of fixed configuration meshes with the same scalability properties as current load-balanced routers. However, by adopting a novel contention-resolution architecture that is scalable and distributed, the CMS architecture enforces packet ordering throughout the switch. Using the CMS architecture, we show that scalability, 100% throughput, packet ordering, and O(1) amortized time complexity with sequential hardware per linecard can all be achieved. We further demonstrate a delay analysis for the CMS architecture.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1330–1343},
numpages = {14},
keywords = {throughput guarantees, load-balanced routers, high-performance switches, concurrent matching switch (CMS)}
}

@article{10.1145/1842733.1842736,
author = {Nygren, Erik and Sitaraman, Ramesh K. and Sun, Jennifer},
title = {The Akamai network: a platform for high-performance internet applications},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5980},
url = {https://doi.org/10.1145/1842733.1842736},
doi = {10.1145/1842733.1842736},
abstract = {Comprising more than 61,000 servers located across nearly 1,000 networks in 70 countries worldwide, the Akamai platform delivers hundreds of billions of Internet interactions daily, helping thousands of enterprises boost the performance and reliability of their Internet applications. In this paper, we give an overview of the components and capabilities of this large-scale distributed computing platform, and offer some insight into its architecture, design principles, operation, and management.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {aug},
pages = {2–19},
numpages = {18},
keywords = {streaming media, quality of service, overlay networks, fault tolerance, content delivery, application acceleration, HTTP, DNS, CDN, Akamai}
}

@article{10.14778/2536222.2536225,
author = {Elmeleegy, Khaled},
title = {Piranha: optimizing short jobs in Hadoop},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536225},
doi = {10.14778/2536222.2536225},
abstract = {Cluster computing has emerged as a key parallel processing platform for large scale data. All major internet companies use it as their major central processing platform. One of cluster computing's most popular examples is MapReduce and its open source implementation Hadoop. These systems were originally designed for batch and massive-scale computations. Interestingly, over time their production workloads have evolved into a mix of a small fraction of large and long-running jobs and a much bigger fraction of short jobs. This came about because these systems end up being used as data warehouses, which store most of the data sets and attract ad hoc, short, data-mining queries. Moreover, the availability of higher level query languages that operate on top of these cluster systems proliferated these ad hoc queries. Since existing systems were not designed for short, latency-sensistive jobs, short interactive jobs suffer from poor response times.In this paper, we present Piranha--a system for optimizing short jobs on Hadoop without affecting the larger jobs. It runs on existing unmodified Hadoop clusters facilitating its adoption. Piranha exploits characteristics of short jobs learned from production workloads at Yahoo! clusters to reduce the latency of such jobs. To demonstrate Piranha's effectiveness, we evaluated its performance using three realistic short queries. Piranha was able to reduce the queries' response times by up to 71%.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {985–996},
numpages = {12}
}

@inproceedings{10.1145/2393347.2396477,
author = {Jiang, Yu-Gang and Dai, Qi and Zheng, Yingbin and Xue, Xiangyang and Liu, Jie and Wang, Dong},
title = {A fast video event recognition system and its application to video search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396477},
doi = {10.1145/2393347.2396477},
abstract = {Techniques for recognizing complex events in diverse Internet videos are important in many applications. State-of-the-art video event recognition approaches normally involve modules that demand extensive computation, which prevents their application to large scale problems. In this demonstration, we present a fast video event recognition system, which requires just a few seconds to process a general YouTube video with a few minutes of duration. The development of this system is grounded on several important findings from a large set of empirical studies, where we systematically evaluated many technical options for each critical module of a present-day video event recognition framework. Pooling the insights gained from this study leads to a speeded-up event recognition system that is 220-times faster than a decent baseline while still has a high degree of recognition accuracy. We also demonstrate the technical feasibility of using event recognition results as the sole clue for video search, where the similarity of videos is determined based on the consistency of the event recognition confidence scores. We showcase this capability using an Internet video dataset containing about 10 thousands of YouTube videos. Very promising results were observed.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1347–1348},
numpages = {2},
keywords = {video search, speed efficiency, internet videos, fast video event recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2063576.2063988,
author = {Pu, Xu and Wang, Jianyong and Luo, Ping and Wang, Min},
title = {AWETO: efficient incremental update and querying in rdf storage system},
year = {2011},
isbn = {9781450307178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063576.2063988},
doi = {10.1145/2063576.2063988},
abstract = {With the fast growth of the knowledge bases built over the Internet, storing and querying millions or billions of RDF triples in a knowledge base have attracted increasing research interests. Although the latest RDF storage systems achieve good querying performance, few of them pay much attention to the characteristic of dynamic growth of the knowledge base. In this paper, to consider the efficiency of both querying and incremental update in RDF data, we propose a hAsh-based tWo-tiEr rdf sTOrage system (abbr. to AWETO) with new index architecture and query execution engine. The performance of our system is systematically measured over two large-scale datesets. Compared with the other three state-of-the-art RDF storage systems, our system achieves the best incremental update efficiency, meanwhile, the query efficiency is competitive.},
booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
pages = {2445–2448},
numpages = {4},
keywords = {rdf, query, index, incremental update, aweto},
location = {Glasgow, Scotland, UK},
series = {CIKM '11}
}

@inproceedings{10.1145/2441776.2441843,
author = {Wang, Yiran and Mark, Gloria},
title = {Trust in online news: comparing social media and official media use by chinese citizens},
year = {2013},
isbn = {9781450313315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2441776.2441843},
doi = {10.1145/2441776.2441843},
abstract = {Since 2006 social media use has grown dramatically in China. Social media has become a stage for citizens to report and disseminate news and to vocalize viewpoints, at times competing with reports from highly curated official media sources. These competing news channels, oftentimes presenting contradictory information, raise questions about citizens' trust in these different media. This study explores the level of trust Chinese Internet users place on news from social media versus official media. We conducted a large-scale anonymous survey in China that revealed that official and citizen news attract different audience groups and each group uses different features to assess news trustworthiness. We present a model for predicting preference for news from citizen media. The results reveal features of social media that explain why some citizens trust it as a channel for news. The results also suggest that in highly regulated news environments, citizen media has the potential to become an alternative news channel where citizens can trust each other for information.},
booktitle = {Proceedings of the 2013 Conference on Computer Supported Cooperative Work},
pages = {599–610},
numpages = {12},
keywords = {trust, social media, news, citizen journalism, authoritarian government},
location = {San Antonio, Texas, USA},
series = {CSCW '13}
}

@inproceedings{10.1145/2077378.2077398,
author = {Bao, Guanbo and Meng, Weiliang and Li, Hongjun and Liu, Jia and Zhang, Xiaopeng},
title = {Hardware instancing for real-time realistic forest rendering},
year = {2011},
isbn = {9781450311380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2077378.2077398},
doi = {10.1145/2077378.2077398},
abstract = {Real-time rendering of vegetation is important in many applications, such as video games, internet graphics applications, landscape design and visualization. However, the visualization of large-scale forests has always been a challenge not only due to the high geometric complexity but also due to the small batch problem. Moreover, generating real-time shadows for forests will heavily increase the burden. The batch problem is caused by a large number of graphics API draw calls launched in every frame. Normally, rendering a tree model requires at least one graphics API draw call. As a forest usually consists of thousands of trees and the graphics API invocation is a relatively high cost for CPU, the large-scale forest rendering is often CPU bound.},
booktitle = {SIGGRAPH Asia 2011 Sketches},
articleno = {16},
numpages = {2},
location = {Hong Kong, China},
series = {SA '11}
}

@inproceedings{10.1145/2413176.2413211,
author = {Wang, Zhi and Li, Baochun and Sun, Lifeng and Yang, Shiqiang},
title = {Cloud-based social application deployment using local processing and global distribution},
year = {2012},
isbn = {9781450317757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2413176.2413211},
doi = {10.1145/2413176.2413211},
abstract = {Social applications represent a paradigm shift on how the Internet is to be used, and have already changed the way we work, live, and play. When it comes to deploying social applications, cloud computing platforms are used to meet the Internet-scale, self-propagating, and fast-growing demands from these applications. Yet, to deploy social media applications in the most effective and economic fashion, we need to strategically design and follow a set of theoretical and practical principles. In this paper, we seek to design a set of new principles to guide social application deployment. Learning from large-scale measurement-based observations using a real-world social application, the gist of our principles is to detach the typically integrated "collection → processing → distribution" work ows in social applications into separate local processing and global distribution procedures, which can be effectively deployed using different cloud services. Moreover, based on a predictive model of regional propagation, we formulate the resource allocation problems in the processes of collecting/processing and distributing content as two optimization problems, which can be solved by efficient algorithms. Finally, based on our theoretical design, we have implemented an example social application on Amazon EC2 and Google AppEngine, where IaaS-based computation instances perform content collection and processing, and the PaaS-based platform is employed to distribute the contents that are widely propagating. Our PlanetLab-based trace-driven experiments have further confirmed the superiority of our design.},
booktitle = {Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies},
pages = {301–312},
numpages = {12},
keywords = {social application deployment, online social network, cloud computing},
location = {Nice, France},
series = {CoNEXT '12}
}

@article{10.1145/2661229.2661290,
author = {Shi, Fuhao and Wu, Hsiang-Tao and Tong, Xin and Chai, Jinxiang},
title = {Automatic acquisition of high-fidelity facial performances using monocular videos},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2661229.2661290},
doi = {10.1145/2661229.2661290},
abstract = {This paper presents a facial performance capture system that automatically captures high-fidelity facial performances using uncontrolled monocular videos (e.g., Internet videos). We start the process by detecting and tracking important facial features such as the nose tip and mouth corners across the entire sequence and then use the detected facial features along with multilinear facial models to reconstruct 3D head poses and large-scale facial deformation of the subject at each frame. We utilize per-pixel shading cues to add fine-scale surface details such as emerging or disappearing wrinkles and folds into large-scale facial deformation. At a final step, we iterate our reconstruction procedure on large-scale facial geometry and fine-scale facial details to further improve the accuracy of facial reconstruction. We have tested our system on monocular videos downloaded from the Internet, demonstrating its accuracy and robustness under a variety of uncontrolled lighting conditions and overcoming significant shape differences across individuals. We show our system advances the state of the art in facial performance capture by comparing against alternative methods.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {222},
numpages = {13},
keywords = {facial performance capture, facial modeling, facial editing, facial detection and tracking, face animation}
}

@inproceedings{10.4108/ICST.SIMUTOOLS2010.8698,
author = {Bless, Roland and R\"{o}hricht, Martin},
title = {Integration of a GIST implementation into OMNeT++},
year = {2010},
isbn = {9789639799875},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/ICST.SIMUTOOLS2010.8698},
doi = {10.4108/ICST.SIMUTOOLS2010.8698},
abstract = {The General Internet Signaling Transport (GIST) protocol was specified by the IETF in order to provide a generic transport protocol for signaling messages. A group at the Institute of Telematics implemented the GIST protocol and evaluated it already in smaller testbed setups. An evaluation of GIST in large-scale scenarios can, however, only be accomplished by using a simulation framework. In this paper we describe how the existing Linux-based and multi-threaded NSIS-ka implementation was ported to the OMNeT++ simulation framework. First we provide an analysis of the different design principles used and then describe related challenges. Then we describe a methodology for integrating an existing real protocol implementation into OMNeT++. The feasibility of the chosen approach is finally demonstrated by a set of evaluations.},
booktitle = {Proceedings of the 3rd International ICST Conference on Simulation Tools and Techniques},
articleno = {19},
numpages = {4},
keywords = {simulation, OMNeT++, NSIS, GIST},
location = {Torremolinos, Malaga, Spain},
series = {SIMUTools '10}
}

@inproceedings{10.1145/1854273.1854298,
author = {Zhao, Jisheng and Shirako, Jun and Nandivada, V. Krishna and Sarkar, Vivek},
title = {Reducing task creation and termination overhead in explicitly parallel programs},
year = {2010},
isbn = {9781450301787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1854273.1854298},
doi = {10.1145/1854273.1854298},
abstract = {There has been a proliferation of task-parallel programming systems to address the requirements of multicore programmers. Current production task-parallel systems include Cilk++, Intel Threading Building Blocks, Java Concurrency, .Net Task Parallel Library, OpenMP 3.0, and current research task-parallel languages include Cilk, Chapel, Fortress, X10, and Habanero-Java (HJ). It is desirable for the programmer to express all the parallelism intrinsic to their algorithm in their code for forward scalability and portability, but the overhead incurred by doing so can be prohibitively large in today's systems. In this paper, we address the problem of reducing the total amount of overhead incurred by a program due to excessive task creation and termination. We introduce a transformation framework to optimize task-parallel programs with finish, forall and next statements. Our approach includes elimination of redundant task creation and termination operations as well as strength reduction of termination operations (finish) to lighter-weight synchronizations (next). Experimental results were obtained on three platforms: a dual-socket 128-thread (16-core) Niagara T2 system, a quad-socket 16-way Intel Xeon SMP and a quad-socket 32-way Power7 SMP. The results showed maximum speedup 66.7x, 11.25x and 23.1x respectively on each platform and 4.6x, 2.1x and 6.4x performance improvements respectively in geometric mean related to non-optimized parallel codes. The original benchmarks in this study were written with medium-grained parallelism; a larger relative improvement can be expected for programs written with finer-grained parallelism. However, even for the medium-grained parallel benchmarks studied in this paper, the significant improvement obtained by the transformation framework underscores the importance of the compiler optimizations introduced in this paper.},
booktitle = {Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
pages = {169–180},
numpages = {12},
keywords = {useful parallelism, redundant tasks, optimization, ideal parallelism, barriers},
location = {Vienna, Austria},
series = {PACT '10}
}

@inproceedings{10.1109/DISCS.2014.11,
author = {Yin, Yanlong and Kougkas, Antonios and Feng, Kun and Eslami, Hassan and Lu, Yin and Sun, Xian-He and Thakur, Rajeev and Gropp, William},
title = {Rethinking key-value store for parallel I/O optimization},
year = {2014},
isbn = {9781479970384},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DISCS.2014.11},
doi = {10.1109/DISCS.2014.11},
abstract = {Key-Value Stores (KVStore) are being widely used as the storage system for large-scale Internet services and cloud storage systems. However, they are rarely used in HPC systems, where parallel file systems (PFS) are the dominant storage systems. In this study, we carefully examine the architecture difference and performance characteristics of PFS and KVStore. We propose that it is valuable to utilize KVStore to optimize the overall I/O performance, especially for the workloads that PFS cannot handle well, such as the cases with hurtful data synchronization or heavy metadata operations. To verify this proposal, we conducted comprehensive experiments with several synthetic benchmarks, an I/O benchmark, and a real application. The results show that our proposal is promising.},
booktitle = {Proceedings of the 2014 International Workshop on Data Intensive Scalable Computing Systems},
pages = {33–40},
numpages = {8},
location = {New Orleans, Louisiana},
series = {DISCS '14}
}

@inproceedings{10.1145/2393347.2393400,
author = {Xu, Qianqian and Huang, Qingming and Yao, Yuan},
title = {Online crowdsourcing subjective image quality assessment},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393400},
doi = {10.1145/2393347.2393400},
abstract = {Recently, HodgeRank on random graphs has been proposed as an effective framework for multimedia quality assessment problem based on paired comparison method. With the random design on large graphs, it is particularly suitable for large scale crowdsourcing experiments on Internet. However, to make it more practical toward this purpose, it is necessary to develop online algorithms to deal with sequential or streaming data. In this paper, we propose an online rating scheme based on HodgeRank on random graphs, to assess image quality when assessors and image pairs enter the system in a sequential way in a crowdsourceable scenario. The scheme is shown in both theory and experiments to be effective by exhibiting similar performance to batch learning under the Erd\"{o}s-R\'{e}nyi random graph model for sampling. It enables us to derive global rating and monitor intrinsic inconsistency in the real time. We demonstrate the effectiveness of the proposed framework on LIVE and IVC databases.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {359–368},
numpages = {10},
keywords = {triangular curl, topology evolution, subjective image quality assessment, random graphs, persistent homology, paired comparison, online, hodgerank, crowdsourcing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2089016.2089025,
author = {Inoue, Tomoya and Takano, Yuuki and Miwa, Shinsuke and Shinoda, Yoichi},
title = {A design and implementation of nickname-based sockets for applications inside NATed network},
year = {2011},
isbn = {9781450310628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2089016.2089025},
doi = {10.1145/2089016.2089025},
abstract = {The central IPv4 address pool managed by Internet Assigned Numbers Authority (IANA) was depleted in January 2011. Nevertheless, almost all nodes on the Internet still continue communicating with each other by using Internet protocol version 4 (IPv4). Since IPv4 addresses have been employed for a long time, it is difficult to immediately shift network layer protocol from IPv4 to IPv6. Therefore, the connectivity using IPv4 address is still required. There is a technology called Large Scale NAT (LSN) which keeps IPv4 address network being connected even though IPv4 addresses are depleted. In the case of home networks and small business networks by LSN, there is usually only a single private IPv4 address on the outside of network interface. Although NAT mechanism has many advantages, it has the negative effect which makes behavior of server side applications unavailable to the Internet. As a result, LSN also has the similar issue.To overcome this issue, we propose a nickname-based network socket software library for future server-side applications. We developed a software which is P2P network based distributed directory service system called "CAGE" having NAT traversal mechanism. Further, using the functions of CAGE software, we also developed a software library of nickname-based sockets called "PRISON". Our proposed software library is available on operating systems such as Linux and MacOS X. In this paper, we discuss the design and implementation of CAGE software and PRISON software library.},
booktitle = {Proceedings of the 7th Asian Internet Engineering Conference},
pages = {48–55},
numpages = {8},
keywords = {naming service, P2P network, NAT traversal},
location = {Bangkok, Thailand},
series = {AINTEC '11}
}

@inproceedings{10.4108/ICST.SIMUTOOLS2010.8630,
author = {Alvarez, Alberto and Orea, Rafael and Cabrero, Sergio and Pa\~{n}eda, Xabiel G. and Garc\'{\i}a, Roberto and Melendi, David},
title = {Limitations of network emulation with single-machine and distributed ns-3},
year = {2010},
isbn = {9789639799875},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/ICST.SIMUTOOLS2010.8630},
doi = {10.4108/ICST.SIMUTOOLS2010.8630},
abstract = {Research on large-scale internet services requires an extensive evaluation prior to deployment. A good analysis must include tests over large networks, using real devices and a considerable number of users. However, how to test in these scenarios with many users is an open question. Network emulation can be a good alternative before real deployments, which are complex and expensive. In this paper, we examine the new ns-3 network simulator/emulator in order to determine its capacity in the evaluation of large scale services. For that purpose, a real client/server video service is deployed over an emulated network. The service is progressively scaled up by increasing the number of clients on a single machine. In addition, we have extended ns-3 to support a distributed architecture for network nodes, thus, we repeat the experiments with a distributed set-up. Advantages, disadvantages, possibilities and limitations of both approaches are thoroughly discussed.},
booktitle = {Proceedings of the 3rd International ICST Conference on Simulation Tools and Techniques},
articleno = {67},
numpages = {9},
keywords = {ns-3, network-simulator, emulation, distributed},
location = {Torremolinos, Malaga, Spain},
series = {SIMUTools '10}
}

@inproceedings{10.1145/2079296.2079310,
author = {Liao, Yongjun and Du, Wei and Geurts, Pierre and Leduc, Guy},
title = {Decentralized prediction of end-to-end network performance classes},
year = {2011},
isbn = {9781450310413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2079296.2079310},
doi = {10.1145/2079296.2079310},
abstract = {In large-scale networks, full-mesh active probing of end-to-end performance metrics is infeasible. Measuring a small set of pairs and predicting the others is more scalable. Under this framework, we formulate the prediction problem as matrix completion, whereby unknown entries of an incomplete matrix of pairwise measurements are to be predicted. This problem can be solved by matrix factorization because performance matrices have a low rank, thanks to the correlations among measurements. Moreover, its resolution can be fully decentralized without actually building matrices nor relying on special landmarks or central servers.In this paper we demonstrate that this approach is also applicable when the performance values are not measured exactly, but are only known to belong to one among some predefined performance classes, such as "good" and "bad". Such classification-based formulation not only fulfills the requirements of many Internet applications but also reduces the measurement cost and enables a unified treatment of various performance metrics. We propose a decentralized approach based on Stochastic Gradient Descent to solve this class-based matrix completion problem. Experiments on various datasets, relative to two kinds of metrics, show the accuracy of the approach, its robustness against erroneous measurements and its usability on peer selection.},
booktitle = {Proceedings of the Seventh COnference on Emerging Networking EXperiments and Technologies},
articleno = {14},
numpages = {12},
keywords = {stochastic gradient descent, network performance classification, matrix factorization, matrix completion},
location = {Tokyo, Japan},
series = {CoNEXT '11}
}

@inproceedings{10.1145/2479871.2479924,
author = {Gao, Yuqing},
title = {Data centric computing for internet scale enterprises},
year = {2013},
isbn = {9781450316361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479871.2479924},
doi = {10.1145/2479871.2479924},
abstract = {In the era of exploding internet usage, social and mobile, enterprises are facing both the challenges and business opportunities that are introduced by Big Data, which has the characteristics of high volume, high velocity, and high variety. Big Data and the emergence of Internet-facing workloads will blur the separation between traditional transactional and analytics workloads. To extract business value and make actionable insight from the unprecedented volume of the data with the agility required from the business, it requires transformational innovations from many fronts. For example, in data management layer, how unstructured data is stored and retrieved efficiently, how data-intensive analytic computation can be done on commercial systems effectively, how the distributed cache should be designed to make use of the latest network protocols so the network-connected memory data can be accessed remotely and seamlessly. Moreover, the trend also motivates many architectural and technological advancement, such as moving from a transaction-centric to a data-centric architecture that supports extreme low and predicable latency, massive scale-out, high concurrency, and real-time situational awareness and analytics, and that requires orders of magnitude improvement over existing systems across each of these characteristics. At the same time, new applications in the Mobile and social space leverage new open source software stacks written in multiple programming languages, e.g., Java, JavaScript, Ruby, PHP, where the developer chooses the best tool for the job. How a polyglot runtime platform can be built that serves as a best practice platform for the programmers' community and in the meantime, optimized for enterprises with elastic, lightweight, resilient, agile runtime for business computing. Last, but not least, how the benchmarks should be enriched to measure the new runtimes, new data-centric systems and architectures.In this talk, I will talk about some of the research activities at IBM Research that addresses these challenges. We examined several enterprise-grade java workloads running on commercial multicore systems for massive parallelization, identified lock contentions and worked towards a streamlined methodology for lock-contention analysis of Java workloads. I will use this to describe the excitement around node.js framework. I will also talk about our design of data centric computing systems, particularly, in the area of data access latency, data ingestion, and massive scale-out distributed caching in the exemplary context of an eCommerce application. I will describe the architecture of a global secondary index to greatly improve data access latency of Hadoop Database (HBase), an open-source key-value distributed datastore. I will describe an innovative distributed caching system that exploits low latency interconnects to utilize hash maps of data keys on each server for local lookup while data resides and are accessed across clustered systems. The distributed cache can achieves 100 to 1000-fold performance gain over many caching methods. Last, I will talk about our early activities in developing technologies for an elastic, scalable, resilient polyglot runtime system. I will conclude with my views on the challenges for benchmarking community for next decade.},
booktitle = {Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering},
pages = {347–348},
numpages = {2},
keywords = {internet scale, data centric computing},
location = {Prague, Czech Republic},
series = {ICPE '13}
}

@article{10.1145/2677046.2677049,
author = {Ghali, Cesar and Tsudik, Gene and Uzun, Ersin},
title = {Network-Layer Trust in Named-Data Networking},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {5},
issn = {0146-4833},
url = {https://doi.org/10.1145/2677046.2677049},
doi = {10.1145/2677046.2677049},
abstract = {In contrast to today's IP-based host-oriented Internet architecture, Information-Centric Networking (ICN) emphasizes content by making it directly addressable and routable. Named Data Networking (NDN) architecture is an instance of ICN that is being developed as a candidate next-generation Internet architecture. By opportunistically caching content within the network, NDN appears to be well-suited for large-scale content distribution and for meeting the needs of increasingly mobile and bandwidth-hungry applications that dominate today's Internet.One key feature of NDN is the requirement for each content object to be digitally signed by its producer. Thus, NDN should be, in principle, immune to distributing fake (aka "poisoned") content. However, in practice, this poses two challenges for detecting fake content in NDN routers: (1) overhead due to signature verification and certificate chain traversal, and (2) lack of trust context, i.e., determining which public keys are trusted to verify which content. Because of these issues, NDN does not force routers to verify content signatures, which makes the architecture susceptible to content poisoning attacks.This paper explores root causes of, and some cures for, content poisoning attacks in NDN. In the process, it becomes apparent that meaningful mitigation of content poisoning is contingent upon a network-layer trust management architecture, elements of which we construct, while carefully justifying specific design choices. This work represents the initial effort towards comprehensive trust management for NDN.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {oct},
pages = {12–19},
numpages = {8},
keywords = {trust, named-data networking, future internet architecture}
}

@inproceedings{10.1145/2324796.2324805,
author = {Jiang, Yu-Gang},
title = {SUPER: towards real-time event recognition in internet videos},
year = {2012},
isbn = {9781450313292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2324796.2324805},
doi = {10.1145/2324796.2324805},
abstract = {Event recognition in unconstrained Internet videos has great potential in many applications. State-of-the-art systems usually include modules that need extensive computation, such as the extraction of spatial-temporal interest points, which poses a big challenge for large-scale video processing. This paper presents SUPER, a Speeded UP Event Recognition framework for efficient Internet video analysis. We take a multimodal baseline that has produced strong performance on popular benchmarks, and systematically evaluate each component in terms of both computational cost and contribution to recognition accuracy. We show that, by choosing suitable features, classifiers, and fusion strategies, recognition speed can be greatly improved with minor performance degradation. In addition, we also evaluate how many visual and audio frames are needed for event recognition in Internet videos, a question left unanswered in the literature. Results on a rigorously designed dataset indicate that similar recognition accuracy can be attained using only 14 frames per video on average. We also observe that, different from the visual channel, the soundtracks contains little redundant information for video event recognition. Integrating all the findings, our suggested SUPER framework is 220-fold faster than the baseline approach with merely 3.8% drop in recognition accuracy. It classifies an 80-second video sequence using models of 20 classes in just 4.56 seconds.},
booktitle = {Proceedings of the 2nd ACM International Conference on Multimedia Retrieval},
articleno = {7},
numpages = {8},
keywords = {multimodal features, internet videos, frame selection, event recognition, efficiency},
location = {Hong Kong, China},
series = {ICMR '12}
}

@proceedings{10.1145/1996029,
title = {LSAP '11: Proceedings of the third international workshop on Large-scale system and application performance},
year = {2011},
isbn = {9781450307031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the third Workshop on Large-scale System and Application Performance -- LSAP2011. We initiated this workshop two years ago as we think it addresses a very important and timely subject. Over the last decade, computer systems and applications in everyday use have grown to unprecedented scales. Large clusters serving millions of search requests per day, grids executing large workflows and parameter sweeps consisting of thousands of jobs, and supercomputers running complex e-science applications, have now hundreds of thousands of processing cores. In addition, clouds are quickly emerging as a large-scale computing infrastructure. Peer-to-peer systems and centralized video distribution systems that dominate the internet and complicated internet applications such as massive multiplayer online games are used by millions of people every day.In view of this tremendous growth, understanding the performance of large-scale computer systems and applications has become vital to institutional, commercial, and private interests. This workshop intends to be a venue for papers on performance evaluation methods, tools, and studies focusing on the challenges of large scale, such as decentralization, predictable performance, reliability, and scalability. It aims to bring together system designers and researchers involved with the modeling and performance evaluation of largescale systems and applications.The call for papers of the workshop attracted 8 submissions, out of which the program committee accepted 5 papers that cover a variety of topics, ranging from lock thrashing in multicore systems and file systems for exascale computing, to the visual analysis of I/O system behavior, the multi-scale analysis of large distributed systems, and the analysis of social gaming networks. In addition, the workshop program features a keynote presentation by Marc Snir of the University of Illinois at Urbana-Champaign, USA, on performance engineering for petascale systems and beyond.},
location = {San Jose, California, USA}
}

@article{10.1109/TNET.2009.2023322,
author = {Lee, Sanghwan and Zhang, Zhi-Li and Sahu, Sambit and Saha, Debanjan},
title = {On suitability of Euclidean embedding for host-based network coordinate systems},
year = {2010},
issue_date = {February 2010},
publisher = {IEEE Press},
volume = {18},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2009.2023322},
doi = {10.1109/TNET.2009.2023322},
abstract = {In this paper, we investigate the suitability of embedding Internet hosts into a Euclidean space given their pairwise distances (as measured by round-trip time). Using the classical scaling and matrix perturbation theories, we first establish the (sum of the) magnitude of negative eigenvalues of the (doubly centered, squared) distance matrix as a measure of suitability of Euclidean embedding.We then show that the distance matrix among Internet hosts contains negative eigenvalues of large magnitude, implying that embedding the Internet hosts in a Euclidean spacewould incur relatively large errors. Motivated by earlier studies, we demonstrate that the inaccuracy of Euclidean embedding is caused by a large degree of triangle inequality violation (TIV) in the Internet distances, which leads to negative eigenvalues of large magnitude. Moreover, we show that the TIVs are likely to occur locally; hence the distances among these close-by hosts cannot be estimated accurately using a global Euclidean embedding. In addition, increasing the dimension of embedding does not reduce the embedding errors. Based on these insights, we propose a new hybrid model for embedding the network nodes using only a two-dimensional Euclidean coordinate system and small error adjustment terms. We show that the accuracy of the proposed embedding technique is as good as, if not better than, that of a seven-dimensional Euclidean embedding.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {27–40},
numpages = {14},
keywords = {triangle inequality, suitability, Euclidean embedding}
}

@inproceedings{10.1145/2089016.2089017,
author = {Seskar, Ivan and Nagaraja, Kiran and Nelson, Sam and Raychaudhuri, Dipankar},
title = {MobilityFirst future internet architecture project},
year = {2011},
isbn = {9781450310628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2089016.2089017},
doi = {10.1145/2089016.2089017},
abstract = {This short paper presents an overview of the MobilityFirst network architecture, which is a clean-slate project being conducted as part of the NSF Future Internet Architecture (FIA) program. The proposed architecture is intended to directly address the challenges of wireless access and mobility at scale, while also providing new multicast, anycast, multi-path and context-aware services needed for emerging mobile Internet application scenarios. Key protocol components of the proposed architecture are: (a) separation of naming from addressing; (b) public key based self-certifying names (called globally unique identifiers or GUIDs) for network-attached objects; (c) global name resolution service (GNRS) for dynamic name-to-address binding; (d) delay-tolerant and storage-aware routing (GSTAR) capable of dealing with wireless link quality fluctuations and disconnections; (e) hop-by-hop transport of large protocol data units; and (f) location or context-aware services. The basic operations of a MobilityFirst router are outlined. This is followed by a discussion of ongoing proof-of-concept prototyping and experimental evaluation efforts for the MobilityFirst protocol stack. In conclusion, a brief description of an ongoing multi-site experimental deployment of the MobilityFirst protocol stack on the GENI testbed is provided.},
booktitle = {Proceedings of the 7th Asian Internet Engineering Conference},
pages = {1–3},
numpages = {3},
keywords = {storage-aware routing, name resolution, mobile networks, future internet architecture, GENI prototyping},
location = {Bangkok, Thailand},
series = {AINTEC '11}
}

@inproceedings{10.1007/978-3-642-13645-0_1,
author = {Romero, Daniel and Hermosillo, Gabriel and Taherkordi, Amirhosein and Nzekwa, Russel and Rouvoy, Romain and Eliassen, Frank},
title = {RESTful integration of heterogeneous devices in pervasive environments},
year = {2010},
isbn = {3642136443},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-13645-0_1},
doi = {10.1007/978-3-642-13645-0_1},
abstract = {More and more home devices are equipped with advanced computational capabilities to improve the user satisfaction (e.g., programmable heating system, Internet TV). Although these devices exhibit communication capabilities, their integration into a larger home monitoring system remains a challenging task, partly due to the strong heterogeneity of technologies and protocols. In this paper, we therefore propose to reconsider the architecture of home monitoring systems by focusing on data and events that are produced and triggered by home devices. In particular, our middleware platform, named DigiHome, applies i) the REST (REpresentational State Transfer) architectural style to leverage on the integration of multi-scale systems-of-systems (from Wireless Sensor Networks to the Internet) and ii) a CEP (Complex Event Processing) engine to collect information from heterogeneous sources and detect application-specific situations. The benefits of the DigiHome platform are demonstrated on smart home scenarios covering home automation, emergency detection, and energy saving situations.},
booktitle = {Proceedings of the 10th IFIP WG 6.1 International Conference on Distributed Applications and Interoperable Systems},
pages = {1–14},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {DAIS'10}
}

@inproceedings{10.1145/2304696.2304714,
author = {Yu, Jian and Han, Jun and Schneider, Jean-Guy and Hine, Cameron and Versteeg, Steve},
title = {A virtual deployment testing environment for enterprise software systems},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304714},
doi = {10.1145/2304696.2304714},
abstract = {Modern enterprise software systems often need to interact with a large number of heterogeneous systems in an enterprise IT environment. The distributedness, large-scale-ness, and heterogeneity of such environment makes it difficult to test a system's quality attributes such as performance and scalability before it is actually deployed in the environment. In this paper, we present a Coloured Petri nets (CPN) based system behaviour emulation approach and a lightweight virtual testing framework for provisioning the deployment testing environment of an enterprise system so that its quality attributes, especially scalability, can be evaluated without physically connecting to the real production environment. This testing environment is scalable and has a flexible pluggable architecture to support the emulation of the behaviour of heterogeneous systems in the environment. To validate the feasibility of this approach, a CPN emulation model for LDAP has been developed and applied in testing the scalability of a real-life identity management system. An in-lab performance study has been conducted to demonstrate the effectiveness of this approach.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {101–110},
numpages = {10},
keywords = {system emulation, petri nets, enterprise software systems, deployment testing},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@inproceedings{10.1145/1774088.1774597,
author = {Baude, Fran\c{c}oise and Filali, Imen and Huet, Fabrice and Legrand, Virginie and Mathias, Elton and Merle, Philippe and Ruz, Cristian and Krummenacher, Reto and Simperl, Elena and Hammerling, Christophe and Lorre, Jean-Pierre},
title = {ESB federation for large-scale SOA},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774597},
doi = {10.1145/1774088.1774597},
abstract = {Embracing service-oriented architectures in the context of large systems, such as the Web, rises a set of new and challenging issues: increased size and load in terms of users and services, distribution, and dynamicity. A top-down federation of service infrastructure support that we name "service cloud" and that is capable of growing to the scale of the Internet, is seen as a promising response to such new challenges. In this paper, we define the service cloud concept, its promises and the requirements in terms of architecture and the corresponding middleware. We present some preliminary proofs of concept through the integration of a JBI-compliant enterprise service bus, extended to our needs, and a scalable semantic space infrastructure, both relying on an established grid middleware environment. The new approach offers service consumers and providers a fully transparent, distributed and federated means to access, compose and deploy services on the Internet. Technically, our contribution advances core service bus technology towards the service cloud by scaling the registries and message routers to the level of federations via a hierarchical approach, and by incorporating the communication and coordination facilities offered by a global semantic space.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2459–2466},
numpages = {8},
keywords = {service oriented architecture, semantic storage, grid and cloud computing, federation, JBI, ESB},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/1787275.1787345,
author = {Stone, Andrew Ian and DiBenedetto, Steven and Mills Strout, Michelle and Massey, Daniel},
title = {Scalable simulation of complex network routing policies},
year = {2010},
isbn = {9781450300445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1787275.1787345},
doi = {10.1145/1787275.1787345},
abstract = {Modern routing protocols for the internet implement complex policies that take more into account than just path length. However, current routing protocol simulators are limited to either working with hard-coded policies or working on small networks (1000 nodes or less). It is currently not possible to ask questions about how the routing tables will change on all of the autonomous systems (e.g., AT&amp;T, Sprint, etc.) in the internet, given a change in the routing protocol. This paper presents a routing policy simulation framework that enables such simulations to be done on resources that are readily available to researchers, such as a small set of typical desktops. We base the policy simulation framework on the Routing Algebra Meta-Language (RAML), which is a formal framework for specifying routing policies. Our theoretical contributions include proving that the signatures and the meet operation induced by the preference operator in RAML define a semilattice and that routing policy simulation frameworks are analogous to data-flow analysis frameworks.The main problem we address is that direct implementation of routing policy simulation has scaling issues due to the O(n^2) memory requirements for routing tables. However, due to properties of routing algebras specified in RAML, we are able to segment the simulation problem into multiple runs that propagate route information for subsets of the network on each run. This strategy enables us to perform a simulation that does not exceed system memory on typical desktops and enables the 43 minute, parallel simulation of a real network topology (33k nodes) and an approximation of the common BGP protocol.},
booktitle = {Proceedings of the 7th ACM International Conference on Computing Frontiers},
pages = {347–356},
numpages = {10},
keywords = {simulation, routing, performance, parallel, metarouting, data-flow analysis},
location = {Bertinoro, Italy},
series = {CF '10}
}

@inproceedings{10.1145/2145204.2145215,
author = {Mark, Gloria and Bagdouri, Mossaab and Palen, Leysia and Martin, James and Al-Ani, Ban and Anderson, Kenneth},
title = {Blogs as a collective war diary},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145215},
doi = {10.1145/2145204.2145215},
abstract = {Disaster-related research in human-centered computing has typically focused on the shorter-term, emergency period of a disaster event, whereas effects of some crises are long-term, lasting years. Social media archived on the Internet provides researchers the opportunity to examine societal reactions to a disaster over time. In this paper we examine how blogs written during a protracted conflict might reflect a collective view of the event. The sheer amount of data originating from the Internet about a significant event poses a challenge to researchers; we employ topic modeling and pronoun analysis as methods to analyze such large-scale data. First, we discovered that blog war topics temporally tracked the actual, measurable violence in the society suggesting that blog content can be an indicator of the health or state of the affected population. We also found that people exhibited a collective identity when they blogged about war, as evidenced by a higher use of first-person plural pronouns compared to blogging on other topics. Blogging about daily life decreased as violence in the society increased; when violence waned, there was a resurgence of daily life topics, potentially illustrating how a society returns to normalcy.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {37–46},
numpages = {10},
keywords = {war, topic modeling, longitudinal study, crisis informatics, crisis, collective identity, blogs},
location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, <country>USA</country>, </conf-loc>},
series = {CSCW '12}
}

@inproceedings{10.1145/2133601.2133607,
author = {Yang, Yuhao and Lutes, Jonathan and Li, Fengjun and Luo, Bo and Liu, Peng},
title = {Stalking online: on user privacy in social networks},
year = {2012},
isbn = {9781450310918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2133601.2133607},
doi = {10.1145/2133601.2133607},
abstract = {With the extreme popularity of Web and online social networks, a large amount of personal information has been made available over the Internet. On the other hand, advances in information retrieval, data mining and knowledge discovery technologies have enabled users to efficiently satisfy their information needs over the Internet or from large-scale data sets. However, such technologies also help the adversaries such as web stalkers to discover private information about their victims from mass data.In this paper, we study privacy-sensitive information that are accessible from the Web, and how these information could be utilized to discover personal identities. In the proposed scenario, an adversary is assumed to possess a small piece of "seed" information about a targeted user, and conduct extensive and intelligent search to identify the target over both the Web and an information repository collected from the Web. In particular, two types of attackers are modeled, namely tireless attackers and resourceful attackers. We then analyze detailed attacking mechanisms that could be performed by these attackers, and quantify the threats of both types of attacks to general Web users. With extensive experiments and sophisticated analysis, we show that a large portion of users with online presence are highly identifiable, even when only a small piece of (possibly inaccurate) seed information is known to the attackers.},
booktitle = {Proceedings of the Second ACM Conference on Data and Application Security and Privacy},
pages = {37–48},
numpages = {12},
keywords = {web, social networks, privacy, attacks},
location = {San Antonio, Texas, USA},
series = {CODASPY '12}
}

@article{10.1145/1764873.1764880,
author = {Choffnes, David R. and Bustamante, Fabian E.},
title = {Pitfalls for testbed evaluations of internet systems},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/1764873.1764880},
doi = {10.1145/1764873.1764880},
abstract = {Today's open platforms for network measurement and distributed system research, which we collectively refer to as testbeds in this article, provide opportunities for controllable experimentation and evaluations of systems at the scale of hundreds or thousands of hosts. In this article, we identify several issues with extending results from such platforms to Internet wide perspectives. Specifically, we try to quantify the level of inaccuracy and incompleteness of testbed results when applied to the context of a large-scale peer-to-peer (P2P) system. Based on our results, we emphasize the importance of measurements in the appropriate environment when evaluating Internet-scale systems.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {apr},
pages = {43–50},
numpages = {8},
keywords = {peer-to-peer, internet-scale systems, evaluation}
}

@inproceedings{10.5555/2151054.2151125,
author = {Baumgart, Ingmar and Gamer, Thomas and H\"{u}bsch, Christian and Mayer, Christoph P.},
title = {Realistic underlays for overlay simulation},
year = {2011},
isbn = {9781936968008},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Overlay networks have become an enabler for innovation in today's Internet through cost-efficient and flexible deployment of novel services. The self-organization and scalability properties that peer-to-peer-based overlay networks provide have created real-world large-scale systems like Kad, or Amazon's Dynamo. Building upon the OMNeT++ simulation environment, the OverSim framework provides widely used simulation of a large and growing set of overlay networks. Realistic environments for evaluation of such networks are crucial to obtain meaningful results, yet complex to develop and validate. The ReaSE topology and traffic generator allows to create Internet-like network topologies, background traffic, and attack traffic. In this work we integrate ReaSE with OverSim, therewith allowing for evaluation of overlay protocols upon realistic underlays and realistic background traffic. This integration provides an important step for design and evaluation of overlay-based systems and allows for meaningful results. We provide insights into runtime and memory consumptions of overlay simulations on the new ReaSE-based underlay on the one hand, and show effects on overlay protocols caused by the realistic underlay on the other hand.},
booktitle = {Proceedings of the 4th International ICST Conference on Simulation Tools and Techniques},
pages = {402–405},
numpages = {4},
location = {Barcelona, Spain},
series = {SIMUTools '11}
}

@inproceedings{10.5555/2048476.2048480,
author = {Mittal, Saurabh and Douglass, Scott A.},
title = {Net-centric act-R-based cognitive architecture with DEVS unified process},
year = {2011},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Air Force Research Lab (AFRL) research efforts employing cognitive and behavioral modeling are growing in scope and complexity as they work to integrate models into larger distributed systems as cognitive agents, synthetic teammates or human operator surrogates. Efforts to transition cognitive modeling from the laboratory to operational environments are stymied by the isolated nature of current cognitive modeling software tools that are not readily extensible, interoperable or scalable e.g. ACT-R. In this paper, we describe an attempt to build a component-based architecture using the Discrete Event Systems Unified Process (DUNIP) on a distributed net-centric platform that eliminates these impediments. We show how the ACT-R architecture is extensible and can serve as a component in larger net-centric systems of systems frameworks such as Department of Defense Architecture Framework. We will also address the issue of platform independent modeling and how Domain Specific Languages (DSLs) can be integrated within DUNIP. We then demonstrate how developing the architecture and related software infrastructure in DUNIP gives it net-centric capabilities that support large-scale integration.},
booktitle = {Proceedings of the 2011 Symposium on Theory of Modeling &amp; Simulation: DEVS Integrative M&amp;S Symposium},
pages = {34–44},
numpages = {11},
keywords = {SOA, DoDAF, DUNIP, DSL, DEVSML, ACT-R},
location = {Boston, Massachusetts},
series = {TMS-DEVS '11}
}

@inproceedings{10.1109/PADS.2010.5471662,
author = {Hogie, Luc and Papadimitriou, Dimitri and Tahiri, Issam and Majorczyk, Frederic},
title = {Simulating Routing Schemes on Large-Scale Topologies},
year = {2010},
isbn = {9781424472925},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PADS.2010.5471662},
doi = {10.1109/PADS.2010.5471662},
abstract = {The expansion of the Internet routing system results in a number of research challenges, in particular, the Border Gateway Protocol (BGP) starts to show its limits a.o. in terms of the number of routing table entries it can dynamically process and control. Dynamic routing protocols showing better scaling properties are thus under investigation. However, because deploying under-development routing protocols on the Internet is not practicable at a large-scale (due to the size of the Internet topology), simulation is an unavoidable step to validate the properties of a newly proposed routing scheme. Unfortunately, the simulation of inter- domain routing protocols over large networks (order of tens of thousands of nodes) poses real challenges due to the limited memory and computational power that computers impose. This paper presents the Dynamic Routing Model simulator (DRMsim) which addresses the specific problem of large-scale simulations of (inter-domain) routing models on large networks. The motivation for developing a new simulator lies in the limitation of existing simulation tools in terms of the number of nodes they can handle and in the models they propose.},
booktitle = {Proceedings of the 2010 IEEE Workshop on Principles of Advanced and Distributed Simulation},
pages = {132–141},
numpages = {10},
keywords = {routing table number, routing schemes simulation, large scale topologies, inter-domain routing, dynamic routing protocols, dynamic routing model simulator, border gateway protocol, Internet routing system, DRMsim, BGP},
series = {PADS '10}
}

@inproceedings{10.1145/1998076.1998131,
author = {Li, Ruifeng and Zhang, Yin and Yu, Haihan and Wang, Xiaojun and Wu, Jiangqin and Wei, Baogang},
title = {A social network-aware top-N recommender system using GPU},
year = {2011},
isbn = {9781450307444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1998076.1998131},
doi = {10.1145/1998076.1998131},
abstract = {A book recommender system is very useful for a digital library. Good book recommender systems can effectively help users find interesting and relevant books from the massive resources, by providing individual recommendation book list for each end-user. By now, a variety of collaborative filtering algorithms have been invented, which are the cores of most recommender systems. However, because of the explosion of information, especially in the Internet, the improvement of the efficiency of the collaborative filting (CF) algorithm becomes more and more important. In this paper, we first propose a parallel Top-N recommendation algorithm in CUDA (Compute Unified Device Architecture) which combines the collaborative filtering and trust-based approach to deal with the cold-start user problem. Then based on this algorithm, we present a parallel book recommender system on a GPU (Graphics Processor unit) for CADAL digital library platform. Our experimental results show our algorithm is very efficient to process the large-scale datasets with good accuracy, and we report the impact of different values of parameters on the recommendation performance.},
booktitle = {Proceedings of the 11th Annual International ACM/IEEE Joint Conference on Digital Libraries},
pages = {287–296},
numpages = {10},
keywords = {recommendation, digital library, collaborative filtering, GPU, CUDA},
location = {Ottawa, Ontario, Canada},
series = {JCDL '11}
}

@inproceedings{10.1145/2642687.2642694,
author = {Sanchez-Iborra, Ramon and Cano, Maria-Dolores},
title = {Qoe-based performance evaluation of video transmission using the BATMAN routing protocol},
year = {2014},
isbn = {9781450330275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642687.2642694},
doi = {10.1145/2642687.2642694},
abstract = {Video communications has become one of the most demanded services among end users. Furthermore, telcos forecast a great growth of this kind of transmissions during the next years, representing the majority of total Internet traffic. The strict requirements of multimedia services pose a great challenge to beat in order to provide acceptable levels of quality; even more in wireless systems, whose inherent characteristics difficult the achievement of the desired multimedia quality. Regarding wireless accessing, Mobile Ad-hoc NETworks (MANETs) are receiving a great attention due to its ease-of-deployment and decentralized architecture. The dynamic nature of these networks adds further issues for the quality provisioning of video services. For that reason, efficient ad-hoc routing protocols are needed. In this work, we evaluate the performance of the proactive routing protocol BATMAN supporting video traffic over different mesh topologies. Using computer simulation, we compare the results obtained with BATMAN with those attained for the extended and well-known routing protocol OLSR. From the results, we conclude that BATMAN presents more robustness than OLSR against the impairments introduced by the wireless medium for video transmission. However, BATMAN presents some scalability issues compared to OLSR. We show that BATMAN can be improved in terms of scalability with a proper tuning of the control-packets flooding interval.},
booktitle = {Proceedings of the 10th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {9–16},
numpages = {8},
keywords = {video, multimedia, manet, QoS, QoE, BATMAN},
location = {Montreal, QC, Canada},
series = {Q2SWinet '14}
}

@inproceedings{10.1145/1993744.1993782,
author = {Sengupta, Sudipta},
title = {Cloud data center networks: technologies, trends, and challenges},
year = {2011},
isbn = {9781450308144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993744.1993782},
doi = {10.1145/1993744.1993782},
abstract = {Large scale data centers are enabling the new era of Internet cloud computing. The computing platform in such data centers consists of low-cost commodity servers that, in large numbers and with software support, match the performance and reliability of expensive enterprise-class servers of yesterday, at a fraction of the cost. The network interconnect within the data center, however, has not seen the same scale of commoditization or dropping price points. Today's data centers use expensive enterprise-class networking equipment and associated best-practices that were not designed for the requirements of Internet-scale data center services -- they severely limit server-to-server network capacity, create fragmented pools of servers that do not allow any service to run on any server, and have poor reliability and utilization. The commoditization and redesign of data center networks to meet cloud computing requirements is the next frontier of innovation in the data center.Recent research in data center networks addresses many of these aspects involving both scale and commoditization. By creating large flat Layer 2 networks, data centers can provide the view of a flat unfragmented pool of servers to hosted services. By using traffic engineering methods (based on both oblivious and adaptive routing techniques) on specialized network topologies, the data center network can handle arbitrary and rapidly changing communication patterns between servers. By making data centers modular for incremental growth, the up-front investment in infrastructure can be reduced, thus increasing their economic feasibility. This is an exciting time to work in the data center networking area, as the industry is on the cusp of big changes, driven by the need to run Internet-scale services, enabled by the availability of low-cost commodity switches/routers, and fostered by creative and novel architectural innovations.What the Tutorial will cover: We will begin with an introduction to data centers for Internet/cloud services. We will survey several next-generation data center network designs that meet the criteria of allowing any service to run on any server in a flat unfragmented pool of servers and providing bandwidth guarantees for arbitrary communication patterns among servers (limited only by server line card rates). These span efforts from academia and industry research labs, including VL2, Portland, SEATTLE, Hedera, and BCube, and ongoing standardization activities like IEEE Data Center Ethernet (DCE) and IEEE TRILL. We will also cover other emerging aspects of data center networking like energy proportionality for greener data center networks.},
booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {355–356},
numpages = {2},
keywords = {scalable commodity networking, data center traffic measurement, data center networks, cloud data centers},
location = {San Jose, California, USA},
series = {SIGMETRICS '11}
}

@article{10.1145/2007116.2007175,
author = {Sengupta, Sudipta},
title = {Cloud data center networks: technologies, trends, and challenges},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2007116.2007175},
doi = {10.1145/2007116.2007175},
abstract = {Large scale data centers are enabling the new era of Internet cloud computing. The computing platform in such data centers consists of low-cost commodity servers that, in large numbers and with software support, match the performance and reliability of expensive enterprise-class servers of yesterday, at a fraction of the cost. The network interconnect within the data center, however, has not seen the same scale of commoditization or dropping price points. Today's data centers use expensive enterprise-class networking equipment and associated best-practices that were not designed for the requirements of Internet-scale data center services -- they severely limit server-to-server network capacity, create fragmented pools of servers that do not allow any service to run on any server, and have poor reliability and utilization. The commoditization and redesign of data center networks to meet cloud computing requirements is the next frontier of innovation in the data center.Recent research in data center networks addresses many of these aspects involving both scale and commoditization. By creating large flat Layer 2 networks, data centers can provide the view of a flat unfragmented pool of servers to hosted services. By using traffic engineering methods (based on both oblivious and adaptive routing techniques) on specialized network topologies, the data center network can handle arbitrary and rapidly changing communication patterns between servers. By making data centers modular for incremental growth, the up-front investment in infrastructure can be reduced, thus increasing their economic feasibility. This is an exciting time to work in the data center networking area, as the industry is on the cusp of big changes, driven by the need to run Internet-scale services, enabled by the availability of low-cost commodity switches/routers, and fostered by creative and novel architectural innovations.What the Tutorial will cover: We will begin with an introduction to data centers for Internet/cloud services. We will survey several next-generation data center network designs that meet the criteria of allowing any service to run on any server in a flat unfragmented pool of servers and providing bandwidth guarantees for arbitrary communication patterns among servers (limited only by server line card rates). These span efforts from academia and industry research labs, including VL2, Portland, SEATTLE, Hedera, and BCube, and ongoing standardization activities like IEEE Data Center Ethernet (DCE) and IEEE TRILL. We will also cover other emerging aspects of data center networking like energy proportionality for greener data center networks.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {355–356},
numpages = {2},
keywords = {scalable commodity networking, data center traffic measurement, data center networks, cloud data centers}
}

@inproceedings{10.1145/2619287.2619305,
author = {Mohammed, A. A. Alaa and Okamura, Koji},
title = {Distributed GA for popularity based partial cache management in ICN},
year = {2014},
isbn = {9781450329422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619287.2619305},
doi = {10.1145/2619287.2619305},
abstract = {Information Centric Networks (ICNs) is a new architecture for the Future Internet to deliver content at large-scale. It relies on named data and caching features, which consists of storing content across the delivery path to serve forthcoming requests. In this paper, we study the problem of finding the optimal assignment of the objects in the available caches in ICN. The optimization problem is to cache objects in order to minimize overall network overhead. We formulate this problem as a combinatorial optimization problem. We show that this optimization problem is NP complete. Metaheuristic methods are considered as effective methods for solving this problem, Genetic Algorithm (GA) is one of those algorithms that can solve this problem efficiently. We will adapt cache management system based on GA for solving the considered problem. In contrast to traditional locally caching systems this algorithm consider both global and local search and make caching decisions about where and which item will be cached in order to minimize overall network overhead.},
booktitle = {Proceedings of The Ninth International Conference on Future Internet Technologies},
articleno = {18},
numpages = {2},
keywords = {information centric networks (ICNs), genetic algorithm (GA)},
location = {Tokyo, Japan},
series = {CFI '14}
}

@inproceedings{10.1145/2037556.2037581,
author = {Linders, Dennis},
title = {We-Government: an anatomy of citizen coproduction in the information age},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037556.2037581},
doi = {10.1145/2037556.2037581},
abstract = {This paper examines whether the tools of the Information Age---principally but not exclusively the Internet---make citizen coproduction of government services more viable and effective. The paper first discusses the re-emergence of citizen coproduction as a fashionable policy option in the face of persistent budget deficits, the rise of "government by network," and the advent of mass "peer-production." Finding a plethora of competing labels, models, and concepts for Internet-facilitated coproduction, the paper proposes a formal taxonomy to provide a more robust framework for systematic analysis. The paper then applies this framework to evaluate the impact of the tools of the Information Age on citizen coproduction. Its findings cautiously support the claim that the Information Age enables and advances new forms of citizen coproduction, namely large-scale "Do-It-Yourself Government" and "Government as a Platform." The paper concludes with a discussion of the potential implications for public administration, including the possible emergence of a new social contract that empowers the public to play a far more active role in the functioning of their government.},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
pages = {167–176},
numpages = {10},
keywords = {service delivery, public administration, information age, government as platform, e-government, digital government, crowdsourcing, coproduction, collaborative governance},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/2342356.2342398,
author = {Gao, Peter Xiang and Curtis, Andrew R. and Wong, Bernard and Keshav, Srinivasan},
title = {It's not easy being green},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342398},
doi = {10.1145/2342356.2342398},
abstract = {Large-scale Internet applications, such as content distribution networks, are deployed across multiple datacenters and consume massive amounts of electricity. To provide uniformly low access latencies, these datacenters are geographically distributed and the deployment size at each location reflects the regional demand for the application. Consequently, an application's environmental impact can vary significantly depending on the geographical distribution of end-users, as electricity cost and carbon footprint per watt is location specific. In this paper, we describe FORTE: Flow Optimization based framework for request-Routing and Traffic Engineering. FORTE dynamically controls the fraction of user traffic directed to each datacenter in response to changes in both request workload and carbon footprint. It allows an operator to navigate the three-way tradeoff between access latency, carbon footprint, and electricity costs and to determine an optimal datacenter upgrade plan in response to increases in traffic load. We use FORTE to show that carbon taxes or credits are impractical in incentivizing carbon output reduction by providers of large-scale Internet applications. However, they can reduce carbon emissions by 10% without increasing the mean latency nor the electricity bill.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {211–222},
numpages = {12},
keywords = {green computing, energy},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

@article{10.1145/2377677.2377719,
author = {Gao, Peter Xiang and Curtis, Andrew R. and Wong, Bernard and Keshav, Srinivasan},
title = {It's not easy being green},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2377677.2377719},
doi = {10.1145/2377677.2377719},
abstract = {Large-scale Internet applications, such as content distribution networks, are deployed across multiple datacenters and consume massive amounts of electricity. To provide uniformly low access latencies, these datacenters are geographically distributed and the deployment size at each location reflects the regional demand for the application. Consequently, an application's environmental impact can vary significantly depending on the geographical distribution of end-users, as electricity cost and carbon footprint per watt is location specific. In this paper, we describe FORTE: Flow Optimization based framework for request-Routing and Traffic Engineering. FORTE dynamically controls the fraction of user traffic directed to each datacenter in response to changes in both request workload and carbon footprint. It allows an operator to navigate the three-way tradeoff between access latency, carbon footprint, and electricity costs and to determine an optimal datacenter upgrade plan in response to increases in traffic load. We use FORTE to show that carbon taxes or credits are impractical in incentivizing carbon output reduction by providers of large-scale Internet applications. However, they can reduce carbon emissions by 10% without increasing the mean latency nor the electricity bill.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {211–222},
numpages = {12},
keywords = {green computing, energy}
}

@inproceedings{10.1145/2647868.2654897,
author = {Ejembi, Oche and Bhatti, Saleem N.},
title = {Help Save The Planet: Please Do Adjust Your Picture},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2654897},
doi = {10.1145/2647868.2654897},
abstract = {Allowing digital video users to make choices of picture size and codec would significantly reduce energy usage, electricity costs and the carbon footprint of Internet users. Our empirical investigation shows a difference of up to a factor of 3 in energy usage for video decoding using different codecs at the same picture size and bitrate, on a desktop client system. With video traffic already responsible for the largest and fastest growing proportion of traffic on the Internet, a significant amount of energy, money and carbon output is due to video. We present a simple methodology and metrics that can be used to give an intuitive, quantitative and comparable assessment of the energy usage of video decoding. Providing energy usage information to users would empower them to make sensible choices. We demonstrate how small energy savings for individual client systems could give significant energy savings when considered at a global scale.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {427–436},
numpages = {10},
keywords = {video, energy, codec},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.1145/2345316.2345325,
author = {Pino, Robinson E.},
title = {Computational intelligence and neuromorphic computing potential for geospatial research and applications},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345325},
doi = {10.1145/2345316.2345325},
abstract = {In today's highly mobile, networked, and interconnected internet world, the flow and volume of information is overwhelming and continuously increasing. Therefore, it is believed that the next frontier in technological evolution and development will rely in our ability to develop intelligent systems that can help us process, analyze, and make-sense of information autonomously just as a well-trained and educated human expert. In computational intelligence, neuromorphic computing promises to allow for the development of computing systems able to imitate natural neuro-biological processes that will form the foundation for intelligent system architectures. This is achieved by artificially re-creating the highly parallelized computing architecture of the mammalian brain. As an interdisciplinary technology inspired from biology, artificial neural systems have been successfully utilized in many applications, such as control systems, signal processing, pattern recognition, vision systems, and robotics etc. In addition, the emerging neuromorphic computing field can also exploit the characteristic behavior of novel material systems with advanced processing techniques to achieve very large scale integration with highly parallel neural architectures for the fabrication physical architectures. This talk will focus on the technological challenges that we are seeking to overcome to enable intelligent parallel neuromorphic computing systems.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {7},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1109/WI-IAT.2010.21,
author = {Huang, Zicheng and Huai, Jinpeng and Liu, Xudong and Zhu, Jiangjun},
title = {Business Process Decomposition Based on Service Relevance Mining},
year = {2010},
isbn = {9780769541914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2010.21},
doi = {10.1109/WI-IAT.2010.21},
abstract = {Reuse is an important mechanism for improving the efficiency of software development. For Internet-scale software produced through service composition, the simple reuse granularity at service is often inefficient due to the large number of available services. This paper proposes a novel architecture which enables efficient reuse of process fragments. In the proposed architecture, services are organized into a network, called Service Composition Network (SCN), based on their co-occurence in the existing composite services. The reusable process fragments are extracted by decomposing existing composite services according to both the structural constraint of the process and the relevance of services in the same process fragment. The design principles and a prototype implementation of this architecture are presented, the performance of the proposed approach is analyzed, and an application is described to demonstrate the effectiveness of it.},
booktitle = {Proceedings of the 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {573–580},
numpages = {8},
keywords = {Service Relevance Mining, Service Composition, Process Decomposition},
series = {WI-IAT '10}
}

@inproceedings{10.1145/1835804.1835926,
author = {Liu, Xiaojiang and Nie, Zaiqing and Yu, Nenghai and Wen, Ji-Rong},
title = {BioSnowball: automated population of Wikis},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835926},
doi = {10.1145/1835804.1835926},
abstract = {Internet users regularly have the need to find biographies and facts of people of interest. Wikipedia has become the first stop for celebrity biographies and facts. However, Wikipedia can only provide information for celebrities because of its neutral point of view (NPOV) editorial policy. In this paper we propose an integrated bootstrapping framework named BioSnowball to automatically summarize the Web to generate Wikipedia-style pages for any person with a modest web presence. In BioSnowball, biography ranking and fact extraction are performed together in a single integrated training and inference process using Markov Logic Networks (MLNs) as its underlying statistical model. The bootstrapping framework starts with only a small number of seeds and iteratively finds new facts and biographies. As biography paragraphs on the Web are composed of the most important facts, our joint summarization model can improve the accuracy of both fact extraction and biography ranking compared to decoupled methods in the literature. Empirical results on both a small labeled data set and a real Web-scale data set show the effectiveness of BioSnowball. We also empirically show that BioSnowball outperforms the decoupled methods.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {969–978},
numpages = {10},
keywords = {summarization, markov logic networks, fact extraction, bootstrapping},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.1145/1971519.1971586,
author = {Lee, Seung-Que and Park, Nam-Hoon and Kim, Young-Jin},
title = {Evolving toward ubiquitous access by femtocell systems},
year = {2010},
isbn = {9781450304405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1971519.1971586},
doi = {10.1145/1971519.1971586},
abstract = {Femtocell is a small scale base station installed at home, apartment or office connecting core network of mobile company via broadband wired internet of indoors. In this paper, we introduce the femtocell systems considered as ubiquitous access by giving wireless Internet access in home at anytime while on moving with a low access cost and high data rate. An overview of the femtocell system is presented and its additional features are explored followed by migrating from the macrocell systems to the femtocell systems.},
booktitle = {Proceedings of the 8th International Conference on Advances in Mobile Computing and Multimedia},
pages = {393–396},
numpages = {4},
keywords = {femto cell, CSG(closed subscriber group)},
location = {Paris, France},
series = {MoMM '10}
}

@inproceedings{10.1145/1877953.1877963,
author = {Shen, Jialie and Yan, Shuicheng and Hua, Xian-Sheng},
title = {The e-recall environment for cloud based mobile rich media data management},
year = {2010},
isbn = {9781450301688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1877953.1877963},
doi = {10.1145/1877953.1877963},
abstract = {With the pervasiveness of mobile and wireless devices and rapid growth of the Internet, the access availability of rich media continues to accelerate in amount, variety, complexity and scale. This has been exemplified by various online media service portals including Flickr, Facebook and Last.fm. Unfortunately, lack of proper data processing techniques has now become major obstacle for effectively personal data management, especially in a mobile environment. While the related technical developments have been attracted a lot of research attentions recently, many open problems still remain unsolved. Among them, two major challenges need to be addressed. The first one is how to construct comprehensive way to describe 1) queries - model user information needs and 2) contents of rich media data. Moreover, size of media data collected by modern personal digital device (PDA) could be huge and will continue to grow. Consequently, fast data processing and associated scalability issues are becoming more important than ever before, and yet, little serious research has been conducted in this field.In this paper, we report ongoing efforts to develop E-Recall system - a novel platform for cloud based mobile rich media data management. It aims to provide an intelligent and comprehensive infrastructure for (1) scalable media data processing, (2) flexible media content sharing and publishing and (3) personalized media content integration under mobile environment.},
booktitle = {Proceedings of the 2010 ACM Multimedia Workshop on Mobile Cloud Media Computing},
pages = {31–34},
numpages = {4},
keywords = {rich media, personalization, information retrieval, evaluation, data management, cloud computing, classification},
location = {Firenze, Italy},
series = {MCMC '10}
}

@inproceedings{10.1145/2491661.2481428,
author = {Soltero, Philip and Bridges, Patrick and Arnold, Dorian and Lang, Michael},
title = {A gossip-based approach to exascale system services},
year = {2013},
isbn = {9781450321464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491661.2481428},
doi = {10.1145/2491661.2481428},
abstract = {Large-scale server deployments in the commercial internet space have been using group based protocols such as peer-to-peer and gossip to allow coordination of services and data across global distributed data centers. Here we look at applying these methods, which are themselves derived from early work in distributed systems, to large-scale, tightly-coupled systems used in high performance computing.In this paper, we study Gossip protocols and their ability to aggregate data across large-scale systems in support of system services. We report accuracy and performance of these estimated results and then focus on a simulated power-capping service to show the tradeoffs of this approach in practice.},
booktitle = {Proceedings of the 3rd International Workshop on Runtime and Operating Systems for Supercomputers},
articleno = {3},
numpages = {7},
location = {Eugene, Oregon},
series = {ROSS '13}
}

@inproceedings{10.1145/2335484.2335526,
author = {Wen, Jimi Y. C. and Lin, Gu Yuan and Sung, Today and Wu, David and Wang, Ping Feng and Feng, Ming Whei},
title = {An energy-aware web of people and things, events of interaction &amp; complex interaction of events},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335526},
doi = {10.1145/2335484.2335526},
abstract = {The continuous evolution of computing and networking technologies is creating a new world populated by many sensors on physical and social environments. At the same time, these applications are also mission-critical with serious quality of service requirements such as real-time performance, continuous availability, high security and privacy. Pu argues that the traditional process-oriented programming languages and software architectures should be augmented by distributed event-based facilities and abstractions for the construction of large-scale distributed IOT applications [4]. Bohli et al. state in one of their theses: The value of the IoT market grows more than linearly with the number of consumers. This thesis inherently assumes the value of 'connectedness' of the network [1]. One supporting experience with 'connectedness' that follows such thesis in a favourable way is with social networks, e. g. facebook, twitter etc [2]. Scharmen argues similarly from more abstract view that spaces, physical, virtual or physical-virtual are just culture containers, for people to present, curate, mediate via these interconnections [5].},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {369–370},
numpages = {2},
keywords = {web of things, event processing},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.5555/2414276.2414333,
author = {Carroll, Raymond},
title = {Bio-inspired future service environments},
year = {2012},
isbn = {9781450318969},
publisher = {International Teletraffic Congress},
abstract = {The rise in the popularity of the Internet over the last number of years has been phenomenal. Driven by increasing numbers of users, networked devices and services, the Internet has developed into a dominant social tool as well as a critical business environment. A number of trends in research today (e.g. Service Oriented Architectures) point towards a Future Internet which is very much service centric, with large numbers of diverse, dynamic and distributed services. At the same time, the move towards cloud and utility-based computing models, as well as the emergence of resource-light mobile devices, suggest a Future Internet highly reliant on large-scale data-centres.},
booktitle = {Proceedings of the 24th International Teletraffic Congress},
articleno = {44},
numpages = {1},
location = {Krakow, Poland},
series = {ITC '12}
}

@proceedings{10.1145/2382416,
title = {BADGERS '12: Proceedings of the 2012 ACM Workshop on Building analysis datasets and gathering experience returns for security},
year = {2012},
isbn = {9781450316613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is my great pleasure to welcome you to the 2012 ACM Workshop on Building Analysis Datasets and Gathering Experience Returns for Security -- BADGERS'12. The BADGERS workshop is intended to encourage the development of Internet-scale security-related data collection and analysis initiatives. As such, the workshop is positioned at the confluence of computer security and general purpose large-scale data processing and aims at bringing together researchers, practitioners, system administrators, and security analysts active in the emerging domain of security-related data collection and analysis for Internet-scale computer systems and networks. By giving visibility to existing solutions, the workshop promotes and encourages the better sharing of data and knowledge. I expect that the increasing availability of tools and techniques to process large-scale data (aka "Big Data") will benefit computer security.The call for papers attracted 7 submissions from Asia, Europe, and the United States, each of which received at least three reviews from the program committee. The 4 accepted papers cover a variety of topics, including Internet-wide intrusion detection and vulnerability analysis, probing using darknets, and residential privacy in location-based social networks. In addition, the program includes keynote speeches by Samuel Weber on challenges for Big Data privacy and security and by Peter Mell on Big Data technologies for security research, and two invited talks describing realworld experiences with Big Data in industrial research lab settings. I hope that these proceedings will serve as a valuable reference for security researchers and developers.},
location = {Raleigh, North Carolina, USA}
}

@article{10.1145/1671948.1671950,
author = {Girdzijauskas, \v{S}ar\={u}nas and Datta, Anwitaman and Aberer, Karl},
title = {Structured overlay for heterogeneous environments: Design and evaluation of oscar},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4665},
url = {https://doi.org/10.1145/1671948.1671950},
doi = {10.1145/1671948.1671950},
abstract = {Recent years have seen advances in building large Internet-scale index structures, generally known as structured overlays. Early structured overlays realized distributed hash tables (DHTs) which are ill suited for anything but exact queries. The need to support range queries necessitates systems that can handle uneven load distributions. However such systems suffer from practical problems—including poor latency, disproportionate bandwidth usage at participating peers, or unrealistic assumptions on peers' homogeneity, in terms of available storage or bandwidth resources. In this article we consider a system that is not only able to support uneven load distributions but also to operate in heterogeneous environments, where each peer can autonomously decide how much of its resources to contribute to the system. We provide the theoretical foundations of realizing such a network and present a newly proposed system Oscar based on these principles. Oscar can construct efficient overlays given arbitrary load distributions by employing a novel scalable network sampling technique. The simulations of our system validate the theory and evaluate Oscar's performance under typical challenges, encountered in real-life large-scale networked systems, including participant heterogeneity, faults, and skewed and dynamic load-distributions. Thus the Oscar distributed index fills in an important gap in the family of structured overlays, bringing into life a practical Internet-scale index, which can play a crucial role in enabling data-oriented applications distributed over wide-area networks.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {feb},
articleno = {2},
numpages = {25},
keywords = {structured overlays, small-world graphs, skewed key distributions, routing, Peer-to-peer systems}
}

@inproceedings{10.1145/2578726.2578748,
author = {Gao, Yue and Wang, Fanglin and Luan, Huanbo and Chua, Tat-Seng},
title = {Brand Data Gathering From Live Social Media Streams},
year = {2014},
isbn = {9781450327824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578726.2578748},
doi = {10.1145/2578726.2578748},
abstract = {Social media streams, such as Twitter, Facebook, and Sina Weibo, have become essential real-time information resources with a wide range of users and applications. The rapidly increasing amount of live information in social media streams has important societal and marketing values for large corporations and government organizations. There is a strong need for effective techniques for data gathering and content analysis. This problem is particularly challenging due to the short and conversational nature of posts, the huge data volume, and the increasing heterogeneous multimedia content in social media streams. Moreover, as the focus of "conversation" often shifts quickly in social media space, the traditional keywords based approach to gather data with respect to a target brand is grossly inadequate. To address these problems, we propose a multi-faceted brand tracking method that gathers relevant data based on not just evolving keywords, but also social factors (users, relations and locations) as well as visual contents as increasing number of social media posts are in multimedia form. For evaluation, we set up a large scale microblog dataset (Brand-Social-Net) on brand/product information, containing 3 million microblogs with over 1.2 million images for 100 famous brands. Experiments on this dataset have demonstrated that the proposed framework is able to gather a more complete set of relevant brand-related data from live social media streams. We have released this dataset to promote social media research.},
booktitle = {Proceedings of International Conference on Multimedia Retrieval},
pages = {169–176},
numpages = {8},
keywords = {Visual content, Social media, Social context, Extended data gathering, Brand tracking},
location = {Glasgow, United Kingdom},
series = {ICMR '14}
}

@inproceedings{10.1145/2018436.2018450,
author = {Otto, John S. and S\'{a}nchez, Mario A. and Choffnes, David R. and Bustamante, Fabi\'{a}n E. and Siganos, Georgos},
title = {On blind mice and the elephant: understanding the network impact of a large distributed system},
year = {2011},
isbn = {9781450307970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018436.2018450},
doi = {10.1145/2018436.2018450},
abstract = {A thorough understanding of the network impact of emerging large-scale distributed systems -- where traffic flows and what it costs -- must encompass users' behavior, the traffic they generate and the topology over which that traffic flows. In the case of BitTorrent, however, previous studies have been limited by narrow perspectives that restrict such analysis.This paper presents a comprehensive view of BitTorrent, using data from a representative set of 500,000 users sampled over a two year period, located in 169 countries and 3,150 networks. This unique perspective captures unseen trends and reveals several unexpected features of the largest peer-to-peer system. For instance, over the past year total BitTorrent traffic has increased by 12%, driven by 25% increases in per-peer hourly download volume despite a 10% decrease in the average number of online peers. We also observe stronger diurnal usage patterns and, surprisingly given the bandwidth-intensive nature of the application, a close alignment between these patterns and overall traffic. Considering the aggregated traffic across access links, this has potential implications on BitTorrent-associated costs for Internet Service Providers (ISPs). Using data from a transit ISP, we find a disproportionately large impact on a commonly used burstable (95th-percentile) billing model. Last, when examining BitTorrent traffic's paths, we find that for over half its users, most network traffic never reaches large transit networks, but is instead carried by small transit ISPs. This raises questions on the effectiveness of most in-network monitoring systems to capture trends on peer-to-peer traffic and further motivates our approach.},
booktitle = {Proceedings of the ACM SIGCOMM 2011 Conference},
pages = {110–121},
numpages = {12},
keywords = {peer-to-peer, internet-scale systems, evaluation},
location = {Toronto, Ontario, Canada},
series = {SIGCOMM '11}
}

@inproceedings{10.1145/2483574.2483585,
author = {Song, Jingkuan},
title = {Effective hashing for large-scale multimedia search},
year = {2013},
isbn = {9781450321556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483574.2483585},
doi = {10.1145/2483574.2483585},
abstract = {With the rapid development of the Internet and multimedia technologies over the last decade, a huge amount of data has become available, from text corpus, to collections of online images and videos. Cheap storage cost and modern database technologies have made it possible to accumulate large-scale datasets. However, the ever-growing sizes of the datasets make it harder to search useful information from such data. A fundamental computational primitive for dealing with massive multimedia datasets is the similarity search problem. Multimedia similarity search aims to preprocess a database so that given a query object, one can quickly find its similar objects in the database. Searching similar objects from a large dataset in high-dimensional spaces is at the heart of many multimedia applications, such as near-duplicate retrieval, multimedia tagging, recommendation, and so on. Driven by its significance, lots of efforts have been made on this topic. The goal of my research is to design efficient hashing methods for large-scale multimedia search. In this paper, we first present the general framework for multimedia similarity search and discuss the latest improvements and progresses in the field. Then we describe the contributions we have made to effectively and efficiently search similar multimedia objects from large-scale databases. Finally, we discuss the future work and draw a conclusion.},
booktitle = {Proceedings of the 2013 SIGMOD/PODS Ph.D. Symposium},
pages = {55–60},
numpages = {6},
keywords = {multimedia retrieval, indexing, hashing, binary codes},
location = {New York, New York, USA},
series = {SIGMOD'13 PhD Symposium}
}

@inproceedings{10.1145/2674005.2674981,
author = {Moradi, Mehrdad and Wu, Wenfei and Li, Li Erran and Mao, Zhuoqing Morley},
title = {SoftMoW: Recursive and Reconfigurable Cellular WAN Architecture},
year = {2014},
isbn = {9781450332798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674005.2674981},
doi = {10.1145/2674005.2674981},
abstract = {The current LTE network architecture is organized into very large regions, each having a core network and a radio access network. The core network contains an Internet edge comprised of packet data network gateways (PGWs). The radio network consists of only base stations. There are minimal interactions among regions other than interference management at the edge. The current architecture has several problems. First, mobile application performance is seriously impacted by the lack of Internet egress points per region. Second, the continued exponential growth of mobile traffic puts tremendous pressure on the scalability of PGWs. Third, the fast growth of signaling traffic known as the signaling storm problem poses a major challenge to the scalability of the control plane. To address these problems, we present SoftMoW, a recursive and reconfigurable cellular WAN architecture that supports seamlessly inter-connected core networks, reconfigurable control plane, and global optimization.To scale the control plane nation-wide, SoftMoW recursively builds up the hierarchical control plane with novel abstractions of both control plane and data plane entities. To enable scalable end-to-end path setup, SoftMoW presents a novel label swapping mechanism such that each controller only operates on its logical topology and each switch along the path only sees at most one label. SoftMoW supports new network-wide optimization functions such as optimal routing and inter-region handover minimization. We demonstrate that SoftMoW improves the performance, flexibility and scalability of cellular WAN using real LTE network traces with thousands of base stations and millions of subscribers. Our evaluation shows that path inflation and inter-region handovers can be reduced by up to 60% and 44% respectively.},
booktitle = {Proceedings of the 10th ACM International on Conference on Emerging Networking Experiments and Technologies},
pages = {377–390},
numpages = {14},
keywords = {software-defined networking, hierarchi- cal controllers, cellular networks},
location = {Sydney, Australia},
series = {CoNEXT '14}
}

@inproceedings{10.1145/2377576.2377599,
author = {Lenzi, Nicholas and Bachrach, Benjamin and Manikonda, Vikram},
title = {DCF® a JAUS and TENA compliant agent-based framework for UAS performance evaluation},
year = {2010},
isbn = {9781450302906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377576.2377599},
doi = {10.1145/2377576.2377599},
abstract = {Real-world applications for UAS teams continue to grow, and the scale and complexity of the teams is continually increasing. To reduce life cycle costs and improve T&amp;E, there is increasing need for a generalized framework that can support the design and development of T&amp;E approaches for multi-UAS teams and validate the feasibility of the concepts, architectures and algorithms. This challenge is most significant in the cognitive/social domains, where the development of test approaches and methodologies are difficult because of the emergent nature of behaviors in response to dynamic changes in the battlespace. Current DOD T&amp;E capabilities and methodologies are insufficient to address these needs. Today much of the initial validation effort is done using simulations, which unfortunately very rarely capture the complexity of this problem. Current simulations rarely capture the complexity of real world effects related to net-centric communications, vehicle dynamics, distributed sensors, the physical environment (terrain), external disturbances, etc. Furthermore, very often high fidelity simulations do not scale as the number of UAS increases. On the other extreme, directly implementing hardware platforms without high resolution simulations to help refine the design induces significant risk. For large unmanned system teams, shortcomings in design decisions related to the control architecture, information flow, sensor fusion, assumptions on communication bandwidths, and robustness of the algorithms may only become apparent when deployment on several hardware platforms has been completed, resulting in a significant loss of time and resources. In response to this need, under a recently completed effort with TRMC, IAI has developed the Distributed Control Framework (DCF), an Integrated Agent-based T&amp;E Framework for Simulated, Mixed-Model (Virtual and Live/hardware in the loop) and Live Testing of Teams of Unmanned Autonomous Systems. In recent efforts, DCF has been made JAUS compliant, and integration with TENA has been achieved. In this paper we discuss the development of this framework, details of JAUS compliance implementation and initial results of its deployment at ARDEC in Picatinny Arsenal on use cases involving teams of FireAnt Robots performing cooperative surveillance tasks.},
booktitle = {Proceedings of the 10th Performance Metrics for Intelligent Systems Workshop},
pages = {119–126},
numpages = {8},
keywords = {unmanned systems teams, unmanned and autonomous system testing, T&amp;E of unmanned systems, JAUS compliance},
location = {Baltimore, Maryland},
series = {PerMIS '10}
}

@inproceedings{10.1145/2187836.2187862,
author = {Mishra, Abhinav and Rastogi, Rajeev},
title = {Semi-supervised correction of biased comment ratings},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187862},
doi = {10.1145/2187836.2187862},
abstract = {In many instances, offensive comments on the internet attract a disproportionate number of positive ratings from highly biased users. This results in an undesirable scenario where these offensive comments are the top rated ones. In this paper, we develop semi-supervised learning techniques to correct the bias in user ratings of comments. Our scheme uses a small number of comment labels in conjunction with user rating information to iteratively compute user bias and unbiased ratings for unlabeled comments. We show that the running time of each iteration is linear in the number of ratings, and the system converges to a unique fixed point. To select the comments to label, we devise an active learning algorithm based on empirical risk minimization. Our active learning method incrementally updates the risk for neighboring comments each time a comment is labeled, and thus can easily scale to large comment datasets. On real-life comments from Yahoo! News, our semi-supervised and active learning algorithms achieve higher accuracy than simple baselines, with few labeled examples.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {181–190},
numpages = {10},
keywords = {semi-supervised learning, iterative technique, bias, active learning},
location = {Lyon, France},
series = {WWW '12}
}

@article{10.14778/2733004.2733018,
author = {Vemuri, Srinivas and Varshney, Maneesh and Puttaswamy, Krishna and Liu, Rui},
title = {Execution primitives for scalable joins and aggregations in map reduce},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733018},
doi = {10.14778/2733004.2733018},
abstract = {Analytics on Big Data is critical to derive business insights and drive innovation in today's Internet companies. Such analytics involve complex computations on large datasets, and are typically performed on MapReduce based frameworks such as Hive and Pig. However, in our experience, these systems are still quite limited in performing at scale. In particular, calculations that involve complex joins and aggregations, e.g. statistical calculations, scale poorly on these systems.In this paper we propose novel primitives for scaling such calculations. We propose a new data model for organizing datasets into calculation data units that are organized based on user-defined cost functions. We propose new operators that take advantage of these organized data units to significantly speed up joins and aggregations. Finally, we propose strategies for dividing the aggregation load uniformly across worker processes that are very effective in avoiding skews and reducing (or in some cases even removing) the associated overheads.We have implemented all our proposed primitives in a framework called Rubix, which has been in production at LinkedIn for nearly a year. Rubix powers several applications and processes TBs of data each day. We have seen remarkable improvements in speed and cost of complex calculations due to these primitives.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1462–1473},
numpages = {12}
}

@inproceedings{10.1145/2342356.2342430,
author = {Tian, Chen and Alimi, Richard and Yang, Yang Richard and Zhang, David},
title = {ShadowStream: performance evaluation as a capability in production internet live streaming networks},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342430},
doi = {10.1145/2342356.2342430},
abstract = {As live streaming networks grow in scale and complexity, they are becoming increasingly difficult to evaluate. Existing evaluation methods including lab/testbed testing, simulation, and theoretical modeling, lack either scale or realism. The industrial practice of gradually-rolling-out in a testing channel is lacking in controllability and protection when experimental algorithms fail, due to its passive approach. In this paper, we design a novel system called ShadowStream that introduces evaluation as a built-in capability in production Internet live streaming networks. ShadowStream introduces a simple, novel, transparent embedding of experimental live streaming algorithms to achieve safe evaluations of the algorithms during large-scale, real production live streaming, despite the possibility of large performance failures of the tested algorithms. ShadowStream also introduces transparent, scalable, distributed experiment orchestration to resolve the mismatch between desired viewer behaviors and actual production viewer behaviors, achieving experimental scenario controllability. We implement ShadowStream based on a major Internet live streaming network, build additional evaluation tools such as deterministic replay, and demonstrate the benefits of ShadowStream through extensive evaluations.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {347–358},
numpages = {12},
keywords = {streaming, performance evaluation, live testing},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

@article{10.1145/2377677.2377751,
author = {Tian, Chen and Alimi, Richard and Yang, Yang Richard and Zhang, David},
title = {ShadowStream: performance evaluation as a capability in production internet live streaming networks},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2377677.2377751},
doi = {10.1145/2377677.2377751},
abstract = {As live streaming networks grow in scale and complexity, they are becoming increasingly difficult to evaluate. Existing evaluation methods including lab/testbed testing, simulation, and theoretical modeling, lack either scale or realism. The industrial practice of gradually-rolling-out in a testing channel is lacking in controllability and protection when experimental algorithms fail, due to its passive approach. In this paper, we design a novel system called ShadowStream that introduces evaluation as a built-in capability in production Internet live streaming networks. ShadowStream introduces a simple, novel, transparent embedding of experimental live streaming algorithms to achieve safe evaluations of the algorithms during large-scale, real production live streaming, despite the possibility of large performance failures of the tested algorithms. ShadowStream also introduces transparent, scalable, distributed experiment orchestration to resolve the mismatch between desired viewer behaviors and actual production viewer behaviors, achieving experimental scenario controllability. We implement ShadowStream based on a major Internet live streaming network, build additional evaluation tools such as deterministic replay, and demonstrate the benefits of ShadowStream through extensive evaluations.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {347–358},
numpages = {12},
keywords = {streaming, performance evaluation, live testing}
}

@inproceedings{10.1145/2002396.2002403,
author = {Xin, Yufeng and Baldine, Ilia and Mandal, Anirban and Heermann, Chris and Chase, Jeff and Yumerefendi, Aydan},
title = {Embedding virtual topologies in networked clouds},
year = {2011},
isbn = {9781450308212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002396.2002403},
doi = {10.1145/2002396.2002403},
abstract = {Embedding virtual topologies in physical network infrastructure has been an area of active research for the future Internet and network testbeds. Virtual network embedding is also useful for linking virtual compute clusters allocated from cloud providers. Using advanced networking technologies to interconnect distributed cloud sites is a promising way to provision on-demand large-scale virtualized networked systems for production and experimental purposes.In this paper, we study the virtual topology embedding problem in a networked cloud environment, in which a number of cloud provider sites are connected by multi-domain wide-area networks that support virtual networking technology. A user submits a request for a virtual topology, and the system plans a low-cost embedding and orchestrates requests to multiple cloud providers and network transit providers to instantiate the virtual topology according to the plan. We describe an efficient heuristic algorithm design and a prototype implementation within a GENI control framework candidate called ORCA.},
booktitle = {Proceedings of the 6th International Conference on Future Internet Technologies},
pages = {26–29},
numpages = {4},
keywords = {virtual topology, embedding, cloud},
location = {Seoul, Republic of Korea},
series = {CFI '11}
}

@inproceedings{10.1145/2488388.2488393,
author = {Ahmed, Amr and Shervashidze, Nino and Narayanamurthy, Shravan and Josifovski, Vanja and Smola, Alexander J.},
title = {Distributed large-scale natural graph factorization},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488393},
doi = {10.1145/2488388.2488393},
abstract = {Natural graphs, such as social networks, email graphs, or instant messaging patterns, have become pervasive through the internet. These graphs are massive, often containing hundreds of millions of nodes and billions of edges. While some theoretical models have been proposed to study such graphs, their analysis is still difficult due to the scale and nature of the data.We propose a framework for large-scale graph decomposition and inference. To resolve the scale, our framework is distributed so that the data are partitioned over a shared-nothing set of machines. We propose a novel factorization technique that relies on partitioning a graph so as to minimize the number of neighboring vertices rather than edges across partitions. Our decomposition is based on a streaming algorithm. It is network-aware as it adapts to the network topology of the underlying computational hardware. We use local copies of the variables and an efficient asynchronous communication protocol to synchronize the replicated values in order to perform most of the computation without having to incur the cost of network communication. On a graph of 200 million vertices and 10 billion edges, derived from an email communication network, our algorithm retains convergence properties while allowing for almost linear scalability in the number of computers.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {37–48},
numpages = {12},
keywords = {matrix factorization, large-scale machine learning, graph factorization, graph algorithms, distributed optimization, asynchronous algorithms},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.5555/2151054.2151138,
author = {Nakasone, Arturo and Prendinger, Helmut and Miska, Marc and Lindner, Martin and Horiguchi, Ryota and Kuwahara, Masao},
title = {OpenEnergySim: a 3D internet based experimental framework for integrating traffic simulation and multi-user immersive driving},
year = {2011},
isbn = {9781936968008},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {In recent years, the use of computer-based simulations in the transportation domain has become increasingly important to analyze and test measures for Intelligent Transport System (ITS) policies. Simulators were built to address several aspects of transport, including traffic, driving experience, and pedestrian behavior. However, as the majority of available simulators are single-user stand-alone systems, traffic engineers cannot easily analyze more complex phenomena, such as the interaction between multiple human drivers or pedestrians. Furthermore, this limitation makes it difficult to collect large-scale behavioral data, which is necessary to draw valid conclusions on driving behavior. Emerging virtual world technology offers a convenient alternative. As a networked multi-user framework that allows users to immerse in the virtual world via a graphical self-representation (an 'avatar'), it allows to develop integrated simulation applications that are conveniently accessible by Internet. In this paper, we present OpenEnergySim, a virtual world based visualization application that integrates traffic simulation and immersive multi-user driving.},
booktitle = {Proceedings of the 4th International ICST Conference on Simulation Tools and Techniques},
pages = {490–498},
numpages = {9},
keywords = {virtual worlds, traffic simulation, open-simulator, driving simulation},
location = {Barcelona, Spain},
series = {SIMUTools '11}
}

@inproceedings{10.1145/2487788.2487933,
author = {Hoi, Steven C.H. and Wang, Dayong and Cheng, I. Yeu and Lin, Elmer Weijie and Zhu, Jianke and He, Ying and Miao, Chunyan},
title = {FANS: face annotation by searching large-scale web facial images},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487933},
doi = {10.1145/2487788.2487933},
abstract = {Auto face annotation is an important technique for many real-world applications, such as online photo album management, new video summarization, and so on. It aims to automatically detect human faces from a photo image and further name the faces with the corresponding human names. Recently, mining web facial images on the internet has emerged as a promising paradigm towards auto face annotation. In this paper, we present a demonstration system of search-based face annotation: FANS - Face ANnotation by Searching large-scale web facial images. Given a query facial image for annotation, we first retrieve a short list of the most similar facial images from a web facial image database, and then annotate the query facial image by mining the top-ranking facial images and their corresponding labels with sparse representation techniques. Our demo system was built upon a large-scale real-world web facial image database with a total of 6,025 persons and about 1 million facial images. This paper demonstrates the potential of searching and mining web-scale weakly labeled facial images on the internet to tackle the challenging face annotation problem, and addresses some open problems for future exploration by researchers in web community. The live demo of FANS is available online at http://msm.cais.ntu.edu.sg/FANS/.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {317–320},
numpages = {4},
keywords = {web facial images, web data mining, search-based face annotation},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/1982185.1982353,
author = {Ha, Jiwoon and Bae, Duck-Ho and Kim, Sang-Wook and Baek, Seok-Chul and Jeong, Byeong-Soo},
title = {Analyzing a Korean blogosphere: a social network analysis perspective},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982353},
doi = {10.1145/1982185.1982353},
abstract = {Due to their popularity and widespread use, blogs have become an important medium through which to communicate and exchange information on the World Wide Web. The advent of the blogosphere may provide opportunities for establishing a new business model that investigates social relationships. In Korea, there are many blogospheres that appear to maintain different characteristics from foreign blogospheres on the Internet. Consequently, it is inappropriate to apply analysis methods used for the foreign blogosphere directly to Korean blogospheres. To establish successful business policies in Korean blogospheres, the characteristics of Korean blogospheres and the behavioral patterns of bloggers should be understood. In this paper, we analyze the characteristics of the Korean blog network, wherein each blogger forms a node and scraps by bloggers as edges. First, we demonstrate that the Korean blog network is also a scale-free network, like the World Wide Web. Second, we compare the Bow-tie structure of the Korean blog network with that of the World Wide Web. We expect that these analysis results will be helpful in developing effective algorithms and in establishing new business models targeted at the Korean blogosphere.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {773–777},
numpages = {5},
keywords = {social network analysis, graph mining, data mining, bow-tie, blogosphere, Korean blogosphere},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/1835804.1835807,
author = {Feldman, Konrad},
title = {The quantification of advertising: (+ lessons from building businesses based on large scale data mining)},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835807},
doi = {10.1145/1835804.1835807},
abstract = {As electronic communication, media and commerce increasingly permeate every aspect of modern life, real-time personalization of consumer experience through data-mining becomes practical. Effective classification, prediction and change modeling of consumer interests, behaviors and purchasing habits using machine learning and statistical methods drives efficiency, insights and consumer relevance that were never before possible.The internet has brought on a rapid evolution in advertising. Everything about behavior on the internet can be quantified and responses to behavior can occur in real time. This dynamic interaction with the user has created opportunities to better understand the way in which individuals move from awareness of a product to considering a purchase, through to intent and ultimately a sale for the marketer. When a marketer can answer the question 'did those TV ads cause consumers to switch shampoo brands?' they can model behavior change and adjust marketing strategies accordingly.Underpinning this shift in how the world's trillion dollar marketing budget is spent is transactional data on an unprecedented scale, creating new challenges for software that must interpret this stream and make real time decisions tens, even hundreds of thousands of times every second.I will explore advances in modeling media consumption, advertising response and the real-time evaluation of media opportunities through reference to Quantcast, a business launched in September 2006 which today interprets in excess of 10 billion new digital media consumption records every day. We will examine the challenges of applying machine learning to non-search advertising and in doing so explore the creation of business environments - organization, infrastructure, tools, processes (and costs considerations) - in which scientists can quickly develop new petabyte scale algorithmic approaches, migrate them rapidly to real-time production and deliver fully customized experiences for marketers, publishers and consumers alike.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {5–6},
numpages = {2},
keywords = {modeling media consumption, data mining, advertising response, advertising},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.1145/2505515.2505589,
author = {Li, Xiaoyi and Gao, Jing and Li, Hui and Yang, Le and Srihari, Rohini K.},
title = {A multimodal framework for unsupervised feature fusion},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2505589},
doi = {10.1145/2505515.2505589},
abstract = {With the overwhelming amounts of visual contents on the Internet nowadays, it is very important to generate meaningful and succinct descriptions of multimedia contents including images and videos. Although human taggings and annotations can partially label some of the images or videos, it is impossible to exhaustively describe all the multimedia data due to its huge scale. Therefore, the key to this important task is to develop an effective algorithm that can automatically generate a description of an image or a frame. In this paper, we propose a multimodal feature fusion framework which can model any given image-description pair using semantically meaningful features. This framework is trained as a combination of multi-modal deep networks having two integral components: An ensemble of image descriptors and a recursive bigram encoder with fixed length output feature vector. These two components are then integrated into a joint model characterizing the correlations between images and texts. The proposed framework can not only model the unique characteristics of images or texts, but also take into account their correlations at the semantic level. Experiments on real image-text data sets show that the proposed framework is effective and efficient in indexing and retrieving semantically similar pairs, which will be very useful to help people locate interesting images or videos in large-scale databases.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management},
pages = {897–902},
numpages = {6},
keywords = {restricted boltzmann machine, multimodal framework, feature fusion},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.5555/2591272.2591275,
author = {Shetty, Pradeep and Spillane, Richard and Malpani, Ravikant and Andrews, Binesh and Seyster, Justin and Zadok, Erez},
title = {Building workload-independent storage with VT-trees},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {As the Internet and the amount of data grows, the variability of data sizes grows too--from small MP3 tags to large VM images. With applications using increasingly more complex queries and larger data-sets, data access patterns have become more complex and randomized. Current storage systems focus on optimizing for one band of workloads at the expense of other workloads due to limitations in existing storage system data structures. We designed a novel workload-independent data structure called the VT-tree which extends the LSM-tree to efficiently handle sequential and file-system workloads. We designed a system based solely on VT-trees which offers concurrent access to data via file system and database APIs, transactional guarantees, and consequently provides efficient and scalable access to both large and small data items regardless of the access pattern. Our evaluation shows that our user-level system has 2-6.6\texttimes{} better performance for random-write workloads and only a small average overhead for other workloads.},
booktitle = {Proceedings of the 11th USENIX Conference on File and Storage Technologies},
pages = {17–30},
numpages = {14},
location = {San Jose, CA},
series = {FAST'13}
}

@inproceedings{10.1109/DS-RT.2014.13,
author = {Hanai, Masatoshi and Shudo, Kazuyuki},
title = {Optimistic Parallel Simulation of Very Large-Scale Peer-to-Peer Systems},
year = {2014},
isbn = {9781479961443},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DS-RT.2014.13},
doi = {10.1109/DS-RT.2014.13},
abstract = {There have been P2P systems with simultaneous millions of nodes on Internet. But existing simulators and techniques cannot simulate such a large scale. Even parallelized simulators have not solved the scale problem. They provide large memory and hold a large number of nodes but the speed of simulation degrades significantly than sequential simulation due to much overhead of inter-server synchronization. We propose a simulation technique for large-scale P2P systems based on an optimistic parallel discrete event simulation model. The technique employs low cost synchronization techniques that are effective for P2P simulation.},
booktitle = {Proceedings of the 2014 IEEE/ACM 18th International Symposium on Distributed Simulation and Real Time Applications},
pages = {35–42},
numpages = {8},
keywords = {peer-to-peer systems, optimistic simulation, discrete event simulation},
series = {DS-RT '14}
}

@inproceedings{10.1145/2448096.2448105,
author = {Lan, Mars and Samy, Lauren and Alshurafa, Nabil and Suh, Myung-Kyung and Ghasemzadeh, Hassan and Macabasco-O'Connell, Aurelia and Sarrafzadeh, Majid},
title = {WANDA: an end-to-end remote health monitoring and analytics system for heart failure patients},
year = {2012},
isbn = {9781450317603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2448096.2448105},
doi = {10.1145/2448096.2448105},
abstract = {Recent advances in wireless sensors, mobile technologies, and cloud computing have made continuous remote monitoring of patients possible. In this paper, we introduce the design and implementation of WANDA, an end-to-end remote health monitoring and analytics system designed for heart failure patients. The system consists of a smartphone-based data collection gateway, an Internet-scale data storage and search system, and a backend analytics engine for diagnostic and prognostic purposes. The system supports the collection of data from a wide range of sensory devices that measure patients' vital signs as well as self-reported questionnaires. The main objective of the analytics engine is to predict future events by examining physiological readings of the patients.We demonstrate the efficiency of the proposed analytics engine using the data gathered from a pilot study of 18 heart failure patients. In particular, our results show that the advanced analytic algorithms used in our system are capable of predicting the worsening of patients' heart failure symptoms with up to 74% accuracy while improving the sensitivity performance by more than 45% compared to the commonly used thresholding algorithm based on daily weight change. Moreover, the accuracy attained by our system is only 9% lower than the theoretical upper bound. The proposed framework is currently deployed in a large ongoing heart failure study that targets 1500 congestive heart failure patients.},
booktitle = {Proceedings of the Conference on Wireless Health},
articleno = {9},
numpages = {8},
keywords = {wireless health, remote health monitoring, medical data mining, machine learning},
location = {San Diego, California},
series = {WH '12}
}

@inproceedings{10.1145/2591513.2597168,
author = {Parhi, Keshab K.},
title = {VLSI systems for neurocomputing and health informatics},
year = {2014},
isbn = {9781450328166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591513.2597168},
doi = {10.1145/2591513.2597168},
abstract = {Ubiquitous access to computers, cell phones, internet, personal digital devices, cameras and TV can be attributed to advances in the very large scale integration (VLSI) technology and the advances in circuit design to operate circuits at Gigahertz rates. One of the mysteries that we have not been able to unravel is the understanding of how the brain works from different perspectives. Reverse engineering the brain has been identified as one of the grand challenge problems by the National Academies. Advances in sensor technologies and imaging modalities such as electroencephalogram (EEG), intra-cranial electroencephalogram (iEEG), magnetoencephalogram (MEG), and magnetic resonance imaging (MRI) allow us to collect data from hundreds of electrodes from the brain at sample rates ranging from 256 Hz to 15kHz. These data can be key to not only understanding brain functioning and brain connectivity at macro and micro levels in healthy subjects but also in identifying patients with neurological and mental disorder. Extracting the appropriate biomarkers using spectral-temporal-spatial signal processing approaches and classifying states using machine learning approaches can assist clinicians in predicting and detecting seizures in epileptic patients, and in identifying patients with mental disorder such as schizophrenia, depression and personality disorder. The biomarkers can be tracked to design personalized therapy and effectiveness of therapy by closed loop drug delivery or closed loop neuromodulation, i.e., brain stimulation either by invasive or non-invasive means using electrical or magnetic stimulation. High-performance VLSI system design is critical to not-only increasing battery life of VLSI chips for neuromodulation but also for reducing computation time by orders of magnitude in analyzing MRI signals. Another grand challenge problem identified by the National Academies is Advanced Health Informatics. Analysis of health data is key to monitoring biomarkers and delivering drugs as needed. VLSI system design of biomarkers and disease state classification is again critical in improving the health and quality of life of human beings.In this talk, I will highlight the emerging opportunities in high-performance low-power VLSI system design for neurocomputing and health informatics at various scales. At macroscale, the goal is to design small low-power implantable or wearable devices that can be used to monitor biomarkers and trigger an alarm signal to alert an abnormal state of the brain such as an impending seizure. At microscale, extracting thousands of connections from structural and functional MRI can require many hours or even a day for one subject and one set of parameters using parallel computers. The challenge here is to design parallel multicore computer architectures and compiler tools that can reduce the time for microscale analysis of MRI to an hour or less. I will describe research in my group in use of signal processing and machine learning approaches to identify and track various neurological and mental disorders. I will present some results on VLSI design of feature extractors such as power spectral density (PSD) and classifiers such as support vector machines (SVMs). I will present diabetic retinopathy screening using fundus image analysis and machine learning as an example to illustrate opportunities in design of embedded systems for health informatics. Significant research needs to be pursued in this area. My presentation will hopefully inspire further research in this emerging and important field embedded VLSI system design for neuro, bio and health informatics.},
booktitle = {Proceedings of the 24th Edition of the Great Lakes Symposium on VLSI},
pages = {1–2},
numpages = {2},
keywords = {support vector machine, signal processing, retinopathy, power spectral density, pattern recognition, neuromodulation, mental disorder, machine learning, health informatics, epilepsy, embedded system design, classification, biomarker},
location = {Houston, Texas, USA},
series = {GLSVLSI '14}
}

@article{10.1109/TNET.2012.2190093,
author = {Seibert, Jeff and Torres, Ruben and Mellia, Marco and Munafo, Maurizio M. and Nita-Rotaru, Cristina and Rao, Sanjay},
title = {The internet-wide impact of P2P traffic localization on ISP profitability},
year = {2012},
issue_date = {December 2012},
publisher = {IEEE Press},
volume = {20},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2012.2190093},
doi = {10.1109/TNET.2012.2190093},
abstract = {We conduct a detailed simulation study to examine how localizing P2P traffic within network boundaries impacts the profitability of an ISP. A distinguishing aspect of our work is the focus on Internet-wide implications, i.e., how adoption of localization within an ISP affects both itself and other ISPs. Our simulations are based on detailed models that estimate inter-autonomous system (AS) P2P traffic and inter-AS routing, localization models that predict the extent to which P2P traffic is reduced, and pricing models that predict the impact of changes in traffic on the profit of an ISP. We evaluate our models by using a large-scale crawl of BitTorrent containing over 138 million users sharing 2.75 million files. Our results show that the benefits of localization must not be taken for granted. Some of our key findings include: 1) residential ISPs can actually lose money when localization is employed, and some of them will not see increased profitability until other ISPs employ localization; 2) the reduction in costs due to localization will be limited for small ISPs and tends to grow only logarithmically with client population; and 3) some ISPs can better increase profitability through alternate strategies to localization by taking advantage of the business relationships they have with other ISPs.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {1910–1923},
numpages = {14},
keywords = {peer-to-peer (P2P), localization, ISP profit}
}

@inproceedings{10.1145/2523616.2523620,
author = {Xu, Yunjing and Bailey, Michael and Noble, Brian and Jahanian, Farnam},
title = {Small is better: avoiding latency traps in virtualized data centers},
year = {2013},
isbn = {9781450324281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523616.2523620},
doi = {10.1145/2523616.2523620},
abstract = {Public clouds have become a popular platform for building Internet-scale applications. Using virtualization, public cloud services grant customers full control of guest operating systems and applications, while service providers still retain the management of their host infrastructure. Because applications built with public clouds are often highly sensitive to response time, infrastructure builders strive to reduce the latency of their data center's internal network. However, most existing solutions require modification to the software stack controlled by guests. We introduce a new host-centric solution for improving latency in virtualized cloud environments. In this approach, we extend a classic scheduling principle---Shortest Remaining Time First---from the virtualization layer, through the host network stack, to the network switches. Experimental and simulation results show that our solution can reduce median latency of small flows by 40%, with improvements in the tail of almost 90%, while reducing throughput of large flows by less than 3%.},
booktitle = {Proceedings of the 4th Annual Symposium on Cloud Computing},
articleno = {7},
numpages = {16},
keywords = {virtualization, latency, cloud computing},
location = {Santa Clara, California},
series = {SOCC '13}
}

@inproceedings{10.1145/2312005.2312060,
author = {Das Sarma, Atish and Dinitz, Michael and Pandurangan, Gopal},
title = {Efficient computation of distance sketches in distributed networks},
year = {2012},
isbn = {9781450312134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2312005.2312060},
doi = {10.1145/2312005.2312060},
abstract = {Distance computation (e.g., computing shortest paths) is one of the most fundamental primitives used in communication networks. The cost of effectively and accurately computing pairwise network distances can become prohibitive in large-scale networks such as the Internet and Peer-to-Peer (P2P) networks. To negotiate the rising need for very efficient distance computation at scales never imagined before, approximation techniques for numerous variants of this question have recently received significant attention in the literature. Several different areas of theoretical research have emerged centered around this problem, such as metric embeddings, distance labelings, spanners, and distance oracles. The goal is to preprocess the graph and store a small amount of information such that whenever a query for any pairwise distance is issued, the distance can be well approximated (i.e., with small stretch) very quickly in an online fashion. Specifically, the pre-processing (usually) involves storing a small sketch with each node, such that at query time only the sketches of the concerned nodes need to be looked up to compute the approximate distance. Techniques derived from metric embeddings have been considered extensively by the networking community, usually under the name of network coordinate systems. On the other hand, while the computation of distance oracles has received considerable attention in the context of web graphs and social networks, there has been little work towards similar algorithms within the networking community. In this paper, we present the first theoretical study of distance sketches derived from distance oracles in a distributed network. We first present a fast distributed algorithm for computing approximate distance sketches, based on a distributed implementation of the distance oracle scheme of [Thorup-Zwick, JACM 2005]. We also show how to modify this basic construction to achieve different tradeoffs between the number of pairs for which the distance estimate is accurate, the size of the sketches, and the time and message complexity necessary to compute them. These tradeoffs can then be combined to give an efficient construction of small sketches with provable average-case as well as worst-case performance. Our algorithms use only small-sized messages and hence are suitable for bandwidth-constrained networks, and can be used in various networking applications such as topology discovery and construction, token management, load balancing, monitoring overlays, and several other problems in distributed algorithms.},
booktitle = {Proceedings of the Twenty-Fourth Annual ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {318–326},
numpages = {9},
keywords = {distributed computing, distance sketches},
location = {Pittsburgh, Pennsylvania, USA},
series = {SPAA '12}
}

@inproceedings{10.1145/2420950.2420969,
author = {Bilge, Leyla and Balzarotti, Davide and Robertson, William and Kirda, Engin and Kruegel, Christopher},
title = {Disclosure: detecting botnet command and control servers through large-scale NetFlow analysis},
year = {2012},
isbn = {9781450313124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420950.2420969},
doi = {10.1145/2420950.2420969},
abstract = {Botnets continue to be a significant problem on the Internet. Accordingly, a great deal of research has focused on methods for detecting and mitigating the effects of botnets. Two of the primary factors preventing the development of effective large-scale, wide-area botnet detection systems are seemingly contradictory. On the one hand, technical and administrative restrictions result in a general unavailability of raw network data that would facilitate botnet detection on a large scale. On the other hand, were this data available, real-time processing at that scale would be a formidable challenge. In contrast to raw network data, NetFlow data is widely available. However, NetFlow data imposes several challenges for performing accurate botnet detection.In this paper, we present Disclosure, a large-scale, wide-area botnet detection system that incorporates a combination of novel techniques to overcome the challenges imposed by the use of NetFlow data. In particular, we identify several groups of features that allow Disclosure to reliably distinguish C&amp;C channels from benign traffic using NetFlow records (i.e., flow sizes, client access patterns, and temporal behavior). To reduce Disclosure's false positive rate, we incorporate a number of external reputation scores into our system's detection procedure. Finally, we provide an extensive evaluation of Disclosure over two large, real-world networks. Our evaluation demonstrates that Disclosure is able to perform real-time detection of botnet C&amp;C channels over datasets on the order of billions of flows per day.},
booktitle = {Proceedings of the 28th Annual Computer Security Applications Conference},
pages = {129–138},
numpages = {10},
location = {Orlando, Florida, USA},
series = {ACSAC '12}
}

@inproceedings{10.1145/2484028.2484234,
author = {Hasibi, Faegheh},
title = {Indexing and querying overlapping structures},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484234},
doi = {10.1145/2484028.2484234},
abstract = {Structural information retrieval is mostly based on hierarchy. However, in real life information is not purely hierarchical and structural elements may overlap each other. The most common example is a document with two distinct structural views, where the logical view is section/ subsection/ paragraph and the physical view is page/ line. Each single structural view of this document is a hierarchy and the components are either disjoint or nested inside each other. The overlapping issue arises when one structural element cannot be neatly nested into others. For instance, when a paragraph starts in one page and terminates in the next page. Similar situations can appear in videos and other multimedia contents, where temporal or spatial constituents of a media file may overlap each other.Querying over overlapping structures is one of the challenges of large scale search engines. For instance, FSIS (FAST Search for Internet Sites) [1] is a Microsoft search platform, which encounters overlaps while analysing content of textual data. FSIS uses a pipeline process to extract structure and semantic information of documents. The pipeline contains several components, where each component writes annotations to the input data. These annotations consist of structural elements and some of them may overlap each other. Handling overlapping structures in search engines will add a novel capability of searching, where users can ask queries such as "Find all the words that overlap two lines" or "Find the music played during Intro scene of Avatar movie". There are also other use cases, where the user of the search engine is not a person, but is a specific program with complex, non-traditional information retrieval needs.This research attempts to index overlapping structures and provide efficient query processing for large-scale search engines. The current research on overlapping structures revolves around encoding and modelling data, while indexing and query processing methods need investigations. Moreover, due to intrinsic complexity of overlaps, XML indexing and query processing techniques cannot be used for overlapping structures. Hence, my research on overlapping structures comprises three main parts: (1) an indexing method that supports both hierarchies and overlaps; (2) a query processing method based on the indexing technique and (3) a query language that is close to natural language and supports both full text and structural queries.Our approach for indexing overlaps is to adapt the PrePost [3] XML indexing method to overlapping structures. This method labels each node with its start and end positions and requires modest storage space. However, PrePost indexing cannot be used for overlapping nodes. To overcome this issue, we need to define a data model for overlapping structures. Since hierarchies are not sufficient to describe overlapping components, several data structures have been introduced by scholars. One of the most interesting data models is GODDAG [2]. GODDAG is a tree-like graph, where nodes can have multiple parentage. This model can support overlaps as well as simple inheritance. Our proposed data model for indexing overlaps is such a tree-like structure, where we can define overlapping, parent-child and ancestor-descendant relationships.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1144},
numpages = {1},
keywords = {semi-structured data, query processing, overlapping structures, indexing},
location = {Dublin, Ireland},
series = {SIGIR '13}
}

@inproceedings{10.1145/1723112.1723167,
author = {Le, Hoang and Yang, Yi-Hua and Prasanna, Viktor K.},
title = {Memory efficient string matching: a modular approach on FPGAs (abstract only)},
year = {2010},
isbn = {9781605589114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1723112.1723167},
doi = {10.1145/1723112.1723167},
abstract = {Network Intrusion Detection Systems (NIDSs) have emerged as powerful tools for detecting and preventing malicious attacks over both the Internet and Intranet. String matching, which is one of the most important functions of NIDS, demands exceptionally high performance to match the content of network traffic against a predefined database of malicious patterns. Much work has been done in this field; however, they result in low memory efficiencyfootnote{The memory efficiency (in bytes/char) is defined as the ratio of the amount of the required storage memory (in bytes), and the size of the dictionary (number of characters).}. Due to the available on-chip memory and the number of I/O pins of Field Programmable Gate Arrays (FPGAs), state-of-the-art designs cannot support large dictionaries without using high-latency external DRAM. We propose a novel Memory efficient Architecture for large-scale String Matching, namely MASM, based on pipelined binary search tree. Our design provides a high-throughput matching module, which can be used as the building block to process arbitrary-length patterns. With memory efficiency close to 1 byte/char, MASM can support a dictionaryfootnote The size of a dictionary is the total number of characters in all the patterns in the dictionary. of over 4 MB (regardless of the size of the alphabet), using a single state-of-the-art FPGA device. This efficiency is comparable to that of a Ternary Content Addressable Memory (TCAM)-based solution. The architecture can also be easily partitioned, so as to use external SRAM to handle even larger dictionaries of over 8 MB. Our implementation results show a sustained throughput of 3.2 Gbps, even when external SRAM is used. The MASM module can be simply duplicated to accept multiple characters per cycle, leading to scalable throughput with respect to the number of characters processed in each cycle. Dictionary update involves only rewriting the memory content, which can be done quickly without reconfiguring the chip.},
booktitle = {Proceedings of the 18th Annual ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {285},
numpages = {1},
keywords = {packet filtering, fpga, deep packet classification},
location = {Monterey, California, USA},
series = {FPGA '10}
}

@inproceedings{10.1145/2480347.2480349,
author = {Szymanski, Ted H.},
title = {Low latency energy efficient communications in global-scale cloud computing systems},
year = {2013},
isbn = {9781450319805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480347.2480349},
doi = {10.1145/2480347.2480349},
abstract = {This paper explores technologies to achieve low-latency energy-efficient communications in Global-Scale Cloud Computing systems. A global-scale cloud computing system linking 100 remote data-centers can interconnect potentially 5M servers, considerably larger than the size of traditional High-Performance-Computing (HPC) machines. Traditional HPC machines use tightly coupled processors and networks which rarely drop packets. In contrast, today's IP Internet is a relatively loosely-coupled Best-Effort network with poor latency and energy-efficiency guarantees, with relatively high packet loss rates. This paper explores the use of a recently-proposed Future-Internet network, which uses a QoS-aware router scheduling algorithm combined with a new IETF resource reservation signalling technology, to achieve improved latency and energy-efficiency in cloud computing systems. A Maximum-Flow Minimum-Energy routing algorithm is used to route high-capacity "trunks" between data-centers distributed over the continental USA, using a USA IP network topology. The communications between virtual machines in remote data-centers are aggregated and multiplexed onto the trunks, to achieve significantly improved energy-efficiency. According to theory and simulations, the large and variable queueing delays of traditional Best-Effort Internet links can be eliminated, and the latency over the cloud can be reduced to near-minimal values, i.e., the fiber latency. The maximum fiber latencies over the Sprint USA network are approx. 20 milliseconds, comparable to hard disk drive latencies, and multithreading in virtual machines can be used to hide these latencies. Furthermore, if existing dark-fiber over the continental network is activated, the bisection bandwidth available in a global-scale cloud computing system can rival that achievable in commercial HPC machines.},
booktitle = {Proceedings of the 2013 Workshop on Energy Efficient High Performance Parallel and Distributed Computing},
pages = {13–22},
numpages = {10},
keywords = {routing, quality of service, minimum latency, minimum energy, minimum cost, maximum flow, internet, global scale, energy efficiency, data centers, cloud computing},
location = {New York, New York, USA},
series = {EEHPDC '13}
}

@inproceedings{10.1145/2390231.2390234,
author = {Vulimiri, Ashish and Michel, Oliver and Godfrey, P. Brighten and Shenker, Scott},
title = {More is less: reducing latency via redundancy},
year = {2012},
isbn = {9781450317764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390231.2390234},
doi = {10.1145/2390231.2390234},
abstract = {Low latency is critical for interactive networked applications. But while we know how to scale systems to increase capacity, reducing latency --- especially the tail of the latency distribution --- can be much more difficult.We argue that the use of redundancy in the context of the wide-area Internet is an effective way to convert a small amount of extra capacity into reduced latency. By initiating redundant operations across diverse resources and using the first result which completes, redundancy improves a system's latency even under exceptional conditions. We demonstrate that redundancy can significantly reduce latency for small but critical tasks, and argue that it is an effective general-purpose strategy even on devices like cell phones where bandwidth is relatively constrained.},
booktitle = {Proceedings of the 11th ACM Workshop on Hot Topics in Networks},
pages = {13–18},
numpages = {6},
location = {Redmond, Washington},
series = {HotNets-XI}
}

@techreport{10.5555/2581999,
author = {Chan, Vincent and Chapin, John and Elson, Pat and Fisher, Darleen and Frost, Victor and Jones, Kevin and Miller, Grant},
title = {Future Heterogeneous Networks},
year = {2011},
publisher = {National Science Foundation},
address = {USA},
abstract = {The workshop FutureHetNets 2011 on "Highly Controllable Dynamic Heterogeneous Networking" was held on March 24--25, 2011 at NASA Ames Research Center in Mountain View, California. It was sponsored by the Large Scale Networking Coordinating Group (LSN) of the Networking and Information Technology Research and Development (NITRD) interagency community, and supported by the National Science Foundation (NSF) and NASA. The workshop brought together 74 leading researchers from US institutions to discuss the research and development activities needed to enable the end-to-end, scalable, highly controllable, secure heterogeneous networks of the future.The workshop was designed to uncover tough networking problems, especially those driven by the properties of new physical layer communication systems, and explore architecture constructs that may provide realistic solutions for the realization of an integrated heterogeneous network. Workshop participants were encouraged to consider fundamental architectural changes, covering the entire stack from the Physical to the Application Layers that hold the potential for major breakthroughs in heterogeneous network performance.Computing power will increase dramatically in the near future with the development of advanced multi-core processors and cloud computing and storage. The limiting factor on how fast new applications will develop is the availability of high network speeds and much better quality of service at reasonable costs. Device technologies and hardware subsystems are mature enough to provide at least two orders of magnitude increase in network speeds. However, left to incremental developments, the current Internet architecture will not be able to support heterogeneous applications over heterogeneous networks at affordable costs. The principal reason is that the current architectural partitioning of networks into layers has run its course and is approaching the saturation point for further major improvements.}
}

@proceedings{10.1145/2608020,
title = {DIDC '14: Proceedings of the sixth international workshop on Data intensive distributed computing},
year = {2014},
isbn = {9781450329132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the Sixth International Workshop on Data-intensive Distributed Computing (DIDC 2014), which is held in conjunction with the International ACM Symposium on High Performance Distributed Computing (HPDC 2014).The data needs of scientific as well as commercial applications from a diverse range of fields have been increasing exponentially over the recent years. Digital data generated from various sources such as scientific instruments, sensors, internet transactions, email, video and click streams can be large, diverse, longitudinal and distributed which poses new challenges and requirements for offline and real time processing where extraction of meaningful information can open novel application areas and lead to new breakthroughs. This data deluge and the increase in the demand for large-scale data processing has necessitated collaboration and sharing of data collections among the world's leading education, research, and industrial institutions and use of distributed resources owned by collaborating parties. In a widely distributed environment, data is often not locally accessible and has thus to be remotely retrieved and stored. While traditional distributed systems work well for computation that requires limited data handling, they may fail in unexpected ways when the computation accesses, creates, and moves large amounts of data especially over wide-area networks. Further, data accessed and created is often poorly described, lacking both metadata and provenance. Scientists, researchers, and application developers are boften forced to solve basic data-handling issues, such as physically locating data, how to access it, and/or how to move it to visualization and/or compute resources for further analysis. Although many efforts have been made to develop new programming paradigms and models that can handle the data needs of the application automatically, the results are far from being optimized.DIDC focuses on the challenges imposed by data-intensive applications on distributed systems, and on the different state-of-the-art solutions proposed to overcome these challenges. It brings together the collaborative and distributed computing community and the data management community in an effort to generate productive conversations on the planning, management, and scheduling of data handling tasks and data storage resources.This year's workshop continues with the tradition of gathering distinguished speakers and providing a diverse program with a variety of topics ranging from parallel programming models for data-intensive applications to Cloud architecture and testbed design. We also include a Hot Topics session that presents and discusses current trends and upcoming challenges in DIDC such as data distribution for GPU processing models in exa-scale computing and challenges and future directions in big data processing.},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/2554850.2555010,
author = {Karumanchi, Sushama and Squicciarini, Anna Cinzia},
title = {In the wild: a large scale study of web services vulnerabilities},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2555010},
doi = {10.1145/2554850.2555010},
abstract = {The pervasiveness of Web Services, compounded with seamless interoperability characteristics, introduces security concerns that are to be carefully considered with the envisioned internet architecture. In this paper, we propose a comprehensive study on Web Service vulnerabilities. We consider not only well known Web-based vulnerabilities such as SQL injection, session replay etc, but we also analyze Web-Service specific vulnerabilities and their potential of attacks due to poor service construction and service maintenance. In our analysis, we classify each of the studied vulnerability according to a new taxonomy, discuss remedies and impact, and propose methods of detection based on real-time analysis. Our analysis is supported by the results of a large scale study involving over 2,000 real-world Web Services. We note that many of the least studied vulnerabilities are present in the wild.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {1239–1246},
numpages = {8},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.1145/2619239.2626318,
author = {Peter, Simon and Javed, Umar and Zhang, Qiao and Woos, Doug and Anderson, Thomas and Krishnamurthy, Arvind},
title = {One tunnel is (often) enough},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626318},
doi = {10.1145/2619239.2626318},
abstract = {A longstanding problem with the Internet is that it is vulnerable to outages, black holes, hijacking and denial of service. Although architectural solutions have been proposed to address many of these issues, they have had difficulty being adopted due to the need for widespread adoption before most users would see any benefit. This is especially relevant as the Internet is increasingly used for applications where correct and continuous operation is essential.In this paper, we study whether a simple, easy to implement model is sufficient for addressing the aforementioned Internet vulnerabilities. Our model, called ARROW (Advertised Reliable Routing Over Waypoints), is designed to allow users to configure reliable and secure end to end paths through participating providers. With ARROW, a highly reliable ISP offers tunneled transit through its network, along with packet transformation at the ingress, as a service to remote paying customers. Those customers can stitch together reliable end to end paths through a combination of participating and non-participating ISPs in order to improve the fault-tolerance, robustness, and security of mission critical transmissions. Unlike efforts to redesign the Internet from scratch, we show that ARROW can address a set of well-known Internet vulnerabilities, for most users, with the adoption of only a single transit ISP. To demonstrate ARROW, we have added it to a small-scale wide-area ISP we control. We evaluate its performance and failure recovery properties in both simulation and live settings.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {99–110},
numpages = {12},
keywords = {source routing, reliability, overlay networks, internet, BGP},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}

@proceedings{10.1145/2307836,
title = {HotPlanet '12: Proceedings of the 4th ACM international workshop on Hot topics in planet-scale measurement},
year = {2012},
isbn = {9781450313186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 4th ACM International Workshop on Hot Topics in Planet-Scale Measurement -- HotPlanet'12. This year's workshop is motivated by the fact that successfully researching, designing and building new mobile, ad-hoc, mesh, and opportunistic networking systems and algorithms requires access to large-scale data on human mobility, encounter, and social network patterns. Unfortunately, the wireless and mobile research communities lack such large-scale data. We believe that large-scale datasets are important, not only in communication network design, but also for fundamental study in other academic disciplines, e.g., epidemiology, urban planning, and social science. Complex networks research has flourished since 1989 when the first large Internet (and later WWW) datasets became available. To achieve similar improvements in mobile networking and related fields, large-scale, and ideally planet-scale, datasets must be collected and made available.Following three previous successful editions of the workshop at ACM MobiSys 2009, 2010, and 2011, the fourth HotPlanet workshop will challenge the community to collect large-scale human mobility traces as well as to propose novel mobility data processing and knowledge discovery techniques.The program committee accepted 7 papers that cover a variety of topics including emerging applications involving large-scale human mobility data collection, human dynamics characterization and modelling, knowledge discovery from mobility data, and methods for choosing and collecting large-scale human mobility datasets. In addition, the program includes a keynote speech on Through a Graph, Darkly by Jon Crowcroft, who has a lot of experience in collecting human mobility datasets and investigating social network patterns. We hope that these proceedings will serve as a valuable reference for large-scale wireless networking measurement.},
location = {Low Wood Bay, Lake District, UK}
}

@article{10.1145/2001269.2001293,
author = {Agarwal, Sameer and Furukawa, Yasutaka and Snavely, Noah and Simon, Ian and Curless, Brian and Seitz, Steven M. and Szeliski, Richard},
title = {Building Rome in a day},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2001269.2001293},
doi = {10.1145/2001269.2001293},
abstract = {We present a system that can reconstruct 3D geometry from large, unorganized collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo-sharing sites. Our system is built on a set of new, distributed computer vision algorithms for image matching and 3D reconstruction, designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.},
journal = {Commun. ACM},
month = {oct},
pages = {105–112},
numpages = {8}
}

@proceedings{10.1145/2661118,
title = {GeoMM '14: Proceedings of the 3rd ACM Multimedia Workshop on Geotagging and Its Applications in Multimedia},
year = {2014},
isbn = {9781450331272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In recent years, the rise of widespread use of GPS sensors and the increasing availability of open geographical databases has motivated a large volume of work on geotagging. The increase in the use of geotagging and improvements in location based multimedia open up a new dimension for the description, organization and manipulation of multimedia data. This new dimension radically expands the usefulness of multimedia data both for daily users of the internet and social networking sites as well as for experts in particular application scenarios.GeoMM 2014 is the third ACM workshop on Geotagging and its Applications in Multimedia. The first workshop was held in 2012 at Nara, Japan, which successfully built a forum for the presentation and synthesis of vision and insight from leading experts. The second workshop was help in 2013 at Barcelona, Spain, which extended the platform to more audience and encouraged practitioners on the developing directions of geotagging research related to multimedia.Following the success in 2012 and 2013, GeoMM 2014 aims to bring together cutting-edge research in geotagging as well as novel applications related to location based service and multimedia. The workshop aims to facilitate in-depth discussions, the sharing of existing tools, and ultimately the enhancement of research efforts in this area. Six papers are presented in GeoMM'14 covering topics including, but not limited to, innovative models of geotagging and interesting phenomenon in geotagging: Chen et al discuss how to use social media based profiling for business location analysis. Almajed and Boutell explore capture time classification of mobile sunset photos with spatiotemporal cues. Phan et al consider collaborative recommendation of photo taking geolocations. Ferracani et al propose to enrich social network trace for urban computing. Zheng et al show that we can infer home location from user's photo collections. At last, we invite the organizers of MediaEval Placing Task to introduce this exciting challenge with large scale datasets.Moreover, we are proud to host a number of keynote speakers including Dr. David J. Crandall, Dr. Martha Larson, Dr. Jia Li, Dr. Mubarak Shah, and Dr. David Ayman Shamma, who are the leading experts in their research fields. We believe their talks will provide cutting edge overviews of the state of the art in this field.},
location = {Orlando, Florida, USA}
}

@inproceedings{10.1145/2335484.2335518,
author = {Cao, Bin and Yin, Jianwei and Deng, Shuiguang and Xu, Yueshen and Xiao, Youneng and Wu, Zhaohui},
title = {A highly efficient cloud-based architecture for large-scale STB event processing: industry article},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335518},
doi = {10.1145/2335484.2335518},
abstract = {With the popularization and ripeness of DTV (Digital television) and IPTV (Internet Protocol television), the STB (Set-Top Box) devices are playing more and more important roles in improving the interactive TV applications. Through STB devices, millions of users can publish/subscribe their interested events when they are participating in different applications. Furthermore, it is likely that a very large number of events would be published simultaneously and processing these events under critical timing constrains is often required by lots of applications. How to subscribe and process this large amount of events in real time is rather a challenge to both academia and industry. The combination of Cloud Computing and DDS (Data Distribution Service) middleware brings us a promising way to address this issue. Cloud computing can not only provide sufficient computing capacity but also reduce the processing cost in a great lot. Besides, DDS middleware supports real-time publish/subscribe communication model and is often used in military scenario. In this paper, in order to process such a large scale and high concurrency events efficiently and real-time, we propose a cloud-based architecture for STB event processing. Besides, we discuss the implementation details of reliability and load balancing. To demonstrate the effectiveness of our approach some experiments on the prototype are evaluated.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {314–323},
numpages = {10},
keywords = {real-time event processing, publish/subscribe, distribution, concurrent, cloud-based, DDS middleware},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1007/978-3-642-13789-1_10,
author = {Pinheiro, Billy and Nascimento, Vagner and Cerqueira, Eduardo and Moreira, Waldir and Abel\'{e}m, Ant\^{o}nio},
title = {Abare: a coordinated and autonomous framework for deployment and management of wireless mesh networks},
year = {2010},
isbn = {3642137881},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-13789-1_10},
doi = {10.1007/978-3-642-13789-1_10},
abstract = {The adoption of Wireless Mesh Networks (WMNs) is a solution to provide last mile indoor and outdoor Internet access and is gaining an important attention form academic and industry research groups in recent years. WMNs will support the distribution of diverse type of services, ranging from battlefield surveillance to high quality mobile audio and video applications. However, the existence of open source and proprietary approaches that are not interoperable as well as the delay in the standardization process makes the deployment of a large-scale WMN time-consuming and very complex. This paper is an extension of the framework Abare with autonomic capability as well as performance evaluation results are presented. Abare defines a set of components and practices in order to assist the implementation and management of WMN systems, as well as to provide autonomic features in routers to decrease the manager workload and reduce time-consuming.},
booktitle = {Proceedings of the Third International Conference on Future Multimedia Networking},
pages = {100–111},
numpages = {12},
keywords = {wireless mesh networks, management, autonomic},
location = {Krak\'{o}w, Poland},
series = {FMN'10}
}

@article{10.1109/TNET.2012.2187674,
author = {Zhang, Lu and Tang, Xueyan},
title = {Optimizing client assignment for enhancing interactivity in distributed interactive applications},
year = {2012},
issue_date = {December 2012},
publisher = {IEEE Press},
volume = {20},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2012.2187674},
doi = {10.1109/TNET.2012.2187674},
abstract = {Distributed interactive applications (DIAs) are networked systems that allow multiple participants at different locations to interact with each other. Wide spreads of client locations in large-scale DIAs often require geographical distribution of servers to meet the latency requirements of the applications. In the distributed server architecture, the network latencies involved in the interactions between clients are directly affected by how the clients are assigned to the servers. In this paper, we focus on the problem of assigning clients to appropriate servers in DIAs to enhance their interactivity. We formulate the problem as a combinational optimization problem and prove that it is NP-complete. Then, we propose several heuristic algorithms for fast computation of good client assignments and theoretically analyze their approximation ratios. The proposed algorithms are also experimentally evaluated with real Internet latency data. The results show that the proposed algorithms are efficient and effective in reducing the interaction time between clients, and our proposed Distributed-Modify-Assignment adapts well to the dynamics of client participation and network conditions. For the special case of tree network topologies, we develop a polynomial-time algorithm to compute the optimal client assignment.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {1707–1720},
numpages = {14},
keywords = {interactivity, distributed interactive application (DIA), client assignment, NP-complete}
}

@inproceedings{10.5555/2725669.2725684,
author = {Preschern, Christopher and Kajtazovic, Nermin and Kreiner, Christian},
title = {Security analysis of safety patterns},
year = {2013},
isbn = {9781941652008},
publisher = {The Hillside Group},
address = {USA},
abstract = {Architectural safety patterns provide knowledge about large scale design decisions for safety-critical systems. Safety-critical systems are nowadays increasingly subject to attacks due to their increased connectivity to the Internet. Therefore, we extend existing architectural safety patterns to include security considerations. We apply a STRIDE approach on the safety patterns to obtain relevant threats for each pattern and we structure these threats in a Goal Structuring Notation diagram. We present a catalog of security enhanced safety patterns and we apply one of the patterns to a case study to show how the security-enhanced safety patterns can help for security reasoning.},
booktitle = {Proceedings of the 20th Conference on Pattern Languages of Programs},
articleno = {12},
numpages = {38},
location = {Monticello, Illinois},
series = {PLoP '13}
}

@article{10.14778/2733004.2733073,
author = {Venkataraman, Shivakumar and Agrawal, Divyakant},
title = {Datacenters as computers: Google engineering &amp; database research perspectives},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733073},
doi = {10.14778/2733004.2733073},
abstract = {In this collaborative keynote address, we will share Google's experience in building a scalable data infrastructure that leverages datacenters for managing Google's advertising data over the last decade. In order to support the massive online advertising platform at Google, the data infrastructure must simultaneously support both transactional and analytical workloads. The focus of this talk will be to highlight how the datacenter architecture and the cloud computing paradigm has enabled us to manage the exponential growth in data volumes and user queries, make our services highly available and fault tolerant to massive datacenter outages, and deliver results with very low latencies. We note that other Internet companies have also undergone similar growth in data volumes and user queries. In fact, this phenomenon has resulted in at least two new terms in the technology lexicon: big data and cloud computing. Cloud computing (and datacenters) have been largely responsible for scaling the data volumes from terabytes range just a few years ago to now reaching in the exabyte range over the next couple of years. Delivering solutions at this scale that are fault-tolerant, latency sensitive, and highly available requires a combination of research advances with engineering ingenuity at Google and elsewhere. Next, we will try to answer the following question: is a datacenter just another (very large) computer? Or, does it fundamentally change the design principles for data-centric applications and systems. We will conclude with some of the unique research challenges that need to be addressed in order to sustain continuous growth in data volumes while supporting high throughput and low latencies.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1720–1721},
numpages = {2}
}

@inproceedings{10.1145/1871437.1871564,
author = {Wang, Qihua and Jin, Hongxia},
title = {Exploring online social activities for adaptive search personalization},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871564},
doi = {10.1145/1871437.1871564},
abstract = {The web has largely become a very social environment and will continue to become even more so. People are not only enjoying their social visibility on the Web but also increasingly participating in various social activities delivered through the Web. In this paper, we propose to explore a user's public social activities, such as blogging and social bookmarking, to personalize Internet services. We believe that public social data provides a more acceptable way to derive user interests than more private data such as search histories and desktop data. We propose a framework that learns about users' preferences from their activities on a variety of online social systems. As an example, we illustrate how to apply the user interests derived by our system to personalize search results. Furthermore, our system is adaptive; it observes users' choices on search results and automatically adjusts the weights of different social systems during the information integration process, so as to refine its interest profile for each user. We have implemented our approach and performed experiments on real-world data collected from three large-scale online social systems. Over two hundred users from worldwide who are active on the three social systems have been tested. Our experimental results demonstrate the effectiveness of our personalized search approach. Our results also show that integrating information from multiple social systems usually leads to better personalized results than relying on the information from a single social system, and our adaptive approach further improves the performance of the personalization solution.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {999–1008},
numpages = {10},
keywords = {personalized search, online social activities, adaptivity},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}

@inproceedings{10.1145/1940761.1940874,
author = {Knox, Emily},
title = {A confirmatory factor analysis of library use},
year = {2011},
isbn = {9781450301213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1940761.1940874},
doi = {10.1145/1940761.1940874},
abstract = {For the past 25 years, public library administrators have pondered their institution's relationship to the computer. Since the 1980s, the integrated library system, which is based on a server-client model, has been a ubiquitous part of the library. These systems, however, are solely for use by the library staff. There have long been computers available for patrons but for many years these only provided access to the online catalog and databases on CD-ROM. Over time more applications were added to the patron computers including integrated office suites and web browsers. Public computers for patrons with access to the Internet and loaded with various productivity applications are now available in almost all public libraries across the country.At the same time that personal computers were proliferating, in both the private and public sphere more knowledge content became available digitally. Materials that were once available only in print, including books, periodicals, and databases, can now be accessed through the Internet. It is not surprising that it has been difficult for libraries to adapt to this change. An institution that was originally established to collect print materials now has to offer access to knowledge in many different formats.This study investigates one aspect of this change. The concept of "library use" has changed with the proliferation of digital media. Libraries of all types and the various administrative boards that control their funding allocated revenue budgets solely on circulation statistics. That is, the amount of money given to the library was based on how many books were checked out in a given year. This statistic does not adequately account for the actual services that libraries provide. In 2009, the editors of Library Journal [1] sponsored research for the development of a new scale measurement for rating library services based on four indicators.This study is a confirmatory factor analysis of this scale in order to validate this new measurement model.The initial data consisted of three hundred three (N =303) from the 2008 New Jersey Public Library Statistics [2]. 23 libraries did not respond to the survey, leaving a total of two hundred eighty libraries in the data set (n=280). These data are collected every year by the New Jersey State Library and are freely available on the institution's website (www.njstatelib.org). The New Jersey State Library collects these data via an electronic survey. An additional 38 libraries were removed because their scores fell more than three standard deviations beyond the mean on any one of the variables that were tested. Eight libraries were removed because they were multivariate outliers. This resulted in a sample size of two hundred thirty-four public libraries in New Jersey (n = 234).All questions on the survey (except for identification information) required whole number (ratio level) responses from the libraries. Items regarding revenue and expenses required whole dollar amounts. Computer Readable Materials Budget refers to the total amount spent on software, electronic books, and other items that must be used on a computer. Databases Owned indicates the total number of licensed databases for which the library pays. Libraries also indicated the number of computers available for public use. The survey also included two categorical questions. One asked whether or not the library made password free Wi-Fi available to the general public. The other asked whether or not the library made JerseyClicks, a full-text search portal funded by the state, available on their website.According to the new Library Journal Index mentioned above, four indicators are used to construct the library use score: library visits, circulation, program attendance, and public Internet computer use. Library visits refers to the total number of people who enter the library for any purpose. Circulation indicates the total number of "check-outs" for all materials including any renewals. It does not include virtual circulation or interlibrary loan. The total number of people at all programs either sponsored or hosted by the library is indicated by program attendance. Finally, public Internet computer use refers to the number of individuals who used public accessible computer terminals in a given year. These indicators were transformed into per-capita data by dividing each reported amount by the population of the library's municipality as indicated in the 2000 Census. Each number was then transformed into a z-score.A confirmatory factor analysis was conducted to assess the fit between the NJPL data and proposed factor structure for the Library Use Index proposed in Library Journal. The chi-square value for the overall model fit was not significant (χ2 = .720, df = 2, p &gt;.001) indicating that there is an adequate fit between the factor structure of the Library Use Index and the data.A "library use" score was then computed using the sum of the four standardized indicators. In accordance with the procedure given in Library Journal [3], since this preliminary score included scores with negative values (with a minimum value of -4.99), a correction factor of 5 was added to each score so that the variable Library Use Score would not include negative values. The Library Use Score for the libraries was M = 4.97, SD = 3.26 with a range of .007 to 15.981. The Library Use Score provides a single measure that can be used to compare individual public libraries with peer institutions.},
booktitle = {Proceedings of the 2011 IConference},
pages = {697–698},
numpages = {2},
location = {Seattle, Washington, USA},
series = {iConference '11}
}

@article{10.1145/1865106.1865115,
author = {Vu, Long and Gupta, Indranil and Nahrstedt, Klara and Liang, Jin},
title = {Understanding overlay characteristics of a large-scale peer-to-peer IPTV system},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/1865106.1865115},
doi = {10.1145/1865106.1865115},
abstract = {This article presents results from our measurement and modeling efforts on the large-scale peer-to-peer (p2p) overlay graphs spanned by the PPLive system, the most popular and largest p2p IPTV (Internet Protocol Television) system today. Unlike other previous studies on PPLive, which focused on either network-centric or user-centric measurements of the system, our study is unique in (a) focusing on PPLive overlay-specific characteristics, and (b) being the first to derive mathematical models for its distributions of node degree, session length, and peer participation in simultaneous overlays.Our studies reveal characteristics of multimedia streaming p2p overlays that are markedly different from existing file-sharing p2p overlays. Specifically, we find that: (1) PPLive overlays are similar to random graphs in structure and thus more robust and resilient to the massive failure of nodes, (2) Average degree of a peer in the overlay is independent of the channel population size and the node degree distribution can be fitted by a piecewise function, (3) The availability correlation between PPLive peer pairs is bimodal, that is, some pairs have highly correlated availability, while others have no correlation, (4) Unlike p2p file-sharing peers, PPLive peers are impatient and session lengths (discretized, per channel) are typically geometrically distributed, (5) Channel population size is time-sensitive, self-repeated, event-dependent, and varies more than in p2p file-sharing networks, (6) Peering relationships are slightly locality-aware, and (7) Peer participation in simultaneous overlays follows a Zipf distribution. We believe that our findings can be used to understand current large-scale p2p streaming systems for future planning of resource usage, and to provide useful and practical hints for future design of large-scale p2p streaming systems.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {nov},
articleno = {31},
numpages = {24},
keywords = {streaming, overlay, multimedia, Peer-to-peer, PPLive, IPTV}
}

@article{10.1145/1961209.1961213,
author = {Mei, Tao and Yang, Bo and Hua, Xian-Sheng and Li, Shipeng},
title = {Contextual Video Recommendation by Multimodal Relevance and User Feedback},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/1961209.1961213},
doi = {10.1145/1961209.1961213},
abstract = {With Internet delivery of video content surging to an unprecedented level, video recommendation, which suggests relevant videos to targeted users according to their historical and current viewings or preferences, has become one of most pervasive online video services. This article presents a novel contextual video recommendation system, called VideoReach, based on multimodal content relevance and user feedback. We consider an online video usually consists of different modalities (i.e., visual and audio track, as well as associated texts such as query, keywords, and surrounding text). Therefore, the recommended videos should be relevant to current viewing in terms of multimodal relevance. We also consider that different parts of videos are with different degrees of interest to a user, as well as different features and modalities have different contributions to the overall relevance. As a result, the recommended videos should also be relevant to current users in terms of user feedback (i.e., user click-through). We then design a unified framework for VideoReach which can seamlessly integrate both multimodal relevance and user feedback by relevance feedback and attention fusion. VideoReach represents one of the first attempts toward contextual recommendation driven by video content and user click-through, without assuming a sufficient collection of user profiles available. We conducted experiments over a large-scale real-world video data and reported the effectiveness of VideoReach.},
journal = {ACM Trans. Inf. Syst.},
month = {apr},
articleno = {10},
numpages = {24},
keywords = {relevance feedback, image retrieval, Video recommendation}
}

@inproceedings{10.1145/1989240.1989252,
author = {Niu, Di and Li, Baochun and Zhao, Shuqiao},
title = {Understanding demand volatility in large VoD systems},
year = {2011},
isbn = {9781450307772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989240.1989252},
doi = {10.1145/1989240.1989252},
abstract = {Bandwidth usage in large-scale Video on Demand (VoD) systems varies rapidly over time, due to unpredictable dynamics in user demand and network conditions. Such bandwidth volatility makes it hard to provision the exact amount of server resources that matches the demand in each video channel, posing significant challenges to achieving quality assurance and efficient resource allocation at the same time. In this paper, we seek to statistically model time-varying traffic volatility in VoD servers, leveraging heteroscedastic models first used to interpret economic time series, with the goal of forecasting not only traffic patterns but also traffic volatility. We present the application of volatility forecast to efficient resource allocation that provides probabilistic service level guarantees to user groups. We also discuss volatility reduction from diversification, and its implications to new strategies for cost-effective server management. Our study is based on monitoring the workload of a large-scale commercial VoD system widely deployed on the Internet.},
booktitle = {Proceedings of the 21st International Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {39–44},
numpages = {6},
keywords = {volatility, video-on-demand, traffic forecast, resource allocation, measurement, garch, diversification, demand prediction},
location = {Vancouver, British Columbia, Canada},
series = {NOSSDAV '11}
}

@article{10.1145/2089085.2089090,
author = {Wu, Chuan and Li, Baochun and Zhao, Shuqiao},
title = {Diagnosing network-wide P2P live streaming inefficiencies},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1S},
issn = {1551-6857},
url = {https://doi.org/10.1145/2089085.2089090},
doi = {10.1145/2089085.2089090},
abstract = {Large-scale live peer-to-peer (P2P) streaming applications have been successfully deployed in today's Internet. While they can accommodate hundreds of thousands of users simultaneously with hundreds of channels of programming, there still commonly exist channels and times where and when the streaming quality is unsatisfactory. In this paper, based on more than two terabytes and one year worth of live traces from UUSee, a large-scale commercial P2P live streaming system, we show an in-depth network-wide diagnosis of streaming inefficiencies, commonly present in typical mesh-based P2P live streaming systems. As the first highlight of our work, we identify an evolutionary pattern of low streaming quality in the system, and the distribution of streaming inefficiencies across various streaming channels and in different geographical regions. We then carry out an extensive investigation to explore the causes to such streaming inefficiencies over different times and across different channels/regions at specific times, by investigating the impact of factors such as the number of peers, peer upload bandwidth, inter-peer bandwidth availability, server bandwidth consumption, and many more. The original discoveries we have brought forward include the two-sided effects of peer population on the streaming quality in a streaming channel, the significant impact of inter-peer bandwidth bottlenecks at peak times, and the inefficient utilization of server capacities across concurrent channels. Based on these insights, we identify problems within the existing P2P live streaming design and discuss a number of suggestions to improve real-world streaming protocols operating at a large scale.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {13},
numpages = {19},
keywords = {streaming inefficiency, Peer-to-peer streaming}
}

@article{10.1145/2460276.2462076,
author = {Bailis, Peter and Ghodsi, Ali},
title = {Eventual Consistency Today: Limitations, Extensions, and Beyond: How can applications be built on eventually consistent infrastructure given no guarantee of safety?},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/2460276.2462076},
doi = {10.1145/2460276.2462076},
abstract = {In a July 2000 conference keynote, Eric Brewer, now VP of engineering at Google and a professor at the University of California, Berkeley, publicly postulated the CAP (consistency, availability, and partition tolerance) theorem, which would change the landscape of how distributed storage systems were architected. Brewer’s conjecture--based on his experiences building infrastructure for some of the first Internet search engines at Inktomi--states that distributed systems requiring always-on, highly available operation cannot guarantee the illusion of coherent, consistent single-system operation in the presence of network partitions, which cut communication between active servers. Brewer’s conjecture proved prescient: in the following decade, with the continued rise of large-scale Internet services, distributed-system architects frequently dropped "strong" guarantees in favor of weaker models--the most notable being eventual consistency.},
journal = {Queue},
month = {mar},
pages = {20–32},
numpages = {13}
}

@inproceedings{10.1145/2632168.2638835,
author = {Xu, Guoqing},
title = {Language, compiler, and runtime system support towards highly scalable big data application (invited talk abstract)},
year = {2014},
isbn = {9781450329347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2632168.2638835},
doi = {10.1145/2632168.2638835},
abstract = {Modern computing has entered the era of Big Data. Analyzing data from Twitter, Google, Facebook, Wikipedia, or the Human Genome Project requires the development of scalable platforms that can quickly extract useful information from an ocean of records collected from customers, clinical trial participants, program execution logs, or the Internet. Most of the existing Big Data applications, including Hadoop, Giraph, Hive, Pig, Mahout, or Hyracks are written managed, object-oriented languages such as Java. While the use of such languages simplifies development tasks, the (memory and execution) inefficiencies inherent in these languages can have large impact on the application performance and scalability. When object-orientation meets Big Data, performance problems are significantly magnified, making data-intensive computing systems fail to scale to large datasets. I will talk about several projects we are currently working on to scale Big Data applications by reducing the cost of a managed runtime. Particularly, I will talk about Facade, a compiler and runtime system we have developed to transform a Big Data application into an almost object-bounded application which has been shown to be much more efficient and scale to much larger datasets. I will also briefly mention two other projects, one attempting to provide a memory-oblivious programming model for developers to allow them to write a program without worrying about how to create threads and use memory, and second aiming to trim a big dataset with probabilistic guarantees to facilitate debugging/testing of a Big Data application.},
booktitle = {Proceedings of the 2014 Joint International Workshop on Dynamic Analysis (WODA) and Software and System Performance Testing, Debugging, and Analytics (PERTEA)},
pages = {13},
numpages = {1},
keywords = {System Support, Highly Scalable Big Data Application},
location = {San Jose, CA, USA},
series = {WODA+PERTEA 2014}
}

@proceedings{10.1145/2000172,
title = {HotPlanet '11: Proceedings of the 3rd ACM international workshop on MobiArch},
year = {2011},
isbn = {9781450307420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 3rd ACM International Workshop on Hot Topics in Planet-Scale Measurement -- HotPlanet'11. This year's workshop is motivated by the fact that successfully researching, designing and building new mobile, ad-hoc, mesh and opportunistic networking systems and algorithms requires access to large-scale data on human mobility, encounter, and social network patterns. Unfortunately, the wireless and mobile research communities lack such data, with typical human contact traces consisting of less than 100 nodes. We believe that large-scale datasets are important, not only in communication network design, but also for fundamental study in other academic disciplines, e.g., epidemiology, urban planning, and social science. Complex networks research has flourished since 1989 when the first large Internet (and later WWW) datasets became available. To achieve similar improvements in mobile networking and related fields, large-scale, and ideally planet-scale, datasets must be collected and made available.Following two successful editions of the workshop at ACM MobiSys 2009 and 2010, the third HotPlanet workshop will not only challenge the community to collect large-scale human mobility traces but also to propose novel mobility data processing and knowledge discovery techniques.Our program this year features a keynote by Professor Gaetano Borriello, one of the architects of Open Data Kit, an invited talk by Michael Doering on large-scale air pollution monitoring, and four reviewed papers on the acquisition, analysis, and use of large scale measurements.We are also excited to be organizing the first ever HotPlanet Mobility Data Contest sponsored by Google. The contest will challenge participants to plan an experiment that relies on mobilityrelated data, deriving the results and communicating them to a broader public. Thanks to the generosity of Google, we are able to offer prizes to the top three competitors. The contest will kick off at the HotPlanet workshop, and will continue throughout the entire MobiSys conference.},
location = {Bethesda, Maryland, USA}
}

@inproceedings{10.5555/2685048.2685066,
author = {Chow, Michael and Meisner, David and Flinn, Jason and Peek, Daniel and Wenisch, Thomas F.},
title = {The mystery machine: end-to-end performance analysis of large-scale internet services},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {Current debugging and optimization methods scale poorly to deal with the complexity of modern Internet services, in which a single request triggers parallel execution of numerous heterogeneous software components over a distributed set of computers. The Achilles' heel of current methods is the need for a complete and accurate model of the system under observation: producing such a model is challenging because it requires either assimilating the collective knowledge of hundreds of programmers responsible for the individual components or restricting the ways in which components interact.Fortunately, the scale of modern Internet services offers a compensating benefit: the sheer volume of requests serviced means that, even at low sampling rates, one can gather a tremendous amount of empirical performance observations and apply "big data" techniques to analyze those observations. In this paper, we show how one can automatically construct a model of request execution from pre-existing component logs by generating a large number of potential hypotheses about program behavior and rejecting hypotheses contradicted by the empirical observations. We also show how one can validate potential performance improvements without costly implementation effort by leveraging the variation in component behavior that arises naturally over large numbers of requests to measure the impact of optimizing individual components or changing scheduling behavior.We validate our methodology by analyzing performance traces of over 1.3 million requests to Facebook servers. We present a detailed study of the factors that affect the end-to-end latency of such requests. We also use our methodology to suggest and validate a scheduling optimization for improving Facebook request latency.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {217–231},
numpages = {15},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{10.1145/1823854.1823923,
author = {Ribarsky, William and Subramanian, K. R. and Liu, Jianfei and Obirieze, Onyewuchi and Guest, Jack},
title = {Mobile application for first response and emergency evacuation in Urban settings},
year = {2010},
isbn = {9781450300315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1823854.1823923},
doi = {10.1145/1823854.1823923},
abstract = {This work builds on a system we have developed for situation-aware mobile routing and response in an urban environment. The system permits police or other first responders to carry models of collections of often large scale, multi-floor urban buildings they encounter in urban environments. The responders must enter, find their way around, and coordinate activities with other responders and command personnel. Coordination requires overall understanding by command personnel of where first responders are (including directing these personnel), where occupants or victims are, where they are going, and what they might do. In addition, coordination may need to take into account response to the actions of perverse independent agents, such as shooters or other criminals. The system supports all this in multiple ways including permitting the responder or commander to determine shortest path 3D routes between floors in real-time, including updates of routes when path-blocking obstacles are present, locations of other responders, and locations of things of interest with respect to the responders current position (e.g., victims, flammable or dangerous materials, etc.). This paper will present some first results for this mobile system in terms of two real-world cases involving police responders: (1) response to a shooter or potential shooter in one or more large urban structures; (2) emergency evacuation in a large urban structure. The real world cases are developed as exercises with police and homeland security personnel. The paper demonstrates the need for tight integration of information provided or displayed through the command interface and the mobile interfaces. The command interface, in particular, must organize and disseminate a substantial amount of information in a timely manner. The paper will also describe initial evaluations of the interfaces and plans for improvement.},
booktitle = {Proceedings of the 1st International Conference and Exhibition on Computing for Geospatial Research &amp; Application},
articleno = {62},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '10}
}

@inproceedings{10.1145/1966913.1966930,
author = {Zhang, Junjie and Luo, Xiapu and Perdisci, Roberto and Gu, Guofei and Lee, Wenke and Feamster, Nick},
title = {Boosting the scalability of botnet detection using adaptive traffic sampling},
year = {2011},
isbn = {9781450305648},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1966913.1966930},
doi = {10.1145/1966913.1966930},
abstract = {Botnets pose a serious threat to the health of the Internet. Most current network-based botnet detection systems require deep packet inspection (DPI) to detect bots. Because DPI is a computational costly process, such detection systems cannot handle large volumes of traffic typical of large enterprise and ISP networks. In this paper we propose a system that aims to efficiently and effectively identify a small number of suspicious hosts that are likely bots. Their traffic can then be forwarded to DPI-based botnet detection systems for fine-grained inspection and accurate botnet detection. By using a novel adaptive packet sampling algorithm and a scalable spatial-temporal flow correlation approach, our system is able to substantially reduce the volume of network traffic that goes through DPI, thereby boosting the scalability of existing botnet detection systems. We implemented a proof-of-concept version of our system, and evaluated it using real-world legitimate and botnet-related network traces. Our experimental results are very promising and suggest that our approach can enable the deployment of botnet-detection systems in large, high-speed networks.},
booktitle = {Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security},
pages = {124–134},
numpages = {11},
keywords = {network security, intrusion detection, botnet, adaptive sampling},
location = {Hong Kong, China},
series = {ASIACCS '11}
}

@inproceedings{10.1145/2523514.2527017,
author = {Khani, Zahra and Azmi, Reza},
title = {A protocol for simultaneous use of confidentiality and integrity in large-scale storage systems},
year = {2013},
isbn = {9781450324984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523514.2527017},
doi = {10.1145/2523514.2527017},
abstract = {Large-scale storage systems often contain sensitive information such as medical information in healthcare services, captured traffic in large companies like Internet Service Providers, or personal user information which is stored by different firms. Most of these systems are distributed geographically and data should be able to move between systems in different places, which mean we cannot protect data by limiting them to internal access. Even internal access is not adequate to provide security because we cannot fully trust every operator who has physical access to systems. In this paper we propose a new approach which provides both confidentiality and integrity for large datasets by combining keyed hash tree (KHT) and Merkle hash tree. In addition, we introduce a hash value and key exchange protocol. Theoretical and experimental analysis shows that it is a practical and scalable way to protect large-scale datasets against modification and theft.},
booktitle = {Proceedings of the 6th International Conference on Security of Information and Networks},
pages = {400–403},
numpages = {4},
keywords = {large-scale storage, hash tree, data integrity, confidentiality},
location = {Aksaray, Turkey},
series = {SIN '13}
}

@inproceedings{10.1145/1879141.1879171,
author = {Luckie, Matthew},
title = {Scamper: a scalable and extensible packet prober for active measurement of the internet},
year = {2010},
isbn = {9781450304832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879141.1879171},
doi = {10.1145/1879141.1879171},
abstract = {Large scale active measurement of the Internet requires appropriate software support. The better tools that we have for executing consistent and systematic measurements, the more confidence we can have in the results. This paper presents scamper, a powerful open-source packet-prober for active measurement of the Internet designed to stand alone from coordination mechanisms. We built scamper and populated it with specific measurement techniques, making design decisions aimed at allowing Internet researchers to focus on scientific experiments rather than building accurate instrumentation.},
booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
pages = {239–245},
numpages = {7},
keywords = {tools, software, active measurement},
location = {Melbourne, Australia},
series = {IMC '10}
}

@inproceedings{10.1145/2079296.2079314,
author = {Vasi\'{c}, Nedeljko and Bhurat, Prateek and Novakovi\'{c}, Dejan and Canini, Marco and Shekhar, Satyam and Kosti\'{c}, Dejan},
title = {Identifying and using energy-critical paths},
year = {2011},
isbn = {9781450310413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2079296.2079314},
doi = {10.1145/2079296.2079314},
abstract = {The power consumption of the Internet and datacenter networks is already significant, and threatens to shortly hit the power delivery limits while the hardware is trying to sustain ever-increasing traffic requirements. Existing energy-reduction approaches in this domain advocate recomputing network configuration with each substantial change in demand. Unfortunately, computing the minimum network subset is computationally hard and does not scale. Thus, the network is forced to operate with diminished performance during the recomputation periods. In this paper, we propose REsPoNse, a framework which overcomes the optimality-scalability trade-off. The insight in REsPoNse is to identify a few energy-critical paths off-line, install them into network elements, and use a simple online element to redirect the traffic in a way that enables large parts of the network to enter a low-power state. We evaluate REsPoNse with real network data and demonstrate that it achieves the same energy savings as the existing approaches, with marginal impact on network scalability and application performance.},
booktitle = {Proceedings of the Seventh COnference on Emerging Networking EXperiments and Technologies},
articleno = {18},
numpages = {12},
location = {Tokyo, Japan},
series = {CoNEXT '11}
}

@inproceedings{10.1145/2517351.2517397,
author = {Faggiani, Adriano and Gregori, Enrico and Lenzini, Luciano and Luconi, Valerio and Vecchio, Alessio},
title = {Network sensing through smartphone-based crowdsourcing},
year = {2013},
isbn = {9781450320276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517351.2517397},
doi = {10.1145/2517351.2517397},
abstract = {Portolan is a crowdsourcing system, aimed at monitoring and measuring large-scale networks, that uses smartphones as mobile observation elements. Currently, Portolan is able to collect information about both wired and wireless networks, in particular it is used to obtain the graph of the Internet with unprecedented resolution and to associate performance indexes (received signal strength, maximum throughput) of cellular networks to geographic locations.},
booktitle = {Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems},
articleno = {31},
numpages = {2},
keywords = {smartphone, network sensing, crowdsourcing},
location = {Roma, Italy},
series = {SenSys '13}
}

@inproceedings{10.1145/2365952.2365991,
author = {Herbrich, Ralf},
title = {Distributed, real-time bayesian learning in online services},
year = {2012},
isbn = {9781450312707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2365952.2365991},
doi = {10.1145/2365952.2365991},
abstract = {The last ten years have seen a tremendous growth in Internet-based online services such as search, advertising, gaming and social networking. Today, it is important to analyze large collections of user interaction data as a first step in building predictive models for these services as well as learn these models in real-time. One of the biggest challenges in this setting is scale: not only does the sheer scale of data necessitate parallel processing but it also necessitates distributed models; with over 900 million active users at Facebook, any user-specific sets of features in a linear or non-linear model yields models of a size bigger than can be stored in a single system.In this talk, I will give a hands-on introduction to one of the most versatile tools for handling large collections of data with distributed probabilistic models: the sum-product algorithm for approximate message passing in factor graphs. I will discuss the application of this algorithm for the specific case of generalized linear models and outline the challenges of both approximate and distributed message passing including an in-depth discussion of expectation propagation and Map-Reduce.},
booktitle = {Proceedings of the Sixth ACM Conference on Recommender Systems},
pages = {203–204},
numpages = {2},
keywords = {graphical models, gaming, distributed machine learning},
location = {Dublin, Ireland},
series = {RecSys '12}
}

@inproceedings{10.1145/1859995.1860024,
author = {Bernardi, Giacomo and Calder, Matt and Fenacci, Damon and Macmillan, Alex and Marina, Mahesh K.},
title = {Stix: a goal-oriented distributed management system for large-scale broadband wireless access networks},
year = {2010},
isbn = {9781450301817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1859995.1860024},
doi = {10.1145/1859995.1860024},
abstract = {Stix is a platform managing emerging large-scale broadband wireless access (BWA) networks. It has been developed to make it easy to manage such networks for community deployments and wireless Internet service providers while keeping the network management infrastructure scalable and flexible. Stix is based on the notions of goal-oriented and in-network management. With Stix, administrators graphically specify network management activities as workflows, which are deployed at a distributed set of agents within the network that cooperate in executing those workflows and storing management information. We implement the Stix system on embedded boards and show that the implementation has a low memory footprint. Using real topology and logging data from a large-scale BWA network operator, we show that Stix is significantly more scalable (via reduction in management traffic) compared to the commonly employed centralized management approach. Finally we use two case studies to demonstrate the ease with which Stix platform can be used for carrying out network reconfiguration and performance management tasks, thereby also showing its potential as a flexible platform to realize self-management mechanisms.},
booktitle = {Proceedings of the Sixteenth Annual International Conference on Mobile Computing and Networking},
pages = {245–256},
numpages = {12},
keywords = {workflows, visual programming, self management, scalability, rural internet access, network management, heterogeneity, distributed management, community networks, broadband wireless access networks},
location = {Chicago, Illinois, USA},
series = {MobiCom '10}
}

@article{10.1145/1710115.1710120,
author = {Croce, Daniele and Mellia, Marco and Leonardi, Emilio},
title = {The quest for bandwidth estimation techniques for large-scale distributed systems},
year = {2010},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/1710115.1710120},
doi = {10.1145/1710115.1710120},
abstract = {In recent years the research community has developed many techniques to estimate the end-to-end available bandwidth of an Internet path. This important metric can be potentially exploited to optimize the performance of several distributed systems and, even, to improve the effectiveness of the congestion control mechanism of TCP. Thus, it has been suggested that some existing estimation techniques could be used for this purpose. However, existing tools were not designed for large-scale deployments and were mostly validated in controlled settings, considering only one measurement running at a time. In this paper, we argue that current tools, while offering good estimates when used alone, might not work in large-scale systems where several estimations severely interfere with each other. We analyze the properties of the measurement paradigms employed today and discuss their functioning, study their overhead and analyze their interference. Our testbed results show that current techniques are insufficient as they are. Finally, we will discuss and propose some principles that should be taken into account for including available bandwidth measurements in large-scale distributed systems.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {20–25},
numpages = {6}
}

@inproceedings{10.1145/1827418.1827456,
author = {Girdzijauskas, Sarunas and Chockler, Gregory and Vigfusson, Ymir and Tock, Yoav and Melamed, Roie},
title = {Magnet: practical subscription clustering for Internet-scale publish/subscribe},
year = {2010},
isbn = {9781605589275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1827418.1827456},
doi = {10.1145/1827418.1827456},
abstract = {An effective means for building Internet-scale distributed applications, and in particular those involving group-based information sharing, is to deploy peer-to-peer overlay networks. The key pre-requisite for supporting these types of applications on top of the overlays is efficient distribution of messages to multiple subscribers dispersed across numerous multicast groups.In this paper, we introduce Magnet: a peer-to-peer publish/subscribe system which achieves efficient message distribution by dynamically organizing peers with similar subscriptions into dissemination structures which preserve locality in the subscription space. Magnet is able to significantly reduce the message propagation costs by taking advantage of subscription correlations present in many large-scale group-based applications.We evaluate Magnet by comparing its performance against a strawman pub/sub system which does not cluster similar subscriptions by simulation. We find that Magnet outperforms the strawman by a substantial margin on clustered subscription workloads produced using both generative models and real application traces.},
booktitle = {Proceedings of the Fourth ACM International Conference on Distributed Event-Based Systems},
pages = {172–183},
numpages = {12},
location = {Cambridge, United Kingdom},
series = {DEBS '10}
}

@inproceedings{10.1145/1834616.1834621,
author = {Jardak, Christine and Riihij\"{a}rvi, Janne and M\"{a}h\"{o}nen, Petri},
title = {Extremely large-scale sensing applications for planetary WSNs},
year = {2010},
isbn = {9781450301770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1834616.1834621},
doi = {10.1145/1834616.1834621},
abstract = {The potentially widespread adoption of Wireless Sensor Networks presents a unique challenge both for the communication infrastructure of the Internet as well as for backend systems responsible for processing the data and making it available in useful form. In this paper we explore this space by making careful estimates on the traffic volumes of sensing data in a number of scenarios explored in the literature. Examples of the scenarios covered are patient monitoring, structural monitoring, vehicular applications and environmental monitoring. The results show that if there were a "flag day" for sensor network deployments in the near future they would as a whole dominate over other forms of mobile and wireless network traffic at least until the middle of the next decade. However, the overall data volumes would still appear to be manageable, although only barely. Based on these forecasts, we comment on the feasibility of processing data from large sensor deployments. We also present first results from our ongoing work towards developing a highly scalable framework for storing and processing data obtained from planet-scale WSN deployments. In particular, we study how MapReduce-based data processing could be used to deal with the scalability challenge, and argue using selected case studies that platforms such as Hadoop could indeed be used to deal with such data volumes.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Hot Topics in Planet-Scale Measurement},
articleno = {3},
numpages = {6},
location = {San Francisco, California},
series = {HotPlanet '10}
}

@inproceedings{10.1145/2480362.2480699,
author = {Payet, Pierre and Doup\'{e}, Adam and Kruegel, Christopher and Vigna, Giovanni},
title = {EARs in the wild: large-scale analysis of execution after redirect vulnerabilities},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480699},
doi = {10.1145/2480362.2480699},
abstract = {Execution After Redirect vulnerabilities---logic flaws in web applications where unintended code is executed after a redirect---have received little attention from the research community. In fact, we found a research paper that incorrectly modeled the redirect semantics, causing their static analysis to miss EAR vulnerabilities.To understand the breadth and scope of EARs in the real world, we performed a large-scale analysis to determine the prevalence of EARs on the Internet. We crawled 8,097,283 URLs from 255,957 domains. We employ a black-box approach that finds EARs which manifest themselves by information leakage in the HTTP redirect response. For this type of EAR, we developed a classification system that discovered 2,173 security-critical EARs among 416 domains. This result shows that EARs are a serious and prevalent problem on the Internet today and deserve future research attention.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1792–1799},
numpages = {8},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.1145/2494493.2494501,
author = {Baxter, Gordon and Cartlidge, John},
title = {Flying by the seat of their pants: what can high frequency trading learn from aviation?},
year = {2013},
isbn = {9781450322492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494493.2494501},
doi = {10.1145/2494493.2494501},
abstract = {As we build increasingly large scale systems (and systems of systems), the level of complexity is also rising. We still expect people to intervene when things go wrong, however, and to diagnose and fix the problems. Aviation has a history of developing systems with a very good safety record. Domains such as high frequency trading (HFT), however, have a much more chequered history. We note that there are several parallels that can be drawn between aviation and HFT. We highlight the ironies of automation that apply to HFT, before going on to identify several lessons that have been used to improve safety in aviation and show how they can be applied to increase the resilience of HFT in particular.},
booktitle = {Proceedings of the 3rd International Conference on Application and Theory of Automation in Command and Control Systems},
pages = {56–65},
numpages = {10},
keywords = {socio-technical systems, ironies of automation, human-in-the-loop, high frequency trading, flash crash},
location = {Naples, Italy},
series = {ATACCS '13}
}

@inproceedings{10.5555/2694476.2694509,
author = {Das, Ariyam and Ranganath, Harish},
title = {Making online BigData small: reducing computation cost and latency in web analytics through sampling},
year = {2013},
publisher = {Computer Society of India},
address = {Mumbai, Maharashtra, IND},
abstract = {In the era of big data, the volume, velocity and variety of data are exploding at an unprecedented pace. With the explosion of data at the web, internet companies are working towards building powerful data analytics system that can crunch the big user data available to them and offer rich business insights. However, generating analytical reports and insights from high dimensional online user data involves significant computation cost. In addition, enterprises are now also looking for low latency analytics systems to dramatically accelerate the time from data to decision by minimizing the delay between the transaction and decision. In this work, we attempt to create smaller samples from the actual online big user data, so that analytical reports can be derived from the sample data faster and at a reduced computation cost without significant trade-offs in precision and accuracy. This study empirically analyzes petabytes of traffic data across Yahoo sites and develops efficient sampling and metric computation mechanisms for large scale web traffic. We generated analytical reports containing traffic and user engagement metrics from both sampled and actual Yahoo web data and compared them in terms of latency, computation cost and accuracy to show the effectiveness of our approach.},
booktitle = {Proceedings of the 19th International Conference on Management of Data},
pages = {139–142},
numpages = {4},
keywords = {web analytics, non-additive metrics, high dimensional data, big data, additive metrics},
location = {Ahmedabad, India},
series = {COMAD '13}
}

@inproceedings{10.1145/2009916.2009989,
author = {Wang, Dayong and Hoi, Steven C.H. and He, Ying},
title = {Mining weakly labeled web facial images for search-based face annotation},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2009989},
doi = {10.1145/2009916.2009989},
abstract = {In this paper, we investigate a search-based face annotation framework by mining weakly labeled facial images that are freely available on the internet. A key component of such a search-based annotation paradigm is to build a database of facial images with accurate labels. This is however challenging since facial images on the WWW are often noisy and incomplete. To improve the label quality of raw web facial images, we propose an effective Unsupervised Label Refinement (ULR) approach for refining the labels of web facial images by exploring machine learning techniques. We develop effective optimization algorithms to solve the large-scale learning tasks efficiently, and conduct an extensive empirical study on a web facial image database with 400 persons and 40,000 web facial images. Encouraging results showed that the proposed ULR technique can significantly boost the performance of the promising search-based face annotation scheme.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {535–544},
numpages = {10},
keywords = {web facial images, unsupervised learning, auto face annotation},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1145/1809018.1809024,
author = {Csorba, Mate J. and Meling, Hein and Heegaard, Poul E.},
title = {Ant system for service deployment in private and public clouds},
year = {2010},
isbn = {9781450300865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809018.1809024},
doi = {10.1145/1809018.1809024},
abstract = {Large-scale computing platforms that serve thousands or even millions of users through the Internet are on a path to become a pervasive technology available to companies of all sizes. However, existing technologies to enable this kind of scaling are based on a hierarchically managed approach that does not scale equally well. Moreover, existing systems are also not equipped to handle the dynamism that may emerge as a result of severe failures or load surges.In this paper, we conjecture that using self-organizing techniques for system (re)configuration can improve both the scalability properties of such systems as well as their ability to tolerate churn. Specifically, the paper focuses on deployment of virtual machine images onto physical machines that reside in different parts of the network. The objective is to construct balanced and dependable deployment configurations that are resilient. To accomplish this, a method based on a variant of Ant Colony Optimization is used to find efficient deployment mappings for a large number of virtual machine image replicas that are deployed concurrently. The method is completely decentralized; ants communicate indirectly through pheromone tables located in the nodes.An example scenario is presented and simulation results are obtained for the method.},
booktitle = {Proceedings of the 2nd Workshop on Bio-Inspired Algorithms for Distributed Systems},
pages = {19–28},
numpages = {10},
keywords = {resource discovery, deployment, cross-entropy ant system, cloud computing},
location = {Washington, DC, USA},
series = {BADS '10}
}

@inproceedings{10.1145/2508075.2508465,
author = {Schmidt, Douglas C. and McCormick, Zach},
title = {Producing and delivering a coursera MOOC on pattern-oriented software architecture for concurrent and networked software},
year = {2013},
isbn = {9781450319959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508075.2508465},
doi = {10.1145/2508075.2508465},
abstract = {A massive open online course (MOOC) is a web-based class environment aimed at large-scale global participation and open access via the Internet. MOOCs are also a disruptive trend changing how education is delivered and funded throughout the world. In the spring of 2013, we developed and taught Vanderbilt's first MOOC, entitled "Pattern-Oriented Software Architecture for Concurrent and Networked Software" (known as the POSA MOOC). This ten-week MOOC was an amalgamation of several courses on software design and programming taken by ~600 undergraduate and graduate students at Vanderbilt during the past decade. Enrollment in our POSA MOOC was more than 50 times (31,000+) that number, consisting of students with a wide range of backgrounds, interests, and expertise from scores of countries around the world.This paper describes our experiences producing and delivering the POSA MOOC. Where possible, we ground our observations in data from statistics collected via Coursera, which was the delivery platform we used for the POSA MOOC. We also discuss the broader implications of MOOCs on life-long learning and the role they play in improving the quality and productivity of software professionals in academia and industry.},
booktitle = {Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, &amp; Applications: Software for Humanity},
pages = {167–176},
numpages = {10},
keywords = {software patterns and frameworks, object-oriented design and programming, moocs, coursera},
location = {Indianapolis, Indiana, USA},
series = {SPLASH '13}
}

@inproceedings{10.1145/1807128.1807132,
author = {Wentzlaff, David and Gruenwald, Charles and Beckmann, Nathan and Modzelewski, Kevin and Belay, Adam and Youseff, Lamia and Miller, Jason and Agarwal, Anant},
title = {An operating system for multicore and clouds: mechanisms and implementation},
year = {2010},
isbn = {9781450300360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807128.1807132},
doi = {10.1145/1807128.1807132},
abstract = {Cloud computers and multicore processors are two emerging classes of computational hardware that have the potential to provide unprecedented compute capacity to the average user. In order for the user to effectively harness all of this computational power, operating systems (OSes) for these new hardware platforms are needed. Existing multicore operating systems do not scale to large numbers of cores, and do not support clouds. Consequently, current day cloud systems push much complexity onto the user, requiring the user to manage individual Virtual Machines (VMs) and deal with many system-level concerns. In this work we describe the mechanisms and implementation of a factored operating system named fos. fos is a single system image operating system across both multicore and Infrastructure as a Service (IaaS) cloud systems. fos tackles OS scalability challenges by factoring the OS into its component system services. Each system service is further factored into a collection of Internet-inspired servers which communicate via messaging. Although designed in a manner similar to distributed Internet services, OS services instead provide traditional kernel services such as file systems, scheduling, memory management, and access to hardware. fos also implements new classes of OS services like fault tolerance and demand elasticity. In this work, we describe our working fos implementation, and provide early performance measurements of fos for both intra-machine and inter-machine operations.},
booktitle = {Proceedings of the 1st ACM Symposium on Cloud Computing},
pages = {3–14},
numpages = {12},
keywords = {single system image, scalable operating systems, multicores, cloud computing},
location = {Indianapolis, Indiana, USA},
series = {SoCC '10}
}

@inproceedings{10.1109/ASE.2013.6693074,
author = {Cho, Chia Yuan and D'Silva, Vijay and Song, Dawn},
title = {BLITZ: compositional bounded model checking for real-world programs},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693074},
doi = {10.1109/ASE.2013.6693074},
abstract = {Bounded Model Checking (BMC) for software is a precise bug-finding technique that builds upon the efficiency of modern SAT and SMT solvers. BMC currently does not scale to large programs because the size of the generated formulae exceeds the capacity of existing solvers. We present a new, compositional and property-sensitive algorithm that enables BMC to automatically find bugs in large programs. A novel feature of our technique is to decompose the behaviour of a program into a sequence of BMC instances and use a combination of satisfying assignments and unsatisfiability proofs to propagate information across instances. A second novelty is to use the control - and data-flow of the program as well as information from proofs to prune the set of variables and procedures considered and hence, generate smaller instances. Our tool BLITZ outperforms existing tools and scales to programs with over 100,000 lines of code. BLITZ automatically and efficiently discovers bugs in widely deployed software including new vulnerabilities in Internet infrastructure software.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {136–146},
numpages = {11},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1145/2508859.2516674,
author = {Acar, Gunes and Juarez, Marc and Nikiforakis, Nick and Diaz, Claudia and G\"{u}rses, Seda and Piessens, Frank and Preneel, Bart},
title = {FPDetective: dusting the web for fingerprinters},
year = {2013},
isbn = {9781450324779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508859.2516674},
doi = {10.1145/2508859.2516674},
abstract = {In the modern web, the browser has emerged as the vehicle of choice, which users are to trust, customize, and use, to access a wealth of information and online services. However, recent studies show that the browser can also be used to invisibly fingerprint the user: a practice that may have serious privacy and security implications.In this paper, we report on the design, implementation and deployment of FPDetective, a framework for the detection and analysis of web-based fingerprinters. Instead of relying on information about known fingerprinters or third-party-tracking blacklists, FPDetective focuses on the detection of the fingerprinting itself. By applying our framework with a focus on font detection practices, we were able to conduct a large scale analysis of the million most popular websites of the Internet, and discovered that the adoption of fingerprinting is much higher than previous studies had estimated. Moreover, we analyze two countermeasures that have been proposed to defend against fingerprinting and find weaknesses in them that might be exploited to bypass their protection. Finally, based on our findings, we discuss the current understanding of fingerprinting and how it is related to Personally Identifiable Information, showing that there needs to be a change in the way users, companies and legislators engage with fingerprinting.},
booktitle = {Proceedings of the 2013 ACM SIGSAC Conference on Computer &amp; Communications Security},
pages = {1129–1140},
numpages = {12},
keywords = {web security, tracking, privacy, javascript, flash, dynamic analysis, device fingerprinting},
location = {Berlin, Germany},
series = {CCS '13}
}

@inproceedings{10.1145/2502081.2502210,
author = {Bhattacharya, Subhabrata},
title = {Recognition of complex events in open-source web-scale videos: a bottom up approach},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502210},
doi = {10.1145/2502081.2502210},
abstract = {Recognition of complex events in unconstrained Internet videos is a challenging research problem. In this symposium proposal, we present a systematic decomposition of complex events into hierarchical components and make an in-depth analysis of how existing research are being used to cater to various levels of this hierarchy. We also identify three key stages where we make novel contributions which are necessary to not only improve the overall recognition performance, but also develop richer understanding of these events. At the lowest level, our contributions include (a) compact covariance descriptors of appearance and motion features used in sparse coding framework to recognize realistic actions and gestures, and (b) a Lie-algebra based representation of dominant camera motion present in video shots which can be used as a complementary feature for video analysis. In the next level, we propose an (c) efficient maximum likelihood estimate based representation from low-level features computed from videos which demonstrates state of the art performance in large scale visual concept detection, and finally, we propose to (d) model temporal interactions between concepts detected in video shots through two new discriminative feature spaces derived from Linear dynamical systems which eventually boosts event recognition performance. In all cases, we conduct thorough experiments to demonstrate promising performance gains over some of the prominent approaches.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1035–1038},
numpages = {4},
keywords = {video descriptors, shot classification, riemannian manifolds, multimedia event detection, maximum likelihood estimates, linear dynamical systems, lie algebra, covariance matrices, complex event recognition, cinematographic techniques, block hankel matrices},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/1930286.1930291,
author = {Alim, M. Abdul and Griffin, Timothy G.},
title = {Hybrid link-state, path-vector routing},
year = {2010},
isbn = {9781450304016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1930286.1930291},
doi = {10.1145/1930286.1930291},
abstract = {Traffic engineering in Internet backbones can be improved by off-line optimization algorithms that automatically configure link weights used by routing protocols. However, with existing protocols large networks often require some type of partitioning in order to scale the routing protocol, and these partitions actually complicate the metrics to the extent that link-weight optimization is no longer practical. In this paper we study how an algebraic specification of a path problem can be naturally decomposed into simpler subproblems where each sub-problem can then be solved independently without changing the global metric being used network-wide. In addition, we go on to study four possible combinations of link-state and distance-vector mechanisms in this setting. In particular, we attempt to clarify the tradeoffs between fast convergence of link-state and low space requirements of distance-vector. The results provide a framework for analyzing existing mechanisms and for designing more reliable and robust routing protocols.},
booktitle = {Proceedings of the 6th Asian Internet Engineering Conference},
pages = {32–39},
numpages = {8},
keywords = {routing protocols, path-vector, link-state, algebraic path problem},
location = {Bangkok, Thailand},
series = {AINTEC '10}
}

@article{10.1145/2160803.2160862,
author = {Liu, Zhenhua and Lin, Minghong and Wierman, Adam and Low, Steven H. and Andrew, Lachlan L.H.},
title = {Geographical load balancing with renewables},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/2160803.2160862},
doi = {10.1145/2160803.2160862},
abstract = {Given the significant energy consumption of data centers, improving their energy efficiency is an important social problem. However, energy efficiency is necessary but not sufficient for sustainability, which demands reduced usage of energy from fossil fuels. This paper investigates the feasibility of powering internet-scale systems using (nearly) entirely renewable energy. We perform a trace-based study to evaluate three issues related to achieving this goal: the impact of geographical load balancing, the role of storage, and the optimal mix of renewables. Our results highlight that geographical load balancing can significantly reduce the required capacity of renewable energy by using the energy more efficiently with "follow the renewables" routing. Further, our results show that small-scale storage can be useful, especially in combination with geographical load balancing, and that an optimal mix of renewables includes significantly more wind than photovoltaic solar.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {dec},
pages = {62–66},
numpages = {5}
}

@inproceedings{10.5555/2133429.2133570,
author = {Chuang, Yi-Lin and Nam, Gi-Joon and Alpert, Charles J. and Chang, Yao-Wen and Roy, Jarrod and Viswanathan, Natarajan},
title = {Design-hierarchy aware mixed-size placement for routability optimization},
year = {2010},
isbn = {9781424481927},
publisher = {IEEE Press},
abstract = {Routability is a mandatory metric for modern large-scale mixed-size circuit placement which typically needs to handle hundreds of large macros and millions of small standard cells. However, most existing academic mixed-size placers either focus on wirelength minimization alone, or do not consider the impact of movable macros on routing. To remedy these insufficiencies, this paper formulates design-hierarchy information as a novel fence force in an analytical placement framework. Unlike a state-of-the-art routability-driven placer that simply removes net bounding boxes during placement, this paper utilizes two different optimization forces, the global fence force and the local spreading force, to determine the positions of both standard cells and macros. We utilize design-hierarchy information to determine block distributions globally, and locally we add additional spreading forces to preserve sufficient free space among blocks by a net-topology estimation. With the interactions between these two forces, our placer can well balance routability and wirelength. Experimental results show that our placer can achieve the best routability and routing time among all published works.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
pages = {663–668},
numpages = {6},
location = {San Jose, California},
series = {ICCAD '10}
}

@inproceedings{10.1145/2491159.2491166,
author = {Zhan, Lei and Fu, Tom Z.J. and Chiu, Dah Ming and Lei, Zhibin},
title = {A framework for monitoring and measuring a large-scale distributed system in real time},
year = {2013},
isbn = {9781450321778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491159.2491166},
doi = {10.1145/2491159.2491166},
abstract = {Increasingly, Internet services are supported by a large-scale and distributed infrastructure on top of the Internet. This includes Content Delivery Networks (CDNs), Peer-to-peer (P2P) networks for content distribution, and more recently Cloud Computing built out of distributed data centers. Applications include Social networks (e.g. Facebook), content distribution and sharing (e.g. YouTube) and various location-based services and applications (e.g. Google Maps). To ensure proper operation and good QoE, these services need centralized monitoring and control of its operation. In this paper, we report our experience in building such a monitoring system. The key components include a reporting mechanism for capturing the system's state, and a visualization library to help administrator to keep track of how well the system is operating. We applied our monitoring system to support a P2P-assisted video streaming network for broadcasting the 2012 London Olympic Games in Hong Kong. We explain how our system keep track (in real time) of key performance numbers interested by the administrators, such as number of concurrent users, the average rate of contribution from different peers, and the QoE observed at different peer nodes, and illustrate various ways such information is visualized.},
booktitle = {Proceedings of the 5th ACM Workshop on HotPlanet},
pages = {21–26},
numpages = {6},
keywords = {visualization, real-time, monitoring, measurement, large-scale},
location = {Hong Kong, China},
series = {HotPlanet '13}
}

@article{10.14778/2732977.2732999,
author = {Gupta, Ashish and Yang, Fan and Govig, Jason and Kirsch, Adam and Chan, Kelvin and Lai, Kevin and Wu, Shuo and Dhoot, Sandeep Govind and Kumar, Abhilash Rajesh and Agiwal, Ankur and Bhansali, Sanjay and Hong, Mingsheng and Cameron, Jamie and Siddiqi, Masood and Jones, David and Shute, Jeff and Gubarev, Andrey and Venkataraman, Shivakumar and Agrawal, Divyakant},
title = {Mesa: geo-replicated, near real-time, scalable data warehousing},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732977.2732999},
doi = {10.14778/2732977.2732999},
abstract = {Mesa is a highly scalable analytic data warehousing system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy a complex and challenging set of user and systems requirements, including near real-time data ingestion and queryability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Specifically, Mesa handles petabytes of data, processes millions of row updates per second, and serves billions of queries that fetch trillions of rows per day. Mesa is geo-replicated across multiple datacenters and provides consistent and repeatable query answers at low latency, even when an entire datacenter fails. This paper presents the Mesa system and reports the performance and scale that it achieves.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1259–1270},
numpages = {12}
}

@inproceedings{10.1145/2489295.2489298,
author = {Badia, S\'{e}bastien and Carpen-Amarie, Alexandra and L\`{e}bre, Adrien and Nussbaum, Lucas},
title = {Enabling large-scale testing of IaaS cloud platforms on the grid'5000 testbed},
year = {2013},
isbn = {9781450321624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2489295.2489298},
doi = {10.1145/2489295.2489298},
abstract = {Almost ten years after its premises, the Grid'5000 platform has become one of the most complete testbeds for designing or evaluating large-scale distributed systems. Initially dedicated to the study of High Performance Computing, the infrastructure has evolved to address wider concerns related to Desktop Computing, the Internet of Services and more recently the Cloud Computing paradigm. In this paper, we present the latest mechanisms we designed to enable the automated deployment of the major open-source IaaS cloudkits (i.e., Nimbus, OpenNebula, CloudStack, and OpenStack) on Grid'5000. Providing automatic, isolated and reproducible deployments of cloud environments lets end-users study and compare each solution or simply leverage one of them to perform higher-level cloud experiments (such as investigating Map/Reduce frameworks or applications).},
booktitle = {Proceedings of the 2013 International Workshop on Testing the Cloud},
pages = {7–12},
numpages = {6},
keywords = {virtualization, reproducibility, large-scale experiments, isolation, OpenStack, OpenNebula, Nimbus, IaaS, Grid'5000, Cloudstack, Cloud computing},
location = {Lugano, Switzerland},
series = {TTC 2013}
}

@inproceedings{10.1145/1989323.1989441,
author = {Silberstein, Adam E. and Sears, Russell and Zhou, Wenchao and Cooper, Brian Frank},
title = {A batch of PNUTS: experiences connecting cloud batch and serving systems},
year = {2011},
isbn = {9781450306614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989323.1989441},
doi = {10.1145/1989323.1989441},
abstract = {Cloud data management systems are growing in prominence, particularly at large Internet companies like Google, Yahoo!, and Amazon, which prize them for their scalability and elasticity. Each of these systems trades off between low-latency serving performance and batch processing throughput. In this paper, we discuss our experience running batch-oriented Hadoop on top of Yahoo's serving-oriented PNUTS system instead of the standard HDFS file system. Though PNUTS is optimized for and primarily used for serving, a number of applications at Yahoo! must run batch-oriented jobs that read or write data that is stored in PNUTS.Combining these systems reveals several key areas where the fundamental properties of each system are mismatched. We discuss our approaches to accommodating these mismatches, by either bending the batch and serving abstractions, or inventing new ones. Batch systems like Hadoop provide coarse task-level recovery, while serving systems like PNUTS provide finer record or transaction-level recovery. We combine both types to log record-level errors, while detecting and recovering from large-scale errors. Batch systems optimize for read and write throughput of large requests, while serving systems use indexing to provide low latency access to individual records. To improve latency-insensitive write throughput to PNUTS, we introduce a batch write path. The systems provide conflicting consistency models, and we discuss techniques to isolate them from one another.},
booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
pages = {1101–1112},
numpages = {12},
keywords = {serving, hybrid, hadoop, bulk load, batch, PNUTS},
location = {Athens, Greece},
series = {SIGMOD '11}
}

@inproceedings{10.1145/2379690.2379700,
author = {Inoue, Daisuke and Eto, Masashi and Suzuki, Koei and Suzuki, Mio and Nakao, Koji},
title = {DAEDALUS-VIZ: novel real-time 3D visualization for darknet monitoring-based alert system},
year = {2012},
isbn = {9781450314138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379690.2379700},
doi = {10.1145/2379690.2379700},
abstract = {A darknet is a set of unused IP addresses whose monitoring is an effective way of detecting malicious activities on the Internet. We have developed an alert system called DAEDALUS (direct alert environment for darknet and livenet unified security), which is based on large-scale darknet monitoring. This paper presents a novel real-time 3D visualization engine called DAEDALUS-VIZ that enables operators to grasp visually and in real time a complete overview of alert circumstances and provides highly flexible and tangible interactivity. We describe some case studies and evaluate the performance of DAEDALUS-VIZ.},
booktitle = {Proceedings of the Ninth International Symposium on Visualization for Cyber Security},
pages = {72–79},
numpages = {8},
keywords = {real-time visualization, darknet monitoring, alert system},
location = {Seattle, Washington, USA},
series = {VizSec '12}
}

@inproceedings{10.1109/UCC.2014.72,
author = {Baralis, Elena and Cagliero, Luca and Cerquitelli, Tania and Chiusano, Silvia and Garza, Paolo and Grimaudo, Luigi and Pulvirenti, Fabio},
title = {NEMICO: Mining Network Data through Cloud-Based Data Mining Techniques},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.72},
doi = {10.1109/UCC.2014.72},
abstract = {Thanks to the rapid advances in Internet-based applications, data acquisition and storage technologies, petabyte-sized network data collections are becoming more and more common, thus prompting the need for scalable data analysis solutions. By leveraging today's ubiquitous many-core computer architectures and the increasingly popular cloud computing paradigm, the applicability of data mining algorithms to these large volumes of network data can be scaled up to gain interesting insights. This paper proposes NEMICO, a comprehensive Big Data mining system targeted to network traffic flow analyses (e.g., Traffic flow characterization, anomaly detection, multiple-level pattern mining). NEMICO comprises new approaches that contribute to a paradigm-shift in distributed data mining by addressing most challenging issues related to Big Data, such as data sparsity, horizontal scaling, and parallel computation.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {503–504},
numpages = {2},
keywords = {Data mining, Cloud Computing},
series = {UCC '14}
}

@inproceedings{10.1145/1951365.1951432,
author = {Agrawal, Divyakant and Das, Sudipto and El Abbadi, Amr},
title = {Big data and cloud computing: current state and future opportunities},
year = {2011},
isbn = {9781450305280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1951365.1951432},
doi = {10.1145/1951365.1951432},
abstract = {Scalable database management systems (DBMS)---both for update intensive application workloads as well as decision support systems for descriptive and deep analytics---are a critical part of the cloud infrastructure and play an important role in ensuring the smooth transition of applications from the traditional enterprise infrastructures to next generation cloud infrastructures. Though scalable data management has been a vision for more than three decades and much research has focussed on large scale data management in traditional enterprise setting, cloud computing brings its own set of novel challenges that must be addressed to ensure the success of data management solutions in the cloud environment. This tutorial presents an organized picture of the challenges faced by application developers and DBMS designers in developing and deploying internet scale applications. Our background study encompasses both classes of systems: (i) for supporting update heavy applications, and (ii) for ad-hoc analytics and decision support. We then focus on providing an in-depth analysis of systems for supporting update intensive web-applications and provide a survey of the state-of-the-art in this domain. We crystallize the design choices made by some successful systems large scale database management systems, analyze the application demands and access patterns, and enumerate the desiderata for a cloud-bound DBMS.},
booktitle = {Proceedings of the 14th International Conference on Extending Database Technology},
pages = {530–533},
numpages = {4},
location = {Uppsala, Sweden},
series = {EDBT/ICDT '11}
}

@inproceedings{10.1145/1963405.1963409,
author = {Papadimitriou, Christos H.},
title = {Games, algorithms, and the Internet},
year = {2011},
isbn = {9781450306324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963405.1963409},
doi = {10.1145/1963405.1963409},
abstract = {The advent of the Internet brought parallel paradigm shifts to both Economics and Computer Science. Computer scientists realized that large-scale performing systems can emerge from the interaction of selfish agents and that incentives are a quintessential part of a good system design. And economists saw that the default platforms of economic transactions are computational and interconnected. Algorithmic Game Theory is a subdiscipline that emerged from this turmoil, revisiting some of the most important problems in Economics and Game Theory from a computational and network perspective. This talk will survey some of the major themes, results and challenges in this field.},
booktitle = {Proceedings of the 20th International Conference on World Wide Web},
pages = {5–6},
numpages = {2},
keywords = {www 2011 keynote talk},
location = {Hyderabad, India},
series = {WWW '11}
}

@article{10.1145/2168260.2168267,
author = {Popescu, Razvan and Staikopoulos, Athanasios and Brogi, Antonio and Liu, Peng and Clarke, Siobh\'{a}n},
title = {A formalized, taxonomy-driven approach to cross-layer application adaptation},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1556-4665},
url = {https://doi.org/10.1145/2168260.2168267},
doi = {10.1145/2168260.2168267},
abstract = {Advances in pervasive technology have made it possible to consider large-scale application types that potentially span heterogeneous organizations, technologies, and device types. This class of application will have a multilayer architecture, where each layer is likely to use languages and technologies appropriate to its own concerns. An example application is a geographically large-scale crisis management system. Typically, such applications are required to dynamically adapt their behavior based on current circumstances, with adaptations potentially affecting all layers of the application. The complexities involved in dynamically adapting multilayer applications will significantly benefit from formal approaches to its specification.This article presents a new methodology for flexible, multilayer application adaptation, with layer-specific adaptation solution templates bound to application mismatches that are organized into hierarchical taxonomies. Templates can be linked either through direct invocations or through adaptation events, supporting flexible cross-layer adaptation. The methodology illustrates the use of different formalisms for different elements of its specification. In particular, we combine semiformal metamodeling techniques for the system model specification with formal Petri nets, which are used to capture template matchmaking using reachability analysis. This work demonstrates how existing formalisms can be used for the specification of a generic adaptation model for pervasive applications.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {may},
articleno = {7},
numpages = {30},
keywords = {taxonomies of application mismatches, multilayer applications, matchmaking, context-aware systems, adaptation templates, Petri nets, Cross-layer adaptation}
}

@inproceedings{10.1145/2632951.2633209,
author = {Barros, Jo\~{a}o},
title = {How to build vehicular networks in the real world},
year = {2014},
isbn = {9781450326209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2632951.2633209},
doi = {10.1145/2632951.2633209},
abstract = {There are now 1 billion vehicles in the world waiting to be connected to the Internet. At the same time, vehicular communication technologies have matured to a point in which massive deployment is both possible and feasible. One option for deployment is to wait for car manufacturers to embed DSRC/WAVE interfaces inside their latest models. However, since only 9% of the world's fleet is new every year, this would result in a time span of up to 20 years until 90% of the vehicles are finally connected. Another option is to rely entirely on cellular communications, such as GPRS, EDGE, 3G and LTE. This cellular only approach is impractical due to the capital expenses required for telecom operators to meet the demands of the impending tsunami of mobile data (expected to grow 1800% until 2016). Clearly, there is need for a low-cost wireless networking solution that can be placed in any vehicle and offers reliable connectivity, improved quality of experience and higher safety for drivers and passengers. This solution, we will argue, is vehicular mesh networking.Drawing from five years of research and our experience with a large testbed with hundreds of taxis and buses, currently under deployment in Porto, Portugal, we will address how city-scale deployment can be achieved and what kind of connectivity, bandwidth, quality of service and application support can be provided as the density of Internet gateways and the density of connected vehicles grows towards widespread deployment. Some attention will also be given to relevant issues in system design such as channel modeling, mobility patterns, networking protocols and large-scale simulation with manageable complexity. Finally, we will show how a vehicular mesh network can be used as a highly dense urban scanner, producing real-time data on-the-move, which can be leveraged from the cloud to help manage future cities, protect our environment and improve our quality of life.},
booktitle = {Proceedings of the 15th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {123–124},
numpages = {2},
keywords = {vehicular networks, network protocols, intelligent transportation systems},
location = {Philadelphia, Pennsylvania, USA},
series = {MobiHoc '14}
}

@inproceedings{10.1145/1871840.1871850,
author = {Murakami, Koji and Nichols, Eric and Mizuno, Junta and Watanabe, Yotaro and Masuda, Shouko and Goto, Hayato and Ohki, Megumi and Sao, Chitose and Matsuyoshi, Suguru and Inui, Kentaro and Matsumoto, Yuji},
title = {Statement map: reducing web information credibility noise through opinion classification},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871850},
doi = {10.1145/1871840.1871850},
abstract = {On the Internet, users often encounter noise in the form of spelling errors or unknown words, however, dishonest, unreliable, or biased information also acts as noise that makes it difficult to find credible sources of information. As people come to rely on the Internet for more and more information, reducing this credibility noise grows ever more urgent. The STATEMENT MAP project's goal is to help Internet users evaluate the credibility of information sources by mining the Web for a variety of viewpoints on their topics of interest and presenting them to users together with supporting evidence in a way that makes it clear how they are related.In this paper, we show how a STATEMENT MAP system can be constructed by combining Information Retrieval (IR) and Natural Language Processing (NLP) technologies, focusing on the task of organizing statements retrieved from the Web by viewpoints. We frame this as a semantic relation classification task, and identify 4 semantic relations: [AGREEMENT], [CONFLICT], [CONFINEMENT], and [EVIDENCE]. The former two relations are identified by measuring semantic similarity through sentence alignment, while the latter two are identified through sentence-internal discourse processing. As a prelude to end-to-end user evaluation of STATEMENT MAP, we present a large-scale evaluation of semantic relation classification between user queries and Internet texts in Japanese and conduct detailed error analysis to identify the remaining areas of improvement.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {59–66},
numpages = {8},
keywords = {structural alignment, semantic relation classification, opinion classification, discourse processing, credibility analysis, STATEMENT MAP},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/2576768.2598269,
author = {Ludwig, Simone A.},
title = {MapReduce-based optimization of overlay networks using particle swarm optimization},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598269},
doi = {10.1145/2576768.2598269},
abstract = {An overlay network is a virtual network that is built on top of the real network such as the Internet. Cloud computing, peer-to-peer networks, and client-server applications are examples of overlay networks since their nodes run on top of the Internet. The major needs of overlay networks are content distribution and caching, file sharing, improved routing, multicast and streaming, ordered message delivery, and enhanced security and privacy. The focus of this paper is the optimization of overlay networks using a Particle Swarm Optimization (PSO) approach. However, since the ever growing need for more infrastructure causes the number of network nodes to grow significantly, the parallelization of the PSO approach becomes a necessity. In this paper, the MapReduce concept, proposed by Google, is adopted for the PSO approach in order to be able to optimize large-scale networks. MapReduce is easy to implement since it is based on the divide and conquer method, and implementation frameworks such has Hadoop allow for scalability and fault tolerance. Experiments of the MapReduce based PSO algorithm are performed to investigate the solution quality and scalability of the approach.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1031–1038},
numpages = {8},
keywords = {overlay network optimization, evolutionary computation},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/2095697.2095750,
author = {Ghallali, Mohamed and El Ouadghiri, Driss and Essaaidi, Mohammad and Boulmalf, Mohamed},
title = {Mobile phones security: the spread of malware via MMS and Bluetooth, prevention methods},
year = {2011},
isbn = {9781450307857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095697.2095750},
doi = {10.1145/2095697.2095750},
abstract = {The work presented in this paper describes the evolution impact of mobile devices operating system's features through its increasing use for internet navigation. And thus, the growing risks of getting contaminated by hostile programs, which exploit the vulnerabilities of these systems and spreading to a large scale via services such as Bluetooth and MMS.Following the description of the spreading and infection methods of these equipments via Bluetooth and MMS, two solutions from the industrial and academic world are exposed. Moreover, a solution based on the use of telecom provider's online services to scan and disinfect these mobile devices is proposed at the end.},
booktitle = {Proceedings of the 9th International Conference on Advances in Mobile Computing and Multimedia},
pages = {256–259},
numpages = {4},
keywords = {state of the art in mobile phones, mobile phones security malwares, MMS infection, Bluetooth infection},
location = {Ho Chi Minh City, Vietnam},
series = {MoMM '11}
}

@inproceedings{10.1145/2554850.2555168,
author = {Xavier, Miguel G. and De Oliveira, Israel C. and Dos Passos, Robson D. and De Rose, Cesar A. F.},
title = {Towards better manageability of database clusters on cloud computing platforms},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2555168},
doi = {10.1145/2554850.2555168},
abstract = {Cloud computing (CC) has become a very popular computing model in the last decade, as it has proved to be a cost-effective alternative to large-scale computers hosted in conventional Information Technologies (IT) department. The ability to provide on-demand IT-related resources either through the internet (public clouds) or private networks (private clouds) based on customer's needs, and the scalability they have to deliver resources horizontally, can be considered the main reasons for such popularity and have led such a CC platforms to a scenario involving the hosting of other types of systems, such as those that control large volume of data, like database management systems (DBMS).},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {366–367},
numpages = {2},
keywords = {virtualization, performance, manageability, database clusters, cloud computing},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.1109/PADS.2012.19,
author = {Coudert, David and Hogie, Luc and Lancin, Aurelien and Papadimitriou, Dimitri and Perennes, Stephane and Tahiri, Issam},
title = {Feasibility Study on Distributed Simulations of BGP},
year = {2012},
isbn = {9780769547145},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PADS.2012.19},
doi = {10.1109/PADS.2012.19},
abstract = {The Autonomous System (AS) topology of the Internet (up to 61k ASs) is growing at a rate of about 10% per year. The Border Gateway Protocol (BGP) starts to show its limits in terms of the number of routing table entries it can dynamically process and control. Due to the increasing routing information processing and storage, the same trend is observed for routing model simulators such as DRMSim specialized in large-scale simulations of routing models. Therefore, DRMSim needs enhancements to support the current size of the Internet topology and its evolution (up to 100k ASs). To this end, this paper proposes a feasibility study of the extension of DRMSim so as to support the Distributed Parallel Discrete Event paradigm. We first detail the possible distribution models and their associated communication overhead. Then, we analyze this overhead by executing BGP on a partitioned topology according to different scenarios. Finally, we conclude on the feasibility of such a simulator by computing the expected additional time required by a distributed simulation of BGP compared to its sequential simulation.},
booktitle = {Proceedings of the 2012 ACM/IEEE/SCS 26th Workshop on Principles of Advanced and Distributed Simulation},
pages = {96–98},
numpages = {3},
keywords = {network, distributed simulation, Internet, BGP},
series = {PADS '12}
}

@inproceedings{10.1145/2525194.2525265,
author = {Sachdeva, Niharika and Saxena, Nitesh and Kumaraguru, Ponnurangam},
title = {On the viability of CAPTCHAs for use in telephony systems: a usability field study},
year = {2013},
isbn = {9781450322539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525194.2525265},
doi = {10.1145/2525194.2525265},
abstract = {Usability of security solution has always been a keen area of interest for researchers. CAPTCHA is one such security solution which presents various usability challenges for users. However, it has successfully reduced the abuse of the Internet resources, such as spam. Similar to the Internet, audio-based CAPTCHAs have been proposed as a solution to curb voice spam over telephony. Voice spam is often encountered on telephony in various forms, such as, an automated telemarketing call asking to call a number to win million of dollars. A large percentage of voice spam is generated through automated system which introduces the classical challenge of distinguishing machines from humans on the telephony. We present a large scale evaluation of audio CAPTCHA from the human perspective over telephony through a field study with 90 participants. We study two primary research questions: how much inconvenience does audio CAPTCHA causes to users on telephony, and how different features of the CAPTCHA, e.g., duration and size influence usability of audio CAPTCHA on telephony. We found that captcha could be a viable solution for telephony with improved features, such as better voice and accent. We found that users were relatively close to the expected correct answers, which does suggest the possibility of deploying audio captcha on telephony platforms in the future. However, we did not find strong influence of captcha size and duration on solving accuracy.},
booktitle = {Proceedings of the 11th Asia Pacific Conference on Computer Human Interaction},
pages = {178–182},
numpages = {5},
keywords = {usability, human-mobile interaction, CAPTCHA},
location = {Bangalore, India},
series = {APCHI '13}
}

@inproceedings{10.5555/2330748.2330755,
author = {Wu, Shining and Chen, Yang and Fu, Xiaoming and Li, Jun},
title = {NCShield: securing decentralized, matrix factorization-based network coordinate systems},
year = {2012},
isbn = {9781467312981},
publisher = {IEEE Press},
abstract = {While network coordinate (NC) systems provide scalable Internet distance estimation service and are useful for various Internet applications, decentralized, matrix factorization-based NC (MFNC) systems have received particular attention recently. They can serve large-scale distributed applications (as opposed to centralized NC systems) and do not need to assume triangle inequality (as opposed to Euclidean-based NC systems). However, because of their decentralized nature, MFNC systems are vulnerable to various malicious attacks.In this paper, we provide the first study on attacks toward MFNC systems, and propose a decentralized trust and reputation approach, called NCShield, to counter such attacks. Different from previous approaches, our approach is able to distinguish between legitimate distance variations and malicious distance alterations. Using four representative data sets from the Internet, we show that NCShield can defend against attacks with high accuracy. For example, when selecting node pairs with a shorter distance than a predefined threshold in an online game scenario, even if 30% nodes are malicious, NCShield can reduce the false positive rate from 45.5% to 3.7%.},
booktitle = {Proceedings of the 2012 IEEE 20th International Workshop on Quality of Service},
articleno = {7},
numpages = {9},
location = {Coimbra, Portugal},
series = {IWQoS '12}
}

@inproceedings{10.1145/2658260.2658267,
author = {Perino, Diego and Varvello, Matteo and Linguaglossa, Leonardo and Laufer, Rafael and Boislaigue, Roger},
title = {Caesar: a content router for high-speed forwarding on content names},
year = {2014},
isbn = {9781450328395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658260.2658267},
doi = {10.1145/2658260.2658267},
abstract = {Internet users are interested in content regardless of its location; however, the current client/server architecture still requires requests to be directed to a specific server. Information-centric networking (ICN) is a recent vein that relaxes this requirement through the use of name-based forwarding, where forwarding decisions are based on content names instead of IP addresses. Despite previous name-based forwarding strategies have been proposed, almost none have actually built a content router. To fill this gap, in this paper we design and prototype a content router called Caesar for high-speed forwarding on content names. Caesar introduces several innovative features, including (i) a longest-prefix matching algorithm based on a novel data structure called prefix Bloom filter; (ii) an incremental design which allows for easy integration with existing protocols and network equipment;(iii) a forwarding scheme where multiple line cards collaborate in a distributed fashion; and (iv) support for offloading packet processing to graphics processing units (GPUs). We build Caesar as an enterprise router, and show that every line card sustains up to 10 Gbps using a forwarding table with more than 10 million content prefixes. Distributed forwarding allows the forwarding table to grow even further, and to scale linearly with the number of line cards at the cost of only a few microseconds in the packet processing latency. GPU offloading, in turn, trades off a few milliseconds of latency for a large speedup in the forwarding rate.},
booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {137–148},
numpages = {12},
keywords = {router, icn, forwarding, architecture.},
location = {Los Angeles, California, USA},
series = {ANCS '14}
}

@inproceedings{10.1145/2068816.2068833,
author = {Bauer, Steven and Beverly, Robert and Berger, Arthur},
title = {Measuring the state of ECN readiness in servers, clients,and routers},
year = {2011},
isbn = {9781450310130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2068816.2068833},
doi = {10.1145/2068816.2068833},
abstract = {Better exposing congestion can improve traffic management in the wide-area, at peering points, among residential broadband connections, and in the data center. TCP's network utilization and efficiency depends on congestion information, while recent research proposes economic and policy models based on congestion. Such motivations have driven widespread support of Explicit Congestion Notification (ECN)in modern operating systems. We reappraise the Internet's ECN readiness, updating and extending previous measurements. Across large and diverse server populations, we find a three-fold increase in ECN support over prior studies. Using new methods, we characterize ECN within mobile infrastructure and at the client-side, populations previously unmeasured. Via large-scale path measurements, we find the ECN feedback loop failing in the core of the network 40% of the time, typically at AS boundaries. Finally, we discover new examples of infrastructure violating ECN Internet standards, and discuss remaining impediments to running ECN while suggesting mechanisms to aid adoption.},
booktitle = {Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference},
pages = {171–180},
numpages = {10},
keywords = {router-assisted congestion control, explicit congestion notification, ecn},
location = {Berlin, Germany},
series = {IMC '11}
}

@inproceedings{10.5555/1996039.1996075,
author = {Tan, Yongmin and Venkatesh, Vinay and Gu, Xiaohui},
title = {OLIC: online information compression for scalable hosting infrastructure monitoring},
year = {2011},
publisher = {IEEE Press},
abstract = {Quality-of-service (QoS) management often requires a continuous monitoring service to provide updated information about different hosts and network links in the managed system. However, it is a challenging task to achieve both scalability and precision for monitoring various intra-node and inter-node metrics (e.g., CPU, memory, disk, network delay) in a large-scale hosting infrastructure. In this paper, we present a novel OnLine Information Compression (OLIC) system to achieve scalable fine-grained hosting infrastructure monitoring. OLIC models continuous snapshots of a hosting infrastructure as a sequence of images and performs online monitoring data compression to significantly reduce the monitoring cost. We have implemented a prototype of the OLIC system and deployed it on the PlanetLab and NCSU's virtual computing lab (VCL). We have conducted extensive experiments using a set of real monitoring data from VCL, Planetlab, and a Google cluster as well as a real Internet traffic matrix trace. The experimental results show that OLIC can achieve much higher compression ratios with several orders of magnitude less overhead than previous approaches.},
booktitle = {Proceedings of the Nineteenth International Workshop on Quality of Service},
articleno = {32},
numpages = {9},
location = {San Jose, California},
series = {IWQoS '11}
}

@inproceedings{10.1145/2658861.2658947,
author = {Do, Ellen Yi-Luen},
title = {Creative design computing for happy healthy living},
year = {2014},
isbn = {9781450330350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658861.2658947},
doi = {10.1145/2658861.2658947},
abstract = {The age of ubiquitous/pervasive/ambient computing is upon us. We see more and more connected objects and devices embedded in everyday life. Design and Human-Computer Interaction are crucial components of information technologies that color our experience. As designers and technologists, we have the unique opportunity to imagine, design and create interesting, intelligent and interactive technologies for a smart living environment. A smart living environment is responsive, reconfigurable and transformable, embedded with sensors and actuators, to support everyday happy, healthy living with things that think, spaces that sense and places that play. Specifically, we see opportunity for investigating creative design computing to consider the built environment as an interface. We aim to engage people in playful, creative ways in which computing technologies embedded in the built environment (e.g., objects, furniture, building, and space) can support everyday happy healthy living. We build wellness technologies in different scales (e.g., in human-centered view: hand, body and environment), for various applications (e.g., to support emotional, physical, and intellectual wellness) to unlock human potential and augmenting human capabilities through digital design innovations. Opportunities exist for integrating multidisciplinary perspectives together to create innovation with impacts. For example, the Mobile Music Touch, a light-weight fingerless instrumented glove provides passive haptic learning of piano playing but also works as an effective rehabilitation tool. The ClockMe system, not only converts traditional pencil-and-paper Clock Drawing Test on a digital tablet for automatic scoring and analysis for detection of Alzheimer's disease and related disorders but also provides more information of the drawing behavior such as drawing sequence and pressure. The Digital Box and Block Test employs image processing to automatically detect and record a clinically validated post-stroke rehabilitation assessment for in-home use and a tangible gaming system to increase patient motivation. The Dodo game provides pictures of cute animals for color matching, and serves as a clinical screening test to detect color deficiency in young children. The Taste + bottle and spoon provide stimulation to alter the sense of taste without any chemical flavoring using electric pluses and color LED lights. It could encourage water drinking or enhance taste for people with restrictive diets or diminished taste sensations. The Sensorendipity is a real-time smartphone-based web-enabled sensor platform for designers to create sensor-based applications such as monitoring activities, in exergaming or safe driving. Now is an exciting time to engage in creative design computing, to implement physically and computationally enhanced environment, to explore experience media, to build prototypes, towards a smart living environment. Advancing technology offers new ways to solve problems, discover opportunities, and create new objects and experience that delight our senses and improve the way we live and work. Let's begin with the spark of creativity and enthusiasm and follow up with design and computational thinking towards the goal of creating unique technology for everyone.},
booktitle = {Proceedings of the Second International Conference on Human-Agent Interaction},
pages = {7–8},
numpages = {2},
keywords = {human computer interaction, happy healthy living, creative design computing},
location = {Tsukuba, Japan},
series = {HAI '14}
}

@inproceedings{10.1145/2063384.2063474,
author = {Tantisiriroj, Wittawat and Son, Seung Woo and Patil, Swapnil and Lang, Samuel J. and Gibson, Garth and Ross, Robert B.},
title = {On the duality of data-intensive file system design: reconciling HDFS and PVFS},
year = {2011},
isbn = {9781450307710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063384.2063474},
doi = {10.1145/2063384.2063474},
abstract = {Data-intensive applications fall into two computing styles: Internet services (cloud computing) or high-performance computing (HPC). In both categories, the underlying file system is a key component for scalable application performance. In this paper, we explore the similarities and differences between PVFS, a parallel file system used in HPC at large scale, and HDFS, the primary storage system used in cloud computing with Hadoop. We integrate PVFS into Hadoop and compare its performance to HDFS using a set of data-intensive computing benchmarks. We study how HDFS-specific optimizations can be matched using PVFS and how consistency, durability, and persistence tradeoffs made by these file systems affect application performance. We show how to embed multiple replicas into a PVFS file, including a mapping with a complete copy local to the writing client, to emulate HDFS's file layout policies. We also highlight implementation issues with HDFS's dependence on disk bandwidth and benefits from pipelined replication.},
booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {67},
numpages = {12},
keywords = {file systems, cloud computing, PVFS, Hadoop, HDFS},
location = {Seattle, Washington},
series = {SC '11}
}

@inproceedings{10.1145/1871437.1871477,
author = {Qian, Mingjie and Chen, Bo and Xu, Hongzhi and Qi, Hongwei},
title = {How about utilizing ordinal information from the distribution of unlabeled data},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871477},
doi = {10.1145/1871437.1871477},
abstract = {Problems of ordinal regression arise in many fields such as information retrieval, data mining and knowledge management. In this paper, we consider ordinal regression in a semi-supervised scenario, i.e., we try to utilize the ordinal information from the distribution of unlabeled data. Semi-supervised ordinal regression is more applicable than traditional supervised ordinal regression, because nowadays labeled data is expensive and time-consuming as it needs human labor, whereas a large amount of unlabeled data are far accessible with the development of internet technology. We construct a general semi-supervised ordinal regression framework to formulate this problem. Based on the framework, we then propose a semi-supervised ordinal regression method called Semi-supervised Ordinal SVM (SOSVM). Additionally, in order to make our proposed method more applicable to problems with large scaled labeled data, we put forward a kernel based dual coordinate descent algorithm to efficiently solve SOSVM. Both rigorous theoretical analysis and promising experimental evaluations on real world datasets show the great performance and remarkable efficiency of SOSVM.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {289–298},
numpages = {10},
keywords = {semi-supervised learning, ordinal semi-supervised svm, ordinal regression, dual coordinate descent algorithm},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}

@inproceedings{10.1145/2396761.2398671,
author = {Long, Bo and Bian, Jiang and Dong, Anlei and Chang, Yi},
title = {Enhancing product search by best-selling prediction in e-commerce},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398671},
doi = {10.1145/2396761.2398671},
abstract = {With the rapid growth of E-Commerce on the Internet, online product search service has emerged as a popular and effective paradigm for customers to find desired products and select transactions. Most product search engines today are based on adaptations of relevance models devised for information retrieval. However, there is still a big gap between the mechanism of finding products that customers really desire to purchase and that of retrieving products of high relevance to customers' query. In this paper, we address this problem by proposing a new ranking framework for enhancing product search based on dynamic best-selling prediction in E-Commerce. Specifically, we first develop an effective algorithm to predict the dynamic best-selling, i.e. the volume of sales, for each product item based on its transaction history. By incorporating such best-selling prediction with relevance, we propose a new ranking model for product search, in which we rank higher the product items that are not only relevant to the customer's need but with higher probability to be purchased by the customer. Results of a large scale evaluation, conducted over the dataset from a commercial product search engine, demonstrate that our new ranking method is more effective for locating those product items that customers really desire to buy at higher rank positions without hurting the search relevance.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2479–2482},
numpages = {4},
keywords = {transaction history, product search, e-commerce, best selling prediction},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1145/2328909.2328940,
author = {Holvoet, Tom and Weyns, Danny and Valckenaers, Paul},
title = {Delegate MAS patterns for large-scale distributed coordination and control applications},
year = {2010},
isbn = {9781450302593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2328909.2328940},
doi = {10.1145/2328909.2328940},
abstract = {The patterns in this document are particularly intended for researchers as well as practitioners who study and develop large-scale decentralized systems - including decentralized control systems, internet applications. The reader should be familiar with typical issues and challenges in developing distributed systems, and be acquainted with elementary terminology of agents and multi-agent systems (MAS, [Wooldridge 2009]), and the discrete optimization metaheuristic called Ant Colony Optimization [Dorigo and Di Caro 1999].},
booktitle = {Proceedings of the 15th European Conference on Pattern Languages of Programs},
articleno = {25},
numpages = {16},
location = {Irsee, Germany},
series = {EuroPLoP '10}
}

@article{10.1145/2567561.2567571,
author = {Mellia, Marco},
title = {Feature interview with Antonio Nucci: chief technology officer of narus, winner of the "CTO of the year"},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0146-4833},
url = {https://doi.org/10.1145/2567561.2567571},
doi = {10.1145/2567561.2567571},
abstract = {Dr. Antonio Nucci is the chief technology officer of Narus and is responsible for setting the company's direction with respect to technology and innovation. He oversees the entire technology innovation lifecycle, including incubation, research, and prototyping. He also is responsible for ensuring a smooth transition to engineering for final commercialization. Antonio has published more than 100 technical papers and has been awarded 38 U.S. patents. He authored a book, "Design, Measurement and Management of Large-Scale IP Networks Bridging the Gap Between Theory and Practice", in 2009 on advanced network analytics. In 2007 he was recognized for his vision and contributions with the prestigious Infoworld CTO Top 25 Award. In 2013, Antonio was honored by InfoSecurity Products Guide's 2013 Global Excellence Awards as "CTO of the Year" and Gold winner in the "People Shaping Info Security" category. He served as a technical lead member of the Enduring Security Framework (ESF) initiative sponsored by various U.S. agencies to produce a set of recommendations, policies, and technology pilots to better secure the Internet (Integrated Network Defense). He is also a technical advisor for several venture capital firms.Antonio holds a Ph.D. in computer science, and master's and bachelor's degrees in electrical engineering from Politecnico di Torino, Italy.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {dec},
pages = {53–55},
numpages = {3},
keywords = {interview}
}

@inproceedings{10.1145/1871437.1871515,
author = {Anagnostopoulos, Aris and Becchetti, Luca and Castillo, Carlos and Gionis, Aristides and Leonardi, Stefano},
title = {Power in unity: forming teams in large-scale community systems},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871515},
doi = {10.1145/1871437.1871515},
abstract = {The internet has enabled the collaboration of groups at a scale that was unseen before. A key problem for large collaboration groups is to be able to allocate tasks effectively. An effective task assignment method should consider both how fit teams are for each job as well as how fair the assignment is to team members, in terms that no one should be overloaded or unfairly singled out. The assignment has to be done automatically or semi-automatically given that it is difficult and time-consuming to keep track of the skills and the workload of each person. Obviously the method to do this assignment must also be computationally efficient.In this paper we present a general framework for task assignment problems. We provide a formal treatment on how to represent teams and tasks. We propose alternative functions for measuring the fitness of a team performing a task and we discuss desirable properties of those functions. Then we focus on one class of task-assignment problems, we characterize the complexity of the problem, and we provide algorithms with provable approximation guarantees, as well as lower bounds. We also present experimental results that show that our methods are useful in practice in several application scenarios.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {599–608},
numpages = {10},
keywords = {team formation, task assignment, scheduling},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}

@inproceedings{10.1145/2684793.2684800,
author = {Lertsinsrubtavee, Adisorn and Mekbungwan, Preechai and Weshsuwannarugs, Nunthaphat},
title = {Comparing NDN and CDN Performance for Content Distribution Service in Community Wireless Mesh Network},
year = {2014},
isbn = {9781450332514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684793.2684800},
doi = {10.1145/2684793.2684800},
abstract = {Content distribution has recently become a predominant service on the current Internet while the early Internet architecture was not designed for scalable content delivery. In this paper, we address the issue of content delivery in community wireless mesh networks (CWMN) with narrow and unstable wireless links providing services to communities, where each community represents a small number of users as compared to the normal Internet. To provide high quality content such as high definition video content to these communities, we propose to attach cache storage to each router on the network and use these distributed storage to enhance the delivery of the content. Two different content delivery methods, designed to accommodate scalable content delivery on the Internet, are compared on CWMN by this study. One is a pull based distributed cache or Named Data Networking (NDN) and another is the push based, Content Distribution Network (CDN). In the former approach data chunks get cached along the path to the requesting user while for the latter approach contents are pushed as close to the users as possible a priori. In this paper, we present an experimental performance evaluation in a laboratory environment where the distribution of content and request patterns were based on the log files collected from real village CWMN in the rural area north of Thailand.},
booktitle = {Proceedings of the 10th Asian Internet Engineering Conference},
pages = {43–50},
numpages = {8},
keywords = {Named Data Network, Content Distribution Network, Community Wireless Mesh Network},
location = {Chiang Mai, Thailand},
series = {AINTEC '14}
}

@inproceedings{10.1145/2324796.2324855,
author = {Younessian, Ehsan and Mitamura, Teruko and Hauptmann, Alexander},
title = {Multimodal knowledge-based analysis in multimedia event detection},
year = {2012},
isbn = {9781450313292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2324796.2324855},
doi = {10.1145/2324796.2324855},
abstract = {Multimedia Event Detection (MED) is a multimedia retrieval task with the goal of finding videos of a particular event in a large-scale Internet video archive, given example videos and text descriptions. We focus on the multimodal knowledge-based analysis in MED where we utilize meaningful and semantic features such as Automatic Speech Recognition (ASR) transcripts, acoustic concept indexing (i.e. 42 acoustic concepts) and visual semantic indexing (i.e. 346 visual concepts) to characterize videos in archive. We study two scenarios where we either do or do not use the provided example videos. In the former, we propose a novel Adaptive Semantic Similarity (ASS) to measure textual similarity between ASR transcripts of videos. We also incorporate acoustic concept indexing and classification to retrieve test videos, specially with too few spoken words. In the latter 'ad-hoc' scenario where we do not have any example video, we use only the event kit description to retrieve test videos ASR transcripts and visual semantics. We also propose an event-specific fusion scheme to combine textual and visual retrieval outputs. Our results show the effectiveness of the proposed ASS and acoustic concept indexing methods and their complimentary role. We also conduct a set of experiments to assess the proposed framework for the 'ad-hoc' scenario.},
booktitle = {Proceedings of the 2nd ACM International Conference on Multimedia Retrieval},
articleno = {51},
numpages = {8},
keywords = {visual concept signature, multimedia retrieval, adaptive semantic similarity, acoustic concept indexing},
location = {Hong Kong, China},
series = {ICMR '12}
}

@article{10.1109/TNET.2010.2042726,
author = {Loiseau, Patrick and Gon\c{c}alves, Paulo and Dewaele, Guillaume and Borgnat, Pierre and Abry, Patrice and Primet, Pascale Vicat-Blanc},
title = {Investigating self-similarity and heavy-tailed distributions on a large-scale experimental facility},
year = {2010},
issue_date = {August 2010},
publisher = {IEEE Press},
volume = {18},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2010.2042726},
doi = {10.1109/TNET.2010.2042726},
abstract = {After the seminal work by Taqqu et al. relating self-similarity to heavy-tailed distributions, a number of research articles verified that aggregated Internet traffic time series show self-similarity and that Internet attributes, like Web file sizes and flow lengths, were heavy-tailed. However, the validation of the theoretical prediction relating self-similarity and heavy tails remains unsatisfactorily addressed, being investigated using either numerical or network simulations, or from uncontrolled Web traffic data. Notably, this prediction has never been conclusively verified on real networks using controlled and stationary scenarios, prescribing specific heavy-tailed distributions, and estimating confidence intervals. With this goal in mind, we use the potential and facilities offered by the large-scale, deeply reconfigurable and fully controllable experimental Grid5000 instrument, combined with state-of-the-art estimators, to investigate the prediction's observability on real networks. To this end, we organize a large number of controlled traffic circulation sessions on a nationwide real network involving 200 independent hosts. We use a FPGA-based measurement system to collect the corresponding traffic at packet level. We then estimate both the self-similarity exponent of the aggregated time series and the heavy-tail index of flow-size distributions, independently. Not only do our results complement and validate, with a striking accuracy, some conclusions drawn from a series of pioneering studies, but they also bring in new insights on the controversial role of certain components of real networks.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1261–1274},
numpages = {14},
keywords = {self-similarity, network traffic, monitoring, large-scale experiments, heavy-tailed distributions}
}

@inproceedings{10.1145/1859184.1859187,
author = {Canini, Marco and Novakovi\'{c}, Dejan and Jovanovi\'{c}, Vojin and Kosti\'{c}, Dejan},
title = {Fault prediction in distributed systems gone wild},
year = {2010},
isbn = {9781450304061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1859184.1859187},
doi = {10.1145/1859184.1859187},
abstract = {We consider the problem of predicting faults in deployed, large-scale distributed systems that are heterogeneous and federated. Motivated by the importance of ensuring reliability of the services these systems provide, we argue that the key step in making these systems reliable is the need to automatically predict faults. For example, doing so is vital for avoiding Internet-wide outages that occur due to programming errors or misconfigurations.},
booktitle = {Proceedings of the 4th International Workshop on Large Scale Distributed Systems and Middleware},
pages = {7–11},
numpages = {5},
keywords = {spatial and temporal awareness, shadow snapshot, heterogeneous systems, federated systems, fault prediction, BGP},
location = {Z\"{u}rich, Switzerland},
series = {LADIS '10}
}

@inproceedings{10.1145/2674095.2674098,
author = {Mand, Nowshad Painda and \"{O}berg, Johnny},
title = {Going for Brain-Scale Integration - using FPGAs, TSVs and NOC based Artificial Neural Networks: A Case Study},
year = {2014},
isbn = {9781450331302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674095.2674098},
doi = {10.1145/2674095.2674098},
abstract = {With better understanding of brain's massive parallel processing, brain-scale integration has been announced as one of the key research area in modern times and numerous efforts has been done to mimic such models. Multicore architectures, Network-On-Chip, 3D stacked ICs with TSVs, FPGA's growth beyond Moore's law and new design methodologies like high level synthesis will ultimately lead us toward single- and multi-chip solutions of Artificial Neural Net models comprising of millions or even more neurons per chip.Historically ANNs have been emulated as either software models, ASICs or a hybrid of both. Software models are very slow while ASICs based designs lacks plasticity. FPGA consumes a little more power but offer the flexibility of software and performance of ASICs along with basic requirement of plasticity in the form of reconfigurability. However, the traditional bottom up approach for building large ANN models is no more feasible and wiring along with memory becomes major bottlenecks when considering networks comprised of large number of neurons.The aim of this paper is to present a design space exploration of large-scale ANN models using a scalable NOC based architecture together with high level synthesis tools to explore the feasibility of implementing brain-scale ANNs on FPGAs using 3D stacked memory structures.},
booktitle = {Proceedings of the FPGA World Conference 2014},
articleno = {3},
numpages = {8},
keywords = {TSVs, NoC, FPGA, Brain-Scale Integration, ANN},
location = {Stockholm and Copenhagen, Sweden},
series = {FPGAWorld '14}
}

@article{10.14778/2212351.2212353,
author = {Metwally, Ahmed and Faloutsos, Christos},
title = {V-SMART-join: a scalable mapreduce framework for all-pair similarity joins of multisets and vectors},
year = {2012},
issue_date = {April 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/2212351.2212353},
doi = {10.14778/2212351.2212353},
abstract = {This work proposes V-SMART-Join, a scalable MapReduce-based framework for discovering all pairs of similar entities. The V-SMART-Join framework is applicable to sets, multisets, and vectors. V-SMART-Join is motivated by the observed skew in the underlying distributions of Internet traffic, and is a family of 2-stage algorithms, where the first stage computes and joins the partial results, and the second stage computes the similarity exactly for all candidate pairs. The V-SMART-Join algorithms are very efficient and scalable in the number of entities, as well as their cardinalities. They were up to 30 times faster than the state of the art algorithm, VCL, when compared on a real dataset of a small size. We also established the scalability of the proposed algorithms by running them on a dataset of a realistic size, on which VCL never succeeded to finish. Experiments were run using real datasets of IPs and cookies, where each IP is represented as a multiset of cookies, and the goal is to discover similar IPs to identify Internet proxies.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {704–715},
numpages = {12}
}

@inproceedings{10.1145/1989240.1989244,
author = {Shen, Zhijie and Zimmermann, Roger},
title = {LAN-awareness: improved P2P live streaming},
year = {2011},
isbn = {9781450307772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989240.1989244},
doi = {10.1145/1989240.1989244},
abstract = {The popularity of P2P streaming systems has rapidly created extensive, far-reaching Internet traffic. Recent studies have demonstrated that localizing cross-ISP (Internet service provider) traffic can mitigate this challenge. Another trend shows that households own an increasing number of devices, which are sharing a LAN of 2 or more peers. To this date, however, no study has investigated the potential of localizing traffic within LANs. In our presented work, we propose the concept of LAN-awareness and introduce its threefold benefits: 1) reducing Internet streaming traffic, 2) lowering stream server workload, and 3) improving streaming quality. First we conduct a large-scale measurement on PPLive, confirming that a considerable number of peers (up to 21%) are connected to the LANs having 2 or more peers. Recognizing the opportunity of localizing traffic within LANs, we discuss the principles to construct a LAN-aware overlay and propose a heuristic. The results of our trace-driven simulations confirm the benefits outlined above.},
booktitle = {Proceedings of the 21st International Workshop on Network and Operating Systems Support for Digital Audio and Video},
pages = {3–8},
numpages = {6},
keywords = {traffic locality, streaming, p2p, lan-aware},
location = {Vancouver, British Columbia, Canada},
series = {NOSSDAV '11}
}

@inproceedings{10.1145/2141622.2141679,
author = {Sarne-Fleischmann, Vardit and Tractinsky, Noam and Dwolatzky, Tzvi and Rief, Inbal},
title = {Personalized reminiscence therapy for patients with Alzheimer's disease using a computerized system},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141679},
doi = {10.1145/2141622.2141679},
abstract = {We present the development and initial evaluation of a personalized reminiscence program, which was developed specifically for use by patients and their caregivers in the treatment of mild to moderate Alzheimer's disease (AD). The system is part of a collaborative effort assessing the effects on patients with Alzheimer's disease of two non-pharmacological computer-based interventions, namely: personalized reminiscence therapy and cognitive training.Results from a pilot study indicated high satisfaction levels from those using the initial version of the system as well as a strong tendency towards repeated use. There was also a clear preference for using personal relevant material rather than more general subject matter.Subsequently, we have designed and further developed an internet-based system, which will be accessible from any location (such as medical facilities, clubs for the elderly, or the residence of the patient or caregiver). The system enables independent use and administration for both patients and caregivers. We are currently conducting a large scale randomized controlled study to further evaluate the effects of this system.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {48},
numpages = {4},
keywords = {user-centered design, reminiscence therapy, rehabilitation engineering, multimedia, human computer interaction, dementia, computerized cognitive training, Alzheimer's disease},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@inproceedings{10.1145/1980022.1980328,
author = {Samant, Rahul M. and Sharma, Anuraag},
title = {Detailed survey of cloud computing paradigm},
year = {2011},
isbn = {9781450304498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980022.1980328},
doi = {10.1145/1980022.1980328},
abstract = {Cloud computing these days is here. Running applications on machines in an internet accessible data center can bring plenty of advantages. Yet to run an application we always need a platform. For many applications this platform usually includes an operating system, some way to store data and more. Applications running in the cloud also need some foundation.Cloud Computing is a foundation for running applications and storing data in the cloud. Cloud Computing applications run on machines in the data centers. Rather than providing software that customers can install and run themselves on their own computers, thus it is a service; customers use it to run applications and store data on internet accessible machines owned by the service provider. Those applications might provide services to businesses, to consumers, or both. The applications that could be built like, If an independent software vendor (ISV) could create an application that targets business users, an approach that's often referred to as Software as a Service (SaaS). ISV's can use Cloud computing framework as a foundation for a variety of business-oriented SaaS applications.An ISV might create a SaaS application that targets consumers. Cloud computing framework is designed to support very scalable software, and so a firm that plans to target a large consumer market might well choose it as a platform for a new application.Enterprises might use Cloud computing framework to build and run applications that are used by their own employees.},
booktitle = {Proceedings of the International Conference &amp; Workshop on Emerging Trends in Technology},
pages = {1357–1358},
numpages = {2},
location = {Mumbai, Maharashtra, India},
series = {ICWET '11}
}

@inproceedings{10.5555/2263019.2263053,
author = {Barnes, Peter D. and Brase, James M. and Canales, Thomas W. and Damante, Matthew M. and Horsley, Matthew A. and Jefferson, David R. and Soltz, Ron A.},
title = {Livermore computer network simulation program},
year = {2012},
isbn = {9781450315104},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {The Livermore Lab has embarked on a multi-year effort to develop a large-scale realistic network simulation capability. Specifically, we are developing computer network simulations for realistic networks derived from real and synthetic network maps, and which incorporate real hardware and geographic constraints, at enterprise (10K node) and above scale; incorporate near-realtime updates from the real global Internet; and generate traffic from realistic traffic models matched to observed data. In this poster we describe our approach and specific applications areas of interest.},
booktitle = {Proceedings of the 5th International ICST Conference on Simulation Tools and Techniques},
pages = {223–225},
numpages = {3},
keywords = {simulated applications, network simulation},
location = {Desenzano del Garda, Italy},
series = {SIMUTOOLS '12}
}

@inproceedings{10.1145/1879141.1879162,
author = {Beverly, Robert and Berger, Arthur and Xie, Geoffrey G.},
title = {Primitives for active internet topology mapping: toward high-frequency characterization},
year = {2010},
isbn = {9781450304832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879141.1879162},
doi = {10.1145/1879141.1879162},
abstract = {Current large-scale topology mapping systems require multiple days to characterize the Internet due to the large amount of probing traffic they incur. The accuracy of maps from existing systems is unknown, yet empirical evidence suggests that additional fine-grained probing exposes hidden links and temporal dynamics. Through longitudinal analysis of data from the Archipelago and iPlane systems, in conjunction with our own active probing, we examine how to shorten Internet topology mapping cycle time. In particular, this work develops discriminatory primitives that maximize topological fidelity while being efficient.We propose and evaluate adaptive probing techniques that leverage external knowledge (e.g., common subnetting structures) and data from prior cycle(s) to guide the selection of probed destinations and the assignment of destinations to vantage points. Our Interface Set Cover (ISC) algorithm generalizes previous dynamic probing work. Crucially, ISC runs across probing cycles to minimize probing while detecting load balancing and reacting to topological changes. To maximize the information gain of each trace, our Subnet Centric Probing technique selects destinations more likely to expose their network's internal structure. Finally, the Vantage Point Spreading algorithm uses network knowledge to increase path diversity to destination ingress points.},
booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
pages = {165–171},
numpages = {7},
keywords = {network topology, internet topology, adaptive probing},
location = {Melbourne, Australia},
series = {IMC '10}
}

@inproceedings{10.1145/2390231.2390245,
author = {Mikians, Jakub and Gyarmati, L\'{a}szl\'{o} and Erramilli, Vijay and Laoutaris, Nikolaos},
title = {Detecting price and search discrimination on the internet},
year = {2012},
isbn = {9781450317764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390231.2390245},
doi = {10.1145/2390231.2390245},
abstract = {Price discrimination, setting the price of a given product for each customer individually according to his valuation for it, can benefit from extensive information collected online on the customers and thus contribute to the profitability of e-commerce services. Another way to discriminate among customers with different willingness to pay is to steer them towards different sets of products when they search within a product category (i.e., search discrimination). Our main contribution in this paper is to empirically demonstrate the existence of signs of both price and search discrimination on the Internet, and to uncover the information vectors used to facilitate them. Supported by our findings, we outline the design of a large-scale, distributed watchdog system that allows users to detect discriminatory practices.},
booktitle = {Proceedings of the 11th ACM Workshop on Hot Topics in Networks},
pages = {79–84},
numpages = {6},
keywords = {search discrimination, search, privacy, price discrimination, economics, e-commerce},
location = {Redmond, Washington},
series = {HotNets-XI}
}

@inproceedings{10.1145/2000064.2000103,
author = {Meisner, David and Sadler, Christopher M. and Barroso, Luiz Andr\'{e} and Weber, Wolf-Dietrich and Wenisch, Thomas F.},
title = {Power management of online data-intensive services},
year = {2011},
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000064.2000103},
doi = {10.1145/2000064.2000103},
abstract = {Much of the success of the Internet services model can be attributed to the popularity of a class of workloads that we call Online Data-Intensive (OLDI) services. These workloads perform significant computing over massive data sets per user request but, unlike their offline counterparts (such as MapReduce computations), they require responsiveness in the sub-second time scale at high request rates. Large search products, online advertising, and machine translation are examples of workloads in this class. Although the load in OLDI services can vary widely during the day, their energy consumption sees little variance due to the lack of energy proportionality of the underlying machinery. The scale and latency sensitivity of OLDI workloads also make them a challenging target for power management techniques.We investigate what, if anything, can be done to make OLDI systems more energy-proportional. Specifically, we evaluate the applicability of active and idle low-power modes to reduce the power consumed by the primary server components (processor, memory, and disk), while maintaining tight response time constraints, particularly on 95th-percentile latency. Using Web search as a representative example of this workload class, we first characterize a production Web search workload at cluster-wide scale. We provide a fine-grain characterization and expose the opportunity for power savings using low-power modes of each primary server component. Second, we develop and validate a performance model to evaluate the impact of processor- and memory-based low-power modes on the search latency distribution and consider the benefit of current and foreseeable low-power modes. Our results highlight the challenges of power management for this class of workloads. In contrast to other server workloads, for which idle low-power modes have shown great promise, for OLDI workloads we find that energy-proportionality with acceptable query latency can only be achieved using coordinated, full-system active low-power modes.},
booktitle = {Proceedings of the 38th Annual International Symposium on Computer Architecture},
pages = {319–330},
numpages = {12},
keywords = {servers, power management},
location = {San Jose, California, USA},
series = {ISCA '11}
}

@inproceedings{10.5555/1855711.1855734,
author = {Sen, Siddhartha and Lloyd, Wyatt and Freedman, Michael J.},
title = {Prophecy: using history for high-throughput fault tolerance},
year = {2010},
publisher = {USENIX Association},
address = {USA},
abstract = {Byzantine fault-tolerant (BFT) replication has enjoyed a series of performance improvements, but remains costly due to its replicated work. We eliminate this cost for read-mostly workloads through Prophecy, a system that interposes itself between clients and any replicated service. At Prophecy's core is a trusted sketcher component, designed to extend the semi-trusted load balancer that mediates access to an Internet service. The sketcher performs fast, load-balanced reads when results are historically consistent, and slow, replicated reads otherwise. Despite its simplicity, Prophecy provides a new form of consistency called delay-once consistency. Along the way, we derive a distributed variant of Prophecy that achieves the same consistency but without any trusted components.A prototype implementation demonstrates Prophecy's high throughput compared to BFT systems. We also describe and evaluate Prophecy's ability to scale-out to support large replica groups or multiple replica groups. As Prophecy is most effective when state updates are rare, we finally present a measurement study of popular websites that demonstrates a large proportion of static data.},
booktitle = {Proceedings of the 7th USENIX Conference on Networked Systems Design and Implementation},
pages = {23},
numpages = {1},
location = {San Jose, California},
series = {NSDI'10}
}

@inproceedings{10.5555/2093889.2093950,
author = {Liscano, Ramiro and Dingel, Juergen and Petriu, Dorina and Qureshi, Faisal},
title = {Software modeling for embedded and mobile sensor system},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Advances in sensing hardware, computation and storage platforms and communication technologies have given rise to a plethora of sensory devices, ranging from biological sensors for health monitoring, humidity meters and temperature sensors for ecological monitoring, energy consumption sensors for smart homes and data centers, imaging sensors for surveillance and security applications, etc. In an effort to monitor our surroundings and to create the capability to respond to any situations that might arise in our surroundings, such sensors are being deployed at an ever increasing rate by both companies and individuals. A worthy goal is to develop techniques for managing these sensors and for analyzing and monitoring the data streams generated by these sensors. Traditionally, data streams are collected at a central location (typically a laptop/desktop computer) for monitoring, analysis and archiving. Mobile computing platforms are quickly reaching a point where these too can be effectively used to interact with these sensor systems. Mobile devices will act as gateways to the sensors, processing data streams originating from these sensors on-the-fly to assist the user with the tasks at hand. Indeed mobile devices themselves boast a suite of sensing capabilities: accelerometers, cameras, etc. This observation leads us to believe that it is worthwhile to study small embedded sensors and the slightly more powerful mobile handheld devices concomitantly.Software development for embedded and mobile sensor devices has typically been performed with minimal software design and primarily by directly coding on the device. Sensor devices exhibit many of the same properties as embedded real-time systems, e.g., timely response to communication and sensory events. Thus UML standards like MARTE and SysML that are designed the modeling of real-time systems are a good starting point for software modeling for embedded sensor systems. SysML is a general purpose modeling language for systems engineering applications while MARTE is a UML profile that adds capabilities to UML for model-driven development of Real Time and Embedded Systems. Both of these specifications are important for sensor systems but are missing many some modeling capabilities. For example, they cannot model the network and communications component of sensors without some domain specific knowledge.Advances in sensing hardware, computation and storage platforms and communication technologies have given rise to a plethora of sensory devices, ranging from biological sensors for health monitoring, humidity meters and temperature sensors for ecological monitoring, energy consumption sensors for smart homes and data centers, imaging sensors for surveillance and security applications, etc. In an effort to monitor our surroundings and to create the capability to respond to any situations that might arise in our surroundings, such sensors are being deployed at an ever increasing rate by both companies and individuals. A worthy goal is to develop techniques for managing these sensors and for analyzing and monitoring the data streams generated by these sensors. Traditionally, data streams are collected at a central location (typically a laptop/desktop computer) for monitoring, analysis and archiving. Mobile computing platforms are quickly reaching a point where these too can be effectively used to interact with these sensor systems. Mobile devices will act as gateways to the sensors, processing data streams originating from these sensors on-the-fly to assist the user with the tasks at hand. Indeed mobile devices themselves boast a suite of sensing capabilities: accelerometers, cameras, etc. This observation leads us to believe that it is worthwhile to study small embedded sensors and the slightly more powerful mobile handheld devices concomitantly.Software development for embedded and mobile sensor devices has typically been performed with minimal software design and primarily by directly coding on the device. Sensor devices exhibit many of the same properties as embedded real-time systems, e.g., timely response to communication and sensory events. Thus UML standards like MARTE and SysML that are designed the modeling of real-time systems are a good starting point for software modeling for embedded sensor systems. SysML is a general purpose modeling language for systems engineering applications while MARTE is a UML profile that adds capabilities to UML for model-driven development of Real Time and Embedded Systems. Both of these specifications are important for sensor systems but are missing many some modeling capabilities. For example, they cannot model the network and communications component of sensors without some domain specific knowledge.On the other hand software development for mobile devices has been focused more on Internet based applications as opposed a device for sensor data acquisition and processing. In most cases these Internet access applications do not require any real-time performance so many conventional high level languages are sufficient. For sensor data acquisition developers must be aware of the resource constraints of the platform. In that sense mobile devices are a form of an embedded system except it is usually constrained or governed by operator and/or vendor restrictions that have to be accounted for. Model based development should help to improve and test software development for mobile devices but there seems to be very little work in this area.This workshop explored the following topics in order to facilitate software modeling for embedded and mobile sensor system:• Software Modeling of sensor networks• Performance modeling for embedded and real-time systems• Component-based development of reactive systems• Model checking for embedded real-time systems• Protocol state machines for embedded real-time systems• Bridging the semantic gap between the source and target model• Software laboratories for studying and developing smart sensor networkCommercial software modeling tools for embedded and real-time systems have primarily focused on the design and analysis of processes in a single device. When one has to incorporate the gathering of sensory information it is necessary to incorporate models for distributed communications and power management into these existing tools. A significant challenge in the modeling of sensor network systems is the ability to model network delays and network lifetime at a similar layer of abstraction to that used at the software modeling layer. To complicate matters the performance of sensor networks is highly dependent on the physical layout of the sensors implying the need for the integration of network simulators with software modeling tools. John Khalil presented an overview on adaptations required to MARTE and SysML to support the modeling of sensor networks.Quantitative performance analysis of service-oriented systems can be conducted in the early development phases by transforming a UML software model extended with performance annotations into a performance model (such as queueing networks, Petri nets, stochastic process algebra) which can be solved with existing performance analysis tools. The OMG standard MARTE, can be used for adding performance annotations to a given UML model. Dr. Petriu discussed the type of MARTE performance annotations and the principles for transforming annotated software models into performance models. Such a transformation must bridge a large semantic gap between the source and target model for two main reasons: performance models concentrate on resource usage and abstract away many details of the original software model, and the performance model requires platform information which is not contained in the software application model.Interfaces represent abstractions which are supposed to facilitate the correct use of an entity by listing the data and operations that the entity makes available and separating its externally visible parts from the internal ones. Arguably, this notion is one of the great success stories in computer science. Dr. Dingel discussed the potential of protocol state machines (PSMs) for facilitating the model-driven development of component-based systems in general and of reactive systems in particular. Model checking is used for determining the compatibility of a component with respect to interface specifications using PSMs.Virtual worlds can serve as software laboratories for carrying out sensor networks research. Virtual sensor networks can be conveniently deployed in these visually and behaviorally realistic 3D environments for further study and evaluation. These virtual worlds can be used to determine if the software models are functioning as expected. Dr. Qureshi presented examples of the use of virtual worlds as software laboratories. These examples support the unorthodox view that such software laboratories are indispensable for studying large scale embedded and mobile sensor systems.The workshop provided a forum for researchers, students and developers of embedded and mobile sensor systems, from both academia and industry, to discuss the latest innovations and future works in the field. A special session was allocated for position papers and short discussions. The workshop concluded with a panel discussion on the topic of challenges for software modeling for embedded and mobile sensor systems.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {360–362},
numpages = {3},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.1145/1943513.1943517,
author = {Kostiainen, Kari and Reshetova, Elena and Ekberg, Jan-Erik and Asokan, N.},
title = {Old, new, borrowed, blue --: a perspective on the evolution of mobile platform security architectures},
year = {2011},
isbn = {9781450304665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1943513.1943517},
doi = {10.1145/1943513.1943517},
abstract = {The recent dramatic increase in the popularity of "smartphones" has led to increased interest in smartphone security research. From the perspective of a security researcher the noteworthy attributes of a modern smartphone are the ability to install new applications, possibility to access Internet and presence of private or sensitive information such as messages or location. These attributes are also present in a large class of more traditional "feature phones." Mobile platform security architectures in these types of devices have seen a much larger scale of deployment compared to platform security architectures designed for PC platforms. In this paper we start by describing the business, regulatory and end-user requirements which paved the way for this widespread deployment of mobile platform security architectures. We briefly describe typical hardware-based security mechanism that provide the foundation for mobile platform security. We then describe and compare the currently most prominent open mobile platform security architectures and conclude that many features introduced recently are borrowed, or adapted with a twist, from older platform security architectures. Finally, we identify a number of open problems in designing effective mobile platform security.},
booktitle = {Proceedings of the First ACM Conference on Data and Application Security and Privacy},
pages = {13–24},
numpages = {12},
keywords = {platform security architectures, hardware-security mechanisms},
location = {San Antonio, TX, USA},
series = {CODASPY '11}
}

@inproceedings{10.1145/2676662.2676673,
author = {Bruno, Rodrigo and Ferreira, Paulo},
title = {SCADAMAR: scalable and data-efficient internet MapReduce},
year = {2014},
isbn = {9781450332330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676662.2676673},
doi = {10.1145/2676662.2676673},
abstract = {Recent developments of popular programming models, namely MapReduce, have raised the interest of running MapReduce applications over the large scale Internet. However, current data distribution techniques used in Internet wide computing platforms to distribute the high volumes of information, which are needed to run MapReduce jobs, are naive, and therefore need to be re-thought.Thus, we present a computing platform called SCADAMAR that runs MapReduce jobs over the Internet and provides two new main contributions: i) improves data distribution by using the BitTorrent protocol to distribute all data, and ii) improves intermediate data availability by replicating tasks or data through nodes in order to avoid losing intermediate data and consequently preventing big delays on the MapReduce overall execution time.Along with the design of our solution, we present an extensive set of performance results which confirm the usefulness of the above mentioned contributions, improved data distribution and availability, thus making our platform a feasible approach to run MapReduce jobs.},
booktitle = {Proceedings of the 2nd International Workshop on CrossCloud Systems},
articleno = {2},
numpages = {6},
keywords = {cloud computing, MapReduce, BitTorrent, BOINC},
location = {Bordeaux, France},
series = {CCB '14}
}

@inproceedings{10.1145/2381966.2381976,
author = {Reimann, Sirke and D\"{u}rmuth, Markus},
title = {Timed revocation of user data: long expiration times from existing infrastructure},
year = {2012},
isbn = {9781450316637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2381966.2381976},
doi = {10.1145/2381966.2381976},
abstract = {The way we deal with information has changed significantly over the last years. More and more private data is published on the Internet, and at the same time our capacity to store and process data has vastly increased. Systems to prevent a large-scale data collection by placing an "expiration date" on digital data have been proposed before, but either they only support very short expiration times of a few days (such as Vanish and EphPub), or they require additional infrastructure (such as FaceCloak and X-pire).We propose a system that (i) implements expiration times of several month and does this (ii) based on existing infrastructure only; to the best of our knowledge this is the first system to have both properties at the same time. We exploit the fact that many webpages continuously change over time: We extract several key-shares from random webpages and use a threshold secret sharing scheme to reconstruct the correct key if enough webpages have not yet changed. After several month, enough webpages have changed to completely hide the key.For almost a year, we have collected statistics about the changes of webpages on a large random sample of webpages and have shown that expiration times of several month can be implemented reliably.},
booktitle = {Proceedings of the 2012 ACM Workshop on Privacy in the Electronic Society},
pages = {65–74},
numpages = {10},
keywords = {timed revocation, privacy},
location = {Raleigh, North Carolina, USA},
series = {WPES '12}
}

@inproceedings{10.1109/CCGrid.2013.43,
author = {Fr\"{o}ning, Holger and N\"{u}ssle, Mondrian and Litz, Heiner and Leber, Christian and Br\"{u}ning, Ulrich},
title = {On achieving high message rates},
year = {2013},
isbn = {9780768549965},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2013.43},
doi = {10.1109/CCGrid.2013.43},
abstract = {Computer systems continue to increase in parallelism in all areas. Stagnating single thread performance as well as power constraints prevent a reversal of this trend; on the contrary, current projections show that the trend towards parallelism will accelerate. In cluster computing, scalability, and therefore the degree of parallelism, is limited by the network interconnect and more specifically by the message rate it provides. We designed an interconnection network specifically for high message rates. Among other things, it reduces the burden on the software stack by relying on communication engines that perform a large fraction of the send and receive functionality in hardware. It also supports multi-core environments very efficiently through hardware-level virtualization of the communication engines. We provide details on the overall architecture, the thin software stack, performance results for a set of MPI-based benchmarks, and an in-depth analysis of how application performance depends on the message rate. We vary the message rate by software and hardware techniques, and measure the application-level impact of different message rates. We are also using this analysis to extrapolate performance for technologies with wider data paths and higher line rates.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {498–505},
numpages = {8},
keywords = {performance prediction, performance analysis, high performance networking, computer communications},
location = {Delft, Netherlands},
series = {CCGRID '13}
}

@article{10.1145/2382756.2382764,
author = {Lago, Patricia and Lewis, Grace A. and Metzger, Andreas and Tosic, Vladimir and Bianculli, Domenico and Di Marco, Antinisca and Polini, Andrea and Plebani, Pierluigi},
title = {Report of the 4th international workshop on principles of engineering service-oriented systems (PESOS 2012)},
year = {2013},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382764},
doi = {10.1145/2382756.2382764},
abstract = {The 4th International Workshop on Principles of Engineering Service-Oriented Systems (PESOS 2012) was held at the International Conference on Software Engineering, ICSE 2012, on June 4, 2012. The special theme of this 4th edition of PESOS was "The Internet of Services." PESOS 2012 brought together software engineering researchers from academia and industry, as well as practitioners working in the areas of service-oriented systems, to discuss research challenges, recent developments, novel application scenarios, as well as methods, techniques, experiences, and tools to support engineering, evolution and adaptation of large-scale, highly-dynamic serviceoriented systems. For the first time, PESOS featured a special session on "The Quest for Case Studies." This effort created an initial reference set of case studies, hosted on a publicly-available repository, for the research community in service-oriented systems. The multiple discussions throughout the day resulted in the identification service requirements for the Internet of Services.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {1–7},
numpages = {7},
keywords = {software-as-a-service, software engineering, services, service-oriented systems, service-oriented architecture, internet of services, cloud computing, SaaS, SOA}
}

@article{10.1145/2413038.2382764,
author = {Lago, Patricia and Lewis, Grace A. and Metzger, Andreas and Tosic, Vladimir and Bianculli, Domenico and Di Marco, Antinisca and Polini, Andrea and Plebani, Pierluigi},
title = {Report of the 4th international workshop on principles of engineering service-oriented systems (PESOS 2012)},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2413038.2382764},
doi = {10.1145/2413038.2382764},
abstract = {The 4th International Workshop on Principles of Engineering Service-Oriented Systems (PESOS 2012) was held at the International Conference on Software Engineering, ICSE 2012, on June 4, 2012. The special theme of this 4th edition of PESOS was "The Internet of Services." PESOS 2012 brought together software engineering researchers from academia and industry, as well as practitioners working in the areas of service-oriented systems, to discuss research challenges, recent developments, novel application scenarios, as well as methods, techniques, experiences, and tools to support engineering, evolution and adaptation of large-scale, highly-dynamic serviceoriented systems. For the first time, PESOS featured a special session on "The Quest for Case Studies." This effort created an initial reference set of case studies, hosted on a publicly-available repository, for the research community in service-oriented systems. The multiple discussions throughout the day resulted in the identification service requirements for the Internet of Services.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {35–38},
numpages = {4},
keywords = {software-as-a-service, software engineering, services, service-oriented systems, service-oriented architecture, internet of services, cloud computing, SaaS, SOA}
}

@inproceedings{10.1145/2382196.2382267,
author = {Li, Zhou and Zhang, Kehuan and Xie, Yinglian and Yu, Fang and Wang, XiaoFeng},
title = {Knowing your enemy: understanding and detecting malicious web advertising},
year = {2012},
isbn = {9781450316514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382196.2382267},
doi = {10.1145/2382196.2382267},
abstract = {With the Internet becoming the dominant channel for marketing and promotion, online advertisements are also increasingly used for illegal purposes such as propagating malware, scamming, click frauds, etc. To understand the gravity of these malicious advertising activities, which we call malvertising, we perform a large-scale study through analyzing ad-related Web traces crawled over a three-month period. Our study reveals the rampancy of malvertising: hundreds of top ranking Web sites fell victims and leading ad networks such as DoubleClick were infiltrated.To mitigate this threat, we identify prominent features from malicious advertising nodes and their related content delivery paths, and leverage them to build a new detection system called MadTracer. MadTracer automatically generates detection rules and utilizes them to inspect advertisement delivery processes and detect malvertising activities. Our evaluation shows that MadTracer was capable of capturing a large number of malvertising cases, 15 times as many as Google Safe Browsing and Microsoft Forefront did together, at a low false detection rate. It also detected new attacks, including a type of click-fraud attack that has never been reported before.},
booktitle = {Proceedings of the 2012 ACM Conference on Computer and Communications Security},
pages = {674–686},
numpages = {13},
keywords = {statistical learning, online advertising, malvertising},
location = {Raleigh, North Carolina, USA},
series = {CCS '12}
}

@inproceedings{10.1145/1993744.1993798,
author = {Akella, Aditya and Chawla, Shuchi and Esquivel, Holly and Muthukrishnan, Chitra},
title = {De-ossifying internet routing through intrinsic support for end-network and ISP selfishness},
year = {2011},
isbn = {9781450308144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993744.1993798},
doi = {10.1145/1993744.1993798},
abstract = {We present the S4R supplemental routing system to address the constraints BGP places on ISPs and stub network alike. Technical soundness and economic viability are equal first class design requirements for S4R. In S4R, ISPs announce links connecting different parts of the Internet. ISPs can selfishly price their links to attract maximal amount of traffic. Stub networks can selfishly select paths that best meet their requirements at the lowest cost. We design a variety of practical algorithms for ISP and stub network response that strike a balance between accommodating selfishness of all participants and ensuring efficient and stable operation overall. We employ large scale simulations over realistic scenarios to show that S4R operates at a close-to-optimal state and that it encourages broad participation from stubs and ISPs.},
booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {145–146},
numpages = {2},
keywords = {selfishness, inter-domain routing},
location = {San Jose, California, USA},
series = {SIGMETRICS '11}
}

@article{10.1145/2007116.2007165,
author = {Akella, Aditya and Chawla, Shuchi and Esquivel, Holly and Muthukrishnan, Chitra},
title = {De-ossifying internet routing through intrinsic support for end-network and ISP selfishness},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2007116.2007165},
doi = {10.1145/2007116.2007165},
abstract = {We present the S4R supplemental routing system to address the constraints BGP places on ISPs and stub network alike. Technical soundness and economic viability are equal first class design requirements for S4R. In S4R, ISPs announce links connecting different parts of the Internet. ISPs can selfishly price their links to attract maximal amount of traffic. Stub networks can selfishly select paths that best meet their requirements at the lowest cost. We design a variety of practical algorithms for ISP and stub network response that strike a balance between accommodating selfishness of all participants and ensuring efficient and stable operation overall. We employ large scale simulations over realistic scenarios to show that S4R operates at a close-to-optimal state and that it encourages broad participation from stubs and ISPs.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {337–338},
numpages = {2},
keywords = {selfishness, inter-domain routing}
}

@article{10.1145/1907450.1907538,
author = {Kuller, Samir and Mahoney, Michael W.},
title = {SIGACT news algorithms column: computation in large-scale scientific and internet data applications is a focus of MMDS 2010},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5700},
url = {https://doi.org/10.1145/1907450.1907538},
doi = {10.1145/1907450.1907538},
abstract = {The 2010 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2010) was held at Stanford University, June 15-18. The goals of MMDS 2010 were (1) to explore novel techniques for modeling and analyzing massive, high-dimensional, and nonlinearly-structured scientific and Internet data sets; and (2) to bring together computer scientists, statisticians, applied mathematicians, and data analysis practitioners to promote cross-fertilization of ideas. MMDS 2010 followed on the heels of two previous MMDS workshops. The first, MMDS 2006, addressed the complementary perspectives brought by the numerical linear algebra and theoretical computer science communities to matrix algorithms in modern informatics applications [1]; and the second, MMDS 2008, explored more generally fundamental algorithmic and statistical challenges in modern large-scale data analysis [2].The MMDS 2010 program drew well over 200 participants, with 40 talks and 13 poster presentations from a wide spectrum of researchers in modern large-scale data analysis. This included both academic researchers as well as a wide spectrum of industrial practitioners. As with the previous meetings, MMDS 2010 generated intense interdisciplinary interest and was extremely successful, clearly indicating the desire among many research communities to begin to distill out and establish the algorithmic and statistical basis for the analysis of complex large-scale data sets, as well as the desire to move increasingly-sophisticated theoretical ideas to the solution of practical problems.},
journal = {SIGACT News},
month = {dec},
pages = {65–72},
numpages = {8}
}

@inproceedings{10.1109/PADS.2011.5936762,
author = {Hou, Bonan and Yao, Yiping and Peng, Shaoliang},
title = {Empirical Study on Entity Interaction Graph of Large-Scale Parallel Simulations},
year = {2011},
isbn = {9781457713637},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PADS.2011.5936762},
doi = {10.1109/PADS.2011.5936762},
abstract = {The entity interaction graph is an important metaphor for understanding the simulation execution of complex systems on parallel computing environment. Current performance tuning techniques often explore interrelated factors affecting performance, but ignore systematic analysis on the structure and behavior of entity interactions. This paper reports an empirical study on the entity interaction graphs of three systems chosen from different domains: Internet models, molecular dynamics, and social dynamics, respectively. The results of complex networks analysis on the entity interaction graphs demonstrate that the heterogeneous distribution of connections and highly clustering are universal in these complex systems. Generally, these properties are not obvious at the system modeling stage. Moreover, mutual information theory is used to measure the ``principle of persistence" as the predictability of partitioning on multiple processors. This study facilitates better understanding and quantifying of the interaction complexity and provides implications on performance tuning for parallel simulation of large- scale complex systems.},
booktitle = {Proceedings of the 2011 IEEE Workshop on Principles of Advanced and Distributed Simulation},
pages = {1–6},
numpages = {6},
series = {PADS '11}
}

@inproceedings{10.1145/1951365.1951368,
author = {Weikum, Gerhard},
title = {Database researchers: plumbers or thinkers?},
year = {2011},
isbn = {9781450305280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1951365.1951368},
doi = {10.1145/1951365.1951368},
abstract = {DB researchers have traditionally focused on engine-centered issues such as indexing, query processing, and transactions. Data mining has broadened the community's viewpoint towards algorithmic and statistical issues. However, DB research has always had a tendency to shy away from seemingly elusive long-term challenges with AI flavor. On the other hand, the current explosion of digital content in enterprises and the Internet, is mostly caused by user-created information like text, tags, photos, videos, and not by seeing more well-designed databases of the traditional kind.In this situation, I question the traditional skepticism of DB researchers towards "AI-complete" problems and the DB community's reluctance to embark on seemingly non-DB-ish grand challenges. Big questions that I see as great opportunities also for DB research include: 1) automatic extraction of relational facts from natural-language text and multimodal contexts [4, 6, 21], 2) automatic disambiguation of named-entity mentions and general phrases in text and speech [10, 11], 3) large-scale gathering of factual-knowledge candidates and their reconciliation into comprehensive knowledge bases [1, 2, 8, 13, 19], 4) reasoning on uncertain hypotheses, for knowledge discovery and semantic search [9, 14, 16, 17, 20], 5) deep and real-time question answering, e.g., to enable computers to win quiz game shows [7], 6) machine-reading of scientific publications and fictional literature, to enable corpus-wide analyses and enable researchers in science and humanities to develop hypotheses and quickly focus on the most relevant issues [3, 5].I believe that successfully tackling these topics requires efficient data-centric algorithms, scalable methods and architectures, and system-level thinking - virtues that are richly available in the DB research community. Moreover, I would encourage our community to look across the fence and get more engaged on the exciting challenges outside the traditionally narrow boundaries of the DB realm. I will illustrate these points by examples from my own research on knowledge management [12, 15, 18, 19]. Breakthroughs will require long-term stamina. In the meantime, steady incremental progress is better than not embarking on these important problems at all.},
booktitle = {Proceedings of the 14th International Conference on Extending Database Technology},
pages = {9–10},
numpages = {2},
keywords = {scalability, robustness, machine reading, knowledge management, information extraction, disambiguation, AI applications},
location = {Uppsala, Sweden},
series = {EDBT/ICDT '11}
}

@inproceedings{10.1145/2639108.2642910,
author = {Katevas, Kleomenis and Haddadi, Hamed and Tokarchuk, Laurissa},
title = {Poster: SensingKit: a multi-platform mobile sensing framework for large-scale experiments},
year = {2014},
isbn = {9781450327831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639108.2642910},
doi = {10.1145/2639108.2642910},
abstract = {With the rapid rise in variety of available smartphones today and their rich sensing capabilities, there is an increasing interest in using mobile sensing in large-scale experiments and commercial applications. Motivated by the lack of a universal, multi-platform library, in this paper we present SensingKit, an efficient, open-source, client-server system that supports both iOS and Android mobile devices. SensingKit is capable of continuous sensing the device's motion (Accelerometer, Gyroscope, Magnetometer), location (GPS) and proximity to other smartphones (Bluetooth Smart). The data are temporarily saved to the device's memory and transmitted to a server for further analysis over any Internet connection. We believe that this platform will be beneficial to all researchers and developers who need to perform mobile sensing in their applications and experiments.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Computing and Networking},
pages = {375–378},
numpages = {4},
keywords = {spatio-temporal data, motion data, mobile sensing, ios, android},
location = {Maui, Hawaii, USA},
series = {MobiCom '14}
}

@inproceedings{10.1145/2413247.2413269,
author = {Phan, Khoa Truong and Moulierac, Joanna and Tran, Cuong Ngoc and Thoai, Nam},
title = {Xcast6 treemap islands: revisiting multicast model},
year = {2012},
isbn = {9781450317795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2413247.2413269},
doi = {10.1145/2413247.2413269},
abstract = {Due to the complexity and poor scalability, IP Multicast has not been used on the Internet. Recently, Xcast6 -- a complementary protocol of IP Multicast has been proposed. However, the key limitation of Xcast6 is that it only supports small multicast sessions. To overcome this, we propose Xcast6 Treemap islands (X6Ti) -- a hybrid model of Overlay Multicast and Xcast6. In summary, X6Ti has many advantages: support large multicast groups, simple and easy to deploy on the Internet, no router configuration, no restriction on the number of groups, no multicast routing protocol and no group management protocol. Based on simulation, we compare X6Ti with IP Multicast and NICE protocols to show the benefits of our new model.},
booktitle = {Proceedings of the 2012 ACM Conference on CoNEXT Student Workshop},
pages = {33–34},
numpages = {2},
keywords = {xcast6 treemap islands, overlay network, IP multicast},
location = {Nice, France},
series = {CoNEXT Student '12}
}

@inproceedings{10.1145/2187836.2187838,
author = {Aizenberg, Natalie and Koren, Yehuda and Somekh, Oren},
title = {Build your own music recommender by modeling internet radio streams},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187838},
doi = {10.1145/2187836.2187838},
abstract = {In the Internet music scene, where recommendation technology is key for navigating huge collections, large market players enjoy a considerable advantage. Accessing a wider pool of user feedback leads to an increasingly more accurate analysis of user tastes, effectively creating a "rich get richer" effect. This work aims at significantly lowering the entry barrier for creating music recommenders, through a paradigm coupling a public data source and a new collaborative filtering (CF) model. We claim that Internet radio stations form a readily available resource of abundant fresh human signals on music through their playlists, which are essentially cohesive sets of related tracks. In a way, our models rely on the knowledge of a diverse group of experts in lieu of the commonly used wisdom of crowds. Over several weeks, we aggregated publicly available playlists of thousands of Internet radio stations, resulting in a dataset encompassing millions of plays, and hundreds of thousands of tracks and artists. This provides the large scale ground data necessary to mitigate the cold start problem of new items at both mature and emerging services.Furthermore, we developed a new probabilistic CF model, tailored to the Internet radio resource. The success of the model was empirically validated on the collected dataset. Moreover, we tested the model at a cross-source transfer learning manner -- the same model trained on the Internet radio data was used to predict behavior of Yahoo! Music users. This demonstrates the ability to tap the Internet radio signals in other music recommendation setups. Based on encouraging empirical results, our hope is that the proposed paradigm will make quality music recommendation accessible to all interested parties in the community.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {1–10},
numpages = {10},
keywords = {music recommendation, internet radio, collaborative filtering},
location = {Lyon, France},
series = {WWW '12}
}

@inproceedings{10.1145/1851476.1851562,
author = {Wojciechowski, Maciej and Capot\u{a}, Mihai and Pouwelse, Johan and Iosup, Alexandru},
title = {BTWorld: towards observing the global BitTorrent file-sharing network},
year = {2010},
isbn = {9781605589428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851476.1851562},
doi = {10.1145/1851476.1851562},
abstract = {Today, the BitTorrent Peer-to-Peer file-sharing network is one of the largest Internet applications---it generates massive traffic volumes, it is deployed in thousands of independent communities, and it serves millions of unique users worldwide. Despite a large number of empirical and theoretical studies, observing the state of the global BitTorrent network remains a grand challenge for the BitTorrent community. To address this challenge, in this work we introduce BT-World, an architecture for observing the global BitTorrent network without help from the ISPs. We design BTWorld around three main features specific to BitTorrent measurements. First, our architecture is able to find public trackers, that is, the BitTorrent components that offer unrestricted service to peers around the world. Second, by observing the state of these trackers, BTWorld obtains information about the performance, scalability, and reliability of BitTorrent. Third, BTWorld is designed to pre-process the large volumes of recorded data for later analysis. We demonstrate the viability of our architecture by deploying it in practice, to observe and analyze one week of operation of a large part of the global BitTorrent network--over 10 million swarms and tens of millions of concurrent users. We also show that BT-World can shed light on BitTorrent phenomena, such as the presence of spam trackers and giant swarms.},
booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
pages = {581–588},
numpages = {8},
location = {Chicago, Illinois},
series = {HPDC '10}
}

@article{10.1145/2317307.2317313,
author = {claffy, kc},
title = {Border gateway protocol (BGP) and traceroute data workshop report},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2317307.2317313},
doi = {10.1145/2317307.2317313},
abstract = {On Monday, 22 August 2011, CAIDA hosted a one-day workshop to discuss scalable measurement and analysis of BGP and traceroute topology data, and practical applications of such data analysis including tracking of macroscopic censorship and filtering activities on the Internet. Discussion topics included: the surprisingly stability in the number of BGP updates over time; techniques for improving measurement and analysis of inter-domain routing policies; an update on Colorado State's BGPMon instrumentation; using BGP data to improve the interpretation of traceroute data, both for real-time diagnostics (e.g., AS traceroute) and for large-scale topology mapping; using both BGP and traceroute data to support detection and mapping infrastructure integrity, including different types of of filtering and censorship; and use of BGP data to analyze existing and proposed approaches to securing the interdomain routing system. This report briefly summarizes the presentations and discussions that followed.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jun},
pages = {28–31},
numpages = {4},
keywords = {validation, topology, routing, internet measurement techniques, filtering, data analysis, censorship}
}

@article{10.1145/2036264.2036276,
author = {Wang, Jingdong and Hua, Xian-Sheng},
title = {Interactive Image Search by Color Map},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036276},
doi = {10.1145/2036264.2036276},
abstract = {The availability of large-scale images from the Internet has made the research on image search attract a lot of attention. Text-based image search engines, for example, Google/Microsoft Bing/Yahoo! image search engines using the surrounding text, have been developed and widely used. However, they suffer from an inability to search image content. In this article, we present an interactive image search system, image search by color map, which can be applied to, but not limited to, enhance text-based image search. This system enables users to indicate how the colors are spatially distributed in the desired images, by scribbling a few color strokes, or dragging an image and highlighting a few regions of interest in an intuitive way. In contrast to the conventional sketch-based image retrieval techniques, our system searches images based on colors rather than shapes, and we, technically, propose a simple but effective scheme to mine the latent search intention from the user’s input, and exploit the dominant color filter strategy to make our system more efficient. We integrate our system to existing Web image search engines to demonstrate its superior performance over text-based image search. The user study shows that our system can indeed help users conveniently find desired images.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {oct},
articleno = {12},
numpages = {23},
keywords = {intention map, color map, Image search}
}

@inproceedings{10.1145/2212346.2212348,
author = {Zink, Thomas and Waldvogel, Marcel},
title = {Efficient BitTorrent handshake obfuscation},
year = {2012},
isbn = {9781450311489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212346.2212348},
doi = {10.1145/2212346.2212348},
abstract = {During the last decade, large scale media distribution populated peer-to-peer applications. Faced with ever increasing volumes of traffic, legal threats by copyright holders, and QoS demands of customers, network service providers are urged to apply traffic classification and shaping techniques. These highly integrated systems require constant maintenance, introduce legal issues, and violate both the net neutrality and end-to-end principles.Clients see their freedom and privacy attacked. Users, application programmers, and even commercial service providers laboriously strive to hide their interests and circumvent classification techniques. While changing the network infrastructure is by nature very complex, and it reacts only slowly to new conditions, updating and distributing software between users is easy and practically instantaneous.We present a new obfuscation extension to the BitTorrent protocol, which allows signature free handshaking. The extension requires no changes to the infrastructure and is fully backwards compatible. With only little change to client software, contemporary classification techniques can be rendered ineffective.},
booktitle = {Proceedings of the First Workshop on P2P and Dependability},
articleno = {2},
numpages = {5},
keywords = {traffic obfuscation, traffic hiding, P2P, BitTorrent},
location = {Sibiu, Romania},
series = {P2P-Dep '12}
}

@inproceedings{10.1145/1993744.1993776,
author = {Shafiq, M. Zubair and Ji, Lusheng and Liu, Alex X. and Wang, Jia},
title = {Characterizing and modeling internet traffic dynamics of cellular devices},
year = {2011},
isbn = {9781450308144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993744.1993776},
doi = {10.1145/1993744.1993776},
abstract = {Understanding Internet traffic dynamics in large cellular networks is important for network design, troubleshooting, performance evaluation, and optimization. In this paper, we present the results from our study, which is based upon a week-long aggregated flow level mobile device traffic data collected from a major cellular operator's core network. In this study, we measure and characterize the spatial and temporal dynamics of mobile Internet traffic. We distinguish our study from other related work by conducting the measurement at a larger scale and exploring mobile data traffic patterns along two new dimensions -- device types and applications that generate such traffic patterns. Based on the findings of our measurement analysis, we propose a Zipf-like model to capture the volume distribution of application traffic and a Markov model to capture the volume dynamics of aggregate Internet traffic. We further customize our models for different device types using an unsupervised clustering algorithm to improve prediction accuracy.},
booktitle = {Proceedings of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {305–316},
numpages = {12},
keywords = {mobile devices, internet traffic, cellular network},
location = {San Jose, California, USA},
series = {SIGMETRICS '11}
}

@article{10.1109/TNET.2009.2035047,
author = {Shakkottai, Srinivas and Johari, Ramesh},
title = {Demand-aware content distribution on the internet},
year = {2010},
issue_date = {April 2010},
publisher = {IEEE Press},
volume = {18},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2009.2035047},
doi = {10.1109/TNET.2009.2035047},
abstract = {The rapid growth of media content distribution on the Internet in the past few years has brought with it commensurate increases in the costs of distributing that content. Can the content distributor defray these costs through a more innovative approach to distribution? In this paper, we evaluate the benefits of a hybrid system that combines peer-to-peer and a centralized client-server approach against each method acting alone. A key element of our approach is to explicitly model the temporal evolution of demand. In particular, we employ a word-of-mouth demand evolution model due to Bass [2] to represent the evolution of interest in a piece of content. Our analysis is carried out in an order scaling depending on the total potential mass of customers in the market. Using this approach, we study the relative performance of peer-to-peer and centralized client-server schemes, as well as a hybrid of the two--both from the point of view of consumers as well as the content distributor.We show how awareness of demand can be used to attain a given average delay target with lowest possible utilization of the central server by using the hybrid scheme.We also show how such awareness can be used to take provisioning decisions. Our insights are obtained in a fluid model and supported by stochastic simulations.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {476–489},
numpages = {14},
keywords = {peer-to-peer (P2P), delay guarantees, content distribution, Bass diffusion}
}

@inproceedings{10.1145/1963192.1963311,
author = {Ahmed, Amr and Smola, Alexander},
title = {WWW 2011 invited tutorial overview: latent variable models on the internet},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963311},
doi = {10.1145/1963192.1963311},
abstract = {Graphical models are an effective tool for analyzing structured and relational data. In particular, they allow us to arrive at insights that are implicit, i.e. latent in the data. Dealing with such data on the internet poses a range of challenges. Firstly, the sheer size renders many well-known inference algorithms infeasible. Secondly, the problems arising on the internet do not always fit well into the known categories for latent variable inference such as Latent Dirichlet Allocation or clustering.In this tutorial we address a number of aspects. Firstly, we present a variety of applications ranging from general purpose document analysis, ideology detection, clustering of sequential data, and dynamic user profiling to recommender systems and data integration. Secondly we give an overview over a number of popular models such as mixture models, topic models, nonparametric variants of temporal dependence, and an integrated analysis and clustering approach, all of which can be used to solve a range of data analysis problems at hand. Thirdly, we present a range of sampling based algorithms for large scale distributed inference using multicore systems and clusters of workstations.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {281–282},
numpages = {2},
keywords = {topic models, sampling, latent variables, graphical models, clustering},
location = {Hyderabad, India},
series = {WWW '11}
}

@article{10.1145/2007116.2007148,
author = {Shafiq, M. Zubair and Ji, Lusheng and Liu, Alex X. and Wang, Jia},
title = {Characterizing and modeling internet traffic dynamics of cellular devices},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2007116.2007148},
doi = {10.1145/2007116.2007148},
abstract = {Understanding Internet traffic dynamics in large cellular networks is important for network design, troubleshooting, performance evaluation, and optimization. In this paper, we present the results from our study, which is based upon a week-long aggregated flow level mobile device traffic data collected from a major cellular operator's core network. In this study, we measure and characterize the spatial and temporal dynamics of mobile Internet traffic. We distinguish our study from other related work by conducting the measurement at a larger scale and exploring mobile data traffic patterns along two new dimensions -- device types and applications that generate such traffic patterns. Based on the findings of our measurement analysis, we propose a Zipf-like model to capture the volume distribution of application traffic and a Markov model to capture the volume dynamics of aggregate Internet traffic. We further customize our models for different device types using an unsupervised clustering algorithm to improve prediction accuracy.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {265–276},
numpages = {12},
keywords = {mobile devices, internet traffic, cellular network}
}

@inproceedings{10.1145/2488222.2488262,
author = {Tariq, Muhammad Adnan and Koldehofe, Boris and Rothermel, Kurt},
title = {Efficient content-based routing with network topology inference},
year = {2013},
isbn = {9781450317580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488222.2488262},
doi = {10.1145/2488222.2488262},
abstract = {Content-based publish/subscribe has gained high popularity for large-scale dissemination of dynamic content. Yet it is highly challenging to enable communication-efficient dissemination of content in such systems, especially in the absence of a broker infrastructure. This paper presents a novel approach that exploits the knowledge of event traffic, user subscriptions and topology of the underlying physical network to perform efficient routing in a publish/subscribe system. In particular, mechanisms are developed to discover the underlay topology among subscribers and publishers in a distributed manner. The information of the topology and the proximity between the subscribers to receive similar events is then used to construct a routing overlay with low communication cost. Our evaluations show that for internet-like topologies the proposed inference mechanisms are capable of modeling an underlay in an efficient and accurate manner. Furthermore, the approach yields a significant reduction in routing cost in comparison to the state of the art.},
booktitle = {Proceedings of the 7th ACM International Conference on Distributed Event-Based Systems},
pages = {51–62},
numpages = {12},
keywords = {underlay, qos, publish/subscribe, p2p, delay, core-based tree, content-based},
location = {Arlington, Texas, USA},
series = {DEBS '13}
}

@inproceedings{10.1145/1958746.1958773,
author = {Starnberger, Guenther and Froihofer, Lorenz and Goeschka, Karl M.},
title = {Adaptive run-time performance optimization through scalable client request rate control},
year = {2011},
isbn = {9781450305198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1958746.1958773},
doi = {10.1145/1958746.1958773},
abstract = {Today's Internet-scale computing systems often run at a low average load with only occasional peak performance demands. Consequently, computing resources are often overdimensioned, leading to high costs. While load control techniques between clients and servers can help to better utilize a given system, these techniques can place a significant communication and computation load on servers. To improve on these issues, we contribute with scalable techniques for client-request rate control, achieved through integration of (i) a scalable distributed feedback channel to transmit control information from the server to the clients with (ii) decoupling strategies that allow to constrain and filter client requests directly at the client, illustrated in the area of first-price sealed-bid online auctions, and (iii) a PID (Proportional-Integral-Derivative) controller that adaptively controls the input parameters of those decoupling strategies to facilitate an optimal server utilization. In contrast to related work, we can hence optimize server load directly at the source through rate control of the clients. Our evaluations show that this setup supports large sets of clients before the controller becomes unstable.},
booktitle = {Proceedings of the 2nd ACM/SPEC International Conference on Performance Engineering},
pages = {167–178},
numpages = {12},
location = {Karlsruhe, Germany},
series = {ICPE '11}
}

@inproceedings{10.1145/1963192.1963310,
author = {Baeza-Yates, Ricardo},
title = {Distributed web retrieval},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963310},
doi = {10.1145/1963192.1963310},
abstract = {In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (over 270 millions at the beginning of 2011) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability; in spite of network latency and scattered data. In this tutorial we present the architecture of current search engines and we explore the main challenges behind the design of all the processes of a distributed Web retrieval system crawling, indexing, and query processing.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {279–280},
numpages = {2},
keywords = {web search, query processing, indexing, distributed systems, crawling},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/2678508.2678514,
author = {Benz, Samuel and Marandi, Parisa Jalili and Pedone, Fernando and Garbinato, Beno\^{\i}t},
title = {Building global and scalable systems with atomic multicast},
year = {2014},
isbn = {9781450332200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678508.2678514},
doi = {10.1145/2678508.2678514},
abstract = {The rise of worldwide Internet-scale services demands large distributed systems. Indeed, when handling several millions of users, it is common to operate thousands of servers spread across the globe. Here, replication plays a central role, as it contributes to improve the user experience by hiding failures and by providing acceptable latency. In this work, we claim that atomic multicast, with strong and well-defined properties, is the appropriate abstraction to efficiently design and implement globally scalable distributed systems. We substantiate our claim with the design of two modern online services atop atomic multicast, a strongly consistent key-value store and a distributed log.},
booktitle = {Proceedings of the Posters and Demos Session of the 15th International Middleware Conference},
pages = {11–12},
numpages = {2},
location = {Bordeaux, France},
series = {Middleware Posters and Demos '14}
}

@inproceedings{10.1145/2536853.2536892,
author = {Elmangoush, Asma and Al-hezmi, Adel and Magedanz, Thomas},
title = {Towards Standard M2M APIs for Cloud-based Telco Service Platforms},
year = {2013},
isbn = {9781450321068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536853.2536892},
doi = {10.1145/2536853.2536892},
abstract = {Machine-to-Machine (M2M) communication technologies is emerging as one of the major trends shaping the development of services in the Future Internet sector. Additionally, cloud computing technologies have been utilized in building large-scale M2M systems, as they could potentially offer more advance solutions for managing data monitoring and service elasticity to support the needs of different consumers. Today, service providers are building a new eco-system with partner vendors to offer new innovative services for Smart Cities. From the users/applications perspective, three concepts aim to deliver Smart service: M2M communication, ubiquitous computing and ambient intelligence. In this paper we identify the requirements for centric service enablers to support the development of desired Smart Cities systems. Also the paper reviews the ongoing work in standardization organizations in this field. Providing standard interfaces to M2M platforms in a network independent way will allow applications deployment by 3rd party developers across variety of end user devices.},
booktitle = {Proceedings of International Conference on Advances in Mobile Computing &amp; Multimedia},
pages = {143–149},
numpages = {7},
keywords = {Smart Service, Open API, M2M, Cloud Computing},
location = {Vienna, Austria},
series = {MoMM '13}
}

@article{10.1145/2160803.2160822,
author = {Starnberger, Guenther and Froihofer, Lorenz and Goeschka, Karl M.},
title = {Adaptive run-time performance optimization through scalable client request rate control (abstracts only)},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/2160803.2160822},
doi = {10.1145/2160803.2160822},
abstract = {Today's Internet-scale computing systems often run at a low average load with only occasional peak performance demands. Consequently, computing resources are often overdimensioned, leading to high costs. While load control techniques between clients and servers can help to better utilize a given system, these techniques can place a significant communication and computation load on servers. To improve on these issues, we contribute with scalable techniques for client-request rate control, achieved through integration of (i) a scalable distributed feedback channel to transmit control information from the server to the clients with (ii) decoupling strategies that allow to constrain and filter client requests directly at the client, illustrated in the area of first-price sealed-bid online auctions, and (iii) a PID (Proportional-Integral-Derivative) controller that adaptively controls the input parameters of those decoupling strategies to facilitate an optimal server utilization. In contrast to related work, we can hence optimize server load directly at the source through rate control of the clients. Our evaluations show that this setup supports large sets of clients before the controller becomes unstable.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {dec},
pages = {14},
numpages = {1}
}

@inproceedings{10.5555/2399776.2399806,
author = {Martin, Patrick and Zulkernine, Farhana},
title = {Ultra-large scale services},
year = {2012},
publisher = {IBM Corp.},
address = {USA},
abstract = {Ultra-large-scale (ULS) services are extremely large software systems across all dimensions, such as the size of their code base, the number of users, the amount of data transferred, the number of developers, and the infrastructure for running these services. ULS services are at the core of many current Web 2.0 applications and are the building blocks of future Web 3.0 applications. ULS services provide the facilities needed for communication (the BlackBerry platform and the Rogers or Bell wireless networks), international banking (Interac and Visa), e-commerce (the eBay auction system and the Amazon Elastic Compute Cloud), social communities (Facebook and MySpace), massive multiplayer online games (World of Warcraft) and future e-healthcare deployments (Health Canada's Infostructure). ULS services are causing a revolution in computing. The scale of ULS services is often hard to grasp. Current ULS services are composed of thousands of hardware nodes; petabytes of data in databases; billions of lines of code, and millions of stakeholders.The Software Engineering Institute (SEI at Carnegie Mellon) report, Ultra-Large-Scale Systems: The Software Challenge of the Future (June 2006), highlighted the dire need for unique and modern approaches to cope with the scale, the wide and varied worldwide user base, the frequent failures and the evolving requirements of these systems. The National Science Foundation (NSF) and the UK Engineering and Physical Sciences Research Council (EPSRC) who recently established research and training centres for Ultra-Large-Scale Software Intensive Systems have heard these sentiments. More recently (2010), the IEEE "Future Directions Committee" highlighted the need for a shift within IEEE to cater to this critical and important domain. Research on a variety of challenging topics is needed to ensure that ULS services can be created and managed reliably in a cost-effective manner. Figure 1: Delivering Ultra Large Scale Services (ULSS)Figure 1 presents the four main perspectives of an efficient ULS service namely, the Focused Quality, Flexible Delivery, Scalable Quality, and Adaptive Infrastructure. Focused Quality implies delivering high quality ULS services, which is a top priority for all service providers. However, the characteristics of ULS services make this a very challenging and complex goal to achieve. The dynamic nature of ULS services (being composed of many other ULS services) and their large and varied user base (often consisting of millions of users with varying needs and expectations) increase the complexity of ensuring the quality of ULS services. For instance, it is impossible to test and verify every possible configuration and usage pattern as these configurations and patterns are continuously changing. This situation has led many ULS providers to depend heavily on monitoring approaches as a way to monitor the quality of their ULS services post-release. However, quality issues should be addressed early, rather than delayed until post-release where quality improvement options are very limited and costly. Techniques and approaches are needed to assist practitioners in focusing their limited resources on the quality improvement efforts which have the highest return, that is, those that are most likely to improve the customer experience.In the world of ULS services, services must handle millions of users with varying needs and capabilities; which is why flexible delivery is recognized as an important aspect. For example, a medical remote diagnostic service that is part of Health Canada's Infostructure might be used by a senior citizen with limited knowledge about medical terms and options, or by a seasoned emergency response worker who requires a more elaborate tool to support his decision making. Providing a single service to satisfy the needs of both users is not feasible today. Instead, different services must be created and maintained. Moreover users of such a medical service might need to perform resource-intensive operations (e.g., watching videos or performing complex simulations based on inputted data), these operations must be delivered in flexible manner instead of simply performing all the operations either locally or centrally (i.e., at the data center). ULS Services need to support both flexible personalization of the service and access through a variety of devices, including smart phones and tablets, in order to accommodate all potential users.ULS systems provide computing, storage and abstraction services, allowing applications to access services with limited knowledge of, expertise with, or control over, the technology infrastructure that supports them. For example, the Blackberry platform connects the wired and wireless internet infrastructure to enable the seamless integration of enterprise and entertainment services between both types of networks. ULS systems are deployed at a worldwide scale and most systems provide an application programming interface (API) to extend various functionalities. These APIs create an ecosystem through which ULS systems grow their user base. However, this leads to varying and innovative usage patterns that must be taken into account, increasing the impact of the reliability of ULS systems while also increasing the complexity of ensuring such reliability. Current industrial approaches to cope with the characteristics of ULS services ensure that they are of high quality but are usually ad hoc, last-resort efforts. Principled approaches are needed to provide scalable high quality services and to evolve these services in an efficient manner while handling their continuously growing user base. For example, current monitoring approaches must be enhanced and adapted to deal with the characteristics of ULS services; current testing techniques and approaches must be revised based on an understanding of the impact of very large user bases on the performance and correctness of software systems in general and ULS service in particular, and traditional security approaches must be adapted to cope with the complexity of ULS services and their evolving usage.To remain competitive, the cost of providing ULS services must scale well with new users and/or units are added. Some ULS providers advocate the need for sub-linear growth in cost, that is, the cost per user/unit should decrease as more users are added to their services. Given the vast number of users, the fluctuation in workload intensities and complexities, the large number of services that must be supported and the unprecedented volumes of different types of data to be processed, it is imperative that ULS infrastructures, on which ULS services run, adapt to the growth, expansion and ever-increasing demands placed on them. The infrastructures, therefore, need to provide autonomic management of the resources, elastic provisioning of storage and other resources to services and support for high availability.},
booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {248–250},
numpages = {3},
location = {Toronto, Ontario, Canada},
series = {CASCON '12}
}

@inproceedings{10.1145/1868521.1868562,
author = {Moghaddam, Saeed and Helmy, Ahmed and Ranka, Sanjay and Somaiya, Manas},
title = {Data-driven co-clustering model of internet usage in large mobile societies},
year = {2010},
isbn = {9781450302746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868521.1868562},
doi = {10.1145/1868521.1868562},
abstract = {Design and simulation of future mobile networks will center around human interests and behavior. We propose a design paradigm for mobile networks driven by realistic models of users' on-line behavior, based on mining of billions of wireless-LAN records. We introduce a systematic method for large-scale multi-dimensional co-clustering of web activity for thousands of mobile users at 79 locations. We find surprisingly that users can be consistently modeled using ten clusters with disjoint profiles. Access patterns from multiple locations show differential user behavior. This is the first study to obtain such detailed results for mobile Internet usage.},
booktitle = {Proceedings of the 13th ACM International Conference on Modeling, Analysis, and Simulation of Wireless and Mobile Systems},
pages = {248–256},
numpages = {9},
keywords = {wireless networks, internet usage, data-driven, co-clustering},
location = {Bodrum, Turkey},
series = {MSWIM '10}
}

@article{10.1145/1966394.1966399,
author = {Bosch, Carles and Laffont, Pierre-Yves and Rushmeier, Holly and Dorsey, Julie and Drettakis, George},
title = {Image-guided weathering: A new approach applied to flow phenomena},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1966394.1966399},
doi = {10.1145/1966394.1966399},
abstract = {The simulation of weathered appearance is essential in the realistic modeling of urban environments. A representative and particularly difficult effect to produce on a large scale is the effect of fluid flow. Changes in appearance due to flow are the result of both the global effect of large-scale shape, and local effects, such as the detailed roughness of a surface. With digital photography and Internet image collections, visual examples of flow effects are readily available. These images, however, mix the appearance of flows with the specific local context. We present a methodology to extract parameters and detail maps from existing imagery in a form that allows new target-specific flow effects to be produced, with natural variations in the effects as they are applied in different locations in a new scene. In this article, we focus on producing a library of parameters and detail maps for generating flow patterns; and this methodology can be used to extend the library with additional image exemplars. To illustrate our methodology, we show a rich collection of patterns applied to urban models.},
journal = {ACM Trans. Graph.},
month = {may},
articleno = {20},
numpages = {13},
keywords = {weathering, rendering, Appearance modeling}
}

@inproceedings{10.1145/2405679.2405680,
author = {Alves, Pedro and Ferreira, Paulo},
title = {Context-aware efficient message propagation},
year = {2012},
isbn = {9781450316095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2405679.2405680},
doi = {10.1145/2405679.2405680},
abstract = {Applications such as Facebook, Twitter and Foursquare brought the massification of personal short messages, distributed in (soft) real-time on the Internet to a large number of users. These messages are complemented with rich contextual information such as the identity, time and location of the person sending the message.Such contextual messages raise serious concerns in terms of scalability and delivery delay; this results not only from their huge number but also because the set of user recipients changes for each message (as their interests continuously change), preventing the use of well-know solutions such as pub-sub and multicast trees. This leads to the use of non-scalable broadcast based solutions or point-to-point messaging.We propose Radiator, a middleware to assist application programmers implementing efficient context propagation mechanisms on their applications. Based on each user current context, Radiator continuously adapts each message propagation path and delivery delay, making an efficient use of network bandwidth, arguably the biggest bottleneck in the deployment of large-scale context propagation systems.Our experimental results demonstrate a 20x reduction on consumed bandwidth without affecting the real-time usefulness of the propagated messages.},
booktitle = {Proceedings of the 11th International Workshop on Adaptive and Reflective Middleware},
articleno = {1},
numpages = {7},
keywords = {scalability, pub-sub, context propagation},
location = {Montreal, Quebec, Canada},
series = {ARM '12}
}

@inproceedings{10.1145/1823854.1823877,
author = {Zhang, Jianting and You, Simin},
title = {Dynamic tiled map services: supporting query-based visualization of large-scale raster geospatial data},
year = {2010},
isbn = {9781450300315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1823854.1823877},
doi = {10.1145/1823854.1823877},
abstract = {Query based visual explorations of raster geospatial data plays an important role in stimulating scientific hypothesis and subsequently seeking casual relationships. While it is desirable to enable visual explorations of large-scale raster geospatial data in a Web environment, improving the end-to-end performance between query backend and the client applications remains a challenging technical issue. Techniques for providing tiled map services that are adopted by major commercial Internet maps APIs have been successful in handling static geospatial data. Motivated by the practical needs of supporting query-based visual explorations in a Web environment, we have proposed a dynamic tiled map services approach that integrates and extends existing Web-based standards and best practices in serving tiled images for static raster geospatial data. The approach includes quadtree-based indexing and query processing at the server side and a middleware to efficiently convert quadrants of dynamic query results into tiled images. A prototype system has been developed to demonstrate the feasibility of the proposed approach. Experimental results have showed that the prototype system achieves an end-to-end performance in the order of sub-second for 1024*1024 pixels display area consisting of multiple tiles.},
booktitle = {Proceedings of the 1st International Conference and Exhibition on Computing for Geospatial Research &amp; Application},
articleno = {19},
numpages = {8},
keywords = {web services, visual exploration, tiled map, geospatial data},
location = {Washington, D.C., USA},
series = {COM.Geo '10}
}

@inproceedings{10.1145/1996413.1996414,
author = {Silberstein, Adam and Machanavajjhala, Ashwin and Ramakrishnan, Raghu},
title = {Feed following: the big data challenge in social applications},
year = {2011},
isbn = {9781450306508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996413.1996414},
doi = {10.1145/1996413.1996414},
abstract = {Internet users spend billions of minutes per month on sites like Facebook and Twitter. These sites support feed following, where users "follow" activity streams associated with other users and entities. Followers get personalized feeds that blend streams produced by those followed. The emphasis on recency and relevance, and the highly variable fan-out of the follows graph, make this feature difficult to implement at the scale seen in major social networks.In this paper, we place feed following in the context of existing research areas and highlight the novel data management challenges that it poses, with the goal of stimulating research in this new direction. We discuss solutions based on pub/sub, caching, and materialized views, and argue that none of these existing approaches fully exploit the unique characteristics of feed following. The number of distinct queries and the query rate per second that a feed following system must support are huge, but queries have simple structure and overlap. The system must handle high throughput input streams, but results are heavily biased toward recent events. The number of users is large, but they exhibit diurnal behavior, and we can dynamically modify the system to optimize for currently active users. These characteristics offer many opportunities for optimization, and the potential gains are substantial.},
booktitle = {Databases and Social Networks},
pages = {1–6},
numpages = {6},
location = {Athens, Greece},
series = {DBSocial '11}
}

@inproceedings{10.1145/1851182.1851237,
author = {Yao, Guang and Bi, Jun and Zhou, Zijian},
title = {Passive IP traceback: capturing the origin of anonymous traffic through network telescopes},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851237},
doi = {10.1145/1851182.1851237},
abstract = {IP traceback can be used to find the origin of anonymous traffic; however, Internet-scale IP traceback systems have not been deployed due to a need for cooperation between Internet Service Providers (ISPs). This article presents an Internet-scale Passive IP Trackback (PIT) mechanism that does not require ISP deployment. PIT analyzes the ICMP messages that may scattered to a network telescope as spoofed packets travel from attacker to victim. An Internet route model is then used to help re-construct the attack path. Applying this mechanism to data collected by Cooperative Association for Internet Data Analysis (CAIDA), we found PIT can construct a trace tree from at least one intermediate router in 55.4% the fiercest packet spoofing attacks, and can construct a tree from at least 10 routers in 23.4% of attacks. This initial result shows PIT is a promising mechanism.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {413–414},
numpages = {2},
keywords = {network telescope, IP traceback},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@article{10.1145/2043635.2043640,
author = {Puzis, Rami and Tubi, Meytal and Elovici, Yuval and Glezer, Chanan and Dolev, Shlomi},
title = {A Decision Support System for Placement of Intrusion Detection and Prevention Devices in Large-Scale Networks},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1049-3301},
url = {https://doi.org/10.1145/2043635.2043640},
doi = {10.1145/2043635.2043640},
abstract = {This article describes an innovative Decision Support System (DSS) for Placement of Intrusion Detection and Prevention Systems (PIDPS) in large-scale communication networks. PIDPS is intended to support network security personnel in optimizing the placement and configuration of malware filtering and monitoring devices within Network Service Providers’ (NSP) infrastructure, and enterprise communication networks. PIDPS meshes innovative and state-of-the-art mechanisms borrowed from the domains of graph theory, epidemic modeling, and network simulation. Scalable network exploitation models enable to define the communication patterns induced by network users (thereby establishing a virtual overlay network), and parallel attack models enable a PIDPS user to define various interdependent network attacks such as: Internet worms, Trojans horses, Denial of Service (DoS) attacks, and others. PIDPS incorporates a set of deployment strategies (employing graph-theoretic centrality measures) in order to facilitate intelligent placement of filtering and monitoring devices; as well as a dedicated network simulator in order to evaluate the various deployments. Experiments with PIDPS indicate that incorporating knowledge on the overlay network (network exploitation patterns) into the placement and configuration of malware filtering and monitoring devices substantially improves the effectiveness of intrusion detection and prevention systems in NSP and enterprise networks.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {dec},
articleno = {5},
numpages = {26},
keywords = {intrusion detection, decision support systems, Overlay networks}
}

@inproceedings{10.1145/2663165.2663323,
author = {Benz, Samuel and Marandi, Parisa Jalili and Pedone, Fernando and Garbinato, Beno\^{\i}t},
title = {Building global and scalable systems with atomic multicast},
year = {2014},
isbn = {9781450327855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663165.2663323},
doi = {10.1145/2663165.2663323},
abstract = {The rise of worldwide Internet-scale services demands large distributed systems. Indeed, when handling several millions of users, it is common to operate thousands of servers spread across the globe. Here, replication plays a central role, as it contributes to improve the user experience by hiding failures and by providing acceptable latency. In this paper, we claim that atomic multicast, with strong and well-defined properties, is the appropriate abstraction to efficiently design and implement globally scalable distributed systems. We substantiate our claim with the design of two modern online services atop atomic multicast, a strongly consistent key-value store and a distributed log. In addition to presenting the design of these services, we experimentally assess their performance in a geographically distributed deployment.},
booktitle = {Proceedings of the 15th International Middleware Conference},
pages = {169–180},
numpages = {12},
location = {Bordeaux, France},
series = {Middleware '14}
}

@inproceedings{10.1145/2159430.2159434,
author = {Zhu, Feiwen and Chen, Peng and Yang, Donglei and Zhang, Weihua and Chen, Haibo and Zang, Binyu},
title = {A GPU-based high-throughput image retrieval algorithm},
year = {2012},
isbn = {9781450312332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2159430.2159434},
doi = {10.1145/2159430.2159434},
abstract = {With the development of Internet and cloud computing, multimedia data, such as images and videos, has become one of the most common data types being processed. As the scale of multimedia data being still increasing, it is vitally important to efficiently extract useful information from such a huge amount of multimedia data. However, due to the complexity of the core algorithms, multimedia retrieval applications are not only data intensive but also computationally intensive. Therefore, it has been a major challenge to accelerate the processing speed of such applications to satisfy the real-time requirement.As Graphic Processing Unit (GPU) has entered the general-propose computing domain (GPGPU), it has become one of the most popular accelerators for the applications with real-time requirements. In this paper, we parallelize a widely-used image retrieval algorithm called SURF on GPGPU, which is the core algorithm for many video and image retrieval applications. We first analyze the parallelism within SURF to guarantee that there are sufficient tasks being mapped to the large-scale computation resources in GPGPU. We then exploit some inherent GPGPU characteristics, such as 2D memory, to further boost the performance. Finally, we provide some optimization to the cooperation between CPU and GPGPU, which is generally ignored in previous designs. Experimental results show that our parallelization and optimization achieve a throughput of 340.5 frames/s on a NVIDIA GTX295 GPGPU, which is 15X faster than the maximal optimized CPU version. Compared to CUDA SURF, a state-of-the-art parallelization of SURF on GPGPU, our system achieves a speedup by a factor of 2.3X.},
booktitle = {Proceedings of the 5th Annual Workshop on General Purpose Processing with Graphics Processing Units},
pages = {30–37},
numpages = {8},
keywords = {image retrieval, high throughput, SURF, GPU-based},
location = {London, United Kingdom},
series = {GPGPU-5}
}

@inproceedings{10.1145/2069216.2069220,
author = {Hluch\'{y}, Ladislav},
title = {From grid to cloud computing},
year = {2011},
isbn = {9781450308809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2069216.2069220},
doi = {10.1145/2069216.2069220},
abstract = {With the advance of computational technologies, the applications running on modern distributed systems became more and more complex. Large-scale applications like environmental applications require such an amount of computation powers on demand that a single computing system hardly can provide. One of the solutions is Grid technologies that connect distributed computational resources of dynamic multi-institutional virtual organizations together and provide aggregate computational powers for solving very complex and computation demanding problems. The technologies make the infrastructures for researchers to share resources and knowledge, allows them to collaborate on solving common problems. The other solution is Cloud computing technologies that deliver computational resources via the Internet in a pay-per-use and self-service way. The technologies allow business and enterprises to use external computational resources dynamically on demands what can reduce the cost of IT resources and allow company to adapt quickly to change of business environment. The research and developments in European projects reflect the eventual change of requirements from applications, advantages and obstacles of the mentioned technologies and mainly the transition from Grid to Cloud computing.},
booktitle = {Proceedings of the 2nd Symposium on Information and Communication Technology},
pages = {4},
numpages = {1},
location = {Hanoi, Vietnam},
series = {SoICT '11}
}

@inproceedings{10.1145/2063576.2064031,
author = {Baumann, Peter},
title = {Large-scale array analytics: taming the data tsunami},
year = {2011},
isbn = {9781450307178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063576.2064031},
doi = {10.1145/2063576.2064031},
abstract = {Never before in history mankind has collected data at the rates we face today. Alone in 2002, an estimated 403 Petabyte of data has been acquired, equivalent to all printed information ever created before. Earth orbiting satellites, as well as ground, airborne, and underwater sensors, space observatories scan their environment at unprecedented resolutions, giving rise to "Big Science". The same holds for the life sciences where genomic data, high-resolution scans, and other modalities are collected in steadily increasing streams. Social network analysis, OLAP, and stock exchange trading represent further examples, the latter involving real-time correlation of thousands of ticker time series resulting in Terabytes of data to be analysed per single run. Summarized under Large-Scale Analytics we are witnessing an exploding demand for flexible access to massive volumes of scientific and business data sets. Arguably a large class of these massive data is represented by multi-dimensional arrays. Consequently, large arrays pose new challenges to data modelling, querying, optimization, and maintenance -- in short: we need Large-Scale Array Analytics. This tutorial introduces to the topic from a database perspective. Aspects addressed include modelling, query languages, query optimization and parallelization, and storage management. High emphasis will be devoted to applications in "Big Science", particularly geo, space, and life sciences; real-life use cases will be presented and discussed which stem from our 15 years of experience with the open-source rasdaman array DBMS and our work on geo raster service standardization. We will highlight requirements, achievements, open research issues, and avenues for future research. Discussion will make use of real-life examples, many of which Internet connected participants can replay hands-on.},
booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
pages = {2599–2600},
numpages = {2},
keywords = {raster, array, analytics},
location = {Glasgow, Scotland, UK},
series = {CIKM '11}
}

@inproceedings{10.5555/2694476.2694480,
author = {Das, Sudipto},
title = {Rethinking eventual consistency: can we do better?},
year = {2013},
publisher = {Computer Society of India},
address = {Mumbai, Maharashtra, IND},
abstract = {Today's data-driven internet-facing applications pose unprecedented challenges to the database systems that back these application. In addition to the immense scale of data, thousands of concurrent requests, and low latency response, these applications also strive for 24X7 availability. The big data serving systems empowering these applications must therefore have low request latencies, be highly-available, and be geo-replicated. As a result, there has been a resurgence of work on replicated, distributed database systems to meet the demands of intermittently-connected clients and of disaster-tolerant databases that span data centers spanning the globe. Many of these data serving systems weaken the criteria for replica-consistency or isolation, and in some cases add new mechanisms, to improve partition-tolerance, availability, and performance. In this talk, I'll present a framework for comparing these criteria and mechanisms, to help architects navigate through this complex design space and reason about the various weak forms of replica consistency. Joint work with Philip A. Bernstein.},
booktitle = {Proceedings of the 19th International Conference on Management of Data},
pages = {5},
numpages = {1},
location = {Ahmedabad, India},
series = {COMAD '13}
}

@inproceedings{10.1145/2523649.2523665,
author = {Amann, Johanna and Sommer, Robin and Vallentin, Matthias and Hall, Seth},
title = {No attack necessary: the surprising dynamics of SSL trust relationships},
year = {2013},
isbn = {9781450320153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523649.2523665},
doi = {10.1145/2523649.2523665},
abstract = {Much of the Internet's end-to-end security relies on the SSL/TLS protocol along with its underlying X.509 certificate infrastructure. However, the system remains quite brittle due to its liberal delegation of signing authority: a single compromised certification authority undermines trust globally. Several recent high-profile incidents have demonstrated this shortcoming convincingly. Over time, the security community has proposed a number of counter measures to increase the security of the certificate ecosystem; many of these efforts monitor for what they consider tell-tale signs of man-in-the-middle attacks. In this work we set out to understand to which degree benign changes to the certificate ecosystem share structural properties with attacks, based on a large-scale data set of more than 17 billion SSL sessions. We find that common intuition falls short in assessing the maliciousness of an unknown certificate, since their typical artifacts routinely occur in benign contexts as well. We also discuss what impact our observations have on proposals aiming to improve the security of the SSL ecosystem.},
booktitle = {Proceedings of the 29th Annual Computer Security Applications Conference},
pages = {179–188},
numpages = {10},
location = {New Orleans, Louisiana, USA},
series = {ACSAC '13}
}

@inproceedings{10.1145/2398776.2398794,
author = {Magno, Gabriel and Comarela, Giovanni and Saez-Trumper, Diego and Cha, Meeyoung and Almeida, Virgilio},
title = {New kid on the block: exploring the google+ social graph},
year = {2012},
isbn = {9781450317054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2398776.2398794},
doi = {10.1145/2398776.2398794},
abstract = {This paper presents a detailed analysis of the Google+ social network. We identify the key differences and similarities with other popular networks like Facebook and Twitter, in order to determine whether Google+ is a new paradigm or yet another social network. This work is based on large-scale crawls of over 27 million user profiles that represented nearly 50% of the entire network in 2011. We observe that the average path length between users is slightly higher than other networks, possibly because Google+ is a new system where relationships are still rapidly growing. Google+ shows a higher level of reciprocity than Twitter, which also has directed social links. The newly available "places lived" field could be used to study how users are distributed around the world and how aggressively the service has been adopted in different countries. We find that Google+ is popular in countries with relatively low Internet penetration rate. Based on the amount and types of information publicly shared in user profiles, we also find that the notion of privacy varies significantly across different cultures.},
booktitle = {Proceedings of the 2012 Internet Measurement Conference},
pages = {159–170},
numpages = {12},
keywords = {online social network, google+, geo-location},
location = {Boston, Massachusetts, USA},
series = {IMC '12}
}

@inproceedings{10.1145/2517899.2517937,
author = {Srinivasan, Vivek and Vardhan, Vibhore and Kar, Snigdha and Asthana, Siddhartha and Narayanan, Rajendran and Singh, Pushpendra and Chakraborty, Dipanjan and Singh, Amarjeet and Seth, Aaditeshwar},
title = {Airavat: an automated system to increase transparency and accountability in social welfare schemes in India},
year = {2013},
isbn = {9781450319072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517899.2517937},
doi = {10.1145/2517899.2517937},
abstract = {Activist groups have taken up information dissemination and feedback collection as a means of rights advocacy in India. However, it is not easy given the difficulty in procuring and disseminating information at a large scale. Beneficiaries are often not able to help themselves as information systems are administration facing, because of poor literacy and the inability to access the Internet. Further, beneficiaries are not well informed of their rights and entitlements under different government schemes to know how and when to file grievances. We aim to solve these problems by designing and testing prototypes for information dissemination and feedback collection in various contexts. In our current prototype we describe an automated tool that sifts through the data on an MIS and conveys personalised information to the beneficiaries through voice calls. This is a work in progress, and our first exercise on providing MIS-extracted information to people through phone calls led to 70% of the beneficiaries who noticed a discrepancy in the data to agree to file a grievance on their behalf. We are continuing to scale the work, make it more automated, and run qualitative interviews with all stakeholders to understand causality linkages with transparency led grievance filing, assisted by appropriate ICTs, to increase accountability.},
booktitle = {Proceedings of the Sixth International Conference on Information and Communications Technologies and Development: Notes - Volume 2},
pages = {151–154},
numpages = {4},
keywords = {accessibility, Airavat},
location = {Cape Town, South Africa},
series = {ICTD '13}
}

@inproceedings{10.1145/1963405.1963436,
author = {Canali, Davide and Cova, Marco and Vigna, Giovanni and Kruegel, Christopher},
title = {Prophiler: a fast filter for the large-scale detection of malicious web pages},
year = {2011},
isbn = {9781450306324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963405.1963436},
doi = {10.1145/1963405.1963436},
abstract = {Malicious web pages that host drive-by-download exploits have become a popular means for compromising hosts on the Internet and, subsequently, for creating large-scale botnets. In a drive-by-download exploit, an attacker embeds a malicious script (typically written in JavaScript) into a web page. When a victim visits this page, the script is executed and attempts to compromise the browser or one of its plugins. To detect drive-by-download exploits, researchers have developed a number of systems that analyze web pages for the presence of malicious code. Most of these systems use dynamic analysis. That is, they run the scripts associated with a web page either directly in a real browser (running in a virtualized environment) or in an emulated browser, and they monitor the scripts' executions for malicious activity. While the tools are quite precise, the analysis process is costly, often requiring in the order of tens of seconds for a single page. Therefore, performing this analysis on a large set of web pages containing hundreds of millions of samples can be prohibitive.One approach to reduce the resources required for performing large-scale analysis of malicious web pages is to develop a fast and reliable filter that can quickly discard pages that are benign, forwarding to the costly analysis tools only the pages that are likely to contain malicious code. In this paper, we describe the design and implementation of such a filter. Our filter, called Prophiler, uses static analysis techniques to quickly examine a web page for malicious content. This analysis takes into account features derived from the HTML contents of a page, from the associated JavaScript code, and from the corresponding URL. We automatically derive detection models that use these features using machine-learning techniques applied to labeled datasets.To demonstrate the effectiveness and efficiency of Prophiler, we crawled and collected millions of pages, which we analyzed for malicious behavior. Our results show that our filter is able to reduce the load on a more costly dynamic analysis tools by more than 85%, with a negligible amount of missed malicious pages.},
booktitle = {Proceedings of the 20th International Conference on World Wide Web},
pages = {197–206},
numpages = {10},
keywords = {malicious web page analysis, efficient web page filtering, drive-by download exploits},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/2661714.2661716,
author = {Hua, Xian-Sheng},
title = {Pushing Image Recognition in the Real World: Towards Recognizing Millions of Entities},
year = {2014},
isbn = {9781450331579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661714.2661716},
doi = {10.1145/2661714.2661716},
abstract = {Building a system that can recognize "what," "who," and "where" from arbitrary images has motivated researchers in computer vision, multimedia and machine learning areas for decades. Significant progresses have been made in recently years based on distributed computation and/or deep neural networks techniques. However, it is still very challenging to realize a general purpose real world image recognition engine that has reasonable recognition accuracy, semantic coverage, and recognition speed.In this talk, firstly we will review the current status of this area, analyze the difficulties, and discuss the potential solutions. Then two promising schemes to attack this challenge will be introduced, including (1) learning millions of concepts from search engine click logs, and (2) recognizing whatever you want without data labeling. The first work tries to build large-scale recognition models by mining search engine click logs. Challenges in training data selection and model selection will be discussed, and efficient and scalable approaches for model training and prediction will be introduced. The second work aims at building image recognition engines for any set of entities without using any human labeled training data, which helps generalize image recognition to a wide range of semantic concepts. Automatic training data generation steps will be presented, and techniques for improving recognition accuracy, which effectively leveraging massive amount of Internet data will be discussed. Different parallelization strategies for different computation tasks will be introduced, which guarantee the efficiency and scalability of the entire system. And last, we will discuss possible directions in pushing image recognition in the real world.},
booktitle = {Proceedings of the First International Workshop on Internet-Scale Multimedia Management},
pages = {51},
numpages = {1},
keywords = {image recognition, concept detection, click log mining},
location = {Orlando, Florida, USA},
series = {WISMM '14}
}

@inproceedings{10.1145/2541940.2541959,
author = {Ouyang, Jian and Lin, Shiding and Jiang, Song and Hou, Zhenyu and Wang, Yong and Wang, Yuanzheng},
title = {SDF: software-defined flash for web-scale internet storage systems},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541959},
doi = {10.1145/2541940.2541959},
abstract = {In the last several years hundreds of thousands of SSDs have been deployed in the data centers of Baidu, China's largest Internet search company. Currently only 40% or less of the raw bandwidth of the flash memory in the SSDs is delivered by the storage system to the applications. Moreover, because of space over-provisioning in the SSD to accommodate non-sequential or random writes, and additionally, parity coding across flash channels, typically only 50-70% of the raw capacity of a commodity SSD can be used for user data. Given the large scale of Baidu's data center, making the most effective use of its SSDs is of great importance. Specifically, we seek to maximize both bandwidth and usable capacity.To achieve this goal we propose {em software-defined flash} (SDF), a hardware/software co-designed storage system to maximally exploit the performance characteristics of flash memory in the context of our workloads. SDF exposes individual flash channels to the host software and eliminates space over-provisioning. The host software, given direct access to the raw flash channels of the SSD, can effectively organize its data and schedule its data access to better realize the SSD's raw performance potential.Currently more than 3000 SDFs have been deployed in Baidu's storage system that supports its web page and image repository services. Our measurements show that SDF can deliver approximately 95% of the raw flash bandwidth and provide 99% of the flash capacity for user data. SDF increases I/O bandwidth by 300% and reduces per-GB hardware cost by 50% on average compared with the commodity-SSD-based system used at Baidu.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {471–484},
numpages = {14},
keywords = {solid-state drive(ssd), flash memory, data center},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1145/2656877.2656888,
author = {Venkataramani, Arun and Kurose, James F. and Raychaudhuri, Dipankar and Nagaraja, Kiran and Mao, Morley and Banerjee, Suman},
title = {MobilityFirst: a mobility-centric and trustworthy internet architecture},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2656877.2656888},
doi = {10.1145/2656877.2656888},
abstract = {MobilityFirst is a future Internet architecture with mobility and trustworthiness as central design goals. Mobility means that all endpoints -- devices, services, content, and networks -- should be able to frequently change network attachment points in a seamless manner. Trustworthiness means that the network must be resilient to the presence of a small number of malicious endpoints or network routers. MobilityFirst enhances mobility by cleanly separating names or identifiers from addresses or network locations, and enhances security by representing both in an intrinsically verifiable manner, relying upon a massively scalable, distributed, global name service to bind names and addresses, and to facilitate services including device-to-service, multicast, anycast, and context-aware communication, content retrieval, and more. A key insight emerging from our experience is that a logically centralized global name service can significantly enhance mobility and security and transform network-layer functionality. Recognizing and validating this insight is the key contribution of the MobilityFirst architectural effort.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {74–80},
numpages = {7},
keywords = {security, network architecture, mobility, global name service}
}

@inproceedings{10.1145/1978672.1978676,
author = {Song, Jungsuk and Takakura, Hiroki and Okabe, Yasuo and Eto, Masashi and Inoue, Daisuke and Nakao, Koji},
title = {Statistical analysis of honeypot data and building of Kyoto 2006+ dataset for NIDS evaluation},
year = {2011},
isbn = {9781450307680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978672.1978676},
doi = {10.1145/1978672.1978676},
abstract = {With the rapid evolution and proliferation of botnets, large-scale cyber attacks such as DDoS, spam emails are also becoming more and more dangerous and serious cyber threats. Because of this, network based security technologies such as Network based Intrusion Detection Systems (NIDSs), Intrusion Prevention Systems (IPSs), firewalls have received remarkable attention to defend our crucial computer systems, networks and sensitive information from attackers on the Internet. In particular, there has been much effort towards high-performance NIDSs based on data mining and machine learning techniques. However, there is a fatal problem in that the existing evaluation dataset, called KDD Cup 99' dataset, cannot reflect current network situations and the latest attack trends. This is because it was generated by simulation over a virtual network more than 10 years ago. To the best of our knowledge, there is no alternative evaluation dataset. In this paper, we present a new evaluation dataset, called Kyoto 2006+, built on the 3 years of real traffic data (Nov. 2006 ~ Aug. 2009) which are obtained from diverse types of honeypots. Kyoto 2006+ dataset will greatly contribute to IDS researchers in obtaining more practical, useful and accurate evaluation results. Furthermore, we provide detailed analysis results of honeypot data and share our experiences so that security researchers are able to get insights into the trends of latest cyber attacks and the Internet situations.},
booktitle = {Proceedings of the First Workshop on Building Analysis Datasets and Gathering Experience Returns for Security},
pages = {29–36},
numpages = {8},
keywords = {honeypot data, NIDS, Kyoto 2006+ dataset},
location = {Salzburg, Austria},
series = {BADGERS '11}
}

@inproceedings{10.1145/2393347.2396372,
author = {Nguyen, Viet Anh and Vu, Tien Dung and Yang, Hongsheng and Lu, Jiangbo and Do, Minh N.},
title = {ITEM: immersive telepresence for entertainment and meetings with commodity setup},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396372},
doi = {10.1145/2393347.2396372},
abstract = {This paper presents an Immersive Telepresence system for Entertainment and Meetings (ITEM). The system aims to provide a radically new video communication experience by seamlessly merging participants into the same virtual space to allow a natural interaction among them and shared collaborative contents. With the goal to make a scalable, flexible system for various business solutions as well as easily accessible by massive consumers, we address the challenges in the whole pipeline of media processing, communication, and displaying in our design and realization of such a system. Extensive experiments show the developed system runs reliably and comfortably in real time with a minimal setup requirement (e.g., a webcam, a laptop/desktop connected to the public Internet) for tele-immersive video communication. With such a really minimal deployment requirement, we present a variety of interesting applications and user experiences created by ITEM.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1021–1024},
numpages = {4},
keywords = {video object cutout, video conferencing, tele-immersive system},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2342356.2342381,
author = {W\"{a}hlisch, Matthias and Maennel, Olaf and Schmidt, Thomas C.},
title = {Towards detecting BGP route hijacking using the RPKI},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342381},
doi = {10.1145/2342356.2342381},
abstract = {Prefix hijacking has always been a big concern in the Internet. Some events made it into the international world-news, but most of them remain unreported or even unnoticed. The scale of the problem can only be estimated.The Resource Publication Infrastructure (RPKI) is an effort by the IETF to secure the inter-domain routing system. It includes a formally verifiable way of identifying who owns legitimately which portion of the IP address space. The RPKI has been standardized and prototype implementations are tested by Internet Service Providers (ISPs). Currently the system holds already about 2% of the Internet routing table.Therefore, in theory, it should be easy to detect hijacking of prefixes within that address space. We take an early look at BGP update data and check those updates against the RPKI---in the same way a router would do, once the system goes operational. We find many interesting dynamics, not all can be easily explained as hijacking, but a significant number are likely operational testing or misconfigurations.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {103–104},
numpages = {2},
keywords = {secure inter-domain routing, rpki, deployment, bgp},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

@article{10.1145/2377677.2377702,
author = {W\"{a}hlisch, Matthias and Maennel, Olaf and Schmidt, Thomas C.},
title = {Towards detecting BGP route hijacking using the RPKI},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2377677.2377702},
doi = {10.1145/2377677.2377702},
abstract = {Prefix hijacking has always been a big concern in the Internet. Some events made it into the international world-news, but most of them remain unreported or even unnoticed. The scale of the problem can only be estimated.The Resource Publication Infrastructure (RPKI) is an effort by the IETF to secure the inter-domain routing system. It includes a formally verifiable way of identifying who owns legitimately which portion of the IP address space. The RPKI has been standardized and prototype implementations are tested by Internet Service Providers (ISPs). Currently the system holds already about 2% of the Internet routing table.Therefore, in theory, it should be easy to detect hijacking of prefixes within that address space. We take an early look at BGP update data and check those updates against the RPKI---in the same way a router would do, once the system goes operational. We find many interesting dynamics, not all can be easily explained as hijacking, but a significant number are likely operational testing or misconfigurations.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {103–104},
numpages = {2},
keywords = {secure inter-domain routing, rpki, deployment, bgp}
}

@inproceedings{10.1109/SC.2014.52,
author = {McLaughlin, Adam and Bader, David A.},
title = {Scalable and high performance betweenness centrality on the GPU},
year = {2014},
isbn = {9781479955008},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2014.52},
doi = {10.1109/SC.2014.52},
abstract = {Graphs that model social networks, numerical simulations, and the structure of the Internet are enormous and cannot be manually inspected. A popular metric used to analyze these networks is betweenness centrality, which has applications in community detection, power grid contingency analysis, and the study of the human brain. However, these analyses come with a high computational cost that prevents the examination of large graphs of interest.Prior GPU implementations suffer from large local data structures and inefficient graph traversals that limit scalability and performance. Here we present several hybrid GPU implementations, providing good performance on graphs of arbitrary structure rather than just scale-free graphs as was done previously. We achieve up to 13x speedup on high-diameter graphs and an average of 2.71x speedup overall over the best existing GPU algorithm. We observe near linear speedup and performance exceeding tens of GTEPS when running betweenness centrality on 192 GPUs.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {572–583},
numpages = {12},
keywords = {parallel algorithms, graph algorithms, GPUs},
location = {New Orleans, Louisana},
series = {SC '14}
}

@article{10.1109/TNET.2013.2291244,
author = {Dainotti, Alberto and Squarcella, Claudio and Aben, Emile and Claffy, Kimberly C. and Chiesa, Marco and Russo, Michele and Pescap\'{e}, Antonio},
title = {Analysis of country-wide internet outages caused by censorship},
year = {2014},
issue_date = {December 2014},
publisher = {IEEE Press},
volume = {22},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2013.2291244},
doi = {10.1109/TNET.2013.2291244},
abstract = {In the first months of 2011, Internet communications were disrupted in several North African countries in response to civilian protests and threats of civil war. In this paper, we analyze episodes of these disruptions in two countries: Egypt and Libya. Our analysis relies on multiple sources of large-scale data already available to academic researchers: BGP interdomain routing control plane data, unsolicited data plane traffic to unassigned address space, active macroscopic traceroute measurements, RIR delegation files, and MaxMind's geolocation database. We used the latter two data sets to determine which IP address ranges were allocated to entities within each country, and then mapped these IP addresses of interest to BGP-announced address ranges (prefixes) and origin autonomous systems (ASs) using publicly available BGP data repositories in the US and Europe. We then analyzed observable activity related to these sets of prefixes and ASs throughout the censorship episodes. Using both control plane and data plane data sets in combination allowed us to narrow down which forms of Internet access disruption were implemented in a given region over time. Among other insights, we detected what we believe were Libya's attempts to test firewall-based blocking before they executed more aggressive BGP-based disconnection. Our methodology could be used, and automated, to detect outages or similar macroscopically disruptive events in other geographic or topological regions.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {1964–1977},
numpages = {14},
keywords = {outages, network telescope, internet background radiation, darknet, connectivity disruption, censorship}
}

@inproceedings{10.1145/2070364.2070424,
author = {McGrath, Owen G.},
title = {Visualizing user activity in open e-learning contexts: challenges and techniques for operational management},
year = {2011},
isbn = {9781450310239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070364.2070424},
doi = {10.1145/2070364.2070424},
abstract = {This paper describes the practical application of four visualization techniques that have been developed to deal with high-volume, large time-scaled, and high-dimensional data sets that are characteristic of Internet-based user activity. Visualization techniques can be useful for monitoring and studying online user activity in settings where many thousands of users are involved in web-based educational endeavors. Simple numerical summaries of server requests and server performance trends cannot adequately answer the kinds of questions posed by those who desire to understand e-learner activity in web-based e-learning systems. Tracking and understanding remote users and their distant, round-the-clock activities present major technical and analytical challenges, especially in terms of the sheer scale and volume of the generated usage data. With web-based teaching and learning systems, four aspects of the usage tend to hinder analysis: high density, broad time scales, many variables, and veiled patterns. The paper looks at possible solutions in the application of four visualization techniques, using open source statistics software, in the contexts of key scenarios where such techniques can support operations management and decision making processes.},
booktitle = {Proceedings of the 39th Annual ACM SIGUCCS Conference on User Services},
pages = {229–234},
numpages = {6},
keywords = {web usage visualization, usage analytics, data mining},
location = {San Diego, California, USA},
series = {SIGUCCS '11}
}

@inproceedings{10.1109/CCGrid.2011.49,
author = {Tirado, Juan M. and Higuero, Daniel and Isaila, Florin and Carretero, Jesus},
title = {Predictive Data Grouping and Placement for Cloud-Based Elastic Server Infrastructures},
year = {2011},
isbn = {9780769543956},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CCGrid.2011.49},
doi = {10.1109/CCGrid.2011.49},
abstract = {Workload variations on Internet platforms such as YouTube, Flickr, LastFM require novel approaches to dynamic resource provisioning in order to meet QoS requirements, while reducing the Total Cost of Ownership (TCO) of the infrastructures. The economy of scale promise of cloud computing is a great opportunity to approach this problem, by developing elastic large scale server infrastructures. However, a proactive approach to dynamic resource provisioning requires prediction models forecasting future load patterns. On the other hand, unexpected volume and data spikes require reactive provisioning for serving unexpected surges in workloads. When workload can not be predicted, adequate data grouping and placement algorithms may facilitate agile scaling up and down of an infrastructure. In this paper, we analyze a dynamic workload of an on-line music portal and present an elastic Web infrastructure that adapts to workload variations by dynamically scaling up and down servers. The workload is predicted by an autoregressive model capturing trends and seasonal patterns. Further, for enhancing data locality, we propose a predictive data grouping based on the history of content access of a user community. Finally, in order to facilitate agile elasticity, we present a data placement based on workload and access pattern prediction. The experimental results demonstrate that our forecasting model predicts workload with a high precision. Further, the predictive data grouping and placement methods provide high locality, load balance and high utilization of resources, allowing a server infrastructure to scale up and down depending on workload.},
booktitle = {Proceedings of the 2011 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {285–294},
numpages = {10},
keywords = {prediction, elastic, data-grouping, cloud},
series = {CCGRID '11}
}

@inproceedings{10.1145/2076623.2076627,
author = {Brilhante, Igo Ramalho and de Macedo, Jose Antonio Fernandes and Renso, Chiara and Casanova, Marco Antonio},
title = {Trajectory data analysis using complex networks},
year = {2011},
isbn = {9781450306270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2076623.2076627},
doi = {10.1145/2076623.2076627},
abstract = {A massive amount of data on moving object trajectories is available today. However, it is still a major challenge to process such information in order to explain moving object interactions, which could help in revealing non-trivial behavioral patterns. To that end, we consider a complex networks-based representation of trajectory data. Frequent encounters among moving objects (trajectory encounters) are used to create the network edges whereas nodes represent trajectories. A real trajectory dataset of vehicles moving within the City of Milan allows us to study the structure of vehicle interactions and validate our method. We create seven networks and compute the clustering coefficient, and the average shortest path length comparing them with those of the Erd\H{o}s-R\'{e}nyi model. Our analysis shows that all computed trajectory networks have the small world effect and the scale-free feature similar to the internet and biological networks. Finally, we discuss how these results could be interpreted in the light of the traffic application domain.},
booktitle = {Proceedings of the 15th Symposium on International Database Engineering &amp; Applications},
pages = {17–25},
numpages = {9},
keywords = {trajectory, mobility, complex network},
location = {Lisboa, Portugal},
series = {IDEAS '11}
}

@inproceedings{10.1145/2538862.2539014,
author = {Thomas, Vicraj and Riga, Niky and Edwards, Sarah},
title = {GENI as a virtual laboratory for networking and distributed systems classes (abstract only)},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2539014},
doi = {10.1145/2538862.2539014},
abstract = {This hands-on workshop will introduce GENI to instructors of computer networking and distributed systems classes. Instructors can use GENI [http://groups.geni.net/geni/wiki], an easy-to-use virtual laboratory, to improve the educational experiences of their students by having them experiment with new concepts without requiring expensive laboratory facilities. It has been used by over twenty graduate and undergraduate classes. GENI is being used by over 1200 researchers and educators. It enables them to run large-scale, well-instrumented, end-to-end experiments engaging real users. These experiments may be fully compatible with today's Internet, variations or improvements on today's Internet protocols, or indeed radically novel "clean slate" designs. GENI includes compute and communications resources distributed across the United States. GENI is "deeply programmable" i.e. experimenters can install their custom software or operating systems on the compute nodes and can program the behavior of the switches that connect these nodes. GENI is funded by the National Science Foundation and is free to use for research and education. Workshop participants will have the opportunity to set up and run experiments using GENI. They will also learn about class logistics when using GENI and support resources such as ready-to-use exercises. Those doing the hands-on activity will need a laptop running relatively a recent version of Mac OS, Windows or Linux; at least 4GB of memory; a modern processor (at least dual core and faster than 1.5 GHz) and a WiFi interface.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {740},
numpages = {1},
keywords = {virtual laboratory, programmable infrastructure, networking and distributed systems instruction},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/1811039.1811063,
author = {Shah, Devavrat and Zaman, Tauhid},
title = {Detecting sources of computer viruses in networks: theory and experiment},
year = {2010},
isbn = {9781450300384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1811039.1811063},
doi = {10.1145/1811039.1811063},
abstract = {We provide a systematic study of the problem of finding the source of a computer virus in a network. We model virus spreading in a network with a variant of the popular SIR model and then construct an estimator for the virus source. This estimator is based upon a novel combinatorial quantity which we term rumor centrality. We establish that this is an ML estimator for a class of graphs. We find the following surprising threshold phenomenon: on trees which grow faster than a line, the estimator always has non-trivial detection probability, whereas on trees that grow like a line, the detection probability will go to 0 as the network grows. Simulations performed on synthetic networks such as the popular small-world and scale-free networks, and on real networks such as an internet AS network and the U.S. electric power grid network, show that the estimator either finds the source exactly or within a few hops in different network topologies. We compare rumor centrality to another common network centrality notion known as distance centrality. We prove that on trees, the rumor center and distance center are equivalent, but on general networks, they may differ. Indeed, simulations show that rumor centrality outperforms distance centrality in finding virus sources in networks which are not tree-like.},
booktitle = {Proceedings of the ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {203–214},
numpages = {12},
keywords = {estimation, epidemics},
location = {New York, New York, USA},
series = {SIGMETRICS '10}
}

@inproceedings{10.1145/2660859.2660950,
author = {Rathi, Sheetal and Dhote, C. A. and Bangera, Vivek},
title = {Parallel Pre-processing for XML mining using Graphic Processor},
year = {2014},
isbn = {9781450329088},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660859.2660950},
doi = {10.1145/2660859.2660950},
abstract = {With the advent of ever increasing data availability on internet, mining and converting the information into knowledge is becoming extremely important and challenging task for researchers in data mining community. Mining of association rules is considered as an important research direction of data mining. XML is being extensively and pre-dominantly used as a markup language on web and thus makes it an interesting source for data extraction from large data sets. There is a growing demand for modern tools and technologies which can efficiently handle such large data. This paper proposes a collaborative approach to extract association rules from structured XML data with the help of cost effective, easily affordable and energy efficient Graphic Processors. Parallelism is applied at two levels in our proposed framework. First the deserialization of XML data is done using a parallel approach. Secondly the in-built multithreaded structure of GPU sorts the converted XML data in the pre-processing stage to make the dataset favorable for mining. Using a parallel framework in form of inbuilt hardware based GPU; we try to handle the scalability issue upto a large extent.},
booktitle = {Proceedings of the 2014 International Conference on Interdisciplinary Advances in Applied Computing},
articleno = {38},
numpages = {7},
keywords = {XPath, XML mining, Parallel data Mining, Graphic Processing Unit (GPU), FP-Growth},
location = {Amritapuri, India},
series = {ICONIAAC '14}
}

@inproceedings{10.5555/2381147.2381160,
author = {Jagadeesan, A. P. and Wenzel, J. and Corney, J. R. and Yan, X. and Sherlock, A. and Torres-Sanchez, C. and Regli, W.},
title = {Fast human classification of 3D object benchmarks},
year = {2010},
isbn = {9783905674224},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {Although a significant number of benchmark data sets for 3D object based retrieval systems have been proposed over the last decade their value is dependent on a robust classification of their content being available. Ideally researchers would want hundreds of people to have classified thousands of parts and the results recorded in a manner that explicitly shows how the similarity assessments varies with the precision used to make the judgement. This paper reports a study which investigated the proposition that Internet Crowdsourcing could be used to quickly and cheaply provide benchmark classifications of 3D shapes. The collective judgments of the anonymous workers produce a classification that has surprisingly fine granularity and precision. The paper reports the results of validating Crowdsourced judgements of 3D similarity against Purdue's ESB and concludes with an estimate of the overall costs associated with large scale classification tasks involving many tens of thousands of models.},
booktitle = {Proceedings of the 3rd Eurographics Conference on 3D Object Retrieval},
pages = {55–62},
numpages = {8},
location = {Norrk\"{o}ping, Sweden},
series = {3DOR '10}
}

@article{10.14778/2733004.2733012,
author = {Zhang, Zhuo and Li, Chao and Tao, Yangyu and Yang, Renyu and Tang, Hong and Xu, Jie},
title = {Fuxi: a fault-tolerant resource management and job scheduling system at internet scale},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733012},
doi = {10.14778/2733004.2733012},
abstract = {Scalability and fault-tolerance are two fundamental challenges for all distributed computing at Internet scale. Despite many recent advances from both academia and industry, these two problems are still far from settled. In this paper, we present Fuxi, a resource management and job scheduling system that is capable of handling the kind of workload at Alibaba where hundreds of terabytes of data are generated and analyzed everyday to help optimize the company's business operations and user experiences. We employ several novel techniques to enable Fuxi to perform efficient scheduling of hundreds of thousands of concurrent tasks over large clusters with thousands of nodes: 1) an incremental resource management protocol that supports multi-dimensional resource allocation and data locality; 2) user-transparent failure recovery where failures of any Fuxi components will not impact the execution of user jobs; and 3) an effective detection mechanism and a multi-level blacklisting scheme that prevents them from affecting job execution. Our evaluation results demonstrate that 95% and 91% scheduled CPU/memory utilization can be fulfilled under synthetic workloads, and Fuxi is capable of achieving 2.36T-B/minute throughput in GraySort. Additionally, the same Fuxi job only experiences approximately 16% slowdown under a 5% fault-injection rate. The slowdown only grows to 20% when we double the fault-injection rate to 10%. Fuxi has been deployed in our production environment since 2009, and it now manages hundreds of thousands of server nodes.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1393–1404},
numpages = {12}
}

@inproceedings{10.1145/2379690.2379701,
author = {Fischer, Fabian and Fuchs, Johannes and Vervier, Pierre-Antoine and Mansmann, Florian and Thonnard, Olivier},
title = {VisTracer: a visual analytics tool to investigate routing anomalies in traceroutes},
year = {2012},
isbn = {9781450314138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379690.2379701},
doi = {10.1145/2379690.2379701},
abstract = {Routing in the Internet is vulnerable to attacks due to the insecure design of the border gateway protocol (BGP). One possible exploitation of this insecure design is the hijacking of IP blocks. Such hijacked IP blocks can then be used to conduct malicious activities from seemingly legitimate IP addresses. In this study we actively trace and monitor the routes to spam sources over several consecutive days after having received a spam message from such a source. However, the real challenge is to distinguish between legitimate routing changes and those ones that are related to systematic misuse in so-called spam campaigns. To combine the strengths of human judgement and computational efficiency, we thus present a novel visual analytics tool named Vistracer in this paper. This tool represents analysis results of our anomaly detection algorithms on large traceroute data sets with the help of several scalable representations to support the analyst to explore, identify and analyze suspicious events and their relations to malicious activities. In particular, pixel-based visualization techniques, novel glyph-based summary representations and a combination of temporal glyphs in a graph representation are used to give an overview of route changes to specific destinations over time. To evaluate our tool, real-world case studies demonstrate the usage of Vistracer in practice on large-scale data sets.},
booktitle = {Proceedings of the Ninth International Symposium on Visualization for Cyber Security},
pages = {80–87},
numpages = {8},
keywords = {visual analytics, traceroutes, network security, anomalies},
location = {Seattle, Washington, USA},
series = {VizSec '12}
}

@inproceedings{10.5555/2602339.2602350,
author = {Sch\"{a}fer, Matthias and Strohmeier, Martin and Lenders, Vincent and Martinovic, Ivan and Wilhelm, Matthias},
title = {Bringing up OpenSky: a large-scale ADS-B sensor network for research},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {Automatic Dependent Surveillance-Broadcast (ADS-B) is one of the key components of the next generation air transportation system. Since ADS-B will become mandatory by 2020 for most airspaces, it is important that aspects such as capacity, applications, and security are investigated by an independent research community. However, large-scale real-world data was previously only accessible to a few closed industrial and governmental groups because it required specialized and expensive equipment. To enable researchers to conduct experimental studies based on real data, we developed OpenSky, a sensor network based on low-cost hardware connected over the Internet.OpenSky is based on off-the-shelf ADS-B sensors distributed to volunteers throughout Central Europe. It covers 720,000 sq km2, is able to capture more than 30% of the commercial air traffic in Europe, and enables researchers to analyze billions of ADS-B messages. In this paper, we report on the challenges we faced during the development and deployment of this participatory network and the insights we gained over the last two years of operations as a service to academic research groups. We go on to provide real-world insights about the possibilities and limitations of such low-cost sensor networks concerning air traffic surveillance and further applications such as multilateration.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {83–94},
numpages = {12},
keywords = {sensor networks, opensky, nextgen, air traffic control, ads-b},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.5555/1996039.1996046,
author = {Wang, Zhi and Wu, Chuan and Sun, Lifeng and Yang, Shiqiang},
title = {Peer-assisted online games with social reciprocity},
year = {2011},
publisher = {IEEE Press},
abstract = {Online games and social networks are cross-pollinating rapidly in today's Internet: Online social network sites are deploying more and more games in their systems, while online game providers are leveraging social networks to power their games. An intriguing development as it is, the operational challenge in the previous game persists, i. e., the large server operational cost remains a non-negligible obstacle for deploying high-quality multi-player games. Peer-to-peer based game network design could be a rescue, only if the game players' mutual resource contribution has been fully incentivized and efficiently scheduled. Exploring the unique advantage of social network based games (social games), we advocate to utilize social reciprocities among peers with social relationships for efficient contribution incentivization and scheduling, so as to power a high-quality online game with low server cost. In this paper, social reciprocity is exploited with two give-and-take ratios at each peer: (1) peer contribution ratio (PCR), which evaluates the reciprocity level between a pair of social friends, and (2) system contribution ratio (SCR), which records the give-and-take level of the player to and from the entire network. We design efficient peer-to-peer mechanisms for game state distribution using the two ratios, where each player optimally decides which other players to seek relay help from and help in relaying game states, respectively, based on combined evaluations of their social relationship and historical reciprocity levels. Our design achieves effective incentives for resource contribution, load balancing among relay peers, as well as efficient social-aware resource scheduling. We also discuss practical implementation concerns and implement our design in a prototype online social game. Our extensive evaluations based on experiments on PlanetLab verify that high-quality large-scale social games can be achieved with conservative server costs.},
booktitle = {Proceedings of the Nineteenth International Workshop on Quality of Service},
articleno = {5},
numpages = {9},
location = {San Jose, California},
series = {IWQoS '11}
}

@inproceedings{10.1145/1859995.1860034,
author = {Keralapura, Ram and Nucci, Antonio and Zhang, Zhi-Li and Gao, Lixin},
title = {Profiling users in a 3g network using hourglass co-clustering},
year = {2010},
isbn = {9781450301817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1859995.1860034},
doi = {10.1145/1859995.1860034},
abstract = {With widespread popularity of smart phones, more and more users are accessing the Internet on the go. Understanding mobile user browsing behavior is of great significance for several reasons. For example, it can help cellular (data) service providers (CSPs) to improve service performance, thus increasing user satisfaction. It can also provide valuable insights about how to enhance mobile user experience by providing dynamic content personalization and recommendation, or location-aware services.In this paper, we try to understand mobile user browsing behavior by investigating whether there exists distinct "behavior patterns" among mobile users. Our study is based on real mobile network data collected from a large 3G CSP in North America. We formulate this user behavior profiling problem as a "co-clustering" problem, i.e., we group both users (who share similar browsing behavior), and browsing profiles (of like-minded users) simultaneously. We propose and develop a scalable co-clustering methodology, Phantom, using a novel hourglass model. The proposed hourglass model first reduces the dimensions of the input data and performs divisive hierarchical co-clustering on the lower dimensional data; it then carries out an expansion step that restores the original dimensions. Applying Phantom to the mobile network data, we find that there exists a number of prevalent and distinct behavior patterns that persist over time, suggesting that user browsing behavior in 3G cellular networks can be captured using a small number of co-clusters. For instance, behavior of most users can be classified as either homogeneous (users with very limited set of browsing interests) or heterogeneous (users with very diverse browsing interests), and such behavior profiles do not change significantly at either short (30-min) or long (6 hour) time scales.},
booktitle = {Proceedings of the Sixteenth Annual International Conference on Mobile Computing and Networking},
pages = {341–352},
numpages = {12},
keywords = {phantom bi-clustering, hourglass model, hierarchical coclustering},
location = {Chicago, Illinois, USA},
series = {MobiCom '10}
}

@article{10.1145/2096149.2096161,
author = {Duerig, Jonathon and Ricci, Robert and Stoller, Leigh and Strum, Matt and Wong, Gary and Carpenter, Charles and Fei, Zongming and Griffioen, James and Nasir, Hussamuddin and Reed, Jeremy and Wu, Xiongqi},
title = {Getting started with GENI: a user tutorial},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0146-4833},
url = {https://doi.org/10.1145/2096149.2096161},
doi = {10.1145/2096149.2096161},
abstract = {GENI, the Global Environment for Network Innovations, is a National Science Foundation project to create a "virtual laboratory at the frontiers of network science and engineering for exploring future internets at scale." It provides researchers, educators, and students with resources that they can use to build their own networks that span the country and---through federation---the world. GENI enables experimenters to try out bold new network architectures and designs for networked systems, and to deploy and evaluate these systems on a diverse set of resources over a large footprint.This tutorial is a starting point for running experiments on GENI. It provides an overview of GENI and covers the process of creating a network and running a simple experiment using two tools: the Flack GUI and the INSTOOLS instrumentation service.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jan},
pages = {72–77},
numpages = {6},
keywords = {virtualization, testbed, instrumentation, geni}
}

@article{10.1109/TNET.2010.2051233,
author = {Vojnovi\'{c}, Milan and Gupta, Varun and Karagiannis, Thomas and Gkantsidis, Christos},
title = {Sampling strategies for epidemic-style information dissemination},
year = {2010},
issue_date = {August 2010},
publisher = {IEEE Press},
volume = {18},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2010.2051233},
doi = {10.1109/TNET.2010.2051233},
abstract = {We consider epidemic-style information dissemination strategies that leverage the nonuniformity of host distribution over subnets (e.g., IP subnets) to optimize the information spread. Such epidemic-style strategies are based on random sampling of target hosts according to a sampling rule. In this paper, we consider the metric of total number of samplings (equivalently probes) to reach a given target fraction of the host population. We first identify the minimum number of samplings needed to reach a target fraction of hosts, assuming global information about the host distribution over subnets is available. We show that this optimum can be achieved either by a dynamic strategy, for which the sampling probabilities over subnets are allowed to vary over time, or, surprisingly, even by a static strategy, for which the sampling probabilities over subnets are fixed. These results provide insights about the best achievable performance and how different system parameters affect the number of sampling needed. We then consider simple online sampling strategies that do not require any prior knowledge of the distribution of hosts over subnets, but where each host biases sampling based on its observed sampling outcomes while keeping only O(1) state at any point in time. Using real data-sets from several large-scale Internet measurements, we evaluate significance of the system parameters that determine the sampling requirements and compare the performance of our proposed distribution-oblivious sampling strategies to the theoretical bound. Our results provide insights for the design of efficient information dissemination systems, as well as for the design of countermeasures against worms that use subnet-preferential scanning.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1013–1025},
numpages = {13},
keywords = {sampling, information dissemination, epidemics}
}

@article{10.1145/1944339.1944342,
author = {Meiss, Mark and Menczer, Filippo and Vespignani, Alessandro},
title = {Properties and Evolution of Internet Traffic Networks from Anonymized Flow Data},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/1944339.1944342},
doi = {10.1145/1944339.1944342},
abstract = {Many projects have tried to analyze the structure and dynamics of application overlay networks on the Internet using packet analysis and network flow data. While such analysis is essential for a variety of network management and security tasks, it is infeasible on many networks: either the volume of data is so large as to make packet inspection intractable, or privacy concerns forbid packet capture and require the dissociation of network flows from users’ actual IP addresses. Our analytical framework permits useful analysis of network usage patterns even under circumstances where the only available source of data is anonymized flow records. Using this data, we are able to uncover distributions and scaling relations in host-to-host networks that bear implications for capacity planning and network application design. We also show how to classify network applications based entirely on topological properties of their overlay networks, yielding a taxonomy that allows us to accurately identify the functions of unknown applications. We repeat this analysis on a more recent dataset, allowing us to demonstrate that the aggregate behavior of users is remarkably stable even as the population changes.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {15},
numpages = {23},
keywords = {traffic statistics, power-law networks, latitudinal analysis, functional networks, evolution of networks, behavioral networks, application networks, application identification, Network flows, Internet usage}
}

@inproceedings{10.1145/2492002.2482609,
author = {Engelberg, Roee and Fabrikant, Alex and Schapira, Michael and Wajc, David},
title = {Best-response dynamics out of sync: complexity and characterization},
year = {2013},
isbn = {9781450319621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492002.2482609},
doi = {10.1145/2492002.2482609},
abstract = {In many computational and economic models of multi-agent interaction, each participant repeatedly "best-responds" to the others' actions. Game theory research on the prominent "best-response dynamics" model typically relies on the premise that the interaction between agents is somehow synchronized. However, in many real-life settings, e.g., internet protocols and large-scale markets, the interaction between participants is asynchronous. We tackle the following important questions: (1) When are best-response dynamics guaranteed to converge to an equilibrium even under asynchrony? (2) What is the (computational and communication) complexity of verifying guaranteed convergence? We show that, in general, verifying guaranteed convergence is intractable. In fact, our main negative result establishes that this task is undecidable. We exhibit, in contrast, positive results for several environments of interest, including complete, computationally-tractable, characterizations of convergent systems. We discuss the algorithmic implications of our results, which extend beyond best-response dynamics to applications such as asynchronous Boolean circuits.},
booktitle = {Proceedings of the Fourteenth ACM Conference on Electronic Commerce},
pages = {379–396},
numpages = {18},
keywords = {game theory, convergence, complexity, best response dynamics, asynchronous models},
location = {Philadelphia, Pennsylvania, USA},
series = {EC '13}
}

@inproceedings{10.1145/2181196.2181203,
author = {Fatemi, Maryam and Tokarchuk, Laurissa},
title = {An empirical study on IMDb and its communities based on the network of co-reviewers},
year = {2012},
isbn = {9781450311632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2181196.2181203},
doi = {10.1145/2181196.2181203},
abstract = {The advent of business oriented and social networking sites on the Internet have seen a huge increase in number of people using them in recent years. With the expansion of Web 2.0, new types of websites have emerged such as online social networks, blogs and wikis. Their popularity has resulted in exponential growth of information on the web and interactions overload thus making it harder to access useful or relevant information. Recommender systems are one of the applications employed to address this problem by filtering relevant information and enhancing user experience. They traditionally use either the content of items of the websites (content-filtering recommender systems) or the collaboration between the users and items such as rating (collaborative-filtering recommender systems) or a combination of them (hybrid recommender systems). However due to the nature of data they use, they all have one or more weaknesses such as cold start, sparsity of data, scalability problems and overspecialised recommendation. Social networks and other similar websites have new types of data which can be used in recommender systems thus have the potential to overcome these shortcomings. However without a good understanding of the properties and structure of these online social websites, the applications can not be accurate. This paper presents an empirical measurement study of the properties and structure of one such social websites. It examines an online movie database, and the interactions between reviewers and attempts to construct a social network graph based on the network of reviewers. The resulting network is confirmed as the power-law, small-world and scale-free. It identifies the highly connected clusters and shows that the content of these subgroups are diversified and not limited to similar tags. Finally the implication of these finding is discussed in order to enhance current recommender systems enabling them to provide diverse results while overcome their shortcomings.},
booktitle = {Proceedings of the First Workshop on Measurement, Privacy, and Mobility},
articleno = {7},
numpages = {6},
keywords = {web 2.0, social network, community detections},
location = {Bern, Switzerland},
series = {MPM '12}
}

@inproceedings{10.5555/2399776.2399816,
author = {Jourdan, Guy-Vincent and Mesbah, Ali},
title = {Workshop on Mobile and Rich Internet Application Model Generation},
year = {2012},
publisher = {IBM Corp.},
address = {USA},
abstract = {Two relatively recent developments are drastically changing the way software applications are created, deployed, and used by end-users. The first one is coined "Rich Internet Applications" (RIA). Over the past 10 years, Web-based applications have become the norm. They are easy to deploy and update, since virtually every users have a Web browser installed in their system. However, until recently, the user experience was not as smooth as it could have been with a desktop application. This was changed with the adoption of RIA, where the client-side communicates asynchronously with the server, and updates parts of the user interface in the browser partially as needed. RIA's enable moving away from the static page-sequence model of traditional web applications, and as such, provide a tremendous gain in end-user experience. A RIA "feels" like a desktop application, without the hassle of installation and upgrade. This revolutionizes the way software applications are engineered, providing relative platform-independence at low cost for software engineers.Meanwhile, another more recent evolution in computing is the move to everything mobile. According to recent estimations, by 2015 over 70 percent of all handset shipments will be smartphones, capable of running mobile applications. With smartphones as powerful as desktop computers, extensive fast network connection, excellent graphical display and access to sensors that open the door to new, richer software, mobile applications are bound to replace traditional desktop applications in the coming years. Currently, there are over 600,000 mobile applications on Apple's AppStore and more than 400,000 on Android Market. Some of the challenges involved in mobile application development include handling different devices, multiple operating systems (Android, Apple iOS, Windows Mobile), and different programming languages (Java, Objective-C, Visual C++). Moreover, mobile applications are developed mostly in small-scale, fast-paced projects to meet the competitive market demands. Also, the fact that it can take up to a week for a deployed application to get updated (e.g., on AppStore), puts an indirect pressure on developers to understand and check the quality of their applications before deployment.With the ever increasing demands of Web and smartphone users for dependable new applications, novel software engineering techniques and tools geared towards the Web and mobile platforms are required to support developers in their program comprehension, maintenance, analysis and testing tasks.The challenges of analyzing these applications include:• RIAs are known to be challenging to crawl automatically. A web application that cannot be crawled cannot be indexed, and thus is not searchable. In fact, most of generated RIA content ends up in the hidden-web currently, which hinders the end-user's ability to find the corresponding information using general search engines.• Mobile and rich internet applications are difficult to analyze and model automatically. An application that cannot be modeled accurately cannot be tested for functional properties, or nonfunctional properties such as security and usability. Without dedicated analysis techniques and tools, we risk flooding the market with poorly engineered software, at great cost for all of us.},
booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {265–266},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {CASCON '12}
}

@article{10.1145/2427036.2427038,
author = {Lee, Yeonhee and Lee, Youngseok},
title = {Toward scalable internet traffic measurement and analysis with Hadoop},
year = {2012},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0146-4833},
url = {https://doi.org/10.1145/2427036.2427038},
doi = {10.1145/2427036.2427038},
abstract = {Internet traffic measurement and analysis has long been used to characterize network usage and user behaviors, but faces the problem of scalability under the explosive growth of Internet traffic and high-speed access. Scalable Internet traffic measurement and analysis is difficult because a large data set requires matching computing and storage resources. Hadoop, an open-source computing platform of MapReduce and a distributed file system, has become a popular infrastructure for massive data analytics because it facilitates scalable data processing and storage services on a distributed computing system consisting of commodity hardware. In this paper, we present a Hadoop-based traffic monitoring system that performs IP, TCP, HTTP, and NetFlow analysis of multi-terabytes of Internet traffic in a scalable manner. From experiments with a 200-node testbed, we achieved 14 Gbps throughput for 5 TB files with IP and HTTP-layer analysis MapReduce jobs. We also explain the performance issues related with traffic analysis MapReduce jobs.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jan},
pages = {5–13},
numpages = {9},
keywords = {traffic measurement, pcap, packet, netflow, mapreduce, hive, hadoop, analysis}
}

@inproceedings{10.1145/1989323.1989434,
author = {Hamilton, James},
title = {Internet scale storage},
year = {2011},
isbn = {9781450306614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989323.1989434},
doi = {10.1145/1989323.1989434},
abstract = {The pace of innovation in data center design has been rapidly accelerating over the last five years, driven by the mega-service operators. I believe we have seen more infrastructure innovation in the last five years than we did in the previous fifteen. Most very large service operators have teams of experts focused on server design, data center power distribution and redundancy, mechanical designs, real estate acquisition, and network hardware and protocols. At low scale, with only a data or center or two, it would be crazy to have all these full time engineers and specialist focused on infrastructural improvements and expansion. But, at high scale with tens of data centers, it would be crazy not to invest deeply in advancing the state of the art. Looking specifically at cloud services, the cost of the infrastructure is the difference between an unsuccessful cloud service and a profitable, self-sustaining business. With continued innovation driving down infrastructure costs, investment capital is available, services can be added and improved, and value can be passed on to customers through price reductions. Amazon Web Services, for example, has had eleven price reductions in four years. I don't recall that happening in my first twenty years working on enterprise software. It really is an exciting time in our industry. I started working on database systems twenty years ago during a period of incredibly rapid change. We improved DB2 performance measured using TPC-A by a factor of ten in a single release. The next release, we made a further four-fold improvement. It's rare to be able to improve a product by forty fold in three years but, admittedly, one of the secrets is to begin from a position where work is truly needed. Back then, the database industry was in its infancy. Customers loved the products and were using them heavily, but we were not anywhere close to delivering on the full promise of the technology. That's exactly where cloud computing is today--just where the database world was twenty years ago. Customers are getting great value from cloud computing but, at the same time, we have much more to do and many of the most interesting problems are yet to be solved. I could easily imagine tenfold improvement across several dimensions in over the next five years. What ties these two problems from different decades together is that some of the biggest problems in cloud computing are problems in persistent state management. What's different is that we now have to tackle these problems in a multi-tenant, high-scale, multi-datacenter environment. It's a new vista for database and storage problems. In this talk, we'll analyze an internet-scale data center looking at the cost of power distribution, servers, storage, networking, and cooling on the belief that understanding what drives cost helps us focus on the most valuable research directions. We'll look at some of the fundamental technology limits approached in cloud database and storage solutions on the belief that, at scale, these limits will constrain practical solutions. And we'll consider existing cloud services since they form the foundation on which future solutions might be built.},
booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
pages = {1047–1048},
numpages = {2},
keywords = {storage},
location = {Athens, Greece},
series = {SIGMOD '11}
}

@inproceedings{10.1145/2389376.2389383,
author = {Militaru, Dorin and Zaharia, Costin},
title = {A survey of collaborative filtering-based systems for online recommendation},
year = {2010},
isbn = {9781450314275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2389376.2389383},
doi = {10.1145/2389376.2389383},
abstract = {Internet services operate on a vastly larger scale and permit virtual interactions. The Internet and Web has created vast new opportunities, providing an infrastructure that enables buyers and sellers to find each other online. Companies can now offer many products, services and information easily and with lower costs. It becomes more and more difficult for customers to find quickly what they are looking for. Nevertheless, recommendation systems are playing a major role. Collaborative filtering (CF), or recommender system based-CF, has appeared as one methodology designed to perform such a recommendation task. These systems allow people to use expressed preferences of thousands of other people in order to find the product they desire based on the level of similarity between tastes. The concept has appeared from convergent research on search browsers, intelligent agents and data mining, and it allows to avoid the difficult question of "why" consumers prefer this or that product or brand.Early studies of electronic markets tools and recommender systems took a simplistic view of consumers as economic agents whose behavior was guided by the search for the lowest cost transactions. Moreover, most studies take into account only technical aspects of these systems like algorithms' development and computational problems. No study had been interested in recommendation's efficiency of collaborative filtering-based systems.This article explores the current state of research in recommender systems-based collaborative filtering, and proposes an experiment to find if such electronic recommendations are better than human recommendation.},
booktitle = {Proceedings of the 12th International Conference on Electronic Commerce: Roadmap for the Future of Electronic Business},
pages = {43–47},
numpages = {5},
keywords = {recommender systems, marketing, experimentation, electronic commerce, collaborative filtering},
location = {Honolulu, Hawaii, USA},
series = {ICEC '10}
}

@inproceedings{10.1145/1755952.1755985,
author = {Bestavros, Azer and Kfoury, Assaf and Lapets, Andrei and Ocean, Michael J.},
title = {Safe compositional network sketches: formal framework},
year = {2010},
isbn = {9781605589558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1755952.1755985},
doi = {10.1145/1755952.1755985},
abstract = {NetSketch is a tool for the specification of constrained-flow applications and the certification of desirable safety properties imposed thereon. NetSketch assists system integrators in two types of activities: modeling and design. As a modeling tool, it enables the abstraction of an existing system while retaining sufficient information about it to carry out future analysis of safety properties. As a design tool, NetSketch enables the exploration of alternative safe designs as well as the identification of minimal requirements for outsourced subsystems. NetSketch embodies a lightweight formal verification philosophy, whereby the power (but not the heavy machinery) of a rigorous formalism is made accessible to users via a friendly interface. NetSketch does so by exposing tradeoffs between exactness of analysis and scalability, and by combining traditional whole-system analysis with a more flexible compositional analysis. The compositional analysis is based on a strongly-typed Domain-Specific Language (DSL) for describing and reasoning about constrained-flow networks at various levels of sketchiness along with invariants that need to be enforced thereupon. In this paper, we define the formal system underlying the operation of NetSketch, in particular the DSL behind NetSketch's user-interface when used in "sketch mode", and prove its soundness relative to appropriately-defined notions of validity. In a companion paper [7], we overview NetSketch, highlight its salient features, and illustrate how it could be used in applications that include: the management/shaping of traffic flows in a vehicular network (as a proxy for cyber-physical systems (CPS) applications) and a streaming media network (as a proxy for Internet applications).},
booktitle = {Proceedings of the 13th ACM International Conference on Hybrid Systems: Computation and Control},
pages = {231–241},
numpages = {11},
keywords = {verification, typing, modeling, design, compositionality},
location = {Stockholm, Sweden},
series = {HSCC '10}
}

@techreport{10.5555/2581998,
author = {Albrecht, Jeannie},
title = {Designing Tools and Curricula for Undergraduate Courses in Distributed Systems},
year = {2012},
publisher = {National Science Foundation},
address = {USA},
abstract = {Distributed applications have become a core component of the Internet's infrastructure. However, many undergraduate curricula do not offer courses that focus on the design and implementation of distributed systems. As a result, undergraduates are not as prepared as they should be for graduate study or careers in industry. Historically, the problem has often been caused by a lack of resources, since many schools do not have the computing infrastructure needed to experiment with distributed systems. However, with the growing availability and accessibility of academic and industrial distributed computing platforms, this is no longer true. Even colleges with limited on-campus computing facilities now have the ability to experiment with large-scale systems using state-of-the-art networking technologies. This workshop focused on developing and disseminating new tools and curricula for undergraduate courses in distributed systems and computer networks that leverage the resources available in publicly-accessible testbeds. The 31 attendees came from a variety of backgrounds, including top-tier research universities, liberal arts colleges, and industry. This report summarizes the results of the main discussions and presentations from the workshop.}
}

@inproceedings{10.1145/1873951.1873956,
author = {Shen, Yi and Fan, Jianping},
title = {Leveraging loosely-tagged images and inter-object correlations for tag recommendation},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1873956},
doi = {10.1145/1873951.1873956},
abstract = {Large-scale loosely-tagged images (i.e., multiple object tags are given loosely at the image level) are available on Internet, and it is very attractive to leverage such loosely-tagged images for automatic image annotation applications. In this paper, a multi-task structured SVM algorithm is developed to leverage both the inter-object correlations and the loosely-tagged images for achieving more effective training of a large number of inter-related object classifiers. To leverage the loosely-tagged images for object classifier training, each loosely-tagged image is partitioned into a set of image instances (image regions) and a multiple instance learning algorithm is developed for instance label identification by automatically identifying the correspondences between multiple tags (given at the image level) and the image instances. An object correlation network is constructed for characterizing the inter-object correlations explicitly and identifying the inter-related learning tasks automatically. To enhance the discrimination power of a large number of inter-related object classifiers, a multi-task structured SVM algorithm is developed to model the inter-task relatedness more precisely and leverage the inter-object correlations for classifier training. Our experiments on a large number of inter-related object classes have provided very positive results.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {5–14},
numpages = {10},
keywords = {object correlation network, multiple instance learning, multi-task structured svm, loosely-tagged images},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{10.1145/2500423.2500430,
author = {Liu, Shu and Striegel, Aaron D.},
title = {Exploring the potential in practice for opportunistic networks amongst smart mobile devices},
year = {2013},
isbn = {9781450319997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2500423.2500430},
doi = {10.1145/2500423.2500430},
abstract = {Wireless network providers are under tremendous pressure to deliver unprecedented amounts of data to a variety of mobile devices. A powerful concept that has only gained limited traction in practice has been the concept of opportunistic networks whereby nodes opportunistically communicate with each other when in range to augment or overcome existing wireless systems. One of the key impediments towards the adoption of opportunistic communications has been the inability to demonstrate viability at scale, namely showing that sufficient opportunities exist and more importantly exist when needed to offer significant network performance gains. We demonstrate through a large-scale, longitudinal study of smartphone users that significant opportunities are indeed prevalent, are indeed stable, and end up being reasonably reciprocal both on short and long-term timescales. In this paper, we propose a framework dubbed PSR (Prevalence, Stability, Reciprocity) to capture key aspects that characterize the net potential for opportunistic networks which we feel merit significantly increased attention.},
booktitle = {Proceedings of the 19th Annual International Conference on Mobile Computing &amp; Networking},
pages = {315–326},
numpages = {12},
keywords = {wifi, relay, proximity, opportunistic networks, bluetooth},
location = {Miami, Florida, USA},
series = {MobiCom '13}
}

@inproceedings{10.1145/2371536.2371569,
author = {Frachtenberg, Eitan},
title = {High efficiency at web scale},
year = {2012},
isbn = {9781450315203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371536.2371569},
doi = {10.1145/2371536.2371569},
abstract = {Every day, over half a billion people log in to Facebook to communicate with their contacts. They exchange more than 300 million photos and more than 3 billion likes and comments each day. And almost every day, Facebook releases new code with new features and products to all these users. This staggering amount of information and processing is served from dozens of clusters in four geographical regions. The keys to operating successfully at this almost incomprehensibly large scale are efficiency and automation. Efficiency starts at the hundreds Facebook engineers and the processes they use to develop, test, and deploy code; it continues with scalable models of distributing and constantly monitoring the software on tens of thousands of servers on a daily basis; and ends at the very hardware and datacenters that serves this data, bringing capital and operational expenditures down to make the economic model viable. Automation is the leverage behind each of these relatively few engineers. It lets them focus on quick iteration and experimentation, catching problems early and solving many automatically. This talk will describe the challenges of developing and operating a product that serves a significant percentage of the worldwide internet population. Through several examples, we will see how efficiency and automation drive and enable operation at Web scale.},
booktitle = {Proceedings of the 9th International Conference on Autonomic Computing},
pages = {179–180},
numpages = {2},
keywords = {web, scale, efficiency, automation},
location = {San Jose, California, USA},
series = {ICAC '12}
}

@inproceedings{10.1145/2464464.2464523,
author = {Park, Souneil and Ko, Minsam and Lee, Jaeung and Choi, Aram and Song, Junehwa},
title = {Challenges and opportunities of local journalism: a case study of the 2012 Korean general election},
year = {2013},
isbn = {9781450318891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464464.2464523},
doi = {10.1145/2464464.2464523},
abstract = {Local journalism is a vital element for governance and civic engagement in local communities. It not only serves as an accountability mechanism for key local institutions but also enhances communities' problem solving skills by setting important agendas and opening public discussions. However, the Internet is posing serious challenges to local news outlets that lead to a significant reduction of accountability reporting and decreasing revenue. In this paper, we study the challenges and opportunities of local journalism in online communication space. We first observe the status of local journalism in online discourse by analyzing the public attention to local news outlets. Using Twitter to capture online public discourse, the study provides a large-scale quantitative analysis of the public's attention to local journalism. We also explore the potential to promote local journalism with information systems through a real deployment study. We propose a context-specific, decision-oriented design approach for local journalism, and evaluate it using Informed Citizen, a web portal to local journalistic contents of electoral districts.},
booktitle = {Proceedings of the 5th Annual ACM Web Science Conference},
pages = {286–295},
numpages = {10},
keywords = {web-based interactive systems, social media, local journalism},
location = {Paris, France},
series = {WebSci '13}
}

@inproceedings{10.1145/2556325.2567848,
author = {Speck, Jacquelin and Gualtieri, Eugene and Naik, Gaurav and Nguyen, Thach and Cheung, Kevin and Alexander, Larry and Fenske, David},
title = {ForumDash: analyzing online discussion forums},
year = {2014},
isbn = {9781450326698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556325.2567848},
doi = {10.1145/2556325.2567848},
abstract = {Since introducing Internet-based distance education programs in 1996, Drexel University has gained recognition as an online education leader. Remaining at the vanguard means finding innovative, automated solutions to determine which students are contributing to thoughtful discussion, helping faculty engage with online students more efficiently, and spending less time managing ever more complex Learning Management Systems (LMS). We introduce ForumDash, a BBLearn plugin for the Blackboard LMS1, designed to enhance online learning. Through its three visualization tools, ForumDash shows instructors which students are contributing, struggling, or distracted, thereby helping instructors target their efforts, save time managing online courses, and scale course tools up to the level of Massive Open Online Courses (MOOCs). ForumDash also provides students with performance feedback, showing them whether their participation levels are satisfactory. Initial testing with two Drexel University Online courses produced positive feedback, and larger scale testing is in progress.},
booktitle = {Proceedings of the First ACM Conference on Learning @ Scale Conference},
pages = {139–140},
numpages = {2},
keywords = {online learning, learning management systems, information visualization},
location = {Atlanta, Georgia, USA},
series = {L@S '14}
}

@inproceedings{10.4108/ICST.SIMUTOOLS2010.8677,
author = {Garcia, Alberto E. and Weidlich, Roman and de Lope, Laura Rodriguez and Hackbarth, Klaus D. and Hlavacs, Helmut and Leandro, Caridad San},
title = {Approximation towards energy-efficient distributed environments},
year = {2010},
isbn = {9789639799875},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/ICST.SIMUTOOLS2010.8677},
doi = {10.4108/ICST.SIMUTOOLS2010.8677},
abstract = {Peer-To-Peer (P2P) traffic represents almost 60 % of Internet traffic, and involves personal computers working during periods of full-time operation (Always-ON). The rational use of shared resources opens up the possibility of reducing energy consumption associated with this type of operation. This paper analyses the results previously obtained from simulating an environment that shares resources among multimedia home networks, called Virtual Home Environment (VHE) and sponsored by the European Network of Excellence EuroFGI [1]. The study analyzes the behavior of basic P2P distributed environments, comparing them to the new environment based on the virtualization of specific processes. This proposal includes a cost model that enables the restriction of behaviors associated with hoarding of resources. As a result, the energy consumption associated with this improvement implies a substantial reduction in the number of Always-ON devices, and the reduction and equalization of activity time around the area of the distributed network. The proposed simulation will be applied in future developments related with the activities proposed by Cost Action IC0804: Energy efficiency in large-scale distributed systems, see [2].},
booktitle = {Proceedings of the 3rd International ICST Conference on Simulation Tools and Techniques},
articleno = {81},
numpages = {7},
keywords = {virtualization, virtual home environment, energy efficiency, cost model, P2P},
location = {Torremolinos, Malaga, Spain},
series = {SIMUTools '10}
}

@article{10.1145/2500098.2500104,
author = {Basso, Simone and Meo, Michela and De Martin, Juan Carlos},
title = {Strengthening measurements from the edges: application-level packet loss rate estimation},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2500098.2500104},
doi = {10.1145/2500098.2500104},
abstract = {Network users know much less than ISPs, Internet exchanges and content providers about what happens inside the network. Consequently users cannot either easily detect network neutrality violations or readily exercise their market power by knowledgeably switching ISPs.This paper contributes to the ongoing efforts to empower users by proposing two models to estimate -- via application-level measurements -- a key network indicator, i.e., the packet loss rate (PLR) experienced by FTP-like TCP downloads.Controlled, testbed, and large-scale experiments show that the Inverse Mathis model is simpler and more consistent across the whole PLR range, but less accurate than the more advanced Likely Rexmit model for landline connections and moderate PLR.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {45–51},
numpages = {7},
keywords = {tcp, network neutrality, application level measurements}
}

@inproceedings{10.1145/1743384.1743437,
author = {Cheng, Xiangang and Chia, Liang-Tien},
title = {Stratification-based keyframe cliques for removal of near-duplicates in video search results},
year = {2010},
isbn = {9781605588155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1743384.1743437},
doi = {10.1145/1743384.1743437},
abstract = {The current volume of videos available for distribution or viewing on the internet is increasing exponentially, there is an urgent need for designing effective and efficient video management systems. However, due to the tremendous amounts of video data, it is highly likely that any large scale video systems will provide query results with near-duplicates videos in the return list of videos. In this paper, we introduce our method of identification and removal of near-duplicates in video search results via matching strata of keyframes.To be exact, we detect the near duplicate keyframes in each video separately. Then we partition these keyframes into summarized groups by our quasi-clique based partition. Experiments on the Trecvid dataset confirmed our initial view that a significant number of keyframes from videos in the Trecvid corpus are near-duplicates.Based on the summarized clique representation of each video, we tested our algorithm for detection of near-duplicates in the web videos. Results show that our proposed method greatly speeds up the retrieval process, while maintaining a high detection accuracy.},
booktitle = {Proceedings of the International Conference on Multimedia Information Retrieval},
pages = {313–322},
numpages = {10},
keywords = {web video, stratification-based, quasi-clique, near-duplicate},
location = {Philadelphia, Pennsylvania, USA},
series = {MIR '10}
}

@article{10.14778/2733085.2733096,
author = {Alsubaiee, Sattam and Altowim, Yasser and Altwaijry, Hotham and Behm, Alexander and Borkar, Vinayak and Bu, Yingyi and Carey, Michael and Cetindil, Inci and Cheelangi, Madhusudan and Faraaz, Khurram and Gabrielova, Eugenia and Grover, Raman and Heilbron, Zachary and Kim, Young-Seok and Li, Chen and Li, Guangqiang and Ok, Ji Mahn and Onose, Nicola and Pirzadeh, Pouria and Tsotras, Vassilis and Vernica, Rares and Wen, Jian and Westmann, Till},
title = {AsterixDB: a scalable, open source BDMS},
year = {2014},
issue_date = {October 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {14},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733085.2733096},
doi = {10.14778/2733085.2733096},
abstract = {AsterixDB is a new, full-function BDMS (Big Data Management System) with a feature set that distinguishes it from other platforms in today's open source Big Data ecosystem. Its features make it well-suited to applications like web data warehousing, social data storage and analysis, and other use cases related to Big Data. AsterixDB has a flexible NoSQL style data model; a query language that supports a wide range of queries; a scalable runtime; partitioned, LSM-based data storage and indexing (including B+-tree, R-tree, and text indexes); support for external as well as natively stored data; a rich set of built-in types; support for fuzzy, spatial, and temporal types and queries; a built-in notion of data feeds for ingestion of data; and transaction support akin to that of a NoSQL store.Development of AsterixDB began in 2009 and led to a mid-2013 initial open source release. This paper is the first complete description of the resulting open source AsterixDB system. Covered herein are the system's data model, its query language, and its software architecture. Also included are a summary of the current status of the project and a first glimpse into how AsterixDB performs when compared to alternative technologies, including a parallel relational DBMS, a popular NoSQL store, and a popular Hadoop-based SQL data analytics platform, for things that both technologies can do. Also included is a brief description of some initial trials that the system has undergone and the lessons learned (and plans laid) based on those early "customer" engagements.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {1905–1916},
numpages = {12}
}

@proceedings{10.1145/1851476,
title = {HPDC '10: Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
year = {2010},
isbn = {9781605589428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizing committee, we welcome you to the 19th ACM International Symposium on High Performance Distributed Computing (HPDC-19) and to Chicago, Illinois, a city rich in culture, history, technology, and third largest city in the United States. The HPDC Symposium is the premier venue for presenting the latest research on the design, implementation, evaluation, and use of parallel and distributed systems for high performance and high end computing. We are looking forward to lively discussions about HPDC emerging technologies, applications and their challenges. In the last two decades, we have experienced rapid advances in networks and computing, Internet technologies and services, parallel and distributed programming tools that have lead to the developed and deployment of many HPDC systems and services include Grid Computing, Cloud Computing, Pervasive Computing, just to name a few. These advances and technologies bring with them many challenges and exciting opportunities that can be addressed by the HPDC research community.This year's technical program presents high-quality papers in HPDC Workflows, Cloud Computing and Tools, Data Centers and Virtualization, Scheduling, Storage and I/O, Communications, and Applications. We would like to thank Peter Dinda (Program Chair), Northwestern University, for his outstanding and tireless efforts in assembling an excellent program committee, managing the reviews and selection of the papers, and even addressing many other issues related to sponsorship, proceedings, finance, and more to make sure we have an excellent HPDC program for this year. We owe a great debt to Peter, whose dedications and commitments have produced another strong technical program that we all will enjoy and appreciate when we meet in Chicago.Continuing the HPDC tradition, this year's symposium features eight workshops: Emerging Computation Methods for the Life Sciences, LSAP: Large-Scale System and Application Performance, MDQCS: Managing Data Quality for Collaborative Science, ScienceCloud: Workshop on Scientific Cloud Computing, CLADE: Challenges of Large Applications in Distributed Environments, DIDC: Data Intensive Distributed Computing, MAPREDUCE: MapReduce and its applications, and VTDC: Virtualization Technologies for Distributed Computing. We would like to acknowledge the excellent job done by Douglas Thain (Workshop Chair), University of Notre Dame, in putting together a strong workshop program.},
location = {Chicago, Illinois}
}

@inproceedings{10.1145/1878431.1878446,
author = {Molina-Markham, Andr\'{e}s and Shenoy, Prashant and Fu, Kevin and Cecchet, Emmanuel and Irwin, David},
title = {Private memoirs of a smart meter},
year = {2010},
isbn = {9781450304580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1878431.1878446},
doi = {10.1145/1878431.1878446},
abstract = {Household smart meters that measure power consumption in real-time at fine granularities are the foundation of a future smart electricity grid. However, the widespread deployment of smart meters has serious privacy implications since they inadvertently leak detailed information about household activities. In this paper, we show that even without a priori knowledge of household activities or prior training, it is possible to extract complex usage patterns from smart meter data using off-the-shelf statistical methods. Our analysis uses two months of data from three homes, which we instrumented to log aggregate household power consumption every second. With the data from our small-scale deployment, we demonstrate the potential for power consumption patterns to reveal a range of information, such as how many people are in the home, sleeping routines, eating routines, etc. We then sketch out the design of a privacy-enhancing smart meter architecture that allows an electric utility to achieve its net metering goals without compromising the privacy of its customers.},
booktitle = {Proceedings of the 2nd ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Building},
pages = {61–66},
numpages = {6},
keywords = {smart meters, smart grid, security, privacy},
location = {Zurich, Switzerland},
series = {BuildSys '10}
}

@inproceedings{10.1145/2207676.2208378,
author = {Schwarz, Julia and Klionsky, David and Harrison, Chris and Dietz, Paul and Wilson, Andrew},
title = {Phone as a pixel: enabling ad-hoc, large-scale displays using mobile devices},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208378},
doi = {10.1145/2207676.2208378},
abstract = {We present Phone as a Pixel: a scalable, synchronization-free, platform-independent system for creating large, ad-hoc displays from a collection of smaller devices. In contrast to most tiled-display systems, the only requirement for participation is for devices to have an internet connection and a web browser. Thus, most smartphones, tablets, laptops and similar devices can be used. Phone as a Pixel uses a color-transition encoding scheme to identify and locate displays. This approach has several advantages: devices can be arbitrarily arranged (i.e., not in a grid) and infrastructure consists of a single conventional camera. Further, additional devices can join at any time without re-calibration. These are desirable properties to enable collective displays in contexts like sporting events, concerts and political rallies. In this paper we describe our system, show results from proof-of-concept setups, and quantify the performance of our approach on hundreds of displays.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2235–2238},
numpages = {4},
keywords = {ubiquitous computing, distributed screens, devices, crowd-computer interaction, computer vision},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/1968613.1968688,
author = {Li, Yu-Shiuan and Chen, Kwang-Cheng},
title = {Graph partition and identification of cluster number in data analysis},
year = {2011},
isbn = {9781450305716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1968613.1968688},
doi = {10.1145/1968613.1968688},
abstract = {Modern computing with wide range of applications in different areas such as Internet, biology, and social science, involves large scale of data analysis. The relations of data can be modeled as graphs and graph partitioning problem can be effectively approximated by spectral approaches. A critically important problem in graph partition is determination of the cluster number k. Although eigengap heuristic is a principle for this problem and is supported by theory, it is difficult to be applied for the real-world data and complex graphs. In this paper, by considering the general data analysis scenario, we present an algorithm to determine the cluster number k and perform clustering task simultaneously. The experimental result shows that our algorithm works successfully even for the real world data, which is therefore a promising tool for future data analysis.},
booktitle = {Proceedings of the 5th International Conference on Ubiquitous Information Management and Communication},
articleno = {62},
numpages = {5},
keywords = {graph partition, spectral clustering},
location = {<conf-loc>, <city>Seoul</city>, <country>Korea</country>, </conf-loc>},
series = {ICUIMC '11}
}

@inproceedings{10.1145/1978672.1978675,
author = {Massicotte, Fr\'{e}d\'{e}ric and Couture, Mathieu},
title = {Blueprints of a lightweight automated experimentation system: a building block towards experimental cyber security},
year = {2011},
isbn = {9781450307680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978672.1978675},
doi = {10.1145/1978672.1978675},
abstract = {Many research projects studying security threats require realistic network scenarios while dealing with millions of cyber threats (e.g., exploit programs and malware). For instance, studying the execution of malware may require to take into account different network configurations in which malware can propagate, as well as dealing with thousands (or millions) of different malware samples. The same challenge occurs if one wants to evaluate IDSs, study exploit programs or conduct vulnerability assessment using realistic network scenarios.Moreover, cyber threats are highly dynamic. Every day, new vulnerabilities are identified and documented in software commonly used by computers connected to the Internet, and new malware instances and exploit programs are also identified. Consequently, it is not viable to develop (deploy) an environment every time cyber threats or security products (e.g., IDSs and anti-virus) have to be studied from a different perspective.New research methodologies and tools are needed to systematically conduct cyber security research. Automation is required to deal with the increasingly large number of cyber threats. In this paper, we draw the foundations of an experimental approach to cyber security by providing the blueprints of a lightweight Automated Experimentation System (AES). The AES automatically executes experiments based on a specification file. We describe its usage in different network security research projects, from IDS evaluation to static and dynamic malware analysis. The results we derived from these different research projects show that our experimental approach to cyber security, enabled by the AES, enhances the scope (and scale) of research in this field. Consequently, the AES improves our understanding of cyber threats and our assessment of the current state of security products.},
booktitle = {Proceedings of the First Workshop on Building Analysis Datasets and Gathering Experience Returns for Security},
pages = {19–28},
numpages = {10},
location = {Salzburg, Austria},
series = {BADGERS '11}
}

@inproceedings{10.1145/2434020.2434042,
author = {Zhu, Ting and Mishra, Aditya and Irwin, David and Sharma, Navin and Shenoy, Prashant and Towsley, Don},
title = {The case for efficient renewable energy management in smart homes},
year = {2011},
isbn = {9781450307499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2434020.2434042},
doi = {10.1145/2434020.2434042},
abstract = {Distributed generation (DG) uses many small on-site energy sources deployed at individual buildings to generate electricity. DG has the potential to make generation more efficient by reducing transmission and distribution losses, carbon emissions, and demand peaks. However, since renewables are intermittent and uncontrollable, buildings must still rely, in part, on the electric grid for power. While DG deployments today use net metering to offset costs and balance local supply and demand, scaling net metering for intermittent renewables to many homes is difficult. In this paper, we explore a different approach that combines residential TOU pricing models with on-site renewables and modest energy storage to incentivize DG. We propose a system architecture and control algorithm to efficiently manage the renewable energy and storage to minimize grid power costs at individual buildings. We evaluate our control algorithm by simulation using a collection of real-world data sets. Initial results show that the algorithm decreases grid power costs by 2.7X while nearly eliminating grid demand peaks, demonstrating the promise of our approach.},
booktitle = {Proceedings of the Third ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Buildings},
pages = {67–72},
numpages = {6},
keywords = {smart grid, renewable energy, building energy},
location = {Seattle, Washington},
series = {BuildSys '11}
}

@inproceedings{10.1145/1871437.1871758,
author = {Pavlov, Dmitry Yurievich and Gorodilov, Alexey and Brunk, Cliff A.},
title = {BagBoo: a scalable hybrid bagging-the-boosting model},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871758},
doi = {10.1145/1871437.1871758},
abstract = {In this paper, we introduce a novel machine learning approach for regression based on the idea of combining bagging and boosting that we call BagBoo. Our BagBoo model borrows its high accuracy potential from. Friedman's gradient boosting [2], and high efficiency and scalability through parallelism from Breiman's bagging [1]. We run empirical evaluations on large scale Web ranking data, and demonstrate that BagBoo is not only showing superior relevance than standalone bagging or boosting, but also outperforms most previously published results on these data sets. We also emphasize that BagBoo is intrinsically scalable and parallelizable, allowing us to train order of half a million trees on 200 nodes in 2 hours CPU time and beat all of the competitors in the Internet Mathematics relevance competition sponsored by Yandex and be one of the top algorithms in both tracks of Yahoo ICML-2010 challenge. We conclude the paper by stating that while impressive experimental evaluation results are presented here in the context of regression trees, the hybrid BagBoo model is applicable to other domains, such as classification, and base training models.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {1897–1900},
numpages = {4},
keywords = {ranking, learning to rank, experimentation, boosting, bagging, bagboo},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}

@article{10.1109/TNET.2009.2025927,
author = {Mondal, Amit and Kuzmanovic, Aleksandar},
title = {Upgrading mice to elephants: effects and end-point solutions},
year = {2010},
issue_date = {April 2010},
publisher = {IEEE Press},
volume = {18},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2009.2025927},
doi = {10.1109/TNET.2009.2025927},
abstract = {Short TCP flows may suffer significant response-time performance degradations during network congestion. Unfortunately, this creates an incentive for misbehavior by clients of interactive applications (e.g., gaming, telnet, web): to send "dummy" packets into the network at a TCP-fair rate even when they have no data to send, thus improving their performance in moments when they do have data to send. Even though no "law" is violated in this way, a large-scale deployment of such an approach has the potential to seriously jeopardize one of the core Internet's principles-- statistical multiplexing. We quantify, by means of analytical modeling and simulation, gains achievable by the above misbehavior. Our research indicates that easy-to-implement application-level techniques are capable of dramatically reducing incentives for conducting the above transgressions, still without compromising the idea of statistical multiplexing.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {367–378},
numpages = {12},
keywords = {statistical multiplexing, retransmission timeout, interactive application, TCP}
}

@inproceedings{10.1109/CCGrid.2012.47,
author = {Kantarci, Burak and Mouftah, Hussein T.},
title = {Optimal Reconfiguration of the Cloud Network for Maximum Energy Savings},
year = {2012},
isbn = {9780769546919},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CCGrid.2012.47},
doi = {10.1109/CCGrid.2012.47},
abstract = {With the advent of cloud computing, storage and computing functions are migrating to remote resources such as virtual servers and storage systems which are mostly hosted in the data centers. This migration can ensure significant energy savings as utilization of local resources contribute to 40% of the Greenhouse Gas emissions of the Information and Communication Technologies (ICTs). On the other hand, provisioning of the cloud services needs to be handled carefully since energy consumption of the transport network, as well as the energy consumed by the data centers, is expected to increase. We revisit our previously proposed Mixed Integer Linear Programming (MILP) models that are used to reconfigure the cloud network design with look-ahead demand profile. Due to long runtimes of the MILP models in large-scale scenarios, in this paper, we propose two heuristics to reconfigure the cloud network for provisioning the cloud and Internet computing demands. The first heuristic aims to minimize the propagation delay while the second one targets minimizing the power consumption of the data centers and the transport network. We verify the heuristics through simulations where MILP models are used as the benchmarks. Numerical results show that power minimized provisioning can guarantee significant energy savings in the cloud network with less resource consumption. We also present the energy versus delay trade-off and point out possible solutions.},
booktitle = {Proceedings of the 2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (Ccgrid 2012)},
pages = {835–840},
numpages = {6},
keywords = {virtual topology, energy-efficiency, demand provisioning, data centers, Cloud computing},
series = {CCGRID '12}
}

@inproceedings{10.5555/2499406.2499466,
author = {Bays, Leonardo Richter and Oliveira, Rodrigo Ruas and Buriol, Luciana Salete and Barcellos, Marinho Pilla and Gaspary, Luciano Paschoal},
title = {Security-aware optimal resource allocation for virtual network embedding},
year = {2012},
isbn = {9781450322102},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Network virtualization enables the creation of multiple instances of virtual networks on top of a single physical infrastructure. Given its wide applicability, this technique has attracted a lot of interest both from academic researchers and major companies within the segment of computer networks. Although recent efforts (motivated mainly by the search for mechanisms to evaluate Future Internet proposals) have contributed substantially to materialize this concept, none of them has attempted to combine efficient resource allocation with fulfillment of security requirements (e.g., confidentiality). It is important to note that, in the context of virtual networks, the protection of shared network infrastructures constitutes a fundamental condition to enable its use in large scale. To address this problem, in this paper we propose a virtual network embedding model that satisfies security requirements and, at the same time, optimizes physical resource usage. The results obtained demonstrate that the model is able to correctly and optimally map virtual networks to a physical substrate, minimizing bandwidth costs for infrastructure providers.},
booktitle = {Proceedings of the 8th International Conference on Network and Service Management},
pages = {378–384},
numpages = {7},
location = {Las Vegas, Nevada},
series = {CNSM '12}
}

@inproceedings{10.1145/2348283.2348339,
author = {Zhang, Qi and Wu, Yan and Ding, Zhuoye and Huang, Xuanjing},
title = {Learning hash codes for efficient content reuse detection},
year = {2012},
isbn = {9781450314725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348283.2348339},
doi = {10.1145/2348283.2348339},
abstract = {Content reuse is extremely common in user generated mediums. Reuse detection serves as be the basis for many applications. However, along with the explosion of Internet and continuously growing uses of user generated mediums, the task becomes more critical and difficult. In this paper, we present a novel efficient and scalable approach to detect content reuse. We propose a new signature generation algorithm, which is based on learned hash functions for words. In order to deal with tens of billions of documents, we implement the detection approach on graphical processing units (GPUs). The experimental comparison in this paper involves studies of efficiency and effectiveness of the proposed approach in different types of document collections, including ClueWeb09, Tweets2011, and so on. Experimental results show that the proposed approach can achieve the same detection rates with state-of-the-art systems while uses significantly less execution time than them (from 400X to 1500X speedup).},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {405–414},
numpages = {10},
keywords = {learning to hash, content reuse detection, GPUs},
location = {Portland, Oregon, USA},
series = {SIGIR '12}
}

@proceedings{10.1145/2332432,
title = {PODC '12: Proceedings of the 2012 ACM symposium on Principles of distributed computing},
year = {2012},
isbn = {9781450314503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2012 ACM Symposium on Principles of Distributed Computing -- PODC'12. This year's symposium continues its tradition of being the premier forum for presentation of research results in the area of theoretical distributed computing. During the years PODC has been the stage where many landmark results that have increased our understanding of this exciting and, in the Internet era, fundamental research endeavor have been presented. In the best tradition of theoretical discovery, the insights that have been provided have not only elucidated fundamental conceptual issues but also found their way in the real world of systems and applications. The mission of the symposium remains that of providing a high quality international forum for the timely dissemination and discussion of ideas at the frontier of current knowledge in the area of theoretical distributed computing.The call for papers attracted 142 submissions from the Americas, Asia, and Europe. The program committee met in Rome and accepted 35 papers and 26 brief announcements that cover a wide variety of topics. In addition, this year the program includes an industrial panel where colleagues from leading technological companies will share with us their experience with the challenges presented by real, large-scale distributed systems. The keynote speech will be by David Peleg, whose outstanding research record sets a gold standard for the field. Finally, this year PODC hosts the ceremony for the 2012 Edsger W. Dijkstra Prize.},
location = {Madeira, Portugal}
}

@inproceedings{10.1007/978-3-642-34263-9_27,
author = {Gong, Mingying and Sun, Lifeng and Yang, Shiqiang and Yang, Yun},
title = {Robust place recognition by avoiding confusing features and fast geometric re-ranking},
year = {2012},
isbn = {9783642342622},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34263-9_27},
doi = {10.1007/978-3-642-34263-9_27},
abstract = {There are millions of mobile phone applications based on location. Using a photo to precisely locate users location is useful and necessary. However, real-time location recognition or retrieval system is a challenging problem due to the really big differences between the query and the dataset in scale, viewpoint and lighting, or the noise existed in the foreground or background etc. To address this problem, we design a place recognition system and a new famous buildings dataset with ground truth labels. By adding a fast geometric image matching procedure before using RANSAC and applying a relative camera orientation calculation algorithm to filter the dataset collected from the Internet, we can substantially improve the efficiency of spatial verification and recognition accuracy.},
booktitle = {Proceedings of the First International Conference on Computational Visual Media},
pages = {210–217},
numpages = {8},
keywords = {relative camera orientation, image recognition, geometric verification, confusing features detection},
location = {Beijing, China},
series = {CVM'12}
}

@inproceedings{10.1145/2525528.2525531,
author = {Dzik, Jan and Palladinos, Nick and Rontogiannis, Konstantinos and Tsarpalis, Eirik and Vathis, Nikolaos},
title = {MBrace: cloud computing with monads},
year = {2013},
isbn = {9781450324601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525528.2525531},
doi = {10.1145/2525528.2525531},
abstract = {As cloud computing and big data gain prominence in today's economic landscape, the challenge of effectively articulating complex algorithms in distributed environments becomes ever more important. In this paper we describe MBrace; a novel programming model/framework for performing large scale computation in the cloud. Based on the .NET software stack, it utilizes the power of the F# programming language. MBrace introduces a declarative style for specifying and composing parallelism patterns, in what is known as cloud workflows or a cloud monad. MBrace is also a distributed execution runtime that handles orchestration of cloud workflows in the data centre.},
booktitle = {Proceedings of the Seventh Workshop on Programming Languages and Operating Systems},
articleno = {7},
numpages = {6},
keywords = {distributed programming, cloud monad, big data},
location = {Farmington, Pennsylvania},
series = {PLOS '13}
}

@inproceedings{10.1145/2488388.2488470,
author = {Nath, Abhirup and Mukherjee, Shibnath and Jain, Prateek and Goyal, Navin and Laxman, Srivatsan},
title = {Ad impression forecasting for sponsored search},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488470},
doi = {10.1145/2488388.2488470},
abstract = {A typical problem for a search engine (hosting sponsored search service) is to provide the advertisers with a forecast of the number of impressions his/her ad is likely to obtain for a given bid. Accurate forecasts have high business value, since they enable advertisers to select bids that lead to better returns on their investment. They also play an important role in services such as automatic campaign optimization. Despite its importance the problem has remained relatively unexplored in literature. Existing methods typically overfit to the training data, leading to inconsistent performance. Furthermore, some of the existing methods cannot provide predictions for new ads, i.e., for ads that are not present in the logs. In this paper, we develop a generative model based approach that addresses these drawbacks. We design a Bayes net to capture inter-dependencies between the query traffic features and the competitors in an auction. Furthermore, we account for variability in the volume of query traffic by using a dynamic linear model. Finally, we implement our approach on a production grade MapReduce framework and conduct extensive large scale experiments on substantial volumes of sponsored search data from Bing. Our experimental results demonstrate significant advantages over existing methods as measured using several accuracy/error criteria, improved ability to provide estimates for new ads and more consistent performance with smaller variance in accuracies. Our method can also be adapted to several other related forecasting problems such as predicting average position of ads or the number of clicks under budget constraints.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {943–952},
numpages = {10},
keywords = {sponsored search, dynamic linear model, bayes net, auctions},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@article{10.1109/TNET.2011.2149540,
author = {Lestas, Marios and Pitsillides, Andreas and Ioannou, Petros and Hadjipollas, George},
title = {A new estimation scheme for the effective number of users in internet congestion control},
year = {2011},
issue_date = {October 2011},
publisher = {IEEE Press},
volume = {19},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2011.2149540},
doi = {10.1109/TNET.2011.2149540},
abstract = {Many congestion control protocols have been recently proposed in order to alleviate the problems encountered by TCP in high-speed networks and wireless links. Protocols utilizing an architecture that is in the same spirit as the ABR service in ATM networks require estimates of the effective number of users utilizing each link in the network to maintain stability in the presence of delays. In this paper, we propose a novel estimation algorithm that is based on online parameter identification techniques and is shown through analysis and simulations to converge to the effective number of users utilizing each link. The algorithm does not require maintenance of per-flow states within the network or additional fields in the packet header, and it is shown to outperform previous proposals that were based on pointwise division in time. The estimation scheme is designed independently from the control functions of the protocols and is thus universal in the sense that it operates effectively in a number of congestion control protocols. It can thus be successfully used in the design of new congestion control protocols. In this paper, to illustrate its universality, we use the proposed estimation scheme to design a representative set of Internet congestion control protocols. Using simulations, we demonstrate that these protocols satisfy key design requirements. They guide the network to a stable equilibrium that is characterized by high network utilization, small queue sizes, and max-min fairness. In addition, they are scalable with respect to changing bandwidths, delays, and number of users, and they generate smooth responses that converge quickly to the desired equilibrium.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {1499–1512},
numpages = {14},
keywords = {internet, congestion control, adaptive control}
}

@inproceedings{10.1145/2396761.2398623,
author = {Wang, Chieh-Jen and Chen, Hsin-Hsi},
title = {Learning to predict the cost-per-click for your ad words},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398623},
doi = {10.1145/2396761.2398623},
abstract = {In Internet ad campaign, ranking of an ad on search result pages depends on a cost-per-click (CPC) of ad words offered by an advertiser and a quality score estimated by a search engine. Bidding for ad words with a higher CPC is more competitive than bidding for the same ad words with a lower CPC in the ad ranking competition. However, offering a higher CPC will increase a burden on advertisers. In contrast, offering a lower CPC may decrease the exposure rate of their ads. Thus, how to select an appropriate CPC for ad words is indispensable for advertisers. In this paper, we extract different semantic levels of features, such as named entities, topic terminologies, and individual words from a large-scale real-world ad words corpus, and explore various learning based prediction algorithms. The thorough experimental results show that the CPC prediction models considering more ad words semantics achieve better prediction performance, and the prediction model using the support vector regression (SVR) and features from all semantic levels performs the best.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2291–2294},
numpages = {4},
keywords = {search engine optimization, ad ranking, CPC prediction},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@proceedings{10.1145/2566468,
title = {HiCoNS '14: Proceedings of the 3rd international conference on High confidence networked systems},
year = {2014},
isbn = {9781450326520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2014 3rd ACM International Conference on High Confidence Networked Systems (HiCoNS'14) as part of CPSWeek 2014. HiCoNS aims to foster collaborations between researchers from the fields of control and systems theory, embedded systems, game theory, software verification, formal methods, and computer security who are addressing various aspects of resilience of cyber-physical systems (CPS). HiCoNS continues after growing interest and enthusiasm that was created by the First Workshop on Secure Control Systems (SCS), the Workshop on the Foundations of Dependable and Secure Cyber-Physical Systems (FDSCPS), HiCoNS'12, and HiCoNS'13.CPS govern the operation of critical infrastructures such as power transmission, water distribution, transportation, healthcare, building automation, and process control. At the core of these systems are modern control technologies based on embedded computers and networked systems that monitor and control large-scale physical processes. The use of internet-connected devices and commodity IT solutions and the malicious intents of hackers and cybercriminals have made these control technologies more vulnerable. Despite attempts to develop guidelines for the design and operation of systems via security policies, much remains to be done to achieve a principled, science-based approach to enhance security, trustworthiness, and dependability of networked cyber-physical systems.HiCoNS'14 aimed to bring together novel concepts and theories that will help in the development of the science of high confidence networked systems, in particular those considered cyber-physical systems (CPS) and their interactions with human decision makers. The conference focused on system theoretic approaches to address fundamental challenges to increase the confidence of networked CPS by making them more secure, dependable, and trustworthy.The technical program includes sessions focused on Resilient Monitoring and Estimation, Security of Networked and Distributed Control Systems, Verification of Security Properties, and Security of CPS applications. The technical program also includes an invited session on improving CPS Resilience by integrating Robust Control and Theory of Incentives. During these sessions, presentations will cover both recent research results as well as new directions for future research and development. In addition, the program includes two invited talks and a poster session on emerging topics in resilience of CPS.},
location = {Berlin, Germany}
}

@inproceedings{10.1145/1835804.1835828,
author = {Henderson, Keith and Eliassi-Rad, Tina and Faloutsos, Christos and Akoglu, Leman and Li, Lei and Maruhashi, Koji and Prakash, B. Aditya and Tong, Hanghang},
title = {Metric forensics: a multi-level approach for mining volatile graphs},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835828},
doi = {10.1145/1835804.1835828},
abstract = {Advances in data collection and storage capacity have made it increasingly possible to collect highly volatile graph data for analysis. Existing graph analysis techniques are not appropriate for such data, especially in cases where streaming or near-real-time results are required. An example that has drawn significant research interest is the cyber-security domain, where internet communication traces are collected and real-time discovery of events, behaviors, patterns, and anomalies is desired. We propose MetricForensics, a scalable framework for analysis of volatile graphs. MetricForensics combines a multi-level "drill down" approach, a collection of user-selected graph metrics, and a collection of analysis techniques. At each successive level, more sophisticated metrics are computed and the graph is viewed at finer temporal resolutions. In this way, MetricForensics scales to highly volatile graphs by only allocating resources for computationally expensive analysis when an interesting event is discovered at a coarser resolution first. We test MetricForensics on three real-world graphs: an enterprise IP trace, a trace of legitimate and malicious network traffic from a research institution, and the MIT Reality Mining proximity sensor data. Our largest graph has 3M vertices and 32M edges, spanning 4.5 days. The results demonstrate the scalability and capability of MetricForensics in analyzing volatile graphs; and highlight four novel phenomena in such graphs: elbows, broken correlations, prolonged spikes, and lightweight stars.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {163–172},
numpages = {10},
keywords = {volatile graphs, temporal analysis, graph mining},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.5555/1929757.1929777,
author = {Baumgartner, Norbert and Gottesheim, Wolfgang and Mitsch, Stefan and Retschitzegger, Werner and Schwinger, Wieland},
title = {Situation prediction nets: playing the token game for ontology-driven situation awareness},
year = {2010},
isbn = {3642163726},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Situation awareness in large-scale control systems such as road traffic management aims to predict critical situations on the basis of spatio-temporal relations between real-world objects. Such relations are described by domain-independent calculi, each of them focusing on a certain aspect, for example topology. The fact that these calculi are described independently of the involved objects, isolated from each other, and irrespective of the distances between relations leads to inaccurate and crude predictions. To improve the overall quality of prediction while keeping the modeling effort feasible, we propose a domain-independent approach based on Colored Petri Nets that complements our ontology-driven situation awareness framework BeAware!. These Situation Prediction Nets can be generated automatically and allow increasing (i) prediction precision by exploiting ontological knowledge in terms of object characteristics and interdependencies between relations and (ii) increasing expressiveness by associating multiple distance descriptions with transitions. The applicability of Situation Prediction Nets is demonstrated using real-world traffic data.},
booktitle = {Proceedings of the 29th International Conference on Conceptual Modeling},
pages = {202–218},
numpages = {17},
keywords = {situation awareness, ontology, colored petri nets},
location = {Vancouver, BC, Canada},
series = {ER'10}
}

@article{10.1145/2505805,
author = {Li, Baochun and Wang, Zhi and Liu, Jiangchuan and Zhu, Wenwu},
title = {Two decades of internet video streaming: A retrospective view},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2505805},
doi = {10.1145/2505805},
abstract = {For over two decades, video streaming over the Internet has received a substantial amount of attention from both academia and industry. Starting from the design of transport protocols for streaming video, research interests have later shifted to the peer-to-peer paradigm of designing streaming protocols at the application layer. More recent research has focused on building more practical and scalable systems, using Dynamic Adaptive Streaming over HTTP. In this article, we provide a retrospective view of the research results over the past two decades, with a focus on peer-to-peer streaming protocols and the effects of cloud computing and social media.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {33},
numpages = {20},
keywords = {social media, multimedia streaming, multicast, cloud computing, Video streaming, P2P streaming, HTTP streaming}
}

@inproceedings{10.1145/2623330.2623636,
author = {Liu, Chien-Liang and Tsai, Tsung-Hsun and Lee, Chia-Hoang},
title = {Online chinese restaurant process},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623636},
doi = {10.1145/2623330.2623636},
abstract = {Processing large volumes of streaming data in near-real-time is becoming increasingly important as the Internet, sensor networks and network traffic grow. Online machine learning is a typical means of dealing with streaming data, since it allows the classification model to learn one instance of data at a time. Although many online learning methods have been developed since the development of the Perceptron algorithm, existing online methods assume that the number of classes is available in advance of classification process. However, this assumption is unrealistic for large scale or streaming data sets. This work proposes an online Chinese restaurant process (CRP) algorithm, which is an online and nonparametric algorithm, to tackle this problem. This work proposes a relaxing function as part of the prior and updates the parameters with the likelihood function in terms of the consistency between the true label information and predicted result. This work presents two Gibbs sampling algorithms to perform posterior inference. In the experiments, the online CRP is applied to three massive data sets, and compared with several online learning and batch learning algorithms. One of the data sets is obtained from Wikipedia, which comprises approximately two million documents. The experimental results reveal that the proposed online CRP performs well and efficiently on massive data sets. Finally, this work proposes two methods to update the hyperparameter $alpha$ of the online CRP. The first method is based on the posterior distribution of $alpha$, and the second exploits the property of online learning, namely adapting to change, to adjust $alpha$ dynamically.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {591–600},
numpages = {10},
keywords = {online learning, nonparametric, chinese restaurant process, adaptive learning},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/2185448.2185468,
author = {Schulz, Steffen and Sadeghi, Ahmad-Reza and Zhdanova, Maria and Mustafa, Hossen and Xu, Wenyuan and Varadharajan, Vijay},
title = {Tetherway: a framework for tethering camouflage},
year = {2012},
isbn = {9781450312653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2185448.2185468},
doi = {10.1145/2185448.2185468},
abstract = {The rapidly increasing data usage and overload in mobile broadband networks has driven mobile network providers to actively detect and bill customers who tether tablets and laptops to their mobile phone for mobile Internet access. However, users may not be willing to pay additional fees only because they use their bandwidth dierently, and may consider tethering detection as violation of their privacy. Furthermore, accurate tethering detection is becoming harder for providers as many modern smartphones are under full control of the user, running customized, complex software and applications similar to desktop systems.In this work, we analyze the network characteristics available to network providers to detect tethering customers. We present and categorize possible detection mechanisms and derive cost factors based on how well the approach scales with large customer bases. For those characteristics that appear most reasonable and practical to deploy by large providers, we present elimination or obfuscation mechanisms and substantiate our design with a prototype Android App.},
booktitle = {Proceedings of the Fifth ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {149–160},
numpages = {12},
keywords = {tethering detection, mobile networks},
location = {Tucson, Arizona, USA},
series = {WISEC '12}
}

@inproceedings{10.1145/2038633.2038638,
author = {Weigert, Stefan and Hiltunen, Matti and Fetzer, Christof},
title = {Mining large distributed log data in near real time},
year = {2011},
isbn = {9781450309783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038633.2038638},
doi = {10.1145/2038633.2038638},
abstract = {Analyzing huge amounts of log data is often a difficult task, especially if it has to be done in real time (e.g., fraud detection) or when large amounts of stored data are required for the analysis. Graphs are a data structure often used in log analysis. Examples are clique analysis and communities of interest (COI). However, little attention has been paid to large distributed graphs that allow a high throughput of updates with very low latency.In this paper, we present a distributed graph mining system that is able to process around 39 million log entries per second on a 50 node cluster while providing processing latencies below 10 ms. We validate our approach by presenting two example applications, namely telephony fraud detection and internet attack detection. A thorough evaluation proves the scalability and near real-time properties of our system.},
booktitle = {Managing Large-Scale Systems via the Analysis of System Logs and the Application of Machine Learning Techniques},
articleno = {5},
numpages = {8},
keywords = {log processing, distributed graphs, COI},
location = {Cascais, Portugal},
series = {SLAML '11}
}

@article{10.1145/1859204.1859235,
author = {Morelli, Ralph and de Silva, Chamindra and de Lanerolle, Trishan and Curzon, Rebecca and Mao, Xin Sheng},
title = {A global collaboration to deploy help to China},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1859204.1859235},
doi = {10.1145/1859204.1859235},
abstract = {A firsthand account of an international team effort to install the Sahana disaster-management system in Chengdu, Sichuan after an earthquake.On Monday May 12, 2008, an earthquake measuring 7.9 on the Richter scale struck in Sichuan Province in southwestern China, destroying homes, schools, hospitals, roads, and vital power and communication infrastructure. More than 45 million people were affected---tens of thousands were killed, hundreds of thousands injured, millions of people were evacuated and left homeless, and millions of buildings were destroyed.When the earthquake hit, several members of what became an international, volunteer, disaster-management IT team were attending a workshop in Washington, D.C. The workshop was organized by the IBM Office of Corporate Citizenship and Corporate Affairs department to train IBM personnel and others in the use and deployment of Sahana, a free and open source software (FOSS) disaster management system.Sahana, which means relief in Sinhalese, is a Web-based collaboration tool that helps manage information resources during a disaster recovery effort. It supports a wide range of relief efforts from finding missing persons, to managing volunteers, tracking resources, and coordinating refugee camps. Sahana enables government groups, non-governmental organizations (NGOs) and the victims themselves to work together during a disaster recovery effort.Over the next several weeks, the team members, distributed among several cities (Beijing and Chengdu in China, Hartford and New York in the U.S., and Colombo in Sri Lanka), worked together over global communication channels to configure and deploy Sahana in Chengdu, in order to support the disaster recovery effort there.The organizations involved in the collaboration included:• The Lanka Software Foundation (LSF), developers of the Sahana system. Three LSF members, led by the second author, were conducting the training workshop.• Various departments of IBM, including Business Continuity and Resiliency Services. Ten employees led by the fourth author, who organized the workshop, were receiving instruction in how to deploy and use Sahana.• The Humanitarian FOSS Project (H-FOSS), an NSF-funded effort aimed at revitalizing undergraduate computing education. Four students and their mentors, the first and third authors, were attending the workshop as developers and undergraduate members of the Sahana community.• IBM China. Initially, local teams in Beijing and Chengdu consisting of corporate citizenship, government relations, and technical professionals led in demonstrating Sahana to local officials, securing buy-in, and establishing channels to proceed. Then a large team of developers, language specialists, and others, including a team based in Chengdu, Sichuan, eventually took charge of the deployment effort in Chengdu. The fifth author was a member of the China development team.Almost immediately after the earthquake, discussions were held between IBM, IBM China, China's Ministry of Civil Affairs, and the Chengdu city government in Sichuan province. Once the Chengdu government expressed real interest in deploying Sahana, a team was formed to begin the process of localizing Sahana---that is, translating its user interface into simplified Chinese. The team was led by executives and software developers from IBM-China and assisted by Sahana team members in Colombo and student H-FOSS volunteers in Hartford.The team's organizational structure followed the normal procedure involved in previous Sahana deployments---a local group in close proximity to the incident supported by volunteers from the global Sahana community. In this case IBM-China, including some who were directly affected by the disaster, took the lead in deploying Sahana over an intensive three-week period.The decision by the Chengdu government to proceed with the deployment was taken on May 21, 2008 and a revised and localized version of Sahana was deployed in Chengdu on May 25. On June 12 we learned that 42 families had been reunited with the help of Sahana.This article provides an inside look at the deployment effort. It describes how a diverse, multidisciplinary team---professional programmers, software engineers, executives from a large global enterprise, students, faculty, and humanitarian IT specialists from a global FOSS community---worked together to assist the earthquake recovery effort. The success of the collaboration illustrates the power of virtual communities working across international boundaries using a variety of electronic communication software. It also demonstrates that the Internet has truly made us all neighbors and is constantly forcing us to redefine our concept of community.},
journal = {Commun. ACM},
month = {dec},
pages = {142–149},
numpages = {8}
}

@inproceedings{10.1145/1807167.1807249,
author = {Elmeleegy, Hazem and Ouzzani, Mourad and Elmagarmid, Ahmed and Abusalah, Ahmad},
title = {Preserving privacy and fairness in peer-to-peer data integration},
year = {2010},
isbn = {9781450300322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807167.1807249},
doi = {10.1145/1807167.1807249},
abstract = {Peer-to-peer data integration - a.k.a. Peer Data Management Systems (PDMSs) - promises to extend the classical data integration approach to the Internet scale. Unfortunately, some challenges remain before realizing this promise. One of the biggest challenges is preserving the privacy of the exchanged data while passing through several intermediate peers. Another challenge is protecting the mappings used for data translation. Protecting the privacy without being unfair to any of the peers is yet a third challenge. This paper presents a novel query answering protocol in PDMSs to address these challenges. The protocol employs a technique based on noise selection and insertion to protect the query results, and a commutative encryption-based technique to protect the mappings and ensure fairness among peers. An extensive security analysis of the protocol shows that it is resilient to several possible types of attacks. We implemented the protocol within an established PDMS: the Hyperion system. We conducted an experimental study using real data from the healthcare domain. The results show that our protocol manages to achieve its privacy and fairness goals, while maintaining query processing time at the interactive level.},
booktitle = {Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data},
pages = {759–770},
numpages = {12},
keywords = {privacy, peer-to-peer data integration, peer data management systems, mappings, fairness},
location = {Indianapolis, Indiana, USA},
series = {SIGMOD '10}
}

@inproceedings{10.5555/2387880.2387898,
author = {Wolinsky, David Isaac and Corrigan-Gibbs, Henry and Ford, Bryan and Johnson, Aaron},
title = {Dissent in numbers: making strong anonymity scale},
year = {2012},
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {Current anonymous communication systems make a trade-off between weak anonymity among many nodes, via onion routing, and strong anonymity among few nodes, via DC-nets. We develop novel techniques in Dissent, a practical group anonymity system, to increase by over two orders of magnitude the scalability of strong, traffic analysis resistant approaches. Dissent derives its scalability from a client/server architecture, in which many unreliable clients depend on a smaller and more robust, but administratively decentralized, set of servers. Clients trust only that at least one server in the set is honest, but need not know or choose which server to trust. Unlike the quadratic costs of prior peer-to-peer DC-nets schemes, Dissent's client/server design makes communication and processing costs linear in the number of clients, and hence in anonymity set size. Further, Dissent's servers can unilaterally ensure progress, even if clients respond slowly or disconnect at arbitrary times, ensuring robustness against client churn, tail latencies, and DoS attacks. On DeterLab, Dissent scales to 5,000 online participants with latencies as low as 600 milliseconds for 600-client groups. An anonymous Web browsing application also shows that Dissent's performance suffices for interactive communication within smaller local-area groups.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
pages = {179–192},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}

@inproceedings{10.5555/2691365.2691496,
author = {Wu, Gang and Lin, Tao and Huang, Hsin-Ho and Chu, Chris and Beerel, Peter A.},
title = {Asynchronous circuit placement by lagrangian relaxation},
year = {2014},
isbn = {9781479962778},
publisher = {IEEE Press},
abstract = {Recent asynchronous VLSI circuit placement approach tries to leverage synchronous placement tools as much as possible by manual loop-breaking and creation of virtual clocks. However, this approach produces an exponential number of explicit timing constraints which is beyond the ability of synchronous placement tools to handle. Thus, synchronous placer can only produce suboptimal results. Also, it can be very costly in terms of runtime. This paper proposed a new placement approach for asynchronous VLSI circuits. We formulated the asynchronous timing-driven placement problem and transform this problem into a weighted wirelength minimization problem based on a Lagrangian relaxation framework. The problem can then be efficiently solved using any standard wirelength-driven placement engine that can handle net weights. We demonstrate our approach on QDI PCHB asynchronous circuit with a state-of-art quadratic placer. The experimental results show that our algorithm can effectively improve the asynchronous circuits performance at placement stage. In addition, the runtime of our algorithm is shown to be more scalable to large-scale circuits compared with the loop-breaking approach.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Computer-Aided Design},
pages = {641–646},
numpages = {6},
location = {San Jose, California},
series = {ICCAD '14}
}

@article{10.1145/2362336.2362343,
author = {Shin, Donghwa and Park, Jaehyun and Kim, Younghyun and Seo, Jaeam and Chang, Naehyuck},
title = {Control-theoretic cyber-physical system modeling and synthesis: A case study of an active direct methanol fuel cell},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/2362336.2362343},
doi = {10.1145/2362336.2362343},
abstract = {A joint optimization of the physical system and the cyber world is one of the key problems in the design of a cyber-physical system (CPS). The major mechanical forces and/or chemical reactions in a plant are commonly modified by actuators in the balance-of-plant (BOP) system. More powerful actuators requires more power, but generally increase the response of the physical system powered by the electrical energy generated by the physical system. To maximize the overall output of a power generating plant therefore requires joint optimization of the physical system and the cyber world, and this is a key factor in the design of a CPS.We introduce a systematic approach to the modeling and synthesis of a CPS that emphasize joint power optimization, using an active direct methanol fuel cell (DMFC) as a case study. Active DMFC systems are superior to passive DMFCs in terms of fuel efficiency thanks to their BOP system, which includes pumps, air blowers, and fans. However, designing a small-scale active DMFC with the best overall system efficiency requires the BOP system to be jointly optimized with the DMFC stack operation, because the BOP components are powered by the stack. Our approach to this synthesis problem involves i) BOP system characterization, ii) integrated DMFC system modeling, iii) configuring a system for the maximum net power output through design space exploration, iv) synthesis of feedback control tasks, and v) implementation.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jan},
articleno = {76},
numpages = {24},
keywords = {Direct methanol fuel cell, Cyber-physical systems, Balance of plants system}
}

@inproceedings{10.1145/2009916.2010173,
author = {Agichtein, Eugene and Gabrilovich, Evgeniy},
title = {Information organization and retrieval with collaboratively generated content},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010173},
doi = {10.1145/2009916.2010173},
abstract = {Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content.The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation.However, the quality and comprehensiveness of collaboratively created content vary widely, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial.The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1307–1308},
numpages = {2},
keywords = {collaboratively generated content},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1145/2124295.2124391,
author = {Agichtein, Eugene and Gabrilovich, Evgeniy},
title = {Mining, searching and exploiting collaboratively generated content on the web},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124391},
doi = {10.1145/2124295.2124391},
abstract = {Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content.The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation.However, the quality and comprehensiveness of collaboratively created content vary significantly, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial.The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {763–764},
numpages = {2},
keywords = {collaboratively-generated content},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@article{10.1145/2500098.2500103,
author = {Frank, Benjamin and Poese, Ingmar and Lin, Yin and Smaragdakis, Georgios and Feldmann, Anja and Maggs, Bruce and Rake, Jannis and Uhlig, Steve and Weber, Rick},
title = {Pushing CDN-ISP collaboration to the limit},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2500098.2500103},
doi = {10.1145/2500098.2500103},
abstract = {Today a spectrum of solutions are available for istributing content over the Internet, ranging from commercial CDNs to ISP-operated CDNs to content-provider-operated CDNs to peer-to-peer CDNs. Some deploy servers in just a few large data centers while others deploy in thousands of locations or even on millions of desktops. Recently, major CDNs have formed strategic alliances with large ISPs to provide content delivery network solutions. Such alliances show the natural evolution of content delivery today driven by the need to address scalability issues and to take advantage of new technology and business opportunities.In this paper we revisit the design and operating space of CDN-ISP collaboration in light of recent ISP and CDN alliances. We identify two key enablers for supporting collaboration and improving content delivery performance: informed end-user to server assignment and in-network server allocation. We report on the design and evaluation of a prototype system, NetPaaS, that materializes them. Relying on traces from the largest commercial CDN and a large tier-1 ISP, we show that NetPaaS is able to increase CDN capacity on-demand, enable coordination, reduce download time, and achieve multiple traffic engineering goals leading to a win-win situation for both ISP and CDN.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {34–44},
numpages = {11},
keywords = {network optimization, content delivery, cdn-isp collaboration}
}

@inproceedings{10.1145/2387238.2387252,
author = {Bruno, Raffaele and Conti, Marco and Mordacchini, Matteo and Passarella, Andrea},
title = {An analytical model for content dissemination in opportunistic networks using cognitive heuristics},
year = {2012},
isbn = {9781450316286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2387238.2387252},
doi = {10.1145/2387238.2387252},
abstract = {When faced with large amounts of data, human brains are able to swiftly react to stimuli and assert relevance of discovered information, even under uncertainty and partial knowledge. These efficient decision-making abilities rely on so-called cognitive heuristics, which are rapid, adaptive, light-weight yet very effective schemes used by the brain to solve complex problems. In a content-centric future Internet where users generate and disseminate large amounts of content through opportunistic networking techniques, individual nodes should exhibit those properties to support a scalable content dissemination system. We therefore study whether such cognitive heuristics can also be used in such a networking environment. To this end, in this paper we develop an analytical model that describes a content dissemination mechanism for opportunistic networks based on one such heuristics, known as the recognition heuristic. Our model takes into account the different popularities of content types, and highlights the impact of the shared memory contributed by individual nodes to make the dissemination process more efficient. Furthermore, our model allows us to investigate the performance of the dissemination process for very large number of nodes, which might be very difficult to carry out through a simulation-based study.},
booktitle = {Proceedings of the 15th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems},
pages = {61–68},
numpages = {8},
keywords = {recognition heuristic, opportunistic networks, data dissemination, cognitive heuristics, analytical model},
location = {Paphos, Cyprus},
series = {MSWiM '12}
}

@inproceedings{10.1145/2046707.2046730,
author = {Houmansadr, Amir and Nguyen, Giang T.K. and Caesar, Matthew and Borisov, Nikita},
title = {Cirripede: circumvention infrastructure using router redirection with plausible deniability},
year = {2011},
isbn = {9781450309486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046707.2046730},
doi = {10.1145/2046707.2046730},
abstract = {Many users face surveillance of their Internet communications and a significant fraction suffer from outright blocking of certain destinations. Anonymous communication systems allow users to conceal the destinations they communicate with, but do not hide the fact that the users are using them. The mere use of such systems may invite suspicion, or access to them may be blocked. We therefore propose Cirripede, a system that can be used for unobservable communication with Internet destinations. Cirripede is designed to be deployed by ISPs; it intercepts connections from clients to innocent-looking destinations and redirects them to the true destination requested by the client. The communication is encoded in a way that is indistinguishable from normal communications to anyone without the master secret key, while public-key cryptography is used to eliminate the need for any secret information that must be shared with Cirripede users.Cirripede is designed to work scalably with routers that handle large volumes of traffic while imposing minimal overhead on ISPs and not disrupting existing traffic. This allows Cirripede proxies to be strategically deployed at central locations, making access to Cirripede very difficult to block. We built a proof-of-concept implementation of Cirripede and performed a testbed evaluation of its performance properties.},
booktitle = {Proceedings of the 18th ACM Conference on Computer and Communications Security},
pages = {187–200},
numpages = {14},
keywords = {unobservability, censorship-resistance},
location = {Chicago, Illinois, USA},
series = {CCS '11}
}

@inproceedings{10.1145/1926385.1926399,
author = {Cousot, Patrick and Cousot, Radhia and Logozzo, Francesco},
title = {A parametric segmentation functor for fully automatic and scalable array content analysis},
year = {2011},
isbn = {9781450304900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1926385.1926399},
doi = {10.1145/1926385.1926399},
abstract = {We introduce FunArray, a parametric segmentation abstract domain functor for the fully automatic and scalable analysis of array content properties. The functor enables a natural, painless and efficient lifting of existing abstract domains for scalar variables to the analysis of uniform compound data-structures such as arrays and collections. The analysis automatically and semantically divides arrays into consecutive non-overlapping possibly empty segments. Segments are delimited by sets of bound expressions and abstracted uniformly. All symbolic expressions appearing in a bound set are equal in the concrete. The FunArray can be naturally combined via reduced product with any existing analysis for scalar variables. The analysis is presented as a general framework parameterized by the choices of bound expressions, segment abstractions and the reduction operator. Once the functor has been instantiated with fixed parameters, the analysis is fully automatic.We first prototyped FunArray in Arrayal to adjust and experiment with the abstractions and the algorithms to obtain the appropriate precision/ratio cost. Then we implemented it into Clousot, an abstract interpretation-based static contract checker for .NET. We empirically validated the precision and the performance of the analysis by running it on the main libraries of .NET and on its own code. We were able to infer thousands of non-trivial invariants and verify the implementation with a modest overhead (circa 1%). To the best of our knowledge this is the first analysis of this kind applied to such a large code base, and proven to scale.},
booktitle = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {105–118},
numpages = {14},
keywords = {static analysis, program verification., invariant synthesis, array property inference, array content analysis, array abstraction, abstract interpretation},
location = {Austin, Texas, USA},
series = {POPL '11}
}

@inproceedings{10.5555/1994486.1994508,
author = {Rajam, Sidhant and Cortez, Ruth and Vazhenin, Alexander and Bhalla, Subhash},
title = {Design patterns in enterprise application integration for e-learning arena},
year = {2010},
publisher = {University of Aizu Press},
address = {Fukushima-ken, JPN},
abstract = {Pattern based design is an effective way to avoid an expensive process of reinventing, rediscovering and revalidating agnostic software artifacts. The Enterprise Application Integration (EAI) leverages the reusability factor of an application by applying decoupling and location transparency in the communication of the disparate applications and services. Design patterns are reusable solutions to solve recurring issues pertaining to the Functional, Non-Functional and Implementation tasks. The e-Learning is an ever growing and expanding arena. It has huge number of disparate applications and services that can be exposed over a ubiquitous media, such as the Internet, to the various kinds of end users. Therefore, the EAI is an important aspect in the e-Learning Arena in order to increase the high reusability and application decoupling factors. In this paper, we are imitating the ModelView-Controller (MVC) design patterns in order to explore the other composite patterns for an efficient integration of the applications and services. The demarcation of a Functional (View) and an Implementation (Model) task can be achieved deliberately by inducing an Integrator (Controller). The Controller can be further enriched to encapsulate certain Non-Functional activities such as security, reliability, scalability, and routing of request. This enables the separation of an Integration Logic from that of a Functional Logic (Client Application) and an Implementation Logic (Service). The Controller can be viewed by using the compound design pattern of the Enterprise Service Bus (ESB). This paper discusses how the Dependency Injection pattern is used in the ESB pattern for the integration of the e-Learning applications.},
booktitle = {Proceedings of the 13th International Conference on Humans and Computers},
pages = {81–88},
numpages = {8},
keywords = {web services, service oriented architecture, enterprise service bus, enterprise application integration, e-learning, design patterns, dependency injection},
location = {Aizu-Wakamatsu, Japan},
series = {HC '10}
}

@article{10.1109/TCBB.2010.117,
author = {Tenazinha, Nuno and Vinga, Susana},
title = {A Survey on Methods for Modeling and Analyzing Integrated Biological Networks},
year = {2011},
issue_date = {July 2011},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {8},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2010.117},
doi = {10.1109/TCBB.2010.117},
abstract = {Understanding how cellular systems build up integrated responses to their dynamically changing environment is one of the open questions in Systems Biology. Despite their intertwinement, signaling networks, gene regulation and metabolism have been frequently modeled independently in the context of well-defined subsystems. For this purpose, several mathematical formalisms have been developed according to the features of each particular network under study. Nonetheless, a deeper understanding of cellular behavior requires the integration of these various systems into a model capable of capturing how they operate as an ensemble. With the recent advances in the "omics” technologies, more data is becoming available and, thus, recent efforts have been driven toward this integrated modeling approach. We herein review and discuss methodological frameworks currently available for modeling and analyzing integrated biological networks, in particular metabolic, gene regulatory and signaling networks. These include network-based methods and Chemical Organization Theory, Flux-Balance Analysis and its extensions, logical discrete modeling, Petri Nets, traditional kinetic modeling, Hybrid Systems and stochastic models. Comparisons are also established regarding data requirements, scalability with network size and computational burden. The methods are illustrated with successful case studies in large-scale genome models and in particular subsystems of various organisms.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {943–958},
numpages = {16},
keywords = {survey, modeling methodologies, integrated biological networks., Systems biology}
}

@inproceedings{10.1145/2069105.2069114,
author = {Nie, Pin and V\"{a}h\"{a}-Herttua, Juho and Aura, Tuomas and Gurtov, Andrei},
title = {Performance analysis of HIP diet exchange for WSN security establishment},
year = {2011},
isbn = {9781450308991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2069105.2069114},
doi = {10.1145/2069105.2069114},
abstract = {Wireless Sensor Nodes are powered by limited batteries and equipped with constrained processor and memory. Therefore, security protocol must be highly efficient to fit WSNs. Meanwhile, considering the large variety of WSN applications and wide deployment, scalability and interoperability are two important concerns of adopting standardized communication protocols. HIP DEX, an IETF Internet draft, provides a generic solution to establish secure connections in WSNs. In this paper, we investigate the security features of HIP DEX based on several practical attack models. We evaluate the performance efficiency of HIP DEX in terms of energy consumption and computing latency on an experimental prototype. Our empirical results show that HIP DEX is applicable for resource constrained sensor nodes to establish hop-by-hop secure connection. In order to reinforce identity protection, we also propose tentative improvements to HIP DEX. Finally, we compare HIP DEX with SSL/TLS to highlight their respective advantages in different WSN architectures.},
booktitle = {Proceedings of the 7th ACM Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {51–56},
numpages = {6},
keywords = {wireless sensor networks, security protocol, host identity protocol, elliptic curve cryptography},
location = {Miami, Florida, USA},
series = {Q2SWinet '11}
}

@inproceedings{10.1109/CCGrid.2011.29,
author = {Chiu, David and Shetty, Apeksha and Agrawal, Gagan},
title = {Evaluating and Optimizing Indexing Schemes for a Cloud-Based Elastic Key-Value Store},
year = {2011},
isbn = {9780769543956},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CCGrid.2011.29},
doi = {10.1109/CCGrid.2011.29},
abstract = {Cloud computing has emerged to provide virtual, pay-as-you-go computing and storage services over the Internet, where the usage cost directly depends on consumption. One compelling feature in Clouds is elasticity, where a user can demand, and be immediately given access to, more (or less) resources based on requirements. However, this feature introduces new challenges in developing application and services. In this paper, we focus on the challenges in data management in Cloud environments, in view of elasticity. Particularly, we consider an elastic key-value store, which is used to cache intermediate results in a service-oriented system, and accelerate future queries by reusing the stored values. Such a key-value store can clearly benefit from the elasticity offered by Clouds, by expanding the cache during query-intensive periods. However, supporting an elastic key-value store involves many challenges, including selecting an appropriate indexing scheme, data migration upon elastic resource provisioning, and optimizations to remove certain overheads in the Cloud. This paper focuses on the design of an elastic key-value store. We consider three ubiquitous methods for indexing: B+-Trees, Extendible Hashing, and Bloom Filters, and we show how these schemes can be modified to exploit elasticity in Clouds. We also evaluate various performance aspects associated with the use of these indexing schemes. Furthermore, we have developed a heuristic to request elastic compute resources for expanding the cache such that instance startup overheads are minimized in our scheme. Our evaluation studies show that the index selection depends on various application and system level parameters that we have identified. And while we confirm that B+-Trees, which pervade many of today's key-value systems, would scale well, we showcases when Extendible Hashing would outperform B+-Trees.},
booktitle = {Proceedings of the 2011 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {362–371},
numpages = {10},
keywords = {elasticity, cloud, caching, AWS},
series = {CCGRID '11}
}

@article{10.5555/2317330.2317349,
author = {Elmokashfi, Ahmed and Kvalbein, Amund and Dovrolis, Constantine},
title = {BGP churn evolution: a perspective from the core},
year = {2012},
issue_date = {April 2012},
publisher = {IEEE Press},
volume = {20},
number = {2},
issn = {1063-6692},
abstract = {The scalability limitations of BGP have been a major concern lately. An important aspect of this issue is the rate of routing updates (churn) that BGP routers must process. This paper presents an analysis of the evolution of churn in four networks at the backbone of the Internet over a period of seven years and eight months, using BGP update traces from the RouteViews project. The churn rate varies widely over time and between networks. Instead of descriptive "black-box" statistical analysis, we take an exploratory data analysis approach attempting to understand the reasons behind major observed characteristics of the churn time series. We find that duplicate announcements are a major churn contributor, responsible for most large spikes. Remaining spikes are mostly caused by routing incidents that affect a large number of prefixes simultaneously. More long-term intense periods of churn, on the other hand, are caused by misconfigurations or other special events at or close to the monitored autonomous system (AS). After filtering pathologies and effects that are not related to the long-term evolution of churn, we analyze the remaining "baseline" churn and find that it is increasing at a rate that is similar to the growth of the number of ASs.},
journal = {IEEE/ACM Trans. Netw.},
month = {apr},
pages = {571–584},
numpages = {14},
keywords = {scalability, routing, internetworking, BGP}
}

@inproceedings{10.1145/2435349.2435371,
author = {Perera, Graciela and Miller, Nathan and Mela, John and McGarry, Michael P. and Acosta, Jaime C.},
title = {Emulating internet topology snapshots in deterlab},
year = {2013},
isbn = {9781450318907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2435349.2435371},
doi = {10.1145/2435349.2435371},
abstract = {Investigating the Internet's topology is one component towards developing mechanisms that can protect the communication infrastructure underlying our critical systems and applications. We study the feasibility of capturing and fitting Internet's topology snapshots to an emulated environment called Deterlab. Physical limitations on Deterlab include the number of nodes available (i.e., about 400) and the number of interfaces (i.e., 4) to interconnect them. For example, one Internet's topology snapshot at the Autonomous Systems (AS) level has about 100 nodes with 5 nodes requiring more than 4 interfaces. In this paper, we present a short summary of the Internet's topology snapshots collected and propose a solution on how we can represent the snapshots in Deterlab and overcome the limitation of nodes requiring more than four interfaces. Preliminary results show that all paths from snapshots are maintained if a node requiring more than four interfaces had no more than four other nodes requiring four interfaces. Also, we constructed a proof of concept that captures the main idea of using then snapshots in a security experiment in Deterlab. The topology shows a Multiple Origin Autonomous System (MOAS) conflict for 10 nodes. It is scalable to larger topologies in Deterlab because we have automated the topology creation and protocol configuration.},
booktitle = {Proceedings of the Third ACM Conference on Data and Application Security and Privacy},
pages = {165–168},
numpages = {4},
keywords = {vulnerability analysis, testbed, internet topology},
location = {San Antonio, Texas, USA},
series = {CODASPY '13}
}

@inproceedings{10.1145/2207676.2208351,
author = {Nobarany, Syavash and Oram, Louise and Rajendran, Vasanth Kumar and Chen, Chi-Hsiang and McGrenere, Joanna and Munzner, Tamara},
title = {The design space of opinion measurement interfaces: exploring recall support for rating and ranking},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208351},
doi = {10.1145/2207676.2208351},
abstract = {Rating interfaces are widely used on the Internet to elicit people's opinions. Little is known, however, about the effectiveness of these interfaces and their design space is relatively unexplored. We provide a taxonomy for the design space by identifying two axes: Measurement Scale for absolute rating vs. relative ranking, and Recall Support for the amount of information provided about previously recorded opinions. We present an exploration of the design space through iterative prototyping of three alternative interfaces and their evaluation. Among many findings, the study showed that users do take advantage of recall support in interfaces, preferring those that provide it. Moreover, we found that designing ranking systems is challenging; there may be a mismatch between a ranking interface that forces people to specify a total ordering for a set of items, and their mental model that some items are not directly comparable to each other.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2035–2044},
numpages = {10},
keywords = {review, rating, ranking, opinion, attitude},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/1835449.1835554,
author = {Li, Wei and Liu, Yaduo and Xue, Xiangyang},
title = {Robust audio identification for MP3 popular music},
year = {2010},
isbn = {9781450301534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835449.1835554},
doi = {10.1145/1835449.1835554},
abstract = {Audio identification via fingerprint has been an active research field with wide applications for years. Many technical papers were published and commercial software systems were also employed. However, most of these previously reported methods work on the raw audio format in spite of the fact that nowadays compressed format audio, especially MP3 music, has grown into the dominant way to store on personal computers and transmit on the Internet. It would be interesting if a compressed unknown audio fragment is able to be directly recognized from the database without the fussy and time-consuming decompression-identification-recompression procedure. So far, very few algorithms run directly in the compressed domain for music information retrieval, and most of them take advantage of MDCT coefficients or derived energy type of features. As a first attempt, we propose in this paper utilizing compressed-domain spectral entropy as the audio feature to implement a novel audio fingerprinting algorithm. The compressed songs stored in a music database and the possibly distorted compressed query excerpts are first partially decompressed to obtain the MDCT coefficients as the intermediate result. Then by grouping granules into longer blocks, remapping the MDCT coefficients into 192 new frequency lines to unify the frequency distribution of long and short windows, and defining 9 new subbands which cover the main frequency bandwidth of popular songs in accordance with the scale-factor bands of short windows, we calculate the spectral entropy of all consecutive blocks and come to the final fingerprint sequence by means of magnitude relationship modeling. Experiments show that such fingerprints exhibit strong robustness against various audio signal distortions like recompression, noise interference, echo addition, equalization, band-pass filtering, pitch shifting, and slight time-scale modification etc. For 5s-long query examples which might be severely degraded, an average top-five retrieval precision rate of more than 90% can be obtained in our test data set composed of 1822 popular songs.},
booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {627–634},
numpages = {8},
keywords = {compressed domain, audio identification, MDCT spectral entropy},
location = {Geneva, Switzerland},
series = {SIGIR '10}
}

@inproceedings{10.1145/2494603.2480343,
author = {Healey, Patrick G.T.},
title = {Design for human interaction: communication as a special case of misunderstanding},
year = {2013},
isbn = {9781450321389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494603.2480343},
doi = {10.1145/2494603.2480343},
abstract = {In order to engineer effective and usable interactive computing systems we need to consider not just the human-system interface but the human-human interface. The success of many technologies depends not just on how easy they are to understand and operate but also on how effectively they integrate with the wider ecology of our interactions with others. This point has been made especially clearly by ethnomethodological studies of the use of technology in workplace contexts (e.g. Heath and Luff, 2000). It also helps to explain why, for example, the evolution of video and music technology has been driven as much by ease of sharing as it has been by image or audio quality and why some technologies, such as SMS messaging succeed despite having a poor human-system interface. As Kang (2000) succinctly put it "The killer application of the internet is other people" (p. 1150, cited in Bargh and McKenna 2004).If technology acts, by accident or by design, as an interface between people then we might try to generalise human-system approaches to design by treating them as the basic building blocks of the larger human-system-human interface. This talk will argue, however, that this kind of 'scaling-up' approach is insufficient. In particular, the generalization of human-system models to contexts which involve multiple participants leads us to ignore some critical processes that underpin the effectiveness of human-human interaction. More specifically, a focus on the cognitive, behavioural or communicative capabilities of individual human beings does not provide an adequate understanding of how different people co-ordinate their understanding of what they are doing through communication.This line of argument suggests that in addition to understanding the broader social context of interactive systems we can also benefit from focusing on the specific low level mechanisms that underpin human interaction. The recurrent need to co-ordinate understanding amongst multiple participants, across a variety of contexts, highlights the importance of the processes by which people collaborate to detect and recover from misunderstandings using whatever resources are to hand (Sacks, Schegloff, and Jefferson, 1974; Clark 1996, Healey, 2008).This approach can feed into the design of interactive systems in a number of ways. It moves our understanding of human interaction beyond 'informational bandwith' and 'psychological bandwidth' approaches. It brings into focus co-ordination processes that are often impeded even by tools that are specifically designed to support human communication. This can provide new ideas for design, a diagnostic process for requirements gathering and formative analysis and comparative metrics for assessing how a technology impacts on the success of communication (Healey, Colman and Thirlwell, 2005).},
booktitle = {Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {119–120},
numpages = {2},
keywords = {repair, miscommunication, human interaction, design},
location = {London, United Kingdom},
series = {EICS '13}
}

@inproceedings{10.1145/2531602.2531607,
author = {Priedhorsky, Reid and Culotta, Aron and Del Valle, Sara Y.},
title = {Inferring the origin locations of tweets with quantitative confidence},
year = {2014},
isbn = {9781450325400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2531602.2531607},
doi = {10.1145/2531602.2531607},
abstract = {Social Internet content plays an increasingly critical role in many domains, including public health, disaster management, and politics. However, its utility is limited by missing geographic information; for example, fewer than 1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable, content-based approach to estimate the location of tweets using a novel yet simple variant of gaussian mixture models. Further, because real-world applications depend on quantified uncertainty for such estimates, we propose novel metrics of accuracy, precision, and calibration, and we evaluate our approach accordingly. Experiments on 13 million global, comprehensively multi-lingual tweets show that our approach yields reliable, well-calibrated results competitive with previous computationally intensive methods. We also show that a relatively small number of training data are required for good estimates (roughly 30,000 tweets) and models are quite time-invariant (effective on tweets many weeks newer than the training set). Finally, we show that toponyms and languages with small geographic footprint provide the most useful location signals.},
booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing},
pages = {1523–1536},
numpages = {14},
keywords = {uncertainty quantification, twitter, metrics, location inference, geo-location, gaussian mixture models},
location = {Baltimore, Maryland, USA},
series = {CSCW '14}
}

@article{10.1109/TCBB.2011.28,
author = {Pattengale, Nicholas and Aberer, Andre and Swenson, Krister and Stamatakis, Alexandros and Moret, Bernard},
title = {Uncovering Hidden Phylogenetic Consensus in Large Data Sets},
year = {2011},
issue_date = {July 2011},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {8},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2011.28},
doi = {10.1109/TCBB.2011.28},
abstract = {Many of the steps in phylogenetic reconstruction can be confounded by "rogue” taxa—taxa that cannot be placed with assurance anywhere within the tree, indeed, whose location within the tree varies with almost any choice of algorithm or parameters. Phylogenetic consensus methods, in particular, are known to suffer from this problem. In this paper, we provide a novel framework to define and identify rogue taxa. In this framework, we formulate a bicriterion optimization problem, the relative information criterion, that models the net increase in useful information present in the consensus tree when certain taxa are removed from the input data. We also provide an effective greedy heuristic to identify a subset of rogue taxa and use this heuristic in a series of experiments, with both pathological examples from the literature and a collection of large biological data sets. As the presence of rogue taxa in a set of bootstrap replicates can lead to deceivingly poor support values, we propose a procedure to recompute support values in light of the rogue taxa identified by our algorithm; applying this procedure to our biological data sets caused a large number of edges to move from "unsupported” to "supported” status, indicating that many existing phylogenies should be recomputed and reevaluated to reduce any inaccuracies introduced by rogue taxa. We also discuss the implementation issues encountered while integrating our algorithm into RAxML v7.2.7, particularly those dealing with scaling up the analyses. This integration enables practitioners to benefit from our algorithm in the analysis of very large data sets (up to 2,500 taxa and 10,000 trees, although we present the results of even larger analyses).},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {902–911},
numpages = {10},
keywords = {support values, consensus methods, bootstrapping, Phylogeny, MAST.}
}

@inproceedings{10.1145/2020408.2020580,
author = {Kang, U. and Tong, Hanghang and Sun, Jimeng and Lin, Ching-Yung and Faloutsos, Christos},
title = {GBASE: a scalable and general graph management system},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020580},
doi = {10.1145/2020408.2020580},
abstract = {Graphs appear in numerous applications including cyber-security, the Internet, social networks, protein networks, recommendation systems, and many more. Graphs with millions or even billions of nodes and edges are common-place. How to store such large graphs efficiently? What are the core operations/queries on those graph? How to answer the graph queries quickly? We propose GBASE, a scalable and general graph management and mining system. The key novelties lie in 1) our storage and compression scheme for a parallel setting and 2) the carefully chosen graph operations and their efficient implementation. We designed and implemented an instance of GBASE using MapReduce/Hadoop. GBASE provides a parallel indexing mechanism for graph mining operations that both saves storage space, as well as accelerates queries. We ran numerous experiments on real graphs, spanning billions of nodes and edges, and we show that our proposed GBASE is indeed fast, scalable and nimble, with significant savings in space and time.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1091–1099},
numpages = {9},
keywords = {indexing, graph, distributed computing, compression},
location = {San Diego, California, USA},
series = {KDD '11}
}

@proceedings{10.1145/2048147,
title = {OOPSLA '11: Proceedings of the ACM international conference companion on Object oriented programming systems languages and applications companion},
year = {2011},
isbn = {9781450309424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SPLASH! SPLASH is the new umbrella for OOPSLA, Onward!, and the Dynamic Languages Symposium. This year, SPLASH also hosts the Scheme Workshop. As usual, a couple of other conferences chose to co-locate with SPLASH; this year, we have the conference on Generative Programming and Component Engineering (GPCE) and the Pattern Languages of Programming conference (PLoP).SPLASH has emerged from OOPSLA with the underlying drive to expand from it and to include more contributions than those that were typically accepted at OOPSLA. This transition didn't have a master plan; we tried several models for SPLASH and its relation to OOPSLA and Onward!. One of them was the "federated conference" model, like the ACM FCRC, where several existing conferences co-locate in the same place at about the same time. But that didn't feel quite right---there has always been a strong connection between OOPSLA, Onward!, and DLS. Separating them while co-locating them might make them compete with each other, which would be exactly the opposite of what we intended SPLASH to be.We realized that conferences have many possible views: there are at least internal and external views. The internal view is what the ACM uses for accounting and administration purposes; for this internal view, separation is a good thing, because it keeps every single event financially independent and accountable. This is what the federated conferences model does. The external view is what the attendees see; from the attendees' point of view, separation of all these events in the form of separate registration fees is bad, because people prefer to flow freely from session to session without having to make upfront plans about what to attend. We realized that we needed an accounting model that served the attendees better than the federated conferences model does. So let me explain SPLASH with a picture:In the picture, the SPLASH box and the co-located conference boxes denote accounting borders. What this means is that SPLASH accommodates several conferences and symposia (OOPSLA, Onward!, DLS, etc.) within one single accounting box; participants see a simple registration fee that doesn't separate the different events, and whose price is proportional to the number of days that a participant decides to attend. As a consequence, during the SPLASH days, participants can freely roam to whatever sessions they want without having to register for individual events, which is exactly what all of us want to do! There is one final detail concerning this arrangement: even though we are grouping conferences under the SPLASH administrative and accounting umbrella, we don't want to disturb the intellectual autonomy and branding of those conferences. We want the SPLASH conferences to continue to make their own decisions regarding topics, scope, and criteria for content selection, as well as produce their own separate proceedings. This is crucial for the success of the SPLASH model and the success of each of its conferences.In short: SPLASH gives full autonomy to the different conferences in it, while minimizing their administrative overhead and serving participants the full spectrum of options about which parts to attend under one single registration fee. On top of this, it also supports the more traditional co-location model with other conferences that, for one reason or another, wish to remain financially independent. We hope that in the future more conferences join the SPLASH in whatever way they see fit.Design conferences as you may, SPLASH is the premier conference for researchers, practitioners, educators, and students who are passionate about all aspects of software construction and delivery, and who seek to find deep insights about software that go beyond the shiny surfaces of the latest trends. There is no question that software is having a tremendous impact in Society. The SPLASH community should be proud of the fact that many of the technologies and methodologies that underlie modern software have emerged here at OOPSLA. I decided to choose a theme this year that captures the change in the order of magnitude of computing that happened over the past few years: The Internet as the world-wide Virtual Machine. We're operating at the global scale now. These days software systems are rarely designed in isolation; they connect to pieces written by 3rd parties, they communicate with other pieces over the Internet, they use big data produced elsewhere, they touch millions of interacting users through an ever larger variety of physical devices...in other words, the "machine" is now a global computing network. What does this entail for software development itself?In this publication, you will find the collection of proceedings of the several sponsored conferences, as well as many papers and summaries of sessions that have a more informal arrangement within SPLASH. I believe we have assembled an impressive technical program, and I hope you enjoy it!},
location = {Portland, Oregon, USA}
}

@inproceedings{10.1145/1964858.1964866,
author = {Hui, Peter and Gregory, Michelle},
title = {Quantifying sentiment and influence in blogspaces},
year = {2010},
isbn = {9781450302173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1964858.1964866},
doi = {10.1145/1964858.1964866},
abstract = {The weblog, or blog, has become a popular form of social media, through which authors can write posts, which can in turn generate feedback in the form of user comments. When considered in totality, a collection of blogs can thus be viewed as a sort of informal collection of mass sentiment and opinion. An obvious topic of interest might be to mine this collection to obtain some gauge of public sentiment over the wide variety of topics contained therein. However, the sheer size of the so-called blogosphere, combined with the fact that the subjects of posts can vary over a practically limitless number of topics poses some serious challenges when any meaningful analysis is attempted. Namely, the fact that largely anyone with access to the Internet can author their own blog, raises the serious issue of credibility---should some blogs be considered to be more influential than others, and consequently, when gauging sentiment with respect to a topic, should some blogs be weighted more heavily than others? In addition, as new posts and comments can be made on almost a constant basis, any blog analysis algorithm must be able to handle such updates efficiently. In this paper, we give a formalization of the blog model. We give formal methods of quantifying sentiment and influence with respect to a hierarchy of topics, with the specific aim of facilitating the computation of a per-topic, influence-weighted sentiment measure. Finally, as efficiency is a specific endgoal, we give upper bounds on the time required to update these values with new posts, showing that our analysis and algorithms are scalable.},
booktitle = {Proceedings of the First Workshop on Social Media Analytics},
pages = {53–61},
numpages = {9},
keywords = {social media analytics, sentiment, influence, formal methods},
location = {Washington D.C., District of Columbia},
series = {SOMA '10}
}

@proceedings{10.1145/2048066,
title = {OOPSLA '11: Proceedings of the 2011 ACM international conference on Object oriented programming systems languages and applications},
year = {2011},
isbn = {9781450309400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SPLASH! SPLASH is the new umbrella for OOPSLA, Onward!, and the Dynamic Languages Symposium. This year, SPLASH also hosts the Scheme Workshop. As usual, a couple of other conferences chose to co-locate with SPLASH; this year, we have the conference on Generative Programming and Component Engineering (GPCE) and the Pattern Languages of Programming conference (PLoP).SPLASH has emerged from OOPSLA with the underlying drive to expand from it and to include more contributions than those that were typically accepted at OOPSLA. This transition didn't have a master plan; we tried several models for SPLASH and its relation to OOPSLA and Onward!. One of them was the "federated conference" model, like the ACM FCRC, where several existing conferences co-locate in the same place at about the same time. But that didn't feel quite right---there has always been a strong connection between OOPSLA, Onward!, and DLS. Separating them while co-locating them might make them compete with each other, which would be exactly the opposite of what we intended SPLASH to be.We realized that conferences have many possible views: there are at least internal and external views. The internal view is what the ACM uses for accounting and administration purposes; for this internal view, separation is a good thing, because it keeps every single event financially independent and accountable. This is what the federated conferences model does. The external view is what the attendees see; from the attendees' point of view, separation of all these events in the form of separate registration fees is bad, because people prefer to flow freely from session to session without having to make upfront plans about what to attend. We realized that we needed an accounting model that served the attendees better than the federated conferences model does. So let me explain SPLASH with a picture:In the picture, the SPLASH box and the co-located conference boxes denote accounting borders. What this means is that SPLASH accommodates several conferences and symposia (OOPSLA, Onward!, DLS, etc.) within one single accounting box; participants see a simple registration fee that doesn't separate the different events, and whose price is proportional to the number of days that a participant decides to attend. As a consequence, during the SPLASH days, participants can freely roam to whatever sessions they want without having to register for individual events, which is exactly what all of us want to do! There is one final detail concerning this arrangement: even though we are grouping conferences under the SPLASH administrative and accounting umbrella, we don't want to disturb the intellectual autonomy and branding of those conferences. We want the SPLASH conferences to continue to make their own decisions regarding topics, scope, and criteria for content selection, as well as produce their own separate proceedings. This is crucial for the success of the SPLASH model and the success of each of its conferences.In short: SPLASH gives full autonomy to the different conferences in it, while minimizing their administrative overhead and serving participants the full spectrum of options about which parts to attend under one single registration fee. On top of this, it also supports the more traditional co-location model with other conferences that, for one reason or another, wish to remain financially independent. We hope that in the future more conferences join the SPLASH in whatever way they see fit.Design conferences as you may, SPLASH is the premier conference for researchers, practitioners, educators, and students who are passionate about all aspects of software construction and delivery, and who seek to find deep insights about software that go beyond the shiny surfaces of the latest trends. There is no question that software is having a tremendous impact in Society. The SPLASH community should be proud of the fact that many of the technologies and methodologies that underlie modern software have emerged here at OOPSLA. I decided to choose a theme this year that captures the change in the order of magnitude of computing that happened over the past few years: The Internet as the world-wide Virtual Machine. We're operating at the global scale now. These days software systems are rarely designed in isolation; they connect to pieces written by 3rd parties, they communicate with other pieces over the Internet, they use big data produced elsewhere, they touch millions of interacting users through an ever larger variety of physical devices...in other words, the "machine" is now a global computing network. What does this entail for software development itself?In this publication, you will find the collection of proceedings of the several sponsored conferences, as well as many papers and summaries of sessions that have a more informal arrangement within SPLASH. I believe we have assembled an impressive technical program, and I hope you enjoy it!},
location = {Portland, Oregon, USA}
}

@inproceedings{10.5555/2093889.2093891,
author = {Alexander, Jason S. and Dean, Thomas and Knight, Scott},
title = {Spy vs. Spy: counter-intelligence methods for backtracking malicious intrusions},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Advanced malicious software threats have become commonplace in cyberspace, with large scale cyber threats exploiting consumer, corporate and government systems on a constant basis. Regardless of the target, upon successful infiltration into a target system an attacker will commonly deploy a backdoor to maintain persistent access as well as a rootkit to evade detection on the infected machine. If the attacked system has access to classified or sensitive material, virus eradication may not be the best response. Instead, a counter-intelligence operation may be initiated to track the infiltration back to its source. It is important that the counter-intelligence operations are not visible to the infiltrator.Rootkits can not only hide the malware, they can also be used to hide the detection and analysis operations by the defenders from the malware. This paper surveys the rootkit literature for their applicability to counter-intelligence operations.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {1–14},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@proceedings{10.1145/2095050,
title = {SPLASH '11 Workshops: Proceedings of the compilation of the co-located workshops on DSM'11, TMC'11, AGERE! 2011, AOOPES'11, NEAT'11, &amp; VMIL'11},
year = {2011},
isbn = {9781450311830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SPLASH! SPLASH is the new umbrella for OOPSLA, Onward!, and the Dynamic Languages Symposium. This year, SPLASH also hosts the Scheme Workshop. As usual, a couple of other conferences chose to co-locate with SPLASH; this year, we have the conference on Generative Programming and Component Engineering (GPCE) and the Pattern Languages of Programming conference (PLoP).SPLASH has emerged from OOPSLA with the underlying drive to expand from it and to include more contributions than those that were typically accepted at OOPSLA. This transition didn't have a master plan; we tried several models for SPLASH and its relation to OOPSLA and Onward!. One of them was the "federated conference" model, like the ACM FCRC, where several existing conferences co-locate in the same place at about the same time. But that didn't feel quite right-there has always been a strong connection between OOPSLA, Onward!, and DLS. Separating them while co-locating them might make them compete with each other, which would be exactly the opposite of what we intended SPLASH to be.We realized that conferences have many possible views: there are at least internal and external views. The internal view is what the ACM uses for accounting and administration purposes; for this internal view, separation is a good thing, because it keeps every single event financially independent and accountable. This is what the federated conferences model does. The external view is what the attendees see; from the attendees' point of view, separation of all these events in the form of separate registration fees is bad, because people prefer to flow freely from session to session without having to make upfront plans about what to attend. We realized that we needed an accounting model that served the attendees better than the federated conferences model does. So let me explain SPLASH with a picture:In the picture, the SPLASH box and the co-located conference boxes denote accounting borders. What this means is that SPLASH accommodates several conferences and symposia (OOPSLA, Onward!, DLS, etc.) within one single accounting box; participants see a simple registration fee that doesn't separate the different events, and whose price is proportional to the number of days that a participant decides to attend.As a consequence, during the SPLASH days, participants can freely roam to whatever sessions they want without having to register for individual events, which is exactly what all of us want to do!There is one final detail concerning this arrangement: even though we are grouping conferences under the SPLASH administrative and accounting umbrella, we don't want to disturb the intellectual autonomy and branding of those conferences. We want the SPLASH conferences to continue to make their own decisions regarding topics, scope, and criteria for content selection, as well as produce their own separate proceedings. This is crucial for the success of the SPLASH model and the success of each of its conferences.In short: SPLASH gives full autonomy to the different conferences in it, while minimizing their administrative overhead and serving participants the full spectrum of options about which parts to attend under one single registration fee. On top of this, it also supports the more traditional co-location model with other conferences that, for one reason or another, wish to remain financially independent. We hope that in the future more conferences join the SPLASH in whatever way they see fit.Design conferences as you may, SPLASH is the premier conference for researchers, practitioners, educators, and students who are passionate about all aspects of software construction and delivery, and who seek to find deep insights about software that go beyond the shiny surfaces of the latest trends. There is no question that software is having a tremendous impact in Society. The SPLASH community should be proud of the fact that many of the technologies and methodologies that underlie modern software have emerged here at OOPSLA. I decided to choose a theme this year that captures the change in the order of magnitude of computing that happened over the past few years: The Internet as the world-wide Virtual Machine. We're operating at the global scale now. These days software systems are rarely designed in isolation; they connect to pieces written by 3rd parties, they communicate with other pieces over the Internet, they use big data produced elsewhere, they touch millions of interacting users through an ever larger variety of physical devices...in other words, the "machine" is now a global computing network. What does this entail for software development itself?In this publication, you will find the collection of proceedings of the several sponsored conferences, as well as many papers and summaries of sessions that have a more informal arrangement within SPLASH. I believe we have assembled an impressive technical program, and I hope you enjoy it!},
location = {Portland, Oregon, USA}
}

@article{10.1145/2362394.2362400,
author = {Dinakar, Karthik and Jones, Birago and Havasi, Catherine and Lieberman, Henry and Picard, Rosalind},
title = {Common Sense Reasoning for Detection, Prevention, and Mitigation of Cyberbullying},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/2362394.2362400},
doi = {10.1145/2362394.2362400},
abstract = {Cyberbullying (harassment on social networks) is widely recognized as a serious social problem, especially for adolescents. It is as much a threat to the viability of online social networks for youth today as spam once was to email in the early days of the Internet. Current work to tackle this problem has involved social and psychological studies on its prevalence as well as its negative effects on adolescents. While true solutions rest on teaching youth to have healthy personal relationships, few have considered innovative design of social network software as a tool for mitigating this problem. Mitigating cyberbullying involves two key components: robust techniques for effective detection and reflective user interfaces that encourage users to reflect upon their behavior and their choices.Spam filters have been successful by applying statistical approaches like Bayesian networks and hidden Markov models. They can, like Google’s GMail, aggregate human spam judgments because spam is sent nearly identically to many people. Bullying is more personalized, varied, and contextual. In this work, we present an approach for bullying detection based on state-of-the-art natural language processing and a common sense knowledge base, which permits recognition over a broad spectrum of topics in everyday life. We analyze a more narrow range of particular subject matter associated with bullying (e.g. appearance, intelligence, racial and ethnic slurs, social acceptance, and rejection), and construct BullySpace, a common sense knowledge base that encodes particular knowledge about bullying situations. We then perform joint reasoning with common sense knowledge about a wide range of everyday life topics. We analyze messages using our novel AnalogySpace common sense reasoning technique. We also take into account social network analysis and other factors. We evaluate the model on real-world instances that have been reported by users on Formspring, a social networking website that is popular with teenagers.On the intervention side, we explore a set of reflective user-interaction paradigms with the goal of promoting empathy among social network participants. We propose an “air traffic control”-like dashboard, which alerts moderators to large-scale outbreaks that appear to be escalating or spreading and helps them prioritize the current deluge of user complaints. For potential victims, we provide educational material that informs them about how to cope with the situation, and connects them with emotional support from others. A user evaluation shows that in-context, targeted, and dynamic help during cyberbullying situations fosters end-user reflection that promotes better coping strategies.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {sep},
articleno = {18},
numpages = {30},
keywords = {artificial intelligence, affective computing, Common sense reasoning}
}

@inproceedings{10.1145/1920261.1920300,
author = {Heusser, Jonathan and Malacaria, Pasquale},
title = {Quantifying information leaks in software},
year = {2010},
isbn = {9781450301336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1920261.1920300},
doi = {10.1145/1920261.1920300},
abstract = {Leakage of confidential information represents a serious security risk. Despite a number of novel, theoretical advances, it has been unclear if and how quantitative approaches to measuring leakage of confidential information could be applied to substantial, real-world programs. This is mostly due to the high complexity of computing precise leakage quantities. In this paper, we introduce a technique which makes it possible to decide if a program conforms to a quantitative policy which scales to large state-spaces with the help of bounded model checking.Our technique is applied to a number of officially reported information leak vulnerabilities in the Linux Kernel. Additionally, we also analysed authentication routines in the Secure Remote Password suite and of a Internet Message Support Protocol implementation. Our technique shows when there is unacceptable leakage; the same technique is also used to verify, for the first time, that the applied software patches indeed plug the information leaks.This is the first demonstration of quantitative information flow addressing security concerns of real-world industrial programs.},
booktitle = {Proceedings of the 26th Annual Computer Security Applications Conference},
pages = {261–269},
numpages = {9},
keywords = {quantitative information flow, information leakage, Linux kernel},
location = {Austin, Texas, USA},
series = {ACSAC '10}
}

@inproceedings{10.5555/2429759.2430192,
author = {Balbo, Gianfranco},
title = {Titans talk on "modeling and simulation of complex systems: are petri nets useful?"},
year = {2012},
publisher = {Winter Simulation Conference},
abstract = {Modeling and analysis of complex systems is becoming increasingly popular due to the availability of powerful processors and the possibility of distributing the analysis over a large set of cooperating computers. Within this context, simulation is often the method of choice for studying the validity of a model and for deriving reliable indications on the efficiency and the effectiveness of the system under study. Despite the power of the machines used for these analyses, the complexity of the models often exceeds the capabilities of direct simulation methods and techniques must be developed to exploit the structure of the model to derive faster simulation algorithms and to obtain reliable performance indications. Petri nets (PNS) are a formalism which allows a precise representation of the intricacy of modern systems and thus of the interactions among different system components characterized by internal complex functionalities with a very well defined semantics. In this paper we will discuss the properties of PNs that are useful for a preliminary qualitative validation of the model and we will show how the PN representation can be easily exploited to gain a reasonable confidence about the correctness of the model. Moreover, we will discuss the possibility of using the structure of the PN model to perform multi-scale analysis of systems with many components characterized by large speed differences. Examples from Systems Biology and from immunology will be used to support the arguments discussed in the paper.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {321},
numpages = {1},
location = {Berlin, Germany},
series = {WSC '12}
}

@inproceedings{10.1145/1953563.1953566,
author = {Baldoni, Roberto and Bonomi, Silvia and Cerocchi, Adriano and Querzoni, Leonardo},
title = {Improving validity of query answering in dynamic systems},
year = {2010},
isbn = {9781450306423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1953563.1953566},
doi = {10.1145/1953563.1953566},
abstract = {Let us consider a large scale distributed system and a query executed on top of it where every process has to contribute to the result. Informally, a query satisfies the interval validity property if its result has been calculated by retrieving data from a set of processes containing at least all those ones that have been present in the system during the whole query lifetime. If the system is prone to churn, it is easy to show that a query cannot deterministically satisfy interval validity. In this paper we propose a novel algorithm that can be used to support distributed queries by increasing the probability of a query to satisfy interval validity. The algorithm strives to (i) reduce the query calculation time (to reduce the net effect of churn) and to (ii) increase the robustness of the overlay network it builds by clustering nodes into cliques of limited size in order for their implementation to be still practical. The paper provides a set of experiments that show the tradeoff between the churn rate and the number of times the interval validity is satisfied.},
booktitle = {Proceedings of the Third International Workshop on Reliability, Availability, and Security},
articleno = {4},
numpages = {6},
keywords = {interval validity, dynamic distributed systems, distributed query answering},
location = {Zurich, Switzerland},
series = {WRAS '10}
}

@inproceedings{10.5555/2399776.2399805,
author = {Bist, Gary},
title = {Business process management (BPM) in a day},
year = {2012},
publisher = {IBM Corp.},
address = {USA},
abstract = {Collaboration is the key to agility today; make the most of all of your resources. In the business process management software market, those resources are typically business users and technical users. The business user is focused on optimizing the flow of a business process. Technical users, who are typically developers, are interested in optimizing and automating tasks in the business process with applications.This hands-on workshop showed attendees how to develop business processes rapidly as a business user such as a business analyst and how to use tools to implement services needed by the same business process as a technical user. A key aspect of modern business is the emergence of standards, which allow corporations to create platform independent and vendor neutral business solutions. The workshop began with an overview of some standards like the Business Process Modeling Notation (BPMN) used to design processes. The Web Services Description Language (WSDL) was presented as the one to create interoperable services. The Business Process Execution Language (BPEL) was shown as the means to create standard business processes. Similarly, service oriented architecture (SOA) was described as the backbone to these standards.As explained, these standards come built in when a user works with the Business Process Manager tools. In other words, when someone builds a business process with the tools in the product, the output generated already conforms to these industry standards. The participants examined the three layers of a process: business integration, services and implementation. The separation of a business process in the business integration layer from its implementation is the key to understanding the portability of business processes. It also explains how services which comprise a business process can interact even though they are implemented in a variety of languages and run on a variety of servers in a variety of locations.While the focus of the workshop surrounds technology, the financial forces driving the move to business process management software were presented as well. Businesses must operate at optimal efficiency, must automate processes wherever possible, and be wary of the financial implications if software is not vendor and platform neutral. Interoperability and scalability may sound like technical terms, but as this workshop showed, business needs are what drive technologies like business process management. Putting everything together, the workshop participants were told of Johansson's definition of a business process: A set of linked activities that take an input and transform it to create an output. Ideally, the transformation that occurs in the process should add value to the input and create an output that is more useful and effective to the recipient either upstream or downstream.The first exercise showed CASCON participants how to rapidly create a business process using Process Designer. This visual tool made outlining a sequence of services fast and easy by setting up conditions when business process may go one way or another depending on runtime events. A process application was defined in the Process Center. A process application is like a project container for business processes; participants had to think through some design issues. Business processes can get big and complex quickly. Thus, design decisions are important as poor ones will lead to usability and performance problems. Participants learned to assemble activities before the implementation and add decision points where a business process might change at run time. They learned to wire the services together and added variables to pass data around the business process. They learned how to develop a service top down: defining it in Process Designer and then, as an IT person might, implement the service in another tool, Integration Designer. The service was an Advanced Integration Service, a service that has its design in Process Designer and its implementation in another component, Integration Designer.Furthermore, participants also spent time with the Coach, an assistant that helps users design effective user interfaces. The test tool known as the Inspector was used to test the CASCON participants' partially completed business process. Once tested, they learned how to version a business process by taking a snapshot. A snapshot captures the state of a business process at a point in time.In the next part of the workshop, participants were told how to reuse services they already had; this is known as a bottom up approach. Key concepts like using association were discussed. Association means connecting your existing application with the business process. Additionally, library mirroring was discussed. With library mirroring, one can push already written programs into a business process shared by different development environments. Once the library is updated, the changes to that library are made available to both Process Designer and Integration Designer. Workshop participants were shown how to make interfaces in their existing applications in Integration Developer visible using library mirroring to the Process Designer user. Finally, the participants put these ideas to work in the second exercise and tested it again with the Inspector, where a snapshot was taken.In the next lecture and final exercise, CASCON participants learned about bindings, imports, exports and mediations - important ideas to know when developing sophisticated business processes that work with a variety of external systems resulting in the need to handle different data formats dynamically. This exercise had the CASCON participants working with complex types, which are a common way of passing structured XML data objects. Complex types are also called business objects. In a section on mediation, they used Integration Designer's mapper to map one kind of data element in one system to another kind of data element from another system. They also used Business Process Manager's mediation editor to handle faults, as failures in a system can happen at any time. If a failure does occur, a message needs to be returned to the business process immediately. The Inspector was used again to test the complete business process. The user interface that CASCON participants developed included a generated email note that made use of information found in the business objects. Another snapshot of their work was taken.The workshop concluded with a presentation on some things missed, such as adapters, messaging systems, monitoring and debugging, as well as a recap of the day's work. A list of helpful books and articles on this rapidly growing area were provided.The participants used Business Process Manager Advanced 8.0. This all-inclusive development environment used Process Designer, a design tool; Integration Designer, a tool to build sophisticated services; and the Process Center, an easy-to-use test and runtime environment.},
booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {246–247},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {CASCON '12}
}

@inproceedings{10.1145/2382196.2382332,
author = {Christodorescu, Mihai},
title = {Second workshop on building analysis datasets and gathering experience returns for security (BADGERS'12)},
year = {2012},
isbn = {9781450316514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382196.2382332},
doi = {10.1145/2382196.2382332},
abstract = {As more and more systems are controlled by computers and connected to the Internet, a tremendous amount of activity is generated when these systems interact with each other and with their users. The continuous stream of data related to such Internet-connected and computer-controlled systems is known as Big Data and introduces problems at new scales. Computer security and privacy can both benefit and suffer from Big Data, as tools and techniques are made readily available to process security-event streams and as attacks are harder to detect in the huge volume of event data. The BADGERS'12 workshop is a forum for discussing the challenges in and potential solutions for taking advantage of Big Data for improving computer security and privacy. The workshop builds on the successful first edition of BADGERS which was held in conjunction with the EuroSys 2011 conference in Salzburg, Austria [3].},
booktitle = {Proceedings of the 2012 ACM Conference on Computer and Communications Security},
pages = {1066–1067},
numpages = {2},
keywords = {volume, velocity, variety, security, privacy, data analysis, big data},
location = {Raleigh, North Carolina, USA},
series = {CCS '12}
}

@inproceedings{10.1145/1851182.1851236,
author = {Oprescu, Iuniana M. and Meulle, Mickael and Uhlig, Steve and Pelsser, Cristel and Maennel, Olaf and Owezarski, Philippe},
title = {Rethinking iBGP routing},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851236},
doi = {10.1145/1851182.1851236},
abstract = {The Internet is organized as a collection of administrative domains, known as Autonomous Systems (ASes). These ASes interact through the Border Gateway Protocol (BGP) that allows them to share reachability information. Adjacent routers in distinct ASes use external BGP (eBGP), whereas in a given AS routes are propagated over internal BGP (iBGP) sessions between any pair of routers. In large ASes where a logical full-mesh is not possible, confederations or route reflectors (RRs) are used. However, these somewhat scalable alternatives have introduced their own set of unpredictable effects (persistent routing oscillations and forwarding loops causing an increase of the convergence time) addressed in the literature [1].The solution we propose to these issues consists of a structured routing overlay holding a comprehensive view of the routes. We describe the design of a distributed entity that performs BGP route pre-computation for its clients inside a large backbone network and propagates the paths to the routers. Compared to the current iBGP routing, the advantage of the overlay approach is the separation between the responsibility of the control plane (route storage and best path computation) and the forwarding of the packets.One of the major improvements we bring is the divided routing table tackling the scalability concerns and allowing for parallel computation of paths.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {411–412},
numpages = {2},
keywords = {routing, bgp},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@proceedings{10.1145/2674005,
title = {CoNEXT '14: Proceedings of the 10th ACM International on Conference on emerging Networking Experiments and Technologies},
year = {2014},
isbn = {9781450332798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is a great pleasure to welcome everyone to the 10th ACM International Conference on emerging Networking EXperiments and Technologies (ACM CoNEXT'14). At the close of a decade as one of the major forums for presentations and discussions of novel computing technologies that will shape the future of Internet-working, the conference travels to another continent.The single track program including what we found the most promising submissions on computer and communication networks research at large is a reflection of commitment of all PC members to the three values of CoNEXT: recognizing farsighted excellence, promoting a broad definition of networking research, and assuring authors of submitted papers a fair and thorough review process. We hope it will bring new technical interactions and substantial novelty to the field of networking for the years to come. The program, 27 long papers and 10 short ones, was selected from the 187 papers we received (including 133 long and 54 short).The geographical diversity of our research community, with submission from authors in 25 countries, lead to 18 countries being represented by at least one author in the program. Often illrepresented as US-centric, our community is more balanced than one may have thought: although 21 papers have at least one author affiliated in North America, 19 and 5, contain one author affiliated in Europe or Pacific/Asia, respectively.Looked over topics, contributions in the program reflect our set of submissions: 12 papers on wireless, 10 on theory, 7 in novel application and other fields, and an expected high number 16 on papers on management, including on data-centers and SDN. Acceptance rate for all topics above are around 20%. The only exception we noticed is in systems and security, less represented in submissions and with a lower acceptance rate, ending with only 4 papers in our program.As can be judged by these numbers, the competition has been worldwide and the selection drastic. We would like to report information on how the reviewing process was organized.  Apart from two papers with clear content or formatting problems, each paper was formally reviewed by 3-5 reviewers, producing a full review and a binary 'accept/reject' suggestion. These reviews and suggestions have been used by the chairs and other PC members to start a discussion on the merits of the submission. The online discussion was vigorous (995 comments were posted by PC members, an average of 5 per paper) and members updated their review to reflect additional information gathered and discussed. A plenary PC meeting attended by the vast majority of PC members at SIGCOMM made final decisions on all controversial papers w.r.t. their merits and limitations.Multiple factors are important to select works to be presented, and we summarized them in two criteria: Impact and Technical Quality, graded on a scale from 1 to 4 with 4 being the strongest, allowing reviewers to specify what part of this paper they appreciated to justify their decisions.While the decision to include or not, each paper was done individually, and all papers have been considered independently of raw scores, we thought it might be important to share statistics on the 187 submissions we received19 received only accept scores (of which 100% were accepted)18 received a strict majority of accept (72% accepted)16 received a tie (30% accepted)134 ended with a strict majority of reject (0% accepted) Distributions of impact scores (1, 2, 3, 4) was (9%, 57%, 33%, 1%); technical quality (1, 2, 3, 4) was (11%, 45%, 38%, 6%).A minority of papers received interaction from PC members, a feature we encouraged whenever a short clarification could help reviewers assessing a particular fact about the paper. Note that this was not a rebuttal. In the end, 8 papers received such requests which were all answered promptly and used in the decision; 3 of those papers were accepted into the program. We think this was a useful approach and not one that causes excessive burden to authors and reviewers.  Following last year's CoNEXT, short papers were considered under the same term as long papers (i.e., mature work with novelty and impact), only with a more open mind towards criticism on the scope of evaluation or broadness of the topics. The acceptance ratio for long and short papers was nearly identical, and we will continue to support authors to use a variety of presentation formats while sticking to the same excellent standard of research.},
location = {Sydney, Australia}
}

@inproceedings{10.5555/2151688.2151701,
author = {Harrison, Peter G. and Llad\'{o}, Catalina M.},
title = {Hierarchically constructed Petri-nets and product-forms},
year = {2011},
isbn = {9781936968091},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Stochastic Petri nets (SPNs) provide a convenient, diagrammatic description of concurrent systems, such as computer and communication networks, and can represent quantitative (or performance) aspects such as mean response times and probability of failure. Such models can be supported by performance modelling interchange formats (PMIFs), facilitating sharing and model interoperability. We propose a hierarchical method for constructing a large class of Petri nets, which preserves efficient product-form solutions when they exist. This scalable approach greatly improves the efficiency of finding steady state probabilities in a wide range of SPNs, making much larger SPNs feasible. An existing PMIF is extended by including a new type of node that describes a particular type of small Petri net, called a "building block", the synchronisation primitives for which can be used to specify task-spawning and task-gathering, whilst retaining product-form solutions under specified conditions. When there is no product-form, the whole network is translated into a Petri net and solved directly - either by a Markov chain solver or by simulation. The extended PMIF and the proposed methodology are applied to a model of a computer system with RAID storage.},
booktitle = {Proceedings of the 5th International ICST Conference on Performance Evaluation Methodologies and Tools},
pages = {101–110},
numpages = {10},
keywords = {product-forms, performance modelling interchange formats, performance engineering, model interoperability, RAID systems, Petri nets, Markov processes},
location = {Paris, France},
series = {VALUETOOLS '11}
}

@inproceedings{10.1145/1815396.1815504,
author = {Gr\'{e}millet, O. and Gon\c{c}alves, P. and Primet, P. Vicat-Blanc and Dupas, A.},
title = {Traffic classification techniques supporting semantic networks},
year = {2010},
isbn = {9781450300629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1815396.1815504},
doi = {10.1145/1815396.1815504},
abstract = {The Semantic Networking concept has been introduced to solve the QoS, scalability and complexity challenges for the Future of Internet. Based on traffic awareness and considering flow entities, it contributes to an adaptive management of the network and provides better knowledge of the transported traffic. Studying the processing time of the classification compatible with real-time operation of such networks is a key question for implementation purposes. In this paper, we present interesting techniques for classification of traffic in semantic networks. The Sample &amp; Hold and multi-stage filter schemes are studied to detect the biggest flows. Their performance is evaluated on real traffic traces. In addition the classification of traffic according to the originating application is investigated. In particular, we analyze the influence of many parameters derived from a traffic flow on the performance of application identification and classify them according to their accuracy. By doing this, a light scheme is proposed able to classify accurately the traffic. We finally discuss the architecture of an hardware implementation to validate the concept of semantic networking.},
booktitle = {Proceedings of the 6th International Wireless Communications and Mobile Computing Conference},
pages = {463–467},
numpages = {5},
keywords = {traffic classification, internet traffic, flow aware networking, application identification},
location = {Caen, France},
series = {IWCMC '10}
}

@inproceedings{10.1145/2600057.2602849,
author = {Kelly, Frank and Key, Peter and Walton, Neil},
title = {Incentivized optimal advert assignment via utility decomposition},
year = {2014},
isbn = {9781450325653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600057.2602849},
doi = {10.1145/2600057.2602849},
abstract = {We consider a large-scale Ad-auction where adverts are assigned over a potentially infinite number of searches. We capture the intrinsic asymmetries in information between advertisers, the advert platform and the space of searches: advertisers know and can optimize the average performance of their advertisement campaign; the platform knows and can optimize on each search instance; and, neither party knows the distribution of the infinite number of searches that can occur. We look at maximizing the aggregate utility of the click-through rates of advertisers subject to the matching constraints of online ad allocation.We show that this optimization can be decomposed into subproblems, which occur on timescales relevant to the platform or the advertisers respectively. The interpretation of the subproblems is that advertisers choose prices which are optimal given the average click-through rate they receive, that the platform allocates adverts according to a classical assignment problem per search impression and that prices satisfy a nominal complementary slackness condition.We then place this optimization result in a game-theoretic framework by assuming that advertisers bid strategically to maximize their net benefit. In this setting, we construct a mechanism with a unique Nash equilibrium that achieves the decomposition just described, and thus maximizes aggregate utility. This simple and implementable mechanism is as follows. When a search occurs, the platform allocates advertisement slots in order to maximize the expected bid from a click-throughs - this is a classical assignment problem. If an advert receives a click, the platform then solves the assignment problem a second time with the advertiser's bid replaced by a bid which is uniformly distributed between zero and the original bid. The advertiser is then charged their bid minus a rebate. The rebate is the product of the advertiser's bid and ratio of the advertiser's click-through rate in the second assignment calculation (after a click-through) to the first assignment click-through rate (before the click-through).We demonstrate that, under the assignment and pricing mechanism just described, advertisers bidding strategically will maximize aggregate utility. The novelty of the mechanism just described is that, while maximizing utilitarian objective, it can be implemented by the platform in a strategic environment on the time-scales relevant to the platform (per-impression) and advertiser (on-average) respectively, and neither party requires information on the distribution of searches. We also show that dynamic models, where advertisers adapt their bids smoothly over time, will converge to the solution that maximizes aggregate utility.},
booktitle = {Proceedings of the Fifteenth ACM Conference on Economics and Computation},
pages = {527},
numpages = {1},
keywords = {utility optimization, sponsored search, position auctions, decomposition},
location = {Palo Alto, California, USA},
series = {EC '14}
}

@inproceedings{10.1145/2002396.2002401,
author = {Angel, Mukankunga Bisamaza and Rhaw, Rim and Lee, Jun and Hong, Choong Seon},
title = {HMS: towards hierarchical mapping system for ID/locator separation},
year = {2011},
isbn = {9781450308212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002396.2002401},
doi = {10.1145/2002396.2002401},
abstract = {Recently, Internet Default Free Zone (DFZ) is facing a scalability problem due to the double use of the current Internet Protocol (IP) namespace. IP namespace is used for both the location finding and host identification. However, scalability is not the only problem, mobility is difficult to achieve due to the dual use of IP. Therefore, Identifier and Locator separation is proposed and discussed in the research community as a way to solve the above problems. But, this solution further creates a big challenging issue in designing a mapping system to support efficient mobility of users while providing scalability. In this paper, we propose a hierarchical mapping system based on today's IP allocation/assignment. We evaluate our mapping system and show that scalability, and short lookup time can be achieved. However, to maintain scalability in the case of mobility is a challenge. Therefore, we provide a smooth location management scheme for mobility with a low signaling cost compared to the approach based on distributed hash table. Signals due to node movement are kept locally in each edge network.},
booktitle = {Proceedings of the 6th International Conference on Future Internet Technologies},
pages = {19–25},
numpages = {7},
keywords = {mapping system, locators, identifiers, future internet, addressing, ID/Locator separation},
location = {Seoul, Republic of Korea},
series = {CFI '11}
}

@inproceedings{10.5555/2694476.2694483,
author = {Ramachandra, Karthik and Sudarshan, S.},
title = {Big data: from querying to transaction processing},
year = {2013},
publisher = {Computer Society of India},
address = {Mumbai, Maharashtra, IND},
abstract = {The term Big Data has been used and abused extensively in the past few years, and means different things to different people. A commonly used notion says Big Data is about "volume" (of data), "velocity" (rate at which data is inserted/updated) and "variety" (of data types). In this tutorial, we use the term Big Data to refer to any data processing need that requires a high degree of parallelism. In other words, we focus primarily on the "volume" and "velocity" aspects.As part of this tutorial, we will cover some aspects of Big Data management, in particular scalable storage, scalable query processing, and scalable transaction processing.This is an introductory tutorial for those who are not familiar with the areas that we will be covering. The focus will be conceptual; it is not meant as a tutorial on how to use any specific system.},
booktitle = {Proceedings of the 19th International Conference on Management of Data},
pages = {10},
numpages = {1},
location = {Ahmedabad, India},
series = {COMAD '13}
}

@inproceedings{10.1145/1815396.1815689,
author = {Li, Yu-Shian and Teng, Hung-Yi and Hwang, Ren-Hung},
title = {P2P SVC-encoded video streaming based on network coding},
year = {2010},
isbn = {9781450300629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1815396.1815689},
doi = {10.1145/1815396.1815689},
abstract = {Along with rapid development of Internet technologies and widespread adoption of broadband residential access, video streaming service becomes a promising killer application. In order to solve device diversity, scalable video coding (SVC) has been standardized by the Joint Video Team of the ITU-T VCEG and the ISO/IEC MPEG. Using SVC, each device is capable of determining which layer should be decoded according to its capacities. On the other hand, comparing with traditional client/server architecture, peer-to-peer (P2P) technology can provide high scalability, high resilience, and prevent single point failure. Many studies have been proposed to improve the video quality under different considerations. However, little work has been done on transmitting SVC-encoded video based on P2P mesh topology. In this paper, a P2P SVC-encoded video streaming based on network coding (NC) is proposed. First of all, we propose a novel coding scheme, SVC-NC, for improving error robustness of SVC-encoded video. Second, we apply three scheduling mechanisms based on SVC-NC, startup request scheduling, priority request scheduling, and priority response scheduling to deliver SVC-encoded video more efficiently. Finally, we demonstrate the performance of our approach via simulation. The simulation results indicate that our approach can achieve low startup latency, smooth playback, and high video quality.},
booktitle = {Proceedings of the 6th International Wireless Communications and Mobile Computing Conference},
pages = {1277–1281},
numpages = {5},
keywords = {video streaming, scalable video coding, priority, peer-to-peer, network coding},
location = {Caen, France},
series = {IWCMC '10}
}

@inproceedings{10.1145/2047594.2047653,
author = {Whittinghill, David Matthew and Lutes, Kyle D.},
title = {Teaching enterprise application development: strategies and challenges},
year = {2011},
isbn = {9781450310178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047594.2047653},
doi = {10.1145/2047594.2047653},
abstract = {Enterprise application development requires a skill set that is broader than that provided by traditional programming courses. Enterprise applications are distributed, networked, multi-user, and, in most instances, fairly complex. As such, competent software developers must possess more than the basic understanding of a programming language. Developers of enterprise applications must also possess an understanding of networking fundamentals, network and application security, relational database management systems, concurrency, and application deployment and scaling. They should additionally be knowledgeable with the mechanics of at least one specific application platform, for example J2EE or Microsoft's .NET. This collection of competencies tend not to be taught in the software development curriculum as a collective whole, rather they are touched upon in many different courses in the curriculum. Though this traditional approach is more or less successful in creating adaptable skills for students who are then positioned to learn application domain-specific technologies and concepts, this approach is likely to fail in providing in-depth knowledge of the principles, patterns, and techniques used for developing these types of large enterprise applications. The acquisition of this knowledge must therefore be obtained in either an on-the-job or self-taught fashion. Our department has been addressing these issues for over ten years via a software development course that uses a variety of technologies including COM/DCOM, .NET, and J2EE. Our expressed goal is to educate our students with the knowledge required to design and implement enterprise applications.In this paper, we present the goals and history of our course, the framework we have used for teaching the subject, as well as the challenges that can arise in administering the associated coursework. The observations are based on the two authors' collective experience teaching enterprise application development in a university setting.},
booktitle = {Proceedings of the 2011 Conference on Information Technology Education},
pages = {221–226},
numpages = {6},
keywords = {teaching, enterprise application development, education},
location = {West Point, New York, USA},
series = {SIGITE '11}
}

@inproceedings{10.1145/2488551.2488586,
author = {Teixid\'{o}, Ivan and Usi\'{e}, Anabel and L\'{e}rida, Josep Ll. and Solsona, Francesc and Comas, Jorge and Torres, Nestor and Karathia, Hiren and Alves, Rui},
title = {P-Biblio-MetReS, a parallel data mining tool for the reconstruction of molecular networks},
year = {2013},
isbn = {9781450319034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488551.2488586},
doi = {10.1145/2488551.2488586},
abstract = {Biblio-MetReS is a single-thread data mining application that facilitates the reconstruction of molecular networks based on automated text mining analysis of published scientific literature. This application is very CPU-intensive, requiring High Performace Computing (HPC). Due to the amount of execution tasks, it can be quite slow. Those tasks are repetitive and consist in mining the information from large sets of scientific documents, a process where the time-cost of the application could be improved through paralellization.This paper presents a parallel version of Biblio-MetReS. The multithreading application P(arallel)-Biblio-MetReS distributes the work among copies of the same Java class, each mining a collection of documents obtained in a previous search phase from different literature sources of Internet. In this article, we compare performances between the parallel and non-parallel versions of the application and discuss scalability issues on multi-threading systems in the context of this application. Furthermore, we also optimize memory management and reutilization of document parsing results. Our experimental results corroborate the good performance of P-Biblio-MetReS, pinpointing specific aspects that still need to be improved.},
booktitle = {Proceedings of the 20th European MPI Users' Group Meeting},
pages = {247–252},
numpages = {6},
keywords = {scalability, parallelization, parallel programming, multithreading, molecular network reconstruction, data/text mining},
location = {Madrid, Spain},
series = {EuroMPI '13}
}

@inproceedings{10.1145/2124295.2124308,
author = {Dasgupta, Anirban and Gurevich, Maxim and Zhang, Liang and Tseng, Belle and Thomas, Achint O.},
title = {Overcoming browser cookie churn with clustering},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124308},
doi = {10.1145/2124295.2124308},
abstract = {Many large Internet websites are accessed by users anonymously, without requiring registration or logging-in. However, to provide personalized service these sites build anonymous, yet persistent, user models based on repeated user visits. Cookies, issued when a web browser first visits a site, are typically employed to anonymously associate a website visit with a distinct user (web browser). However, users may reset cookies, making such association short-lived and noisy. In this paper we propose a solution to the cookie churn problem: a novel algorithm for grouping similar cookies into clusters that are more persistent than individual cookies. Such clustering could potentially allow more robust estimation of the number of unique visitors of the site over a certain long time period, and also better user modeling which is key to plenty of web applications such as advertising and recommender systems.We present a novel method to cluster browser cookies into groups that are likely to belong to the same browser based on a statistical model of browser visitation patterns. We address each step of the clustering as a binary classification problem estimating the probability that two different subsets of cookies belong to the same browser. We observe that our clustering problem is a generalized interval graph coloring problem, and propose a greedy heuristic algorithm for solving it. The scalability of this method allows us to cluster hundreds of millions of browser cookies and provides significant improvements over baselines such as constrained K-means.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {83–92},
numpages = {10},
keywords = {similarity measure, distributed computing, clustering algorithms, browser cookie churn, bayes factor},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@proceedings{10.1145/2072572,
title = {J-HGBU '11: Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding},
year = {2011},
isbn = {9781450309981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MA3HO'11: The First International ACM Workshop on Multimedia access to 3D Human Objects (MA3HO'11) is held on November 2011 at Scottsdale, Arizona, USA in conjunction with ACM Multimedia 2011.Motivations behind this initiative are strong: 3D is becoming increasingly popular in a number of economically relevant fields of application, including movies, graphic entertainments, security applications, data archives storage, search and retrieval. 3D cinema, online gaming and virtual reality, surveillance and security, mechanic parts management, medical imaging, structural and molecular biology, cultural heritage asset reproduction, improved human computer interaction, natural and multimodal interactivity are just a few of the potential applications. While 3D digital data was obtained by manual CAD and 3D modeling software until a few years ago, nowadays laser scanners and computer vision technology make it possible to get high resolution textured 3D models from real world data at very fast pace. Dynamic 3D models can be captured from moving targets as well. Low cost devices like the Kinect 3D scanner permit to obtain low resolution 3D full scans in real time at short distance. Computer vision solutions permit fast extraction of interest points in the images, compute their geometrical relationships and perform approximate 3D reconstruction of observed objects or scenes. Smart tracking algorithms, while observing a target from different viewpoints permit to reconstruct its 3D silhouette and provide a realistic avatar of the moving target. Pan Tilt Zoom cameras make it possible to capture high resolution images of far targets and potentially permit their 3D reconstruction from such a sequence. 3D object databases are rapidly emerging in many application fields, so paving the way to large scale 3D content-based retrieval over the Internet. Web3D is near to come and will enable access to 3D materials of high quality. Sharing, retrieving and reusing 3D content will be soon exchanged between professionals of 3D data.To restrict the scope of interest this Workshop was focused on 3D human objects, intended both as parts of 3D human bodies and 3D parts for humans, i.e.: people silhouettes, head and torso models, arms and hand models, body, faces and faces parts such as lips or objects handled and interacting humans and eventually 3D environment where humans acts. We particularly envision the task of matching 3D models with 3D models or 2D image data. In surveillance and security, for example, matching of 2D face images with 3D face models permits to exploit both appearance and structural information to perform target identification, so superseding the limitations of traditional 2D face matching. While 3D face databases are becoming more and more available, 3D face matching is becoming an important topic of investigation for advanced security applications. New recognition and tracking applications will fully exploit 3D body behaviors. Real time reconstruction of the 3D target body and face from multiple 2D views, makes live 3D body modeling, identification, re-identification a new opportunity in surveillance long term tracking and and forensic applications, easing the task of behavior analysis and recognition. Expression analysis, human machine interaction with natural interfaces are all fields where 3D can improve with respect the the current state of the art. A growing number of benchmark and dataset of 3D human objects was made available from research projects. Examples are the TRICTRAC project where a number of video clips were rendered in 3D, the Carnegie Mellon University Motion Capture Database, for human bodies and interactions (http://mocap.cs.cmu.edu/); the Multi-view 3D Human Pose Estimation benchmark at CVPR2009 (http://www.gavrila.net/Research/3-D_Human_Body_Tracking) and the 3D multiview object modeling for re-identification, by the EU project THIS (http://imagelab.ing.unimore.it/3dpes/).The MA3HO workshop is aimed at taking a leap forward in emerging research of multimedia access of 3D human objects, merging researchers in 3D graphics, 3D object recognition and retrieval, Multimedia, with attention to application fields where humans are highly significant, such as security, surveillance and biometry, animation and entertainment, video retrieval, sport analytics, natural interaction, cultural heritage, augmented and virtual reality and world wide web. Main subjects addressed are among the others: 3D human objects reconstruction from 2D views3D pose estimation from 2D information2D to 3D human object matching3D human object categorization3D people identification and re-identification3D object/face similarity matching, indexing, and miningFeature extraction for 3D model segmentationFeature extraction for 3D motion detection and behavior classification3D shape descriptorsRetrieval with large distributed and heterogeneous 3D datasets and benchmarkingSemantics-driven 3D object retrieval and classification3D natural interfaces and search modalitiesThe workshop has attracted 18 good quality submissions fairly distributed among different countries: China, Japan, Canada, USA, France, Italy and Germany. Many of the key arguments of the workshop call were addressed. The MA3HO Technical Program Committee, after careful review and evaluation, only selected 6 papers for oral presentation and 7 papers for poster presentation, in order to have a selective high quality event, in the spirit of the ACM MULTIMEDIA conference.SSPW'11: It is a pleasure and an honor to have organized the Third International Workshop on Social Signal Processing (SSPW'11), held on December 1, 2011, in Scottsdale, Arizona, USA in conjunction with ACM Multimedia 2011.Machine analysis of human social behaviors and machine synthesis of human-like socially-aware interactions is of utmost importance for research on next-generation computing and multimedia including ambient intelligence, smart environments/ multimedia, and perceptual interfaces/multimedia. This field -- widely know as Social Signal Processing -- has witness a surge of interest in the past couple of years and is progressing rapidly with new or pending applications in HCI, psychology, biomedicine, politics, and entertainment technology, among other domains. With these advances come new conceptual and methodological challenges. The SSPW'11 workshop is the third edition of the Social Signal Processing Workshop series and it presents cutting-edge research and new challenges in automatic analysis and synthesis of social interactions and social signals in an interdisciplinary forum of computer and behavioral scientists.The workshop series is the premier forum for presenting research in social signal processing and the related topics. The workshop provides a rich forum for sharing and generating allied technologies: the generation of new ideas, new approaches, new techniques, and new evaluation. The workshop is organized under the auspices of the SSPNet, the FP7 European Network of Excellence on Social Signal Processing (EC FP7 grant agreement no. 231287), and continues the tradition of the previous SSPW workshops by maintaining the high standard set by its predecessors.Main topics discussed during the SSPW workshop series include the following: Social Intelligence, Social Cognition and Social Behavior ModelingFacial behavior analysis and synthesis in social interactionsExpressive speech analysis and synthesis in social interactionsHuman gesture and action recognition and synthesis in social interactionsMultimodal human behavior analysis and synthesis in social interactionsPerceptual, multimodal, and socially-aware user interfacesSocially-adept Embodied Conversational AgentsData Mining, Machine Learning, Information Retrieval, Artificial Intelligence in Social ContextsDatabases for training and testingSocially-aware computing and applications (reality mining, implicit multimedia tagging, etc.)The SSPW'11 workshop program includes a number of Keynote talks and a poster session. For the workshop we have received 13 good quality submissions. Each of these was assessed by no fewer than two reviewers. The final SSPW'11 program consists of four Keynote talks by Hatice Gunes (Queen Mary University London, UK), Shri Narayanan (University of Southern California, USA), Matthias Mehl (University of Arizona, USA), and Louis-Philippe Morency (Institute of Creative Technologies, USC, USA), and a poster session with 4 papers. The Keynote and poster presentations bring together related communities to share the latest findings and ideas and pursue continuing and new collaborations in research on social signal processing.},
location = {Scottsdale, Arizona, USA}
}

@inproceedings{10.1145/1879082.1879088,
author = {Lodhi, Aemen and Dovrolis, Constantine},
title = {A network formation model for internet transit relations},
year = {2010},
isbn = {9781450373043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879082.1879088},
doi = {10.1145/1879082.1879088},
abstract = {Most Autonomous Systems in the Internet need to select one or more transit providers. The provider selection process is complex, influenced by dynamic pricing, contracts, performance, marketing and other factors. We propose a simple dynamic model that captures the salient features of the provider selection process. The model creates a positive feedback effect, where "the bigger a provider is the bigger it gets". We then study the resulting internetwork formation process, showing that it always leads to a stable, but not unique, internetwork. We also use computational experiments to understand how the convergence delay scales with the size of the network, the factor(s) that affect the number of distinct equilibria, and the impact of three key model parameters.},
booktitle = {Proceedings of the 2010 Workshop on Economics of Networks, Systems, and Computation},
articleno = {4},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NetEcon '10}
}

@inproceedings{10.1145/2554688.2554711,
author = {Lam, Ka Chun and Tang, Wai-Chung and Young, Evangeline F.Y.},
title = {A scalable routability-driven analytical placer with global router integration for FPGAs (abstract only)},
year = {2014},
isbn = {9781450326711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554688.2554711},
doi = {10.1145/2554688.2554711},
abstract = {As the sizes of modern circuits become bigger and bigger, implementing those large circuits into FPGA becomes arduous. The state-of-the-art academic FPGA place-and-route tool, VPR, has good quality but needs around a whole day to complete a placement when the input circuit contains millions of lookup tables, excluding the runtime for routing. To expedite the placement process, we propose a routability-driven placement algorithm for FPGA that adopts techniques used in ASIC global placer. Our placer follows the lower-bound-and-upper-bound iterative optimization process in ASIC placers like Ripple. In the lower-bound computation, the total HPWL, modeled using the Bound2Bound net model, is minimized using the conjugate gradient method. In the upper-bound computation, an almost-legalized result is produced by spreading cells linearly in the placement area. Those positions are then served as fixed-point anchors and fed into the next lower-bound computation. Furthermore, global routing will be performed in the upper-bound computation to estimate the routing segment usage, as a mean to consider congestion in placement. We tested our approach using 20 MCNC benchmarks and 4 large benchmarks for performance and scalability. Experimental results show that based on the island-style architecture which VPR is most optimized for, our approach can obtain a placement result 8x faster than VPR with 2% more in channel width, or 3x faster with 1% more in channel width when congestion is being considered. Our approach is even 14x faster than VPR in placing large benchmarks with over 10,000 lookup tables, with only 7% more in channel width.},
booktitle = {Proceedings of the 2014 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {242},
numpages = {1},
keywords = {routability-driven, placement, fpga},
location = {Monterey, California, USA},
series = {FPGA '14}
}

@inproceedings{10.1007/978-3-642-28997-2_19,
author = {Sondhi, Parikshit and Vydiswaran, V. G. Vinod and Zhai, Cheng Xiang},
title = {Reliability prediction of webpages in the medical domain},
year = {2012},
isbn = {9783642289965},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-28997-2_19},
doi = {10.1007/978-3-642-28997-2_19},
abstract = {In this paper, we study how to automatically predict reliability of web pages in the medical domain. Assessing reliability of online medical information is especially critical as it may potentially influence vulnerable patients seeking help online. Unfortunately, there are no automated systems currently available that can classify a medical webpage as being reliable, while manual assessment cannot scale up to process the large number of medical pages on the Web. We propose a supervised learning approach to automatically predict reliability of medical webpages. We developed a gold standard dataset using the standard reliability criteria defined by the Health on Net Foundation and systematically experimented with different link and content based feature sets. Our experiments show promising results with prediction accuracies of over 80%. We also show that our proposed prediction method is useful in applications such as reliability-based re-ranking and automatic website accreditation.},
booktitle = {Proceedings of the 34th European Conference on Advances in Information Retrieval},
pages = {219–231},
numpages = {13},
location = {Barcelona, Spain},
series = {ECIR'12}
}

@inproceedings{10.1145/2382196.2382256,
author = {Yu, Haifeng and Gibbons, Phillip B. and Shi, Chenwei},
title = {DCast: sustaining collaboration in overlay multicast despite rational collusion},
year = {2012},
isbn = {9781450316514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382196.2382256},
doi = {10.1145/2382196.2382256},
abstract = {A key challenge in large-scale collaborative distributed systems is to properly incentivize the rational/selfish users so that they will properly collaborate. Within such a context, this paper focuses on designing incentive mechanisms for overlay multicast systems. A key limitation shared by existing proposals on the problem is that they are no longer able to provide proper incentives and thus will collapse when rational users collude or launch sybil attacks.This work explicitly aims to properly sustain collaboration despite collusion and sybil attacks by rational users. To this end, we propose a new decentralized DCast multicast protocol that uses a novel mechanism with debt-links and circulating debts. We formally prove that the protocol offers a novel concept of safety-net guarantee: A user running the protocol will always obtain a reasonably good utility despite the deviation of any number of rational users that potentially collude or launch sybil attacks. Our prototyping as well as simulation demonstrates the feasibility and safety-net guarantee of our design in practice.},
booktitle = {Proceedings of the 2012 ACM Conference on Computer and Communications Security},
pages = {567–580},
numpages = {14},
keywords = {whitewashing attack, sybil attack, rational collusion, overlay multicast, incentive mechanism, algorithmic mechanism design},
location = {Raleigh, North Carolina, USA},
series = {CCS '12}
}

@inproceedings{10.1145/2499788.2499812,
author = {Chen, Zhikun and Zhang, Lumin and Yang, Shuqiang and Tan, Shuang and He, Li and Zhang, Ge and Yang, Huiyu},
title = {The data partition strategy based on hybrid range consistent hash in NoSQL database},
year = {2013},
isbn = {9781450322522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499788.2499812},
doi = {10.1145/2499788.2499812},
abstract = {With the development of Internet technology and Cloud Computing, more and more applications have to be confronted with the challenges of big data. NoSQL Database is fit to the management of big data because of the characteristics of high scalability, high availability and high fault-tolerance. The data partitioning strategy plays an important role in the NoSQL database. The existing data partitioning strategies will cause some problems such as low scalability, hot spot and low performance and so on. In this paper we proposed a new data partitioning strategy---HRCH, which can partitioning the data in a reasonable way. At last we use some experiments to verify the effectiveness of HRCH. It shows that the HRCH can improve the scalability of the system. It also can avoid the hot spot problem as far as possible. And it also can improve the parallel degree of processing to improve the system's performance in some processing.},
booktitle = {Proceedings of the Fifth International Conference on Internet Multimedia Computing and Service},
pages = {358–363},
numpages = {6},
keywords = {data partition, data management, big data, NoSQL database},
location = {Huangshan, China},
series = {ICIMCS '13}
}

@inproceedings{10.1145/2660129.2660155,
author = {Papalini, Michele and Carzaniga, Antonio and Khazaei, Koorosh and Wolf, Alexander L.},
title = {Scalable routing for tag-based information-centric networking},
year = {2014},
isbn = {9781450332064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660129.2660155},
doi = {10.1145/2660129.2660155},
abstract = {Routing in information-centric networking remains an open problem. The main issue is scalability. Traditional IP routing can be used with name prefixes, but it is believed that the number of prefixes will grow too large. A related problem is the use of per-packet in-network state (to cut loops and return data to consumers). We develop a routing scheme that solves these problems. The service model of our information-centric network supports information pull and push using tag sets as information descriptors. Within this service model, we propose a routing scheme that supports forwarding along multiple loop-free paths, aggregates addresses for scalability, does not require per-packet network state, and leads to near-optimal paths on average. We evaluate the scalability of our routing scheme, both in terms of memory and computational complexity, on the full Internet AS-level topology and on the internal networks of representative ASes using realistic distributions of content and users extrapolated from traces of popular applications. For example, a population of 500 million users requires a routing information base of 3.8GB with an almost flat growth and, in this case, a routing update (one content descriptor) can be processed in 2ms on commodity hardware. We conclude that information-centric networking is feasible, even with (or perhaps thanks to) addresses consisting of expressive content descriptors.},
booktitle = {Proceedings of the 1st ACM Conference on Information-Centric Networking},
pages = {17–26},
numpages = {10},
keywords = {tag-based routing, push/pull, multi-tree routing, icn},
location = {Paris, France},
series = {ACM-ICN '14}
}

@inproceedings{10.5555/2380816.2380882,
author = {Eckle-Kohler, Judith and Gurevych, Iryna},
title = {Subcat-LMF: fleshing out a standardized format for subcategorization frame interoperability},
year = {2012},
isbn = {9781937284190},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes Subcat-LMF, an ISO-LMF compliant lexicon representation format featuring a uniform representation of subcategorization frames (SCFs) for the two languages English and German. Subcat-LMF is able to represent SCFs at a very fine-grained level. We utilized Subcat-LMF to standardize lexicons with large-scale SCF information: the English Verb-Net and two German lexicons, i.e., a subset of IMSlex and GermaNet verbs. To evaluate our LMF-model, we performed a cross-lingual comparison of SCF coverage and overlap for the standardized versions of the English and German lexicons. The Subcat-LMF DTD, the conversion tools and the standardized versions of VerbNet and IMS-lex subset are publicly available.},
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
pages = {550–560},
numpages = {11},
location = {Avignon, France},
series = {EACL '12}
}

@inproceedings{10.1145/2037509.2037538,
author = {Gao, Qian and Liu, Fei and Gilbert, David and Heiner, Monika and Tree, David},
title = {A multiscale approach to modelling planar cell polarity in Drosophila wing using hierarchically coloured Petri nets},
year = {2011},
isbn = {9781450308175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037509.2037538},
doi = {10.1145/2037509.2037538},
abstract = {Modelling across multiple scales is a current challenge in Systems Biology, especially when applied to multicellular organisms. In this paper we present an approach to model at different spatial scales, using the new concept of hierarchically coloured Petri Nets (HCPN). We apply HCPN to model a tissue comprising multiple cells hexagonally packed in a honeycomb formation in order to describe the phenomenon of Planar Cell Polarity (PCP) signalling in Drosophila wing. We illustrate different levels of abstraction that can be used in order to assist the systematic modelling of such a complex system involving intra- and inter-cellular signalling mechanisms, and we provide a design pattern for similar modelling problems. Our initial model describes normal, wild-type PCP signalling, and we illustrate the power of our approach by easily adapting it to various tissue sizes and to describe the phenotype of a well-documented genetic mutation in Drosophila. We have performed a series of analyses on our models which require computational experiments over very large underlying models. All results are reproducible.},
booktitle = {Proceedings of the 9th International Conference on Computational Methods in Systems Biology},
pages = {209–218},
numpages = {10},
keywords = {planar cell polarity, multiscale modelling, hierarchically coloured petri nets, continuous and stochastic simulation},
location = {Paris, France},
series = {CMSB '11}
}

@inproceedings{10.1145/2377310.2377323,
author = {Jung, Heeyoung},
title = {Considerations on the endpoint for future internet},
year = {2012},
isbn = {9781450316903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377310.2377323},
doi = {10.1145/2377310.2377323},
abstract = {Where to put the endpoint is a big issue in Future Internet because many problems of the Internet come from the fact that it uses NAP (Network Attachment Point) as the endpoint of the end-to-end communication. This paper briefly reviews related works regarding the definition and location of endpoint, propose NID (Network Interface Device) as the appropriate endpoint for Future Internet and argue that NID could be the advisable point from the two essential aspects of the Internet-scalability and functionality. This paper also describes the basic model for the NID-based communication.},
booktitle = {Proceedings of the 7th International Conference on Future Internet Technologies},
pages = {43–44},
numpages = {2},
keywords = {network interface device, network attachment point, future internet, endpoint, end-to-end communication, design and mechanism discussion},
location = {Seoul, Korea},
series = {CFI '12}
}

@inproceedings{10.1145/2020408.2020421,
author = {Yuan, Guo-Xun and Ho, Chia-Hua and Lin, Chih-Jen},
title = {An improved GLMNET for l1-regularized logistic regression},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020421},
doi = {10.1145/2020408.2020421},
abstract = {GLMNET proposed by Friedman et al. is an algorithm for generalized linear models with elastic net. It has been widely applied to solve L1-regularized logistic regression. However, recent experiments indicated that the existing GLMNET implementation may not be stable for large-scale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient regardless of loosely or strictly solving the optimization problem. Experiments demonstrate that the improved GLMNET is more efficient than a state-of-the-art coordinate descent method.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {33–41},
numpages = {9},
keywords = {logistic regression, linear classification, l1 regularization},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{10.1145/2502524.2502554,
author = {Zhu, Ting and Huang, Zhichuan and Sharma, Ankur and Su, Jikui and Irwin, David and Mishra, Aditya and Menasche, Daniel and Shenoy, Prashant},
title = {Sharing renewable energy in smart microgrids},
year = {2013},
isbn = {9781450319966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502524.2502554},
doi = {10.1145/2502524.2502554},
abstract = {Renewable energy harvested from the environment is an attractive option for providing green energy to homes. Unfortunately, the intermittent nature of renewable energy results in a mismatch between when these sources generate energy and when homes demand it. This mismatch reduces the efficiency of using harvested energy by either i) requiring batteries to store surplus energy, which typically incurs ~20% energy conversion losses; or ii) using net metering to transmit surplus energy via the electric grid's AC lines, which severely limits the maximum percentage of possible renewable penetration. In this paper, we propose an alternative structure wherein nearby homes explicitly share energy with each other to balance local energy harvesting and demand in microgrids. We develop a novel energy sharing approach to determine which homes should share energy, and when, to minimize system-wide efficiency losses. We evaluate our approach in simulation using real traces of solar energy harvesting and home consumption data from a deployment in Amherst, MA. We show that our system i) reduces the energy loss on the AC line by 60% without requiring large batteries, ii) scales up performance with larger battery capacities, and iii) is robust to changes in microgrid topology.},
booktitle = {Proceedings of the ACM/IEEE 4th International Conference on Cyber-Physical Systems},
pages = {219–228},
numpages = {10},
keywords = {renewable energy, microgrids, energy sharing, battery},
location = {Philadelphia, Pennsylvania},
series = {ICCPS '13}
}

@inproceedings{10.5555/2132325.2132348,
author = {Hsu, Meng-Kai and Chou, Sheng and Lin, Tzu-Hen and Chang, Yao-Wen},
title = {Routability-driven analytical placement for mixed-size circuit designs},
year = {2011},
isbn = {9781457713989},
publisher = {IEEE Press},
abstract = {Due to the significant mismatch between existing wirelength models and the congestion objective in placement, considering routability during placement is particularly significant for modern circuit designs. In this paper, a novel routability-driven analytical placement algorithm for large-scale mixed-size circuit designs is proposed. Unlike most existing works which usually optimize routability by reallocating whitespace or net-based congestion removal, the proposed algorithm optimizes routability from three major aspects: (1) Pin density: Most existing works optimize routability based on net distribution, while our work considers both the density of pins and their routing directions; (2) Routing overflow optimization: Unlike most previous works that use whitespace allocation or net-based congestion removal to improve routability, our work optimizes routing overflow by a novel sigmoid function during global placement; (3) Macro porosity consideration: A virtual macro expansion technique is applied to consider the constrained routing resource incurred by big macros. Routability-driven legalization and detailed placement are also proposed to further optimize routing congestion. Experimental results show the effectiveness and efficiency of our proposed algorithm. Compared with the participating teams for the 2011 ACM ISPD Routability-Driven Placement Contest, our algorithm achieves the best average overflow and routed wirelength.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
pages = {80–84},
numpages = {5},
location = {San Jose, California},
series = {ICCAD '11}
}

@inproceedings{10.1145/2228360.2228442,
author = {Li, Zhuo and Alpert, Charles J. and Nam, Gi-Joon and Sze, Cliff and Viswanathan, Natarajan and Zhou, Nancy Y.},
title = {Guiding a physical design closure system to produce easier-to-route designs with more predictable timing},
year = {2012},
isbn = {9781450311991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2228360.2228442},
doi = {10.1145/2228360.2228442},
abstract = {Physical synthesis has emerged as one of the most important tools in design closure, which starts with the logic synthesis step and generates a new optimized netlist and its layout for the final signoff process. As stated in [1], "it is a wrapper around traditional place and route, whereby synthesis-based optimization are interwoven with placement and routing." A traditional physical synthesis tool generally focuses on design closure with Steiner wire model. It optimizes timing/area/power with the assumption that each net can be routed with optimal Steiner tree. However, advanced design rules, more IP and hierarchical design styles for super-large billion-gate designs, serious buffering problems from interconnect scaling and metal layer stacks make routing a much more challenging problem [2]. This paper discusses a series of techniques that may relieve this problem, and guide the physical design closure system to produce not only easier to route designs, but also better timing quality. Open challenges are also overviewed at the end.},
booktitle = {Proceedings of the 49th Annual Design Automation Conference},
pages = {465–470},
numpages = {6},
keywords = {timing driven routing, physical synthesis},
location = {San Francisco, California},
series = {DAC '12}
}

@inproceedings{10.1145/2593069.2593177,
author = {Moctar, Yehdhih Ould Mohammed and Brisk, Philip},
title = {Parallel FPGA Routing based on the Operator Formulation},
year = {2014},
isbn = {9781450327305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593069.2593177},
doi = {10.1145/2593069.2593177},
abstract = {We have implemented an FPGA routing algorithm on a shared memory multi-processor using the Galois API, which offers speculative parallelism in software. The router is a parallel implementation of PathFinder, which is the basis for most commercial FPGA routers. We parallelize the maze expansion step for each net, while routing nets sequentially to limit the amount of rollback that would likely occur due to misspeculation. Our implementation relies on non-blocking priority queues, which use software transactional memory (SMT), to identify the best route for each net. Our experimental results demonstrate scalability for large benchmarks and that the amount of available parallelism depends primarily on the circuit size, not the interdependence of signals. We achieve an average speedup of approximately 3x compared to the most recently published work on parallel multi-threaded FPGA routing, and up to 6x in comparison to the single-threaded router implemented in the publicly available Versatile Place and Route (VPR) framework.},
booktitle = {Proceedings of the 51st Annual Design Automation Conference},
pages = {1–6},
numpages = {6},
keywords = {Software Transactional Memory (STM), Routing Resource Graph (RRG), Routing, Maze Expansion, Irregular Algorithm, Field Programmable Gate Array (FPGA)},
location = {San Francisco, CA, USA},
series = {DAC '14}
}

@article{10.1145/1710115.1710123,
author = {Hellerstein, Joseph L. and Morrison, Vance and Eilebrecht, Eric},
title = {Applying control theory in the real world: experience with building a controller for the .NET thread pool},
year = {2010},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/1710115.1710123},
doi = {10.1145/1710115.1710123},
abstract = {There has been considerable interest in using control theory to build web servers, database managers, and other systems. We claim that the potential value of using control theory cannot be realized in practice without a methodology that addresses controller design, testing, and tuning. Based on our experience with building a controller for the .NET thread pool, we develop a methodology that: (a) designs for extensibility to integrate diverse control techniques, (b) scales the test infrastructure to enable running a large number of test cases, (c) constructs test cases for which the ideal controller performance is known a priori so that the outcomes of test cases can be readily assessed, and (d) tunes controller parameters to achieve good results for multiple performance metrics. We conclude by discussing how our methodology can be extended, especially to designing controllers for distributed systems.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {38–42},
numpages = {5}
}

@inproceedings{10.1145/2463676.2463689,
author = {Tao, Fangbo and Yu, Xiao and Lei, Kin Hou and Brova, George and Cheng, Xiao and Han, Jiawei and Kanade, Rucha and Sun, Yizhou and Wang, Chi and Wang, Lidan and Weninger, Tim},
title = {Research-insight: providing insight on research by publication network analysis},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463689},
doi = {10.1145/2463676.2463689},
abstract = {A database contains rich, inter-related, multi-typed data and information, forming one or a set of gigantic, intercon- nected, heterogeneous information networks. Much knowl- edge can be derived from such information networks if we systematically develop an effective and scalable database-oriented information network analysis technology. In this system demo, we take a computer science research publica- tion network as an example, which is an information net- work derived from an integration of DBLP, other web-based information about researchers, and partially available cita- tion data, and construct a Research-Insight system in order to demonstrate the power of database-oriented information network analysis. We show that nontrivial research insight can be obtained from such analysis, including (1) ranking, clustering, classification and similarity search of researchers, terms and venues for research subfields and themes, (2) recommending good researchers and good research papers to read or cite when conducting research on certain topics (3) predicting potential collaborators for certain theme-oriented research, and (4) predicting advisor-advisee rela- tionships and affiliation history based on historical research publications. Although some of these functions have been studied in recent research, effective and scalable realization of such functions in large networks still poses challenging research problems. Moreover, some function are our on- going research tasks. By integrating these functionalities, Research-Insight may not only provide with us insightful rec- ommendations in CS research but also help us gain insight on how to perform effective data mining in large databases.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1093–1096},
numpages = {4},
keywords = {recommendation system, heterogeneous information network},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2168836.2168857,
author = {Qian, Zhengping and Chen, Xiuwei and Kang, Nanxi and Chen, Mingcheng and Yu, Yuan and Moscibroda, Thomas and Zhang, Zheng},
title = {MadLINQ: large-scale distributed matrix computation for the cloud},
year = {2012},
isbn = {9781450312233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2168836.2168857},
doi = {10.1145/2168836.2168857},
abstract = {The computation core of many data-intensive applications can be best expressed as matrix computations. The MadLINQ project addresses the following two important research problems: the need for a highly scalable, efficient and fault-tolerant matrix computation system that is also easy to program, and the seamless integration of such specialized execution engines in a general purpose data-parallel computing system.MadLINQ exposes a unified programming model to both matrix algorithm and application developers. Matrix algorithms are expressed as sequential programs operating on tiles (i.e., sub-matrices). For application developers, MadLINQ provides a distributed matrix computation library for .NET languages. Via the LINQ technology, MadLINQ also seamlessly integrates with DryadLINQ, a data-parallel computing system focusing on relational algebra.The system automatically handles the parallelization and distributed execution of programs on a large cluster. It outperforms current state-of-the-art systems by employing two key techniques, both of which are enabled by the matrix abstraction: exploiting extra parallelism using fine-grained pipelining and efficient on-demand failure recovery using a distributed fault-tolerant execution engine. We describe the design and implementation of MadLINQ and evaluate system performance using several real-world applications.},
booktitle = {Proceedings of the 7th ACM European Conference on Computer Systems},
pages = {197–210},
numpages = {14},
keywords = {pipelining, matrix computation, fault-tolerance, distributed systems, dataflow, cluster computing},
location = {Bern, Switzerland},
series = {EuroSys '12}
}

@inproceedings{10.5555/2132325.2132400,
author = {Han, Yiding and Ancajas, Dean Michael and Chakraborty, Koushik and Roy, Sanghamitra},
title = {Exploring high throughput computing paradigm for global routing},
year = {2011},
isbn = {9781457713989},
publisher = {IEEE Press},
abstract = {With aggressive technology scaling, the complexity of the global routing problem is poised to rapidly grow. Solving such a large computational problem demands a high throughput hardware platform such as modern Graphics Processing Units (GPU). In this work, we explore a hybrid GPU-CPU high-throughput computing environment as a scalable alternative to the traditional CPU-based router. We introduce Net Level Concurrency (NLC): a novel parallel model for router algorithms that aims to exploit concurrency at the level of individual nets.To efficiently uncover NLC, we design a Scheduler to create groups of nets that can be routed in parallel. At its core, our Scheduler employs a novel algorithm to dynamically analyze data dependencies between multiple nets. We believe such an algorithm can lay the foundation for uncovering data-level parallelism in routing: a necessary requirement for employing high throughput hardware. Detailed simulation results show an average of 4X speedup over NTHU-Route 2.0 with negligible loss in solution quality. To the best of our knowledge, this is the first work on utilizing GPUs for global routing.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
pages = {298–305},
numpages = {8},
location = {San Jose, California},
series = {ICCAD '11}
}

@inproceedings{10.1145/2345316.2345329,
author = {Pino, Robinson E.},
title = {Cloud/big data computing for defense},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345329},
doi = {10.1145/2345316.2345329},
abstract = {The ever growing necessity for Big Data processing within the industry, government, and specially within defense applications causes the need and requirement for the fast development of new technologies. In addition, the protection of Big Data can be a serious problem because security is commonly an afterthought during technology development, and the exponentially increasing rate at which new data is generated presents many challenges. Although conventional Turing computation has been remarkably successful, it does not scale well and is failing to adapt to novel application domains in cyberspace. Fortunately, Turing formalism for computation represents only a subset of all possible computational possibilities. Unconventional computing - the quest for new algorithms and physical implementations of novel computing paradigms based on and inspired by principles of information processing in physical and biological systems - may help to solve some of the information overflow problems facing the Defense community. These and other topics will be covered by our diverse panel of experts.},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {10},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/2541608.2541611,
author = {Decat, Maarten and Lagaisse, Bert and Joosen, Wouter and Crispo, Bruno},
title = {Introducing concurrency in policy-based access control},
year = {2013},
isbn = {9781450325516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541608.2541611},
doi = {10.1145/2541608.2541611},
abstract = {Policy-based access control aims to decouple access control rules from the application they constrain by expressing these rules in declarative access control policies. Performance of policy-based access control is of growing importance, but concurrent and distributed policy evaluation has received little research attention and current policy evaluation engines are still single-machine and fully sequential to the best of our knowledge. We believe that concurrent policy evaluation is necessary to meet the performance and scalability requirements of next-generation internet applications and aid the maturation of policy-based access control. Therefore, this paper presents an initial exploration of concurrent policy evaluation. We illustrate the performance of current policy evaluation engines, model the performance of policy evaluation in terms of the characteristics of a policy, list opportunities for concurrency, describe the need for concurrency control and specifically show how concurrency can be used to improve throughput based on our prototype.},
booktitle = {Proceedings of the 8th Workshop on Middleware for Next Generation Internet Computing},
articleno = {3},
numpages = {6},
keywords = {performance, concurrency, access control policies, access control},
location = {Beijing, China},
series = {MW4NextGen '13}
}

@proceedings{10.1145/2494621,
title = {CAC '13: Proceedings of the 2013 ACM Cloud and Autonomic Computing Conference},
year = {2013},
isbn = {9781450321723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizing and program committees, we welcome you to the International Conference on Cloud and Autonomic Computing (CAC 2013). The initial conference in this new series is being held in Miami, Florida, a city rich in culture, history, finance, commerce, technology, and fourth-largest urban area in the United States. The CAC 2013 Conference, organized in cooperation with the Association for Computing Machinery (ACM), is a spin-off of the previous and ongoing International Conference on Autonomic Computing with an additional emphasis on cloud topics to address the emerging cyberspace and cloud technology aspects of autonomic computing that we strongly believe will become pervasive and ubiquitous and that will eventually grow to touch all aspects of life and economic activity.As these new technologies start to be developed and deployed, we are experiencing major research and technological challenges in how we manage and secure cyberspace resources and services. The Cloud and Autonomic Computing Conference series will be the main international forum to present the latest research on the design, implementation, evaluation, and use of cloud and autonomic systems and services. This conference, the kickoff event of the new series, will focus on four important areas: Autonomic Cloud Computing, Autonomics for Extreme Scales, Autonomic Cybersecurity, and Autonomic Computing Tools and Applications. We are looking forward to lively discussions about CAC emerging technologies, applications and their challenges.},
location = {Miami, Florida, USA}
}

@inproceedings{10.1145/2593069.2596682,
author = {Chandra, Vikas},
title = {Monitoring Reliability in Embedded Processors - A Multi-layer View},
year = {2014},
isbn = {9781450327305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593069.2596682},
doi = {10.1145/2593069.2596682},
abstract = {Scaling to sub-20nm technology nodes changes the nature of reliability effects from abrupt functional problems to progressive degradation of the performance characteristics of devices and system components. Further, application workloads can significantly affect the overall system reliability. In this work, we have analyzed aging effects on various design hierarchies of an embedded commercial processor in 28nm running real-world applications. We have also quantified the dependencies of aging effects on switching-activity and power-state of workloads. Implementation results show that the processor timing degradation can vary from 2% to 11%, depending on the workload. Due to the dependence of aging on the application workloads, margin based design will be highly pessimistic. We propose an efficient and flexible in situ monitoring methodology, SlackProbe, which inserts timing monitors at both path endpoints and path intermediate nets. We show that SlackProbe reduces the numbers of monitors required by over 15X with ~5% additional delay margin in several commercial processor benchmarks. The real-time data from these monitors can be used for hardware and software adaptation to mitigate failures due to aging.},
booktitle = {Proceedings of the 51st Annual Design Automation Conference},
pages = {1–6},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '14}
}

@inproceedings{10.5555/2685617.2685637,
author = {Gil-Costa, Veronica and Lobos, Jair and Solar, Roberto and Marin, Mauricio},
title = {AMEDS-tool: an automatic tool to model and simulate large scale systems},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Simulating the cost of applications running on large clusters of processors poses difficulties in model definition and simulation. In this paper we propose a methodology to ease this burden. The user specifies a model describing it as a timed coloured Petri Net in a graphical manner by using a tool like CPN tool. The model is automatically converted into a XML specification. Then a code generator converts the XML file into C++ code which is linked to a simulation kernel. The result is an efficient and scalable simulation program that can be executed sequentially or in parallel on either single multi-core processors or cluster of multi-core processors. We illustrate the suitability of our proposal by modelling and simulating the cost of message passing in a Fat-tree network which is commonly used to support communication in cluster of processors.},
booktitle = {Proceedings of the 2014 Summer Simulation Multiconference},
articleno = {20},
numpages = {8},
keywords = {discrete event simulation, automatic code generator, XML specification},
location = {Monterey, California},
series = {SummerSim '14}
}

@inproceedings{10.5555/2147671.2147730,
author = {Cheng, Sujun and Cheng, Zhendong and Luan, Zhongzhi and Qian, Depei},
title = {NEPnet: a scalable monitoring system for anomaly detection of network service},
year = {2011},
isbn = {9783901882449},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {Anomaly detection is very important for modern network service. Yet it is still a big challenge to conduct effective anomaly detection due to the high rate of service data and the complex correlations among them. Owing to the powerful query language and performance potential, complex event processing (CEP) is very suitable for this situation. In this paper, we present NEPnet, a high-performance and scalable monitoring system, which can process events for anomaly detection of network service in real time. NEPnet is based on CEP and provides a SQL-like language supporting various event correlations. On accepting user-defined queries as input, NEPnet builds a tree-based monitoring net for detailed anomaly detection. Considering the anomaly features of network service, the monitoring net utilizes limit trigger, predicate index and route table for different types of processing nodes in it. Our preliminary experiment results show that NEPnet can effectively detect anomaly of network service, with a high-speed of 100,000 events per second and 3~6 times faster than Esper, a general CEP engine.},
booktitle = {Proceedings of the 7th International Conference on Network and Services Management},
pages = {338–342},
numpages = {5},
keywords = {network service, monitoring net, complex event processing, anomaly detection},
location = {Paris, France},
series = {CNSM '11}
}

@inproceedings{10.5555/2048416.2048434,
author = {Rajaei, Hassan and Aldhalaan, Arwa},
title = {Advances in virtual learning environments and classrooms},
year = {2011},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Classrooms and laboratories of tomorrow are most likely enhanced with virtual presence in cyberspace allowing students and instructors to participate in the learning sessions using advanced Virtual Leaning Environments (VLE). These settings let difficult courses of science and engineering to be available for larger student audiences. Despite vast technological advances in recent years, there are significant challenges suggesting further research and developments to ensure virtual classrooms be as good as or even better than what we have now in our physical lecture halls. This paper provides a state-of-the-art survey on a number of existing VLE solutions pinpointing their strengths and limitations. We distinguish between asynchronous distance learning and the synchronous settings whose model allows live participation of the learners as well as collaboration possibilities between them. Furthermore, we examine whether the selected VLEs maintain any virtual presence of the participants and scalability of the platform. Other comparison criteria include user interface, gesture &amp; facial recognition, availability, affordability, compatibility and social awareness. As the virtual world moves closer to represent the real world, how these environments might facilitate different types of interactions inherent in a real classroom will largely dictate their future usefulness.},
booktitle = {Proceedings of the 14th Communications and Networking Symposium},
pages = {133–142},
numpages = {10},
keywords = {web-based simulation, virtual classroom, interactive distant learning, distributed system},
location = {Boston, Massachusetts},
series = {CNS '11}
}

